{"GlobalId": 2, "code": "public void setSplitDataProperties(SplitDataProperties<OUT> splitDataProperties) {\n\t\tthis.splitProperties = splitDataProperties;\n\t}", "comment": " Split properties can help to generate more efficient execution plans.", "commitId": "f0a28bf5345084a0a43df16021e60078e322e087", "issueId": "FLINK-1444", "issueString": "Add data properties for data sources\nThis issue proposes to add support for attaching data properties to data sources. These data properties are defined with respect to input splits.\nPossible properties are:\n\n- partitioning across splits: all elements of the same key (combination) are contained in one split\n- sorting / grouping with splits: elements are sorted or grouped on certain keys within a split\n- key uniqueness: a certain key (combination) is unique for all elements of the data source. This property is not defined wrt. input splits.\n\nThe optimizer can leverage this information to generate more efficient execution plans.\n\nThe InputFormat will be responsible to generate input splits such that the promised data properties are actually in place. Otherwise, the program will produce invalid results. \nI've prototyped something like this here: https://github.com/rmetzger/flink/compare/local_joins\nMaybe its helpful for you.\nImplemented with f0a28bf5345084a0a43df16021e60078e322e087\n"}
{"GlobalId": 4, "code": "public <F> ConnectedIterativeDataStream<IN, F> withFeedbackType(TypeInformation<F> feedbackType) {\n\t\treturn new ConnectedIterativeDataStream<IN, F>(this, feedbackType);\n\t}", "comment": " For type safety the user needs to define the feedback type", "commitId": "128d3bcfe27ed3149be61f0481047f8f855e2c2d", "issueId": "FLINK-2279", "issueString": "Allow treating iteration head as ConnectedDataStream \nCurrently the streaming iterations are restricted to use the same input and feedback types which are routed through the same operator.\n\nThis means that if the user want to distinguish between normal input and feedback record he/she needs to mark it somehow and also a wrapper type is necessary for handling separate input and feedback types.\n\nThis makes implementing iterative algorithms (such as ML) quite ugly at some points.\n\nI propose to let the user treat the normal input if the iteration head operator and the feedback input as a ConnectedDataStream which can be used to apply co-operators both distinguishing the inputs and allowing different feedback types for elegant implementations.\n"}
{"GlobalId": 10, "code": "public SplitDataProperties<OUT> getSplitDataProperties() {\n\t\tif(this.splitDataProperties == null) {\n\t\t\tthis.splitDataProperties = new SplitDataProperties<OUT>(this);\n\t\t}\n\t\treturn this.splitDataProperties;\n\t}", "comment": " SplitDataProperties can help to generate more efficient execution plans.", "commitId": "f0a28bf5345084a0a43df16021e60078e322e087", "issueId": "FLINK-1444", "issueString": "Add data properties for data sources\nThis issue proposes to add support for attaching data properties to data sources. These data properties are defined with respect to input splits.\nPossible properties are:\n\n- partitioning across splits: all elements of the same key (combination) are contained in one split\n- sorting / grouping with splits: elements are sorted or grouped on certain keys within a split\n- key uniqueness: a certain key (combination) is unique for all elements of the data source. This property is not defined wrt. input splits.\n\nThe optimizer can leverage this information to generate more efficient execution plans.\n\nThe InputFormat will be responsible to generate input splits such that the promised data properties are actually in place. Otherwise, the program will produce invalid results. \nI've prototyped something like this here: https://github.com/rmetzger/flink/compare/local_joins\nMaybe its helpful for you.\nImplemented with f0a28bf5345084a0a43df16021e60078e322e087\n"}
{"GlobalId": 14, "code": "public ResourceProfile getResourceProfile() {\n\t\treturn resourceProfile;\n\t}", "comment": " Gets the resource profile of the slot.", "commitId": "f6d8668175fb94f338037fd1ab40d2a2e344d097", "issueId": "FLINK-4373", "issueString": "Introduce SlotID, AllocationID, ResourceProfile\nFor the new version of cluster management, we need some more basic data structures:\n\n* SlotID: Identifier of one single slot located in a task executor\n\n* AllocationID: Slot allocation identifier, created by the JobManager when requesting a slot, constant across re-tries. Used to identify responses by the ResourceManager and to identify deployment calls towards the TaskManager that was allocated from.\n\n* ResourceProfile: The resource profile of the desired slot (currently only cpu cores and memory are supported\n\nAdded via 08cf860169948c93c6c92ab1e2c70cf1a4266c6f\n"}
{"GlobalId": 15, "code": "private static Map<SlotSharingGroupId, Integer> determineSlotsPerSharingGroup(\n            JobInformation jobInformation, int freeSlots) {\n        int numUnassignedSlots = freeSlots;\n        int numUnassignedSlotSharingGroups = jobInformation.getSlotSharingGroups().size();\n\n        final Map<SlotSharingGroupId, Integer> slotSharingGroupParallelism = new HashMap<>();\n\n        for (Map.Entry<SlotSharingGroupId, Integer> slotSharingGroup :\n                sortSlotSharingGroupsByDesiredParallelism(jobInformation)) {\n            final int groupParallelism =\n                    Math.min(\n                            slotSharingGroup.getValue(),\n                            numUnassignedSlots / numUnassignedSlotSharingGroups);\n\n            slotSharingGroupParallelism.put(slotSharingGroup.getKey(), groupParallelism);\n\n            numUnassignedSlots -= groupParallelism;\n            numUnassignedSlotSharingGroups--;\n        }\n\n        return slotSharingGroupParallelism;\n    }", "comment": " Distributes free slots across the slot-sharing groups of the job.", "commitId": "97e1dbc90cc3ea6e6e48e901813ab3bfbb693c6a", "issueId": "FLINK-30895", "issueString": "SlotSharingSlotAllocator may waste slots\nThe allocated evenly distributes slots across slot sharing groups independent of how many slots the vertices in that group actually need.\r\n\r\nThis can cause slots to be unused.\nmaster: 97e1dbc90cc3ea6e6e48e901813ab3bfbb693c6a\r\n1.17: 9e1cf08e24677108758a227a42384a482885b371\n"}
{"GlobalId": 16, "code": "@Experimental\n    public String getJsonPlan() {\n        return tableEnvironment.getJsonPlan(operations);\n    }", "comment": " Get the json plan of the all statements and Tables as a batch.", "commitId": "e16e45f29e0c8b7dc771f4acb50e5b3910a751e2", "issueId": "FLINK-21092", "issueString": "Introduce getJsonPlan, explainJsonPlan and executeJsonPlan in  TableEnvironmentInternal\nWe will introduce the following methods to support json plan operations in TableEnvironmentInternal:\r\n{code:java}\r\n  /**\r\n     * Get the json plan for the given statement.\r\n     *\r\n     * <p>The statement can only be DML.\r\n     *\r\n     * <p>The json plan is the string json representation of an optimized ExecNode plan for the\r\n     * given statement. An ExecNode plan can be serialized to json plan, and a json plan can be\r\n     * deserialized to an ExecNode plan.\r\n     */\r\n    @Experimental\r\n    String getJsonPlan(String stmt);\r\n\r\n    /**\r\n     * Get the json plan for the given {@link ModifyOperation}s.\r\n     */\r\n    @Experimental\r\n    String getJsonPlan(List<ModifyOperation> operations);\r\n\r\n    /**\r\n     * Returns the execution plan for the given json plan.\r\n     */\r\n    @Experimental\r\n    String explainJsonPlan(String jsonPlan, ExplainDetail... extraDetails);\r\n\r\n    /**\r\n     * Execute the given json plan, and return the execution result. A SQL statement can be\r\n     * converted to json plan through {@link #getJsonPlan(String)}.\r\n     */\r\n    @Experimental\r\n    TableResult executeJsonPlan(String jsonPlan);\r\n\r\n{code}\r\n\r\nand we also introduce getJsonPlan method in StatementSetImpl so that we can get the json plan of the all statements and Tables as a batch.\r\n\r\n\r\n{code:java}\r\n  /**\r\n     * Get the json plan of the all statements and Tables as a batch.\r\n     */\r\n    @Experimental\r\n    public String getJsonPlan() {\r\n    }\r\n{code}\r\n\r\n\nFixed in 1.13.0: e16e45f29e0c8b7dc771f4acb50e5b3910a751e2\n"}
{"GlobalId": 18, "code": "public OutType arrayDistinct() {\n        return toApiSpecificExpression(unresolvedCall(ARRAY_DISTINCT, toExpr()));\n    }", "comment": " Keeps ordering of elements.", "commitId": "b9f6a90a9f942e6a69bb31bbe8baf8ab463156d5", "issueId": "FLINK-27471", "issueString": "Add ARRAY_DISTINCT supported in SQL & Table API\nRemoves duplicate values from the array.\r\n\r\nSyntax:\r\narray_distinct(array) \r\n\r\nArguments:\r\n    array: An ARRAY to be handled.\r\n\r\nReturns:\r\n\r\nAn ARRAY. If value is NULL, the result is NULL. Keeps order of elements.\r\nExamples:\r\n{code:sql}\r\nSELECT array_distinct(ARRAY[1, 2, 3, 2, 1]);\r\n-- [1, 2, 3]\r\nSELECT array_distinct(ARRAY[1, NULL, 1]);\r\n-- [1, NULL]\r\n{code}\r\n\r\nSee also \r\nhttps://spark.apache.org/docs/latest/api/sql/index.html#array_distinct\r\n\r\n\r\n\nMerged to master via b9f6a90a9f942e6a69bb31bbe8baf8ab463156d5.\n"}
{"GlobalId": 27, "code": "public boolean isBlockingConnectionsBetweenChains() {\n\t\treturn blockingConnectionsBetweenChains;\n\t}", "comment": " If there are some stream edges that can not be chained and the shuffle mode of edge is not specified, translate these edges into BLOCKING result partition type.", "commitId": "5e27744c94c59f2bcc661b3f94f311221c021773", "issueId": "FLINK-13101", "issueString": "Introduce \"blockingConnectionsBetweenChains\" property on StreamGraph\nThe property\u00a0\"blockingConnectionsBetweenChains\" means, if there are some stream edges that can not be chained and the shuffle mode of edge is not specified, translate these edges into {{BLOCKING}} result partition type.\r\n\r\nThe reason of introducing it is to satisfy the requirement of Blink batch planner. Because the current scheduling strategy is a bit simple. It can not support some complex scenarios, like a batch job with resources limited.\r\n\r\nTo be honest, it's probably a work-around solution. However it's an internal implementation, we can replace it when we are able to support batch job by scheduling strategy.\nAdded on master in\r\n5e27744c94c59f2bcc661b3f94f311221c021773\n"}
{"GlobalId": 33, "code": "public static ApiExpression currentWatermark(Object rowtimeAttribute) {\n        return apiCall(BuiltInFunctionDefinitions.CURRENT_WATERMARK, rowtimeAttribute);\n    }", "comment": " The function returns the watermark with the same type as the rowtime attribute, but with an adjusted precision of 3. If no watermark has been emitted yet, the function will return NULL.", "commitId": "8bd215dc28ae431120e5b1a114a94ddc6e004b16", "issueId": "FLINK-22737", "issueString": "Add support for CURRENT_WATERMARK to SQL\nWith a built-in function returning the current watermark, one could operate on late events without resorting to using the DataStream API.\r\n\r\nCalled with zero parameters, this function returns the current watermark for the current row \u2013 if there is an event time attribute. Otherwise, it returns NULL.\u00a0\nSounds make sense to me. \nI decided with [~twalthr]\u00a0that we'll make this function return TIMESTAMP_LTZ to be in line with other functions, and return NULL if either no event-time attribute exists or no watermark has yet been emitted. It'd actually be very difficult to determine whether an event-time attribute exists, so we rather map the value returned by TimerService#currentWatermark() (which is MIN_VALUE in both cases) correspondingly, as NULL requires careful handling, but semantically fits better than a \"placeholder\" date like 1/1/1970.\n[~jark]\u00a0What do you think about this proposal? The alternative would be returning TIMESTAMP, but TIMESTAMP_LTZ seems to be more in line with some of the other functions like PROCTIME, and generally more useful.\nSmall correction: The alternative would be to return BIGINT, so users would need to call another function to convert it to the target timestamp type. TIMESTAMP_LTZ fits better to CURRENT_TIMESTAMP, CURRENT_ROW_TIMESTAMP and PROCTIME. Also LTZ is the recommended event-time timestamp type that we recommend to users. If we would go with BIGINT, it would actually be better to let `SOURCE_WATERMARK()` also return BIGINT for consistency.\nHi [~airblader], [~twalthr], I prefer to return TIMESTAMP/LTZ rather than BIGINT, because as said in the JIRA description, it is mainly used to compare with event time to determine late events. So a same type with event time attribute would make the comparison easier and less error-prone (no conversion).  \r\n\r\nWhat about asking {{CURRENT_WATERMARK}} to accept an event-time attribute argument? I believe the event-time column should exist if users want to access watermark. This can make event time column and CURRENT_WATERMARK return same type,  also avoid returning NULL values. \nThanks for your input [~jark]! I discussed this with [~twalthr]\u00a0and we agree that passing the event-time attribute as an argument solves a couple of problems, so we'll go with that. However, we would still have NULL values for the case where no watermark has yet been emitted. Or did you have something else in mind for this?\nI like [~jark]'s approach. And it seems to me that in the case where no watermark has yet been emitted, returning MIN_VALUE would be preferable, since one could then compare the event-time attribute with the result of CURRENT_WATERMARK(rowtime) and correctly ascertain that event is not late. Whereas if CURRENT_WATERMARK(rowtime) returns null it will be harder to work with.\n[~alpinegizmo]\u00a0What would \"MIN_VALUE\" be in case of TIMESTAMP_LTZ, though? Returning 1/1/1970 seems rather odd (and would be 0, not MIN_VALUE).\nSemantically, I would argue that returning 0 as an initial watermark is perfectly sensible. Since no timestamps can be less than that watermark, nothing will be considered late, which is correct. 0 is a perfectly logical return value, and shouldn't require any special handling.\nI have some concerns when returning NULL, because it involves three-valued-logic when comparing which may confuse users at sometimes. I think returning 0 as initial watermark is fine which represents \"1970-01-01 00:00:00\" if is TIMESTAMP type, or \"1970-01-01 00:00:00Z\" if is TIMESTAMP_LTZ type. \nI have to admit that I find it quite odd to return 1/1/1970 for \"no watermark yet\", it seems rather random to me to allow that to happen and NULL feels much more semantically correct. But I understand that returning the epoch can make things easier. Timo and I had discussed this as well and decided to propose NULL because e.g. the Flink UI also doesn't show the Long.MIN_VALUE in this case. But ultimately I am also fine with returning the epoch, I think Timo was also kind of on the fence for this. [~twalthr]\u00a0Should we just return the epoch then or do you feel strongly about the NULL now?\n-I discussed this again with Timo, and we will go with the Unix epoch instead of NULL now. To summarize:-\r\n * -CURRENT_WATERMARK(<column>)-\r\n * -Error if the provided column is not a rowtime column-\r\n * -The returned type is that of the provided column (TIMESTAMP / TIMESTAMP_LTZ), but we force the precision to 3.-\r\n * -If no watermark has been emitted yet, we return the Unix epoch.-\r\n\r\nEdit: Sorry, I may have misunderstood my discussion with Timo.\u00a0\nI'm against returning 0 as the initial watermark for the following reasons\r\n- This introduces a third value with special meaning next to Long.MIN_VALUE and LONG.MAX_VALUE.\r\n- It goes against the design of all other components in the code base (from DataStream API to Web UI). The ProcessFunction returns Long.MIN_VALUE when querying the timer service and the Web UI also checks against Long.MIN_VALUE and displays \"No watermark received yet.\".\r\n- It prevents the processing of historical data. This might not be a strong argument but it could happen that data from 1970 is processed (banks? weather data? stock data?). The decision to use Long.MIN_VALUE in streaming operators is definitely safer to prevent unintended side effects. Also, when further combing bounded and unbounded data processing, watermarks could also describe non-real-time data in the near future.\r\n\r\nPersonally, I would go for Long.MIN_VALUE or NULL. The problem we have in SQL is that new type system is actually limiting the precision of years to 4 digits in the definition of `TimestampType`. So NULL is a good alternative.\r\n\r\nIt is true that it introduces a three-valued logic but this should not be a problem for handling late data. {{FROM T WHERE rowtime < CURRENT_WATERMARK}} evaluates to false until a watermark arrives which is correct.\n[~twalthr] [~jark] [~airblader] Thanks for your analyzing. I'm fine with\u00a0Long.MIN_VALUE, bug have a little concern with\u00a0NULL.\r\n\r\nIntroducing a three-valued logical is not a problem for handling late data, however it\u00a0may confuse users when handling non-late data by wrong condition: 'FROM T where rowtime >= CURRENT_WATERMARK', because it also returns false before a valid watermark arrives.\u00a0\r\n\r\n\u00a0\n-Thanks- [~twalthr]-. I'm also fine with returning {{NULL}} now.-\n-{{FROM T where rowtime >= CURRENT_WATERMARK}} is not a valid query, because the condition is also false before watermark emitted even if it returns {{0}} or {{Long.MIN}} instead of {{NULL}}.-\n[~jark]\u00a0Would you please explain a little more about why `T where rowtime >= CURRENT_WATERMARK` would also return false before watermark emitted even\u00a0if it returns\u00a0{{0}}\u00a0or\u00a0{{Long.MIN}}\u00a0instead of\u00a0{{NULL? I thought it would return true\u00a0\u00a0if CURRENT_WATERMARK returns\u00a0}}{{0}}{{\u00a0or\u00a0}}{{Long.MIN}}{{, it would return false only if\u00a0CURRENT_WATERMARK return null.}}\nI thought it wrong. I agree with [~qingru zhang].\u00a0The three-valued logic will confuse users when using condition {{rowtime >= CURRENT_WATERMARK}}, e.g. use it to filter out late data. \nAs I said before, I would be fine with Long.MIN_VALUE or NULL. Long.MIN_VALUE perfectly integrates with DataStream API and stream operators. My only concern was that it might look weird when printing because it would show as {{-292275055-05-16T16:47:04.192Z}} when printing a {{LocalDateTime}} or {{Instant}}. However, we can fix this as part of FLINK-21456 where we improve the printing and will limit the year digits to 4. This would ensure that we don't break the contracts mentioned in the docs of {{TimestampType}}.\nJust for completeness, this is how the SQL client will render MIN_VALUE. I can't say I'm a fan of it, but if we allow a date 300 million years in the past, this will have to be fixed. :)\u00a0\r\n\r\n!screenshot-2021-05-31_14-22-43.png!\n* -1 for\u00a0using *NULL* which will bring the three value logic which makes user confused very much in some cases like filtering out the non-late data.\r\n * slightly +0 for using *Long.MIN_VALUE* to represent a timepoint. Although it's a meaningless timepoint and may overflow under some timezones when we subtract time zone offset, we can check the reasonable range in FLINK-21456.\u00a0\nAfter thinking about this topic again, also from a usability perspective, NULL is way easier to handle in SQL than Long.MIN_VALUE.\r\n\r\nA SQL user cannot access the Long.MIN_VALUE constant and it cannot be declared easily using a SQL literal like {{TIMESTAMP '-292275055-05-16 16:47:04.192'}}.\r\n\r\nSQL users are used to handling NULLs.\r\n\r\nBy using {{TIMESTAMP '1970-01-01'}}, we would (again) relate {{LocalDateTime}} and {{Instant}} semantics with each other and expose internal representations.\r\n\r\nI see NULL as the only valid alternative. Otherwise I would rather vote for returning BIGINT instead of any timestamp type.\nI would really like avoiding going back to BIGINT. This puts an even bigger burden on the user to always explicitly convert to an (appropriate) timestamp type first, and it doesn't really address the question of \"MIN_VALUE vs NULL\", it just changes the type with which this is returned. If we expect users to just convert it to a timestamp, which I think they will do, they'll also end up with the same timestamp, and nothing is really gained.\r\n\r\n\u00a0\r\n\r\n\u00a0\nIt seems no option can make every one happy. \r\n\r\nI would be fine with NULL if we can clearly state on docs the default value of CURRENT_WATERMARK before any watermark emitted is **{{NULL}}**, and users should be careful when using it to drop late data, should use {{WHERE CURRENT_WATERMARK() IS NULL OR  rowtime > CURRENT_WATERMARK()}}. \n[~twalthr]\u00a0has convinced me, and I (reluctantly) agree with returning NULL. And +1 for making sure\u00a0the docs include a good example showing how to use this correctly.\nThanks everyone for the productive discussion. I think we reached a good conclusion now, and I will shortly open a PR for this:\r\n # CURRENT_WATERMARK(<rowtimeAttribute>)\r\n # Returns the same data type as <rowtimeAttribute>, but with a precision of 3\r\n # If no watermark has been emitted, it returns NULL\r\n # We'll make sure to document this very explicitly and with an example.\nThanks [~airblader] for the summary. Sounds good to me. \nFixed in 1.14.0: 8bd215dc28ae431120e5b1a114a94ddc6e004b16\n"}
{"GlobalId": 37, "code": "public DataStreamSource<String> socketTextStream(String hostname, int port, char delimiter) {\n\t\treturn socketTextStream(hostname, port, delimiter, 0);\n\t}", "comment": " The reader is terminated immediately when socket is down.", "commitId": "fd65a2411ddf8f80d69e8de425bd8805de300195", "issueId": "FLINK-1582", "issueString": "SocketStream gets stuck when socket closes\nWhen the server side of the socket closes the socket stream reader does not terminate. When the socket is reinitiated it does not reconnect just gets stuck.\nIt would be nice to add options for the user have the reader should behave when the socket is down: terminate immediately (good for testing and examples) or wait a specified time - possibly forever.\nI've assigned [~qmlmoon] to the issue, because he opened a pull request for it.\nFixed via fd65a24 and d182d04.\n"}
{"GlobalId": 38, "code": "@Override\n\tpublic List<SlotExecutionVertexAssignment> allocateSlotsFor(\n\t\t\tList<ExecutionVertexSchedulingRequirements> executionVertexSchedulingRequirements) {\n\t\tList<ExecutionVertexID> executionVertexIds = executionVertexSchedulingRequirements\n\t\t\t.stream()\n\t\t\t.map(ExecutionVertexSchedulingRequirements::getExecutionVertexId)\n\t\t\t.collect(Collectors.toList());\n\n\t\tSharedSlotProfileRetriever sharedSlotProfileRetriever = sharedSlotProfileRetrieverFactory\n\t\t\t.createFromBulk(new HashSet<>(executionVertexIds));\n\t\tMap<ExecutionVertexID, SlotExecutionVertexAssignment> assignments = executionVertexIds\n\t\t\t.stream()\n\t\t\t.collect(Collectors.groupingBy(slotSharingStrategy::getExecutionSlotSharingGroup))\n\t\t\t.entrySet()\n\t\t\t.stream()\n\t\t\t.flatMap(entry -> allocateLogicalSlotsFromSharedSlot(sharedSlotProfileRetriever, entry.getKey(), entry.getValue()))\n\t\t\t.collect(Collectors.toMap(SlotExecutionVertexAssignment::getExecutionVertexId, a -> a));\n\n\t\treturn executionVertexIds.stream().map(assignments::get).collect(Collectors.toList());\n\t}", "comment": " If a physical slot request fails, associated logical slot requests are canceled within the shared slot Generate SlotExecutionVertexAssignments based on the logical slot futures and returns the results.", "commitId": "ff94e8be5de7cc6341fe49abc99829a755ca91dd", "issueId": "FLINK-18751", "issueString": "Implement SlotSharingExecutionSlotAllocator\nSlotSharingExecutionSlotAllocator maintains a SharedSlot for each ExecutionSlotSharingGroup. SlotSharingExecutionSlotAllocator allocates physical slots for SharedSlot(s) and then allocates logical slots from it for scheduled tasks. In this way, the slot sharing hints can be respected in the ExecutionSlotAllocator. And we no longer need to rely on the SlotProvider to do the slot sharing matching. Co-location constraints will be respected since co-located subtasks will be in the same ExecutionSlotSharingGroup.\r\n\r\nThe physical slot will be lazily allocated for a SharedSlot, upon any hosted subtask asking for the SharedSlot. Each subsequent sharing subtask allocates a logical slot from the SharedSlot. The SharedSlot/physical slot can be released only if all the requested logical slots are released or canceled.\r\nh4. Slot Allocation Process\r\n\r\nWhen SlotSharingExecutionSlotAllocator receives a set of tasks to allocate slots for, it should do the following:\r\n # Map the tasks to ExecutionSlotSharingGroup(s)\r\n # Check which ExecutionSlotSharingGroup(s) _already have_ SharedSlot(s)\r\n # For all involved ExecutionSlotSharingGroup(s) _which do not have a SharedSlot_ yet:\r\n ## Create a SlotProfile future by MergingSharedSlotProfileRetriever and then\r\n ## Allocate a physical slot from the PhysicalSlotProvider\r\n ## Create SharedSlot based on the returned physical slot futures\r\n # Allocate logical slot futures for the tasks from all corresponding SharedSlot(s).\r\n # If physical slot future fails, cancel its pending logical slot requests within the SharedSlot\r\n # Generates SlotExecutionVertexAssignment(s)\u00a0 based on the logical slot futures and returns the results.\nmerged into master by\u00a0ff94e8be5de7cc6341fe49abc99829a755ca91dd\n"}
{"GlobalId": 40, "code": "@Experimental\n\tpublic DataStream<T> rescale() {\n\t\treturn setConnectionType(new RescalePartitioner<T>());\n\t}", "comment": " The subset of downstream operations to which the upstream operation sends elements depends on the degree of parallelism of both the upstream and downstream operation. For example, if the upstream operation has parallelism 2 and the downstream operation has parallelism 4, then one upstream operation would distribute elements to two downstream operations while the other upstream operation would distribute to the other two downstream operations. If, on the other hand, the downstream operation has parallelism 2 while the upstream operation has parallelism 4 then two upstream operations will distribute to one downstream operation while the other two upstream operations will distribute to the other downstream operations. In cases where the different parallelisms are not multiples of each other one or several downstream operations will have a differing number of inputs from upstream operations.", "commitId": "9f6a8b6d075551d1b4090e191df8f56b5ef9d7e9", "issueId": "FLINK-3336", "issueString": "Add Semi-Rebalance Data Shipping for DataStream\nThis feature has recently been requested on the ML: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Distribution-of-sinks-among-the-nodes-td4640.html\n\nThe new data shipping pattern would allow to rebalance data only to a subset of downstream operations.\n\nThe subset of downstream operations to which the upstream operation would send\nelements depends on the degree of parallelism of both the upstream and downstream operation.\nFor example, if the upstream operation has parallelism 2 and the downstream operation\nhas parallelism 4, then one upstream operation would distribute elements to two\ndownstream operations while the other upstream operation would distribute to the other\ntwo downstream operations. If, on the other hand, the downstream operation had parallelism\n2 while the upstream operation has parallelism 4 then two upstream operations would\ndistribute to one downstream operation while the other two upstream operations would\ndistribute to the other downstream operations.\n\nIn cases where the different parallelisms are not multiples of each other one or several\ndownstream operations would have a differing number of inputs from upstream operations.\nI'm open for suggestions on the name.\nAdded in https://github.com/apache/flink/commit/9f6a8b6d075551d1b4090e191df8f56b5ef9d7e9\n"}
{"GlobalId": 41, "code": "public Pattern<T, F> within(Time windowTime, WithinType withinType) {\n        if (windowTime != null) {\n            windowTimes.put(withinType, windowTime);\n        }\n\n        return this;\n    }", "comment": " Defines the maximum time interval in which a matching pattern has to be completed in order to be considered valid. This interval corresponds to the maximum time gap between events.", "commitId": "4a82df07d70d0c8882fe6b92fe69973fc384464b", "issueId": "FLINK-27392", "issueString": "CEP Pattern supports definition of the maximum time gap between events\nAt present, Pattern#withIn defines the maximum time interval in which a matching pattern has to be completed in order to be considered valid. The interval corresponds to the maximum time gap between first and the last event. The maximum time gap between events is needed in the certain scenario, for example, purchases a good within a maximum of 5 minutes after browsing.\nMerged to master via\u00a04a82df07d70d0c8882fe6b92fe69973fc384464b\n"}
{"GlobalId": 45, "code": "public <F> ConnectedIterativeDataStream<IN, F> withFeedbackType(String feedbackTypeString) {\n\t\treturn withFeedbackType(TypeInfoParser.<F> parse(feedbackTypeString));\n\t}", "comment": " For type safety the user needs to define the feedback type", "commitId": "128d3bcfe27ed3149be61f0481047f8f855e2c2d", "issueId": "FLINK-2279", "issueString": "Allow treating iteration head as ConnectedDataStream \nCurrently the streaming iterations are restricted to use the same input and feedback types which are routed through the same operator.\n\nThis means that if the user want to distinguish between normal input and feedback record he/she needs to mark it somehow and also a wrapper type is necessary for handling separate input and feedback types.\n\nThis makes implementing iterative algorithms (such as ML) quite ugly at some points.\n\nI propose to let the user treat the normal input if the iteration head operator and the feedback input as a ConnectedDataStream which can be used to apply co-operators both distinguishing the inputs and allowing different feedback types for elegant implementations.\n"}
{"GlobalId": 46, "code": "public <F> ConnectedIterativeDataStream<IN, F> withFeedbackType(Class<F> feedbackTypeClass) {\n\t\treturn withFeedbackType(TypeExtractor.getForClass(feedbackTypeClass));\n\t}", "comment": " For type safety the user needs to define the feedback type", "commitId": "128d3bcfe27ed3149be61f0481047f8f855e2c2d", "issueId": "FLINK-2279", "issueString": "Allow treating iteration head as ConnectedDataStream \nCurrently the streaming iterations are restricted to use the same input and feedback types which are routed through the same operator.\n\nThis means that if the user want to distinguish between normal input and feedback record he/she needs to mark it somehow and also a wrapper type is necessary for handling separate input and feedback types.\n\nThis makes implementing iterative algorithms (such as ML) quite ugly at some points.\n\nI propose to let the user treat the normal input if the iteration head operator and the feedback input as a ConnectedDataStream which can be used to apply co-operators both distinguishing the inputs and allowing different feedback types for elegant implementations.\n"}
{"GlobalId": 48, "code": "private void registerAtElectionService() {\n\t\ttry {\n\t\t\tleaderElectionService = highAvailabilityServices.getJobMasterLeaderElectionService(jobID);\n\t\t\tleaderElectionService.start(new JobMasterLeaderContender());\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Fail to register at the election of JobMaster\", e);\n\t\t}\n\t}", "comment": " Retrieves the election service and contend for the leadership.", "commitId": "a903f171a53a6ad0e25bebcb298bdf7dfedd06cf", "issueId": "FLINK-4400", "issueString": "Leadership Election among JobManagers\n* All JobMasters are LeaderContenders\n* Once a JobMaster is initialized, the very first thing it has to do is to start the leadership election service and contend for the leadership.\n* A JobMaster starts to perform its functionality when it grants the leadership.\n* If a JobMaster\u2019s leadership is revoked, it will cancel all performed execution and release all acquired resources.\n\nAdded via e4f5aec22a1d86ac3667e3d79ccd9f91e934086a\n"}
{"GlobalId": 49, "code": "public void setQueueLimit(int queueLimit) {\n\t\tcheckArgument(queueLimit > 0, \"queueLimit must be a positive number\");\n\t\tthis.queueLimit = queueLimit;\n\t}", "comment": " The KinesisProducer holds an unbounded queue internally.", "commitId": "7d034d4ef6986ba5ccda6f5e8c587b8fdd88be8e", "issueId": "FLINK-9374", "issueString": "Flink Kinesis Producer does not backpressure\nThe {{FlinkKinesisProducer}} just accepts records and forwards it to a {{KinesisProducer}} from the Amazon Kinesis Producer Library (KPL). The KPL internally holds an unbounded queue of records that have not yet been sent.\r\n\r\nSince Kinesis is rate-limited to 1MB per second per shard, this queue may grow indefinitely if Flink sends records faster than the KPL can forward them to Kinesis.\r\n\r\nOne way to circumvent this problem is to set a record TTL, so that queued records are dropped after a certain amount of time, but this will lead to data loss under high loads.\r\n\r\nCurrently the only time the queue is flushed is during checkpointing: {{FlinkKinesisProducer}} consumes records at arbitrary rate, either until a checkpoint is reached (and will wait until the queue is flushed), or until out-of-memory, whichever is reached first. (This gets worse due to the fact that the Java KPL is only a thin wrapper around a C++ process, so it is not even the Java process that runs out of memory, but the C++ process.) The implicit rate-limit due to checkpointing leads to a ragged throughput graph like this (the periods with zero throughput are the wait times before a checkpoint):\r\n\r\n!file:///home/fthoma/projects/flink/before.png!!before.png! Throughput limited by checkpointing only\r\n\r\nMy proposed solution is to add a config option {{queueLimit}} to set a maximum number of records that may be waiting in the KPL queue. If this limit is reached, the {{FlinkKinesisProducer}} should trigger a {{flush()}} and wait (blocking) until the queue length is below the limit again. This automatically leads to backpressuring, since the {{FlinkKinesisProducer}} cannot accept records while waiting. For compatibility, {{queueLimit}} is set to {{Integer.MAX_VALUE}} by default, so the behavior is unchanged unless a client explicitly sets the value. Setting a \u00bbsane\u00ab default value is not possible unfortunately, since sensible values for the limit depend on the record size (the limit should be chosen so that about 10\u2013100MB of records per shard are accumulated before flushing, otherwise the maximum Kinesis throughput may not be reached).\r\n\r\n!after.png! Throughput with a queue limit of 100000 records (the spikes are checkpoints, where the queue is still flushed completely)\nMerged\r\n\r\n1.6.0:\u00a07d034d4ef6986ba5ccda6f5e8c587b8fdd88be8e\r\n1.5.1:\u00a0b725982e5758043ba3aa53bde1615569336e451e\r\n\r\nThanks a lot for the contribution [~fmthoma]!\n"}
{"GlobalId": 50, "code": "public void setBlockingConnectionsBetweenChains(boolean blockingConnectionsBetweenChains) {\n\t\tthis.blockingConnectionsBetweenChains = blockingConnectionsBetweenChains;\n\t}", "comment": " If there are some stream edges that can not be chained and the shuffle mode of edge is not specified, translate these edges into BLOCKING result partition type.", "commitId": "5e27744c94c59f2bcc661b3f94f311221c021773", "issueId": "FLINK-13101", "issueString": "Introduce \"blockingConnectionsBetweenChains\" property on StreamGraph\nThe property\u00a0\"blockingConnectionsBetweenChains\" means, if there are some stream edges that can not be chained and the shuffle mode of edge is not specified, translate these edges into {{BLOCKING}} result partition type.\r\n\r\nThe reason of introducing it is to satisfy the requirement of Blink batch planner. Because the current scheduling strategy is a bit simple. It can not support some complex scenarios, like a batch job with resources limited.\r\n\r\nTo be honest, it's probably a work-around solution. However it's an internal implementation, we can replace it when we are able to support batch job by scheduling strategy.\nAdded on master in\r\n5e27744c94c59f2bcc661b3f94f311221c021773\n"}
