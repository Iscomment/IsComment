{"aId": 1, "code": "synchronized void markSuspectBlock(String storageId, ExtendedBlock block) {\n    if (!isEnabled()) {\n      LOG.info(\"Not scanning suspicious block {} on {}, because the block \" +\n          \"scanner is disabled.\", block, storageId);\n      return;\n    }\n    VolumeScanner scanner = scanners.get(storageId);\n    if (scanner == null) {\n      // This could happen if the volume is in the process of being removed.\n      // The removal process shuts down the VolumeScanner, but the volume\n      // object stays around as long as there are references to it (which\n      // should not be that long.)\n      LOG.info(\"Not scanning suspicious block {} on {}, because there is no \" +\n          \"volume scanner for that storageId.\", block, storageId);\n      return;\n    }\n    scanner.markSuspectBlock(block);\n  }", "comment": " This means that we should try to rescan it soon.", "issueId": "HDFS-7686", "issueStringList": ["Re-add rapid rescan of possibly corrupt block feature to the block scanner", "When doing a transferTo (aka sendfile operation) from the DataNode to a client, we may hit an I/O error from the disk.", "If we believe this is the case, we should be able to tell the block scanner to rescan that block soon.", "The feature was originally implemented in HDFS-7548 but was removed by HDFS-7430.", "We should re-add it.", "The feature implemented by HDFS-7548 was removed by HDFS-7430.", "[~cmccabe], it looks like your change accidentally dropped it.", "Can you take a look?", "Thanks, guys.", "I have a patch for this one but I want to add a few unit tests to it.", "Will post tomorrow.", "Thanks for finding this Rushabh, thanks Colin for the patch.", "A few light review comments:", "Unused Iterator import in the test, LoadingCache import in VolumeScanner", "I like the cache since it's a nice way of preventing scanning the same blocks over and over again, but it'd be good to also use a LinkedHashMap instead of the LinkedList and also check existence in there before adding.", "That way we never have dupes in the suspect queue.", "It seems possible to have a working set bigger than the 1000 element cache size, like if an entire disk goes bad.", "Otherwise looks good!", "bq.", "Unused Iterator import in the test, LoadingCache import in VolumeScanner", "ok", "bq.", "I like the cache since it's a nice way of preventing scanning the same blocks over and over again, but it'd be good to also use a LinkedHashMap instead of the LinkedList and also check existence in there before adding.", "That way we never have dupes in the suspect queue.", "It seems possible to have a working set bigger than the 1000 element cache size, like if an entire disk goes bad.", "good idea...", "I'll use a {{LinkedHashSet}}.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12698219/HDFS-7686.002.patch", "against trunk revision fe689d3.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover", "org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9550//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9550//console", "This message is automatically generated.", "Just a little nit, the suspectBlocks check is copy pasted from the recentSuspectBlocks check, but shouldn't the log message say \"already queued for scanning\" instead?", "+1 besides that though, thanks Colin.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12698298/HDFS-7686.003.patch", "against trunk revision 8a54384.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestRbwSpaceReservation", "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart", "org.apache.hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots", "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9556//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9556//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12698527/HDFS-7686.004.patch", "against trunk revision f107023.", "{color:red}-1 @author{color}.", "The patch appears to contain  @author tags which the Hadoop community has agreed to not allow in code contributions.", "{color:green}+1 tests included{color}.", "The patch appears to include  new or modified test files.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9563//console", "This message is automatically generated.", "Jenkins continues to be very weird.", "There are no author tags in the patch.", "Re-triggering jenkins.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12698527/HDFS-7686.004.patch", "against trunk revision 2f1e5dc.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9568//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9568//console", "This message is automatically generated.", "lgtm, +1.", "Thanks Colin, Andrew and Rushabh.", "FAILURE: Integrated in Hadoop-trunk-Commit #7108 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7108/])", "HDFS-7686.", "Re-add rapid rescan of possibly corrupt block feature to the block scanner (cmccabe) (cmccabe: rev 8bb9a5000ed06856abbad268c43ce1d5ad5bdd43)", "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockScanner.java", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java", "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java", "SUCCESS: Integrated in Hadoop-trunk-Commit #7109 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7109/])", "update CHANGES.txt for HDFS-7430, HDFS-7721, HDFS-7686 (cmccabe: rev 19be82cd1614000bb26e5684f763c736ea46ff1a)", "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt"], "SplitGT": [" This means that we should try to rescan it soon."], "issueString": "Re-add rapid rescan of possibly corrupt block feature to the block scanner\nWhen doing a transferTo (aka sendfile operation) from the DataNode to a client, we may hit an I/O error from the disk.  If we believe this is the case, we should be able to tell the block scanner to rescan that block soon.  The feature was originally implemented in HDFS-7548 but was removed by HDFS-7430.  We should re-add it.\nThe feature implemented by HDFS-7548 was removed by HDFS-7430. \n[~cmccabe], it looks like your change accidentally dropped it. Can you take a look?\nThanks, guys.  I have a patch for this one but I want to add a few unit tests to it.  Will post tomorrow.\nThanks for finding this Rushabh, thanks Colin for the patch. A few light review comments:\n\n* Unused Iterator import in the test, LoadingCache import in VolumeScanner\n* I like the cache since it's a nice way of preventing scanning the same blocks over and over again, but it'd be good to also use a LinkedHashMap instead of the LinkedList and also check existence in there before adding. That way we never have dupes in the suspect queue. It seems possible to have a working set bigger than the 1000 element cache size, like if an entire disk goes bad.\n\nOtherwise looks good!\nbq. Unused Iterator import in the test, LoadingCache import in VolumeScanner\n\nok\n\nbq. I like the cache since it's a nice way of preventing scanning the same blocks over and over again, but it'd be good to also use a LinkedHashMap instead of the LinkedList and also check existence in there before adding. That way we never have dupes in the suspect queue. It seems possible to have a working set bigger than the 1000 element cache size, like if an entire disk goes bad.\n\ngood idea... I'll use a {{LinkedHashSet}}.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12698219/HDFS-7686.002.patch\n  against trunk revision fe689d3.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:\n\norg.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\norg.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9550//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9550//console\n\nThis message is automatically generated.\nJust a little nit, the suspectBlocks check is copy pasted from the recentSuspectBlocks check, but shouldn't the log message say \"already queued for scanning\" instead?\n\n+1 besides that though, thanks Colin.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12698298/HDFS-7686.003.patch\n  against trunk revision 8a54384.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestRbwSpaceReservation\n                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart\n                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots\n                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9556//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9556//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12698527/HDFS-7686.004.patch\n  against trunk revision f107023.\n\n    {color:red}-1 @author{color}.  The patch appears to contain  @author tags which the Hadoop community has agreed to not allow in code contributions.\n\n    {color:green}+1 tests included{color}.  The patch appears to include  new or modified test files.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9563//console\n\nThis message is automatically generated.\nJenkins continues to be very weird.  There are no author tags in the patch.  Re-triggering jenkins.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12698527/HDFS-7686.004.patch\n  against trunk revision 2f1e5dc.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9568//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9568//console\n\nThis message is automatically generated.\nlgtm, +1. Thanks Colin, Andrew and Rushabh.\nFAILURE: Integrated in Hadoop-trunk-Commit #7108 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7108/])\nHDFS-7686. Re-add rapid rescan of possibly corrupt block feature to the block scanner (cmccabe) (cmccabe: rev 8bb9a5000ed06856abbad268c43ce1d5ad5bdd43)\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockScanner.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java\n\nSUCCESS: Integrated in Hadoop-trunk-Commit #7109 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7109/])\nupdate CHANGES.txt for HDFS-7430, HDFS-7721, HDFS-7686 (cmccabe: rev 19be82cd1614000bb26e5684f763c736ea46ff1a)\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n\n", "issueSearchSentences": ["Just a little nit, the suspectBlocks check is copy pasted from the recentSuspectBlocks check, but shouldn't the log message say \"already queued for scanning\" instead?", "If we believe this is the case, we should be able to tell the block scanner to rescan that block soon.", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java", "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockScanner.java", "Re-add rapid rescan of possibly corrupt block feature to the block scanner"], "issueSearchScores": [0.6760746240615845, 0.6253693699836731, 0.6043670773506165, 0.5849640369415283, 0.5162338018417358]}
{"aId": 2, "code": "public static Collection<URI> getNameServiceUris(Configuration conf,\n      String... keys) {\n    Set<URI> ret = new HashSet<URI>();\n    for (String nsId : getNameServiceIds(conf)) {\n      if (HAUtil.isHAEnabled(conf, nsId)) {\n        // Add the logical URI of the nameservice.\n        try {\n          ret.add(new URI(HdfsConstants.HDFS_URI_SCHEME + \"://\" + nsId));\n        } catch (URISyntaxException ue) {\n          throw new IllegalArgumentException(ue);\n        }\n      } else {\n        // Add the URI corresponding to the address of the NN.\n        for (String key : keys) {\n          String addr = conf.get(concatSuffixes(key, nsId));\n          if (addr != null) {\n            ret.add(createUri(HdfsConstants.HDFS_URI_SCHEME,\n                NetUtils.createSocketAddr(addr)));\n            break;\n          }\n        }\n      }\n    }\n    // Add the generic configuration keys.\n    for (String key : keys) {\n      String addr = conf.get(key);\n      if (addr != null) {\n        ret.add(createUri(\"hdfs\", NetUtils.createSocketAddr(addr)));\n        break;\n      }\n    }\n    return ret;\n  }", "comment": " Get a URI for each configured nameservice. If a nameservice is HA-enabled, then the logical URI of the nameservice is returned.", "issueId": "HDFS-2979", "issueStringList": ["HA: Balancer should use logical uri for creating failover proxy with HA enabled.", "Presently Balancer uses real URI for creating the failover proxy.", "Since the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI.", "Presently will work only with default port.", "java.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.", "at org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)", "at org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)", "at org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)", "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401)", "Attached the patch with addressing the issue.", "Hey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.", "Furthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.", "What do you think about the following?", "# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.", "# Remove the need to convert from inet address -> URI in NameNodeConnector(...).", "Also, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol.", "I've filed HDFS-2986 to address this issue.", "Here's a patch which addresses the issue, using the technique I described above.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME and", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "Otherwise looks good.", "Thanks a lot for the review, Eli.", "Here's an updated patch which addresses your feedback.", "In the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it.", "The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated.", "This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client.", "To fix this, I made the NN make copies of conf objects provided to it in a few places.", "This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.", "bq.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "OK, done.", "bq.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "I revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.", "bq.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME", "Done.", "bq.", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "This is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.", "I also ran the following tests to verify this change, and they all passed:", "TestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "Otherwise looks great.", "Nice find wrt the mutating configuration issue.", "Thanks a lot, Eli.", "Here's an updated patch which adds the wrapper method as you requested.", "+1", "Thanks a lot for the reviews, Eli.", "I've just committed this to the HA branch.", "This patch seems to have made some tests fail (I noticed with TestFileAppend2).", "It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.", "Thanks Todd.", "I've filed HDFS-3033 to address these failures.", "On second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case.", "The conf mutation issue should be fixed in a more general way."], "SplitGT": [" Get a URI for each configured nameservice.", "If a nameservice is HA-enabled, then the logical URI of the nameservice is returned."], "issueString": "HA: Balancer should use logical uri for creating failover proxy with HA enabled.\nPresently Balancer uses real URI for creating the failover proxy.\nSince the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI. Presently will work only with default port.\n\njava.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.\n\tat org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)\n\tat org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)\n\tat org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)\n\tat org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401) \nAttached the patch with addressing the issue.\nHey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.\n\nFurthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.\n\nWhat do you think about the following?\n\n# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.\n# Remove the need to convert from inet address -> URI in NameNodeConnector(...).\n\nAlso, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol. I've filed HDFS-2986 to address this issue.\nHere's a patch which addresses the issue, using the technique I described above.\n- The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n- Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n- getNNUris should use HdfsConstants#HDFS_URI_SCHEME and\n- testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nOtherwise looks good.\nThanks a lot for the review, Eli. Here's an updated patch which addresses your feedback.\n\nIn the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it. The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated. This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client. To fix this, I made the NN make copies of conf objects provided to it in a few places. This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.\n\nbq. The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n\nOK, done.\n\nbq. Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n\nI revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.\n\nbq. getNNUris should use HdfsConstants#HDFS_URI_SCHEME\n\nDone.\n\nbq. testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nThis is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.\n\nI also ran the following tests to verify this change, and they all passed:\n\nTestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes\nLet's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys. Otherwise looks great. Nice find wrt the mutating configuration issue.\nThanks a lot, Eli. Here's an updated patch which adds the wrapper method as you requested.\n+1\nThanks a lot for the reviews, Eli. I've just committed this to the HA branch.\nThis patch seems to have made some tests fail (I noticed with TestFileAppend2). It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.\nThanks Todd. I've filed HDFS-3033 to address these failures.\nOn second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case. The conf mutation issue should be fixed in a more general way.\n", "issueSearchSentences": ["Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys."], "issueSearchScores": [0.7654621601104736, 0.7654621601104736, 0.6461271047592163, 0.57203209400177, 0.57203209400177]}
{"aId": 3, "code": "@Override\n  protected void doTrace(HttpServletRequest req, HttpServletResponse resp)\n      throws ServletException, IOException {\n    resp.sendError(HttpServletResponse.SC_METHOD_NOT_ALLOWED);\n  }", "comment": " Disable TRACE method to avoid TRACE vulnerability.", "issueId": "HADOOP-13299", "issueStringList": ["JMXJsonServlet is vulnerable to TRACE", "Nessus scan shows that JMXJsonServlet is vulnerable to TRACE/TRACK requests.", "We could disable this to avoid such vulnerability.", "The patch overrides the doTrace method in JMXJsonServlet to disable TRACE requests.", "| (/) *{color:green}+1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated.", "{color} |", "| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags.", "{color} |", "| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files.", "{color} |", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 25s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 29s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 19s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 37s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javac {color} | {color:green}  6m 37s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 30s{color} | {color:green} hadoop-common in the patch passed.", "{color} |", "| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings.", "{color} |", "| {color:black}{color} | {color:black} {color} | {color:black} 37m 47s{color} | {color:black} {color} |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| Docker |  Image:yetus/hadoop:e2f6409 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12812025/hadoop13299.001.patch |", "| JIRA Issue | HADOOP-13299 |", "| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |", "| uname | Linux 250499de3aa4 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |", "| Build tool | maven |", "| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |", "| git revision | trunk / 8c1f81d |", "| Default Java | 1.8.0_91 |", "| findbugs | v3.0.0 |", "|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9838/testReport/ |", "| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |", "| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9838/console |", "| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |", "This message was automatically generated.", "Is there a specific CVE here?", "Hi [~steve_l] There is no specific CVE here.", "This is found in a network scan.", "Is there any component relying on the TRACE?", "If not, we can disable it just in case, which is exactly what the patch is doing.", "If this needs to be discussed in the security mailing list first, I can start a discussion there.", "Looks like the issue is a potential hole to allow for cross site tracing (https://www.owasp.org/index.php/Cross_Site_Tracing).", "It could be a false alarm, but it's definitely something that a customer who scans Hadoop will find.", "If we're not doing anything with the TRACE operation, then we should close the hole just to be safe.", "The patch looks good to me.", "+1 (non-binding)", "The patch looks good to me.", "[~haibochen] - could you confirm this on a cluster as well?", "Tested in a psedo-distributed cluster, it worked as expected.", "+1.", "Checking this in.", "Thanks [~haibochen] for the contribution, and [~templedf] for the review.", "Just committed this to trunk, branch-2, and branch-2.8.", "SUCCESS: Integrated in Hadoop-trunk-Commit #10248 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10248/])", "HADOOP-13299.", "JMXJsonServlet is vulnerable to TRACE.", "(Haibo Chen via (kasha: rev 85422bb7c5d3e70a49f620ba1c8800e0ba4b64f2)", "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/jmx/TestJMXJsonServlet.java", "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java"], "SplitGT": [" Disable TRACE method to avoid TRACE vulnerability."], "issueString": "JMXJsonServlet is vulnerable to TRACE \nNessus scan shows that JMXJsonServlet is vulnerable to TRACE/TRACK requests.  We could disable this to avoid such vulnerability.\nThe patch overrides the doTrace method in JMXJsonServlet to disable TRACE requests.\n| (/) *{color:green}+1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 25s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 29s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 19s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 37s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  6m 37s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 30s{color} | {color:green} hadoop-common in the patch passed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 37m 47s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:e2f6409 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12812025/hadoop13299.001.patch |\n| JIRA Issue | HADOOP-13299 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 250499de3aa4 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 8c1f81d |\n| Default Java | 1.8.0_91 |\n| findbugs | v3.0.0 |\n|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9838/testReport/ |\n| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9838/console |\n| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n\nIs there a specific CVE here?\nHi [~steve_l] There is no specific CVE here.  This is found in a network scan. \nIs there any component relying on the TRACE? If not, we can disable it just in case, which is exactly what the patch is doing.\nIf this needs to be discussed in the security mailing list first, I can start a discussion there.\nLooks like the issue is a potential hole to allow for cross site tracing (https://www.owasp.org/index.php/Cross_Site_Tracing).  It could be a false alarm, but it's definitely something that a customer who scans Hadoop will find.  If we're not doing anything with the TRACE operation, then we should close the hole just to be safe.\nThe patch looks good to me. +1 (non-binding)\nThe patch looks good to me. [~haibochen] - could you confirm this on a cluster as well? \nTested in a psedo-distributed cluster, it worked as expected.\n+1. Checking this in. \nThanks [~haibochen] for the contribution, and [~templedf] for the review. \n\nJust committed this to trunk, branch-2, and branch-2.8. \nSUCCESS: Integrated in Hadoop-trunk-Commit #10248 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10248/])\nHADOOP-13299. JMXJsonServlet is vulnerable to TRACE. (Haibo Chen via (kasha: rev 85422bb7c5d3e70a49f620ba1c8800e0ba4b64f2)\n* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/jmx/TestJMXJsonServlet.java\n* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java\n\n", "issueSearchSentences": ["The patch overrides the doTrace method in JMXJsonServlet to disable TRACE requests.", "JMXJsonServlet is vulnerable to TRACE.", "JMXJsonServlet is vulnerable to TRACE", "Nessus scan shows that JMXJsonServlet is vulnerable to TRACE/TRACK requests.", "We could disable this to avoid such vulnerability."], "issueSearchScores": [0.699681282043457, 0.5924400091171265, 0.5733674168586731, 0.4988727569580078, 0.4730268716812134]}
{"aId": 4, "code": "public static boolean verifyLength(XDR xdr, int len) {\n    return xdr.buf.remaining() >= len;\n  }", "comment": " check if the rest of data has more than len bytes", "issueId": "HADOOP-9669", "issueStringList": ["Reduce the number of byte array creations and copies in XDR data manipulation", "XDR.writeXxx(..) methods ultimately use the static XDR.append(..) for writing each data type.", "The static append creates a new array and copy data.", "Therefore, for a singe reply such as RpcAcceptedReply.voidReply(..), there are multiple array creations and array copies.", "For example, there are at least 6 array creations and array copies for RpcAcceptedReply.voidReply(..).", "Here is a more version that utilizes Java's ByteBuffer.", "It should be more efficient.", "The APIs are compatible with the previous version.", "The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization.", "I'm tracking my proposals to the APIs in a separate JIRA.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12603124/HADOOP-9669.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 4 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The following test timeouts occurred in hadoop-common-project/hadoop-nfs:", "org.apache.hadoop.oncrpc.TestFrameDecoder", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3096//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3096//console", "This message is automatically generated.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12603385/HADOOP-9669.001.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 4 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-nfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3102//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3102//console", "This message is automatically generated.", "Thanks, Haohui.", "Some comments:", "1. please try to keep the original javadoc for the same named methods", "2. can you make \"State state\" as final?", "3. please fix the javadoc /** check if the rest of data has more than <len> bytes */", "\"len\" is not visible in generated javadoc", "4. readFixedOpaque still has a copy", "not sure if it's possible to generat a read-only bytebuffer from another bytebuffer", "5. it would be nice to remove the extra copy for writeFixedOpaque", "For 4 and 5, I am ok if you think it's out of scope of this JIRA.", "A patch that addresses Brandon's comments.", "I'll address 4/5 in HADOOP-9966.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12603652/HADOOP-9669.002.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 4 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-nfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3104//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3104//console", "This message is automatically generated.", "+1.", "New patch looks good.", "Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have DEFAULT_INITIAL_CAPACITY as 256 instead of 512.", "A new patch that addresses the problems found by [~brandonli].", "Thanks very much for [~brandonli].", "SUCCESS: Integrated in Hadoop-trunk-Commit #4432 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4432/])", "HADOOP-9669 Reduce the number of byte array creations and copies in XDR data manipulation.", "Contributed by Haohui Mai (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1524259)", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpClient.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpServerHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/XDR.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/TestNfsTime.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/nfs3/TestFileHandle.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestXDR.java", "hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/security/TestCredentialsSys.java", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12603718/HADOOP-9669.003.patch", "against trunk revision .", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3105//console", "This message is automatically generated.", "I've committed the patch.", "Thank you, Haohui."], "SplitGT": [" check if the rest of data has more than len bytes"], "issueString": "Reduce the number of byte array creations and copies in XDR data manipulation\nXDR.writeXxx(..) methods ultimately use the static XDR.append(..) for writing each data type.  The static append creates a new array and copy data.  Therefore, for a singe reply such as RpcAcceptedReply.voidReply(..), there are multiple array creations and array copies.  For example, there are at least 6 array creations and array copies for RpcAcceptedReply.voidReply(..).\n\nHere is a more version that utilizes Java's ByteBuffer. It should be more efficient.\n\nThe APIs are compatible with the previous version.\n\nThe APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12603124/HADOOP-9669.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-common-project/hadoop-nfs:\n\norg.apache.hadoop.oncrpc.TestFrameDecoder\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3096//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3096//console\n\nThis message is automatically generated.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12603385/HADOOP-9669.001.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3102//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3102//console\n\nThis message is automatically generated.\nThanks, Haohui. Some comments:\n1. please try to keep the original javadoc for the same named methods\n2. can you make \"State state\" as final?\n3. please fix the javadoc /** check if the rest of data has more than <len> bytes */\n\"len\" is not visible in generated javadoc\n4. readFixedOpaque still has a copy\nnot sure if it's possible to generat a read-only bytebuffer from another bytebuffer\n5. it would be nice to remove the extra copy for writeFixedOpaque\nFor 4 and 5, I am ok if you think it's out of scope of this JIRA.\n\nA patch that addresses Brandon's comments.\n\nI'll address 4/5 in HADOOP-9966. \n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12603652/HADOOP-9669.002.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3104//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3104//console\n\nThis message is automatically generated.\n+1. New patch looks good. \nNit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have DEFAULT_INITIAL_CAPACITY as 256 instead of 512. \nA new patch that addresses the problems found by [~brandonli]. Thanks very much for [~brandonli].\nSUCCESS: Integrated in Hadoop-trunk-Commit #4432 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4432/])\nHADOOP-9669 Reduce the number of byte array creations and copies in XDR data manipulation. Contributed by Haohui Mai (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1524259)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpClient.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpServerHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/XDR.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/TestNfsTime.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/nfs3/TestFileHandle.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestXDR.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/security/TestCredentialsSys.java\n\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12603718/HADOOP-9669.003.patch\n  against trunk revision .\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3105//console\n\nThis message is automatically generated.\nI've committed the patch. Thank you, Haohui.\n", "issueSearchSentences": ["3. please fix the javadoc /** check if the rest of data has more than <len> bytes */", "HADOOP-9669 Reduce the number of byte array creations and copies in XDR data manipulation.", "The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization.", "XDR.writeXxx(..) methods ultimately use the static XDR.append(..) for writing each data type.", "\"len\" is not visible in generated javadoc"], "issueSearchScores": [0.504760205745697, 0.3975261449813843, 0.3764090836048126, 0.3431493639945984, 0.34055525064468384]}
{"aId": 5, "code": "@Override\n  public void removeAclFeature() {\n    throw new UnsupportedOperationException(\"ACLs are not supported on symlinks\");\n  }", "comment": " getAclFeature is not overridden because it is needed for resolving symlinks.", "issueId": "HDFS-6396", "issueStringList": ["Remove support for ACL feature from INodeSymlink", "Symlinks cannot have ACLs, but we still have support for the ACL feature in INodeSymlink because of class inheritance.", "Let's remove this support for code consistency.", "Here's a patch for this issue.", "Note that I didn't override getAclFeature because it is still needed for resolving symlinks.", "However, if acls can never be set for a symlink then there will never be any to get back.", "TestNameNodeAcl seems to verify that symlinks can never have acls set on them.", "Added extra checks to ensure that ACLs are never set on symlinks.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12646045/HDFS-6396.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6947//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6947//console", "This message is automatically generated.", "Existing tests cover this issue.", "This is just adding an extra safeguard to ensure that acls are never placed on symlinks.", "+1 LGTM, will commit shortly.", "Merged to trunk and branch-2.", "branch-2 required a trivial fixup because of the xattrs merge."], "SplitGT": [" getAclFeature is not overridden because it is needed for resolving symlinks."], "issueString": "Remove support for ACL feature from INodeSymlink\nSymlinks cannot have ACLs, but we still have support for the ACL feature in INodeSymlink because of class inheritance. Let's remove this support for code consistency.\nHere's a patch for this issue. Note that I didn't override getAclFeature because it is still needed for resolving symlinks. However, if acls can never be set for a symlink then there will never be any to get back. TestNameNodeAcl seems to verify that symlinks can never have acls set on them.\n\nAdded extra checks to ensure that ACLs are never set on symlinks.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12646045/HDFS-6396.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6947//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6947//console\n\nThis message is automatically generated.\nExisting tests cover this issue. This is just adding an extra safeguard to ensure that acls are never placed on symlinks.\n\n+1 LGTM, will commit shortly.\nMerged to trunk and branch-2. branch-2 required a trivial fixup because of the xattrs merge.\n", "issueSearchSentences": ["Note that I didn't override getAclFeature because it is still needed for resolving symlinks.", "This is just adding an extra safeguard to ensure that acls are never placed on symlinks.", "Remove support for ACL feature from INodeSymlink", "Added extra checks to ensure that ACLs are never set on symlinks.", "Let's remove this support for code consistency."], "issueSearchScores": [0.6624406576156616, 0.6295345425605774, 0.6139789819717407, 0.5952596664428711, 0.5658619403839111]}
{"aId": 6, "code": "public static INodeFileUnderConstruction valueOf(INode inode, String path\n      ) throws IOException {\n    final INodeFile file = INodeFile.valueOf(inode, path);\n    if (!file.isUnderConstruction()) {\n      throw new IOException(\"File is not under construction: \" + path);\n    }\n    return (INodeFileUnderConstruction)file;\n  }", "comment": " Cast INode to INodeFileUnderConstruction.", "issueId": "HDFS-4107", "issueStringList": ["Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction", "In the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.", "Let's add utility methods for such checks.", "h4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550408/h4107_20121022.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer", "org.apache.hadoop.hdfs.TestDistributedFileSystem", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//console", "This message is automatically generated.", "TestDistributedFileSystem compared exception messsages so that it failed.", "TestBalancerWithEncryptedTransfer timed out.", "It did not seem related to the patch.", "h4107_20121023.patch: updates exception messages.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550517/h4107_20121023.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//console", "This message is automatically generated.", "Went through the call stack to ensure the behavior is not changed.", "Only change I see is throwing ClassCastException instead of NPE in one of the cases.", "Can you please add few unit tests for these new methods?", "h4107_20121024.patch: adds a test and rewrites some exception messages.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550688/h4107_20121024.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//console", "This message is automatically generated.", "+1 for the patch.", "Integrated in Hadoop-trunk-Commit #2928 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2928/])", "HDFS-4107.", "Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.", "(Revision 1402265)", "Result = SUCCESS", "szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402265", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java", "I have committed this."], "SplitGT": [" Cast INode to INodeFileUnderConstruction."], "issueString": "Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction\nIn the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.  Let's add utility methods for such checks. \nh4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550408/h4107_20121022.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer\n                  org.apache.hadoop.hdfs.TestDistributedFileSystem\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//console\n\nThis message is automatically generated.\nTestDistributedFileSystem compared exception messsages so that it failed.  TestBalancerWithEncryptedTransfer timed out.  It did not seem related to the patch.\n\nh4107_20121023.patch: updates exception messages.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550517/h4107_20121023.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//console\n\nThis message is automatically generated.\nWent through the call stack to ensure the behavior is not changed. Only change I see is throwing ClassCastException instead of NPE in one of the cases.\n\nCan you please add few unit tests for these new methods?\nh4107_20121024.patch: adds a test and rewrites some exception messages.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550688/h4107_20121024.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//console\n\nThis message is automatically generated.\n+1 for the patch.\nIntegrated in Hadoop-trunk-Commit #2928 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2928/])\n    HDFS-4107. Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction. (Revision 1402265)\n\n     Result = SUCCESS\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402265\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java\n\nI have committed this.\n", "issueSearchSentences": ["h4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java", "In the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.", "Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.", "Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction"], "issueSearchScores": [0.7169329524040222, 0.6324142813682556, 0.6265650391578674, 0.598992109298706, 0.5917457342147827]}
{"aId": 7, "code": "private static String getNameServiceId(Configuration conf, String addressKey) {\n    String nameserviceId = conf.get(DFS_FEDERATION_NAMESERVICE_ID);\n    if (nameserviceId != null) {\n      return nameserviceId;\n    }\n    \n    Collection<String> ids = getNameServiceIds(conf);\n    if (ids == null || ids.size() == 0) {\n      // Not federation configuration, hence no nameservice Id\n      return null;\n    }\n    \n    // Match the rpc address with that of local address\n    int found = 0;\n    for (String id : ids) {\n      String addr = conf.get(getNameServiceIdKey(addressKey, id));\n      InetSocketAddress s = NetUtils.createSocketAddr(addr);\n      if (NetUtils.isLocalAddress(s.getAddress())) {\n        nameserviceId = id;\n        found++;\n      }\n    }\n    if (found > 1) { // Only one address must match the local address\n      throw new HadoopIllegalArgumentException(\n          \"Configuration has multiple RPC addresses that matches \"\n              + \"the local node's address. Please configure the system with \"\n              + \"the parameter \" + DFS_FEDERATION_NAMESERVICE_ID);\n    }\n    if (found == 0) {\n      throw new HadoopIllegalArgumentException(\"Configuration address \"\n          + addressKey + \" is missing in configuration with name service Id\");\n    }\n    return nameserviceId;\n  }", "comment": " Get the nameservice Id by matching the addressKey with thethe address of the local node.", "issueId": "HDFS-2355", "issueStringList": ["Federation: enable using the same configuration file across all the nodes in the cluster.", "In federation the configuration files xml include another configuration xml.", "This configuration xml is empty for datanodes and defines nameservice ID for namenodes, secondary and backup nodes.", "The need for xml include can be eliminated by namenodes determining the nameservice ID by matching its local address with a configured address (such as namenode rpc address) and on match using the nameservice ID of the corresponding configuration parameter.", "For more details about federation configuration see https://issues.apache.org/jira/browse/HDFS-1689?focusedCommentId=13001749&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13001749", "Attached patch does the following:", "# If \"dfs.federation.nameservice.id\" is defined, then that is used as nameservice Id.", "# If it is not defined, the following addresses are matched to localhost:", "#* Namenode matches for all the nameservices defined, dfs.namenode.rpc-address.<nsid> to that of the localaddress.", "When a match is found, it picks the corresponding nsid as nameservice id.", "#* SecondaryNamenode matches for all the nameservices defined, dfs.namenode.secondary.http-address.<nsid> to that of the localaddress.", "When a match is found, it picks the corresponding nsid as nameservice id.", "#* BackupNode matches for all the nameservices defined, dfs.namenode.backup.address.<nsid> to that of the localaddress.", "When a match is found, it picks the corresponding nsid as nameservice id.", "DFSUtil.java#getNameServiceId", "We should throw HadoopIllegalArgumentException if no match found?", "Currently, no match found and non federation cases are treated as same.", "Remove a System.out.println.", "Addressed comments.", "Also shutting down the nodes when name service ID could not be found.", "+1 for the patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12496916/HDFS-2355.txt", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hdfs.TestDfsOverAvroRpc", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/1312//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1312//console", "This message is automatically generated.", "Integrated in Hadoop-Common-trunk-Commit #983 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/983/])", "HDFS-2355.", "Federation: enable using the same configuration file across all the nodes in the cluster.", "Contributed by Suresh Srinivas.", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java", "Integrated in Hadoop-Hdfs-trunk-Commit #1061 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1061/])", "HDFS-2355.", "Federation: enable using the same configuration file across all the nodes in the cluster.", "Contributed by Suresh Srinivas.", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #1005 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1005/])", "HDFS-2355.", "Federation: enable using the same configuration file across all the nodes in the cluster.", "Contributed by Suresh Srinivas.", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java"], "SplitGT": [" Get the nameservice Id by matching the addressKey with thethe address of the local node."], "issueString": "Federation: enable using the same configuration file across all the nodes in the cluster.\nIn federation the configuration files xml include another configuration xml. This configuration xml is empty for datanodes and defines nameservice ID for namenodes, secondary and backup nodes. The need for xml include can be eliminated by namenodes determining the nameservice ID by matching its local address with a configured address (such as namenode rpc address) and on match using the nameservice ID of the corresponding configuration parameter.\n\nFor more details about federation configuration see https://issues.apache.org/jira/browse/HDFS-1689?focusedCommentId=13001749&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13001749\nAttached patch does the following:\n# If \"dfs.federation.nameservice.id\" is defined, then that is used as nameservice Id.\n# If it is not defined, the following addresses are matched to localhost:\n#* Namenode matches for all the nameservices defined, dfs.namenode.rpc-address.<nsid> to that of the localaddress. When a match is found, it picks the corresponding nsid as nameservice id.\n#* SecondaryNamenode matches for all the nameservices defined, dfs.namenode.secondary.http-address.<nsid> to that of the localaddress. When a match is found, it picks the corresponding nsid as nameservice id.\n#* BackupNode matches for all the nameservices defined, dfs.namenode.backup.address.<nsid> to that of the localaddress. When a match is found, it picks the corresponding nsid as nameservice id.\n\nDFSUtil.java#getNameServiceId\n  - We should throw HadoopIllegalArgumentException if no match found? Currently, no match found and non federation cases are treated as same.\n\nRemove a System.out.println.\nAddressed comments. Also shutting down the nodes when name service ID could not be found.\n+1 for the patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12496916/HDFS-2355.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests:\n                  org.apache.hadoop.hdfs.TestDfsOverAvroRpc\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/1312//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/1312//console\n\nThis message is automatically generated.\nIntegrated in Hadoop-Common-trunk-Commit #983 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/983/])\n    HDFS-2355. Federation: enable using the same configuration file across all the nodes in the cluster. Contributed by Suresh Srinivas.\n\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java\n\nIntegrated in Hadoop-Hdfs-trunk-Commit #1061 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1061/])\n    HDFS-2355. Federation: enable using the same configuration file across all the nodes in the cluster. Contributed by Suresh Srinivas.\n\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java\n\nIntegrated in Hadoop-Mapreduce-trunk-Commit #1005 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1005/])\n    HDFS-2355. Federation: enable using the same configuration file across all the nodes in the cluster. Contributed by Suresh Srinivas.\n\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177100\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestMulitipleNNDataBlockScanner.java\n\n", "issueSearchSentences": ["DFSUtil.java#getNameServiceId", "# If \"dfs.federation.nameservice.id\" is defined, then that is used as nameservice Id.", "#* Namenode matches for all the nameservices defined, dfs.namenode.rpc-address.<nsid> to that of the localaddress.", "#* SecondaryNamenode matches for all the nameservices defined, dfs.namenode.secondary.http-address.<nsid> to that of the localaddress.", "Also shutting down the nodes when name service ID could not be found."], "issueSearchScores": [0.7381678819656372, 0.6341609954833984, 0.5502306222915649, 0.4888230264186859, 0.48591822385787964]}
{"aId": 8, "code": "@Override\n  public String toString() {\n    String str = null;\n\n    if (allAllowed) {\n      str = \"All users are allowed\";\n    }\n    else if (users.isEmpty() && groups.isEmpty()) {\n      str = \"No users are allowed\";\n    }\n    else {\n      String usersStr = null;\n      String groupsStr = null;\n      if (!users.isEmpty()) {\n        usersStr = users.toString();\n      }\n      if (!groups.isEmpty()) {\n        groupsStr = groups.toString();\n      }\n\n      if (!users.isEmpty() && !groups.isEmpty()) {\n        str = \"Users \" + usersStr + \" and members of the groups \"\n            + groupsStr + \" are allowed\";\n      }\n      else if (!users.isEmpty()) {\n        str = \"Users \" + usersStr + \" are allowed\";\n      }\n      else {// users is empty array and groups is nonempty\n        str = \"Members of the groups \"\n            + groupsStr + \" are allowed\";\n      }\n    }\n\n    return str;\n  }", "comment": " Returns descriptive way of users and groups that are part of this ACL.", "issueId": "HADOOP-6715", "issueStringList": ["AccessControlList.toString() returns empty string when we set acl to \"*\"", "AccessControlList.toString() returns empty string when we set the acl to \"\\*\" and also when we set it to empty(i.e. \"", "\").", "This is causing wrong values for ACLs shown on jobdetails.jsp and jobdetailshistory.jsp web pages when acls are set to \"\\*\".", "I think AccessControlList.toString() needs to be changed to return \"\\*\" when we set the acl to \"\\*\".", "Maybe something a little more user-friendly ?", "Like ALL USERS or something like that ?", "There is a similar, but more pertinent, display issue with empty string.", "There, we display an empty string, which in effect shows up as nothing.", "It would be nice to show something like NONE.", "Right.", "Would look good, if .toString() is used for displaying only.", "But if .toString() is used to save it as a String and later set it to a key in some Configuration object, then \"ALL USERS\" will not be considered as \"*\" and will cause an issue ?", "I propose we separate the display functionality from {{toString()}} as the latter can potentially be used for other purposes as Ravi mentioned above.", "So we will", "fix {{toString()}} addressing the bug related to the wild-card '*'", "add a new API say {{AccessControlList.getDisplayString()}}.", "This should return \"ALL USERS\" for '*' and \"NONE\" for ' ' (empty space character) and the output of {{toString}} for every other value of ACL.", "Thoughts?", "Can one of you help me understand what Ravi means by \"save it as a String and later set it to a key in some Configuration object\" ?", "It seems like we need to store the ACL objects in some map, and possibly these need to be reconstructed from a serialized representation (like for task log access, maybe ?)", "and we are using the key of the map as the String that is thus serialized.", "If that's the case, can we serialize the ACL using some representation that stores the actual name and value as separate fields rather than a toString representation, use a hashCode / equals on the ACL object to build a key based on these fields, and use toString for display purposes.", "This seems more canonical to me (inline with what toString is typically used for).", "Right Hemanth.", "People wouldn't expect toString() to give serialized string of object.", "We can have our own messages in it.", "Will modify AccessControlList.toString() itself to give \"ALL USERS\" and \"NONE\" for \"\\*\" and empty acl cases.", "Um.. 'ALL', 'USERS', 'NONE' can themselves be users/groups and will lead to confusions, however corner-cased they might be.", "How about something simpler like the following?", "Essentially, I'm trying to make the space character stand out by using special chars '[' and ']' which are unlikely to end up in user/group names.", "{code}", "Job-ACLs: [*]", "Job-ACLs: [ ]", "Job-ACLs: [ group1]", "Job-ACLs: [user1,user2 group1,group2]", "{code}", "In fact more spaces between user-list and group-list makes things clearer, I think", "{code}", "Job-ACLs: [*]", "Job-ACLs: [             ]", "Job-ACLs: [             group1]", "Job-ACLs: [user1,user2             group1,group2]", "{code}", "We could solve the problem of All, Users, None etc being valid user names by modifying the display string.", "Note that this is all primarily presentation layer changes.", "Firstly, I think displaying allowed users and groups separately in the UI would make it much more user friendly - rather than sticking to our internal representation.", "So, we could say:", "Allowed Users: a,b,c", "Allowed Groups: d,e,f", "When All or No users / groups have access, instead of saying:", "\"Users: All\" or \"Groups: None\"", "we could say", "\"All users can access job\" or \"No groups can access job\"", "Would this work ?", "+1.", "That's looks far better than anything else we considered before.", "So, in summary, what we have currently", "{code}", "Job-ACLs:", "mapreduce.job.acl-view-job:  group1,group2", "mapreduce.job.acl-modify-job: *", "{code}", "will become", "{code}", "Job-ACLs:", "mapreduce.job.acl-view-job:", "Users: No users are allowed", "Groups: group1,group2", "mapreduce.job.acl-modify-job:", "Users: All users are allowed", "Groups: All groups are allowed", "{code}", "Right?", "Yes, that's the general idea.", "+1 from me.", "You may just want to get some opinion (from users or operations folks about whether users will be confused if it says \"No users are allowed\", but some groups are allowed.", "Argh.. that is confusing too.. one final jab at it:", "||ACL||Message||", "|user1,user2 group1,group2|Users user1,user2 and members of the groups group1,group2 are allowed|", "|user1,user2 |Users user1,user2 are allowed|", "| group1,group2|Members of the groups group1,group2 are allowed|", "| (blank space) | No users are allowed|", "|*(asterisk)| All users are allowed|", "How about that?", "(Nothing is easy around here.. )", "Attaching patch for earlier version of hadoop.", "Not for commit here.", "Attaching patch for trunk.", "AccessControlList.toString() returns a descriptive String of users and groups that are part of this ACL.", "This is same as that is there in 6715.20S.6.patch.", "Also added a new public method AccessControlList.getAclString() that returns the exact String that can be used for creating a new instance of AccessControlList by passing it to the constructor.", "Attaching new patch for current trunk because HADOOP-6862 just got merged to trunk.", "Patch looks good.", "+1.", "We should commit this together with MAPREDUCE-1780 so that mapreduce doesn't get broken.", "Hudson didn't come back for 1 day.", "I manually ran unit tests and test-patch.", "All unit tests passed.", "test-patch gave:", "[exec] -1 overall.", "[exec]", "[exec]     +1 @author.", "The patch does not contain any @author tags.", "[exec]", "[exec]     +1 tests included.", "The patch appears to include 3 new or modified tests.", "[exec]", "[exec]     -1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "[exec]", "[exec]     +1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "[exec]", "[exec]     +1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "[exec]", "[exec]     +1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "I verified that the existing javadoc warnings are not related to this patch.", "I just committed this.", "Thanks Ravi !", "Integrated in Hadoop-Common-trunk-Commit #357 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/357/])", "HADOOP-6715.", "Fixes AccessControlList.toString() to return a descriptive String representation of the ACL.", "Contributed by Ravi Gummadi", "Integrated in Hadoop-Common-trunk #422 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/422/])", "HADOOP-6715.", "Fixes AccessControlList.toString() to return a descriptive String representation of the ACL.", "Contributed by Ravi Gummadi"], "SplitGT": [" Returns descriptive way of users and groups that are part of this ACL."], "issueString": "AccessControlList.toString() returns empty string when we set acl to \"*\"\nAccessControlList.toString() returns empty string when we set the acl to \"\\*\" and also when we set it to empty(i.e. \" \"). This is causing wrong values for ACLs shown on jobdetails.jsp and jobdetailshistory.jsp web pages when acls are set to \"\\*\".\n\nI think AccessControlList.toString() needs to be changed to return \"\\*\" when we set the acl to \"\\*\".\nMaybe something a little more user-friendly ? Like ALL USERS or something like that ? There is a similar, but more pertinent, display issue with empty string. There, we display an empty string, which in effect shows up as nothing. It would be nice to show something like NONE.\nRight. Would look good, if .toString() is used for displaying only.\nBut if .toString() is used to save it as a String and later set it to a key in some Configuration object, then \"ALL USERS\" will not be considered as \"*\" and will cause an issue ?\nI propose we separate the display functionality from {{toString()}} as the latter can potentially be used for other purposes as Ravi mentioned above. So we will\n - fix {{toString()}} addressing the bug related to the wild-card '*'\n - add a new API say {{AccessControlList.getDisplayString()}}. This should return \"ALL USERS\" for '*' and \"NONE\" for ' ' (empty space character) and the output of {{toString}} for every other value of ACL.\n\nThoughts?\nCan one of you help me understand what Ravi means by \"save it as a String and later set it to a key in some Configuration object\" ? It seems like we need to store the ACL objects in some map, and possibly these need to be reconstructed from a serialized representation (like for task log access, maybe ?) and we are using the key of the map as the String that is thus serialized.\n\nIf that's the case, can we serialize the ACL using some representation that stores the actual name and value as separate fields rather than a toString representation, use a hashCode / equals on the ACL object to build a key based on these fields, and use toString for display purposes. This seems more canonical to me (inline with what toString is typically used for).\nRight Hemanth. People wouldn't expect toString() to give serialized string of object. We can have our own messages in it.\nWill modify AccessControlList.toString() itself to give \"ALL USERS\" and \"NONE\" for \"\\*\" and empty acl cases.\nUm.. 'ALL', 'USERS', 'NONE' can themselves be users/groups and will lead to confusions, however corner-cased they might be.\n\nHow about something simpler like the following? Essentially, I'm trying to make the space character stand out by using special chars '[' and ']' which are unlikely to end up in user/group names.\n{code}\nJob-ACLs: [*]\nJob-ACLs: [ ]\nJob-ACLs: [ group1]\nJob-ACLs: [user1,user2 group1,group2]\n{code}\nIn fact more spaces between user-list and group-list makes things clearer, I think\n{code}\nJob-ACLs: [*]\nJob-ACLs: [             ]\nJob-ACLs: [             group1]\nJob-ACLs: [user1,user2             group1,group2]\n{code}\nWe could solve the problem of All, Users, None etc being valid user names by modifying the display string. Note that this is all primarily presentation layer changes. \n\nFirstly, I think displaying allowed users and groups separately in the UI would make it much more user friendly - rather than sticking to our internal representation.\nSo, we could say:\n\nAllowed Users: a,b,c\nAllowed Groups: d,e,f\n\nWhen All or No users / groups have access, instead of saying:\n\"Users: All\" or \"Groups: None\"\nwe could say\n\"All users can access job\" or \"No groups can access job\"\n\nWould this work ?\n+1. That's looks far better than anything else we considered before.\n\nSo, in summary, what we have currently\n\n{code}\nJob-ACLs:\n      mapreduce.job.acl-view-job:  group1,group2\n      mapreduce.job.acl-modify-job: *\n{code}\n\nwill become\n\n{code}\nJob-ACLs:\n      mapreduce.job.acl-view-job:\n            Users: No users are allowed\n           Groups: group1,group2\n      mapreduce.job.acl-modify-job:\n             Users: All users are allowed\n            Groups: All groups are allowed\n{code}\n\nRight?\nYes, that's the general idea. +1 from me. You may just want to get some opinion (from users or operations folks about whether users will be confused if it says \"No users are allowed\", but some groups are allowed.\nArgh.. that is confusing too.. one final jab at it:\n\n||ACL||Message||\n|user1,user2 group1,group2|Users user1,user2 and members of the groups group1,group2 are allowed|\n|user1,user2 |Users user1,user2 are allowed|\n| group1,group2|Members of the groups group1,group2 are allowed|\n| (blank space) | No users are allowed|\n|*(asterisk)| All users are allowed|\nHow about that? (Nothing is easy around here.. )\nAttaching patch for earlier version of hadoop. Not for commit here.\nAttaching patch for trunk.\n\nAccessControlList.toString() returns a descriptive String of users and groups that are part of this ACL. This is same as that is there in 6715.20S.6.patch.\n\n Also added a new public method AccessControlList.getAclString() that returns the exact String that can be used for creating a new instance of AccessControlList by passing it to the constructor.\nAttaching new patch for current trunk because HADOOP-6862 just got merged to trunk.\nPatch looks good. +1.\n\nWe should commit this together with MAPREDUCE-1780 so that mapreduce doesn't get broken.\nHudson didn't come back for 1 day.\nI manually ran unit tests and test-patch.\nAll unit tests passed.\ntest-patch gave:\n\n     [exec] -1 overall.\n     [exec]\n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec]\n     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.\n     [exec]\n     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n     [exec]\n     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n     [exec]\n     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n     [exec]\n     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n\nI verified that the existing javadoc warnings are not related to this patch.\nI just committed this. Thanks Ravi !\n\nIntegrated in Hadoop-Common-trunk-Commit #357 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/357/])\n    HADOOP-6715. Fixes AccessControlList.toString() to return a descriptive String representation of the ACL. Contributed by Ravi Gummadi\n\nIntegrated in Hadoop-Common-trunk #422 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/422/])\n    HADOOP-6715. Fixes AccessControlList.toString() to return a descriptive String representation of the ACL. Contributed by Ravi Gummadi\n\n", "issueSearchSentences": ["AccessControlList.toString() returns a descriptive String of users and groups that are part of this ACL.", "Would look good, if .toString() is used for displaying only.", "But if .toString() is used to save it as a String and later set it to a key in some Configuration object, then \"ALL USERS\" will not be considered as \"*\" and will cause an issue ?", "I think AccessControlList.toString() needs to be changed to return \"\\*\" when we set the acl to \"\\*\".", "Firstly, I think displaying allowed users and groups separately in the UI would make it much more user friendly - rather than sticking to our internal representation."], "issueSearchScores": [0.7071061730384827, 0.6320334672927856, 0.6168148517608643, 0.5577724575996399, 0.5521504282951355]}
{"aId": 11, "code": "public void allowSnapshot(String snapshotRoot) throws IOException {\n    namenode.allowSnapshot(snapshotRoot);\n  }", "comment": " Allow snapshot on a directory.", "issueId": "HDFS-4084", "issueStringList": ["provide CLI support for allow and disallow snapshot on a directory", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "@Brandon", "Can we make the new commands case insensitive?", "We can log a different jira to make existing commands also case insensitive.", "Good point, Arpit.", "Will change it and upload a new patch.", "Re-based the patch and addressed Arpit's comment.", "Comments:", "# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods", "{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "{quote}", "Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "Added javadoc for them.", "{quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "{quote}", "Removed, realized these two methods need to be public anyway.", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote}", "Yes.", "removed.", "{quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote}", "Done.", "Thanks.", "I committed the patch to HDFS-2802 branch.", "Thank you Brandon."], "SplitGT": [" Allow snapshot on a directory."], "issueString": "provide CLI support for allow and disallow snapshot on a directory\nTo provide CLI support to allow snapshot, disallow snapshot on a directory.\n@Brandon\n\nCan we make the new commands case insensitive? We can log a different jira to make existing commands also case insensitive. \nGood point, Arpit. Will change it and upload a new patch. \nRe-based the patch and addressed Arpit's comment.\nComments:\n# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.\n# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.\n# FSNamesystem.java is unnecessarily importing SnapshotInfo?\n# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods\n\n{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.{quote} \nMethods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override. Added javadoc for them.\n   {quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.{quote} \nRemoved, realized these two methods need to be public anyway.\n    {quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?{quote} \nYes. removed.\n    {quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote} \nDone. Thanks.\n\nI committed the patch to HDFS-2802 branch. Thank you Brandon.\n", "issueSearchSentences": ["Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "provide CLI support for allow and disallow snapshot on a directory", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?"], "issueSearchScores": [0.6293054819107056, 0.5613719820976257, 0.505608320236206, 0.4870143532752991, 0.4868614673614502]}
{"aId": 12, "code": "public short getUMask() {\n    if (symbolic) {\n      // Return the complement of octal equivalent of umask that was computed\n      return (short) (~umaskMode & 0777);      \n    }\n    return umaskMode;\n  }", "comment": " For octal umask, the specified bits are set in the file mode creation mask.", "issueId": "HADOOP-6710", "issueStringList": ["Symbolic umask for file creation is not consistent with posix", "Currently both octal and symbolic umask are used to reset the file creation mode bits.", "This is not consistent with the behavior defined in posix.", "Making it consistent would avoid confusion to the HDFS users.", "Snippet from the posix specification http://www.opengroup.org/onlinepubs/000095399/utilities/umask.html:", "For a symbolic_mode value, the new value of the file mode creation mask shall be the logical complement of the file permission bits portion of the file mode specified by the symbolic_mode  string.", "In a symbolic_mode value, the permissions op characters '+' and '-' shall be interpreted relative to the current file mode creation mask; '+' shall cause the bits for the indicated permissions to be cleared in the mask; '-' shall cause the bits for the indicated permissions to be set in the mask.", "The interpretation of mode values that specify file mode bits other than the file permission bits is unspecified.", "In the octal integer form of mode, the specified bits are set in the file mode creation mask.", "The file mode creation mask shall be set to the resulting numeric value.", "Attached patch:", "# Adds support for posix compliant symbolic umask support with extensive tests", "# Current symbolic umask of format \"u=,g=,o=\" is not parsed correctly.", "This should be same as octal umask 777.", "Changing the regex to support this.", "+1.", "Looks excellent.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12441987/hadoop-6710.patch", "against trunk revision 937183.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/console", "This message is automatically generated.", "Committed the patch."], "SplitGT": [" For octal umask, the specified bits are set in the file mode creation mask."], "issueString": "Symbolic umask for file creation is not consistent with posix\nCurrently both octal and symbolic umask are used to reset the file creation mode bits. This is not consistent with the behavior defined in posix. Making it consistent would avoid confusion to the HDFS users.\nSnippet from the posix specification http://www.opengroup.org/onlinepubs/000095399/utilities/umask.html:\nFor a symbolic_mode value, the new value of the file mode creation mask shall be the logical complement of the file permission bits portion of the file mode specified by the symbolic_mode  string.\n\nIn a symbolic_mode value, the permissions op characters '+' and '-' shall be interpreted relative to the current file mode creation mask; '+' shall cause the bits for the indicated permissions to be cleared in the mask; '-' shall cause the bits for the indicated permissions to be set in the mask.\n\nThe interpretation of mode values that specify file mode bits other than the file permission bits is unspecified.\n\nIn the octal integer form of mode, the specified bits are set in the file mode creation mask.\n\nThe file mode creation mask shall be set to the resulting numeric value.\nAttached patch:\n# Adds support for posix compliant symbolic umask support with extensive tests\n# Current symbolic umask of format \"u=,g=,o=\" is not parsed correctly. This should be same as octal umask 777. Changing the regex to support this.\n+1. Looks excellent.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12441987/hadoop-6710.patch\n  against trunk revision 937183.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/475/console\n\nThis message is automatically generated.\nCommitted the patch.\n", "issueSearchSentences": ["This should be same as octal umask 777.", "Snippet from the posix specification http://www.opengroup.org/onlinepubs/000095399/utilities/umask.html:", "Currently both octal and symbolic umask are used to reset the file creation mode bits.", "+1 javadoc.", "# Current symbolic umask of format \"u=,g=,o=\" is not parsed correctly."], "issueSearchScores": [0.6198194622993469, 0.5206583738327026, 0.4339345097541809, 0.39618659019470215, 0.3872759938240051]}
{"aId": 17, "code": "private static void verifyAndSetRMHAIdsList(Configuration conf) {\n    Collection<String> ids =\n      conf.getTrimmedStringCollection(YarnConfiguration.RM_HA_IDS);\n    if (ids.size() < 2) {\n      throwBadConfigurationException(\n        getInvalidValueMessage(YarnConfiguration.RM_HA_IDS,\n          conf.get(YarnConfiguration.RM_HA_IDS) +\n          \"\\nHA mode requires atleast two RMs\"));\n    }\n\n    StringBuilder setValue = new StringBuilder();\n    for (String id: ids) {\n      // verify the RM service addresses configurations for every RMIds\n      for (String prefix : YarnConfiguration.RM_SERVICES_ADDRESS_CONF_KEYS) {\n        String confKey = null;\n        try {\n          confKey = addSuffix(prefix, id);\n          if (conf.getTrimmed(confKey) == null) {\n            throwBadConfigurationException(getNeedToSetValueMessage(confKey));\n          }\n        } catch (IllegalArgumentException iae) {\n          String errmsg = iae.getMessage();\n          if (confKey == null) {\n            // Error at addSuffix\n            errmsg = getInvalidValueMessage(YarnConfiguration.RM_HA_ID,\n              getRMHAId(conf));\n          }\n          throwBadConfigurationException(errmsg);\n        }\n      }\n      setValue.append(id);\n      setValue.append(\",\");\n    }\n    conf.set(YarnConfiguration.RM_HA_IDS,\n      setValue.substring(0, setValue.length() - 1));\n  }", "comment": " Verify configuration that there are at least two RM-ids and RPC addresses are specified for each RM-id.", "issueId": "YARN-1485", "issueStringList": ["Enabling HA should verify the RM service addresses configurations have been set for every RM Ids defined in RM_HA_IDs", "After YARN-1325, the YarnConfiguration.RM_HA_IDS will contain multiple RM_Ids.", "We need to verify that the RM service addresses configurations have been set for all of RM_Ids.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12618490/YARN-1485.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:", "org.apache.hadoop.yarn.server.TestContainerManagerSecurity", "org.apache.hadoop.yarn.server.TestRMNMSecretKeys", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2653//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2653//console", "This message is automatically generated.", "I proposed a bunch of renames at YARN-1325.", "Please do them here.", "TestHAUtil: +121, +122.", "Set host:port instead of a RM ID.", "bq.", "TestHAUtil: +121, +122.", "Set host:port instead of a RM ID.", "fixed", "bq.", "I proposed a bunch of renames at YARN-1325.", "Please do them here.", "new patch has renamed verifyAndSetRMHAIds to verifyAndSetRMHAIdsList and verifyAndSetRMHAId to verifyAndSetCurrentRMHAId", "verifyAndSetAllRpcAddresses -> verifyAndSetAllServiceAddresses and YarnConfiguration.RM_RPC_ADDRESS_CONF_KEYS -> RM_SERVICES_ADDRESS_CONF_KEYS have already been done in YARN-1325", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12618673/YARN-1485.2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:", "org.apache.hadoop.yarn.server.TestRMNMSecretKeys", "org.apache.hadoop.yarn.server.TestContainerManagerSecurity", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2657//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2657//console", "This message is automatically generated.", "Again, these test failures are not related", "The method {{HAUtil#verifyAndSetRMHAIdsList}} verifies there are at least two RM-ids and RPC addresses are specified for each RM-id.", "Then, sets the rm-ids.", "However, the method name doesn't capture all this.", "It would be nice to either pick a name that captures this or add a comment at the beginning of the method that explains all this.", "Otherwise, the patch looks good to me.", "Depending on which goes first, this might have to rebase on YARN-1028.", "Thanks for the review.", "bq.It would be nice to either pick a name that captures this or add a comment at the beginning of the method that explains all this.", "Added comments", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12618693/YARN-1485.3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:", "org.apache.hadoop.yarn.server.TestContainerManagerSecurity", "org.apache.hadoop.yarn.server.TestRMNMSecretKeys", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2658//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2658//console", "This message is automatically generated.", "+1, looks good.", "Checked this into trunk and branch-2.", "Thanks Xuan!", "SUCCESS: Integrated in Hadoop-trunk-Commit #4881 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4881/])", "YARN-1485.", "Modified RM HA configuration validation to also ensure that service-address configuration are configured for every RM.", "Contributed by Xuan Gong.", "(vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550854)", "hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/HAUtil.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/conf/TestHAUtil.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java"], "SplitGT": [" Verify configuration that there are at least two RM-ids and RPC addresses are specified for each RM-id."], "issueString": "Enabling HA should verify the RM service addresses configurations have been set for every RM Ids defined in RM_HA_IDs\nAfter YARN-1325, the YarnConfiguration.RM_HA_IDS will contain multiple RM_Ids. We need to verify that the RM service addresses configurations have been set for all of RM_Ids.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12618490/YARN-1485.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:\n\n                  org.apache.hadoop.yarn.server.TestContainerManagerSecurity\n                  org.apache.hadoop.yarn.server.TestRMNMSecretKeys\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2653//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2653//console\n\nThis message is automatically generated.\nI proposed a bunch of renames at YARN-1325. Please do them here.\n\nTestHAUtil: +121, +122. Set host:port instead of a RM ID.\nbq. TestHAUtil: +121, +122. Set host:port instead of a RM ID.\n\nfixed\n\nbq. I proposed a bunch of renames at YARN-1325. Please do them here.\n\nnew patch has renamed verifyAndSetRMHAIds to verifyAndSetRMHAIdsList and verifyAndSetRMHAId to verifyAndSetCurrentRMHAId\n\nverifyAndSetAllRpcAddresses -> verifyAndSetAllServiceAddresses and YarnConfiguration.RM_RPC_ADDRESS_CONF_KEYS -> RM_SERVICES_ADDRESS_CONF_KEYS have already been done in YARN-1325\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12618673/YARN-1485.2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:\n\n                  org.apache.hadoop.yarn.server.TestRMNMSecretKeys\n                  org.apache.hadoop.yarn.server.TestContainerManagerSecurity\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2657//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2657//console\n\nThis message is automatically generated.\nAgain, these test failures are not related\nThe method {{HAUtil#verifyAndSetRMHAIdsList}} verifies there are at least two RM-ids and RPC addresses are specified for each RM-id. Then, sets the rm-ids. However, the method name doesn't capture all this. It would be nice to either pick a name that captures this or add a comment at the beginning of the method that explains all this.\n\nOtherwise, the patch looks good to me. Depending on which goes first, this might have to rebase on YARN-1028. \n\nThanks for the review.\nbq.It would be nice to either pick a name that captures this or add a comment at the beginning of the method that explains all this.\n\nAdded comments \n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12618693/YARN-1485.3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:\n\n                  org.apache.hadoop.yarn.server.TestContainerManagerSecurity\n                  org.apache.hadoop.yarn.server.TestRMNMSecretKeys\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2658//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2658//console\n\nThis message is automatically generated.\n+1, looks good. Checked this into trunk and branch-2. Thanks Xuan!\nSUCCESS: Integrated in Hadoop-trunk-Commit #4881 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4881/])\nYARN-1485. Modified RM HA configuration validation to also ensure that service-address configuration are configured for every RM. Contributed by Xuan Gong. (vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550854)\n* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/HAUtil.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/conf/TestHAUtil.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java\n\n", "issueSearchSentences": ["After YARN-1325, the YarnConfiguration.RM_HA_IDS will contain multiple RM_Ids.", "new patch has renamed verifyAndSetRMHAIds to verifyAndSetRMHAIdsList and verifyAndSetRMHAId to verifyAndSetCurrentRMHAId", "The method {{HAUtil#verifyAndSetRMHAIdsList}} verifies there are at least two RM-ids and RPC addresses are specified for each RM-id.", "Enabling HA should verify the RM service addresses configurations have been set for every RM Ids defined in RM_HA_IDs", "We need to verify that the RM service addresses configurations have been set for all of RM_Ids."], "issueSearchScores": [0.6916323304176331, 0.6404802799224854, 0.6384034156799316, 0.616121768951416, 0.5951627492904663]}
{"aId": 18, "code": "public static void closeAllForUGI(UserGroupInformation ugi) \n  throws IOException {\n    CACHE.closeAll(ugi);\n  }", "comment": " Close all cached filesystems for a given UGI.", "issueId": "HADOOP-6888", "issueStringList": ["Being able to close all cached FileSystem objects for a given UGI", "This is the Common part of MAPREDUCE-1900.", "It adds a utility method to FileSystem that closes all cached filesystems for a given UGI.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450763/c6888-02.patch", "against trunk revision 979944.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/console", "This message is automatically generated.", "The javadoc warnings are due to KerberosName and SecurityUtil, and unrelated to this patch.", "+1 patch looks good.", "I have committed this.", "Thanks, Kan!", "Also thanks Devaraj for working on this."], "SplitGT": [" Close all cached filesystems for a given UGI."], "issueString": "Being able to close all cached FileSystem objects for a given UGI\nThis is the Common part of MAPREDUCE-1900. It adds a utility method to FileSystem that closes all cached filesystems for a given UGI.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450763/c6888-02.patch\n  against trunk revision 979944.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/645/console\n\nThis message is automatically generated.\nThe javadoc warnings are due to KerberosName and SecurityUtil, and unrelated to this patch.\n+1 patch looks good.\nI have committed this.  Thanks, Kan!\nAlso thanks Devaraj for working on this.\n", "issueSearchSentences": ["Being able to close all cached FileSystem objects for a given UGI", "It adds a utility method to FileSystem that closes all cached filesystems for a given UGI.", "+1 release audit.", "I have committed this.", "Here are the results of testing the latest attachment"], "issueSearchScores": [0.6396175622940063, 0.5769612193107605, 0.19216860830783844, 0.16390877962112427, 0.14661771059036255]}
{"aId": 19, "code": "public static Collection<URI> getNameServiceUris(Configuration conf,\n      String... keys) {\n    Set<URI> ret = new HashSet<URI>();\n    for (String nsId : getNameServiceIds(conf)) {\n      if (HAUtil.isHAEnabled(conf, nsId)) {\n        // Add the logical URI of the nameservice.\n        try {\n          ret.add(new URI(HdfsConstants.HDFS_URI_SCHEME + \"://\" + nsId));\n        } catch (URISyntaxException ue) {\n          throw new IllegalArgumentException(ue);\n        }\n      } else {\n        // Add the URI corresponding to the address of the NN.\n        for (String key : keys) {\n          String addr = conf.get(concatSuffixes(key, nsId));\n          if (addr != null) {\n            ret.add(createUri(HdfsConstants.HDFS_URI_SCHEME,\n                NetUtils.createSocketAddr(addr)));\n            break;\n          }\n        }\n      }\n    }\n    // Add the generic configuration keys.\n    for (String key : keys) {\n      String addr = conf.get(key);\n      if (addr != null) {\n        ret.add(createUri(\"hdfs\", NetUtils.createSocketAddr(addr)));\n        break;\n      }\n    }\n    return ret;\n  }", "comment": " Get a URI for each configured nameservice. If a nameservice is HA-enabled, then the logical URI of the nameservice is returned.", "issueId": "HDFS-2979", "issueStringList": ["HA: Balancer should use logical uri for creating failover proxy with HA enabled.", "Presently Balancer uses real URI for creating the failover proxy.", "Since the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI.", "Presently will work only with default port.", "java.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.", "at org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)", "at org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)", "at org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)", "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401)", "Attached the patch with addressing the issue.", "Hey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.", "Furthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.", "What do you think about the following?", "# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.", "# Remove the need to convert from inet address -> URI in NameNodeConnector(...).", "Also, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol.", "I've filed HDFS-2986 to address this issue.", "Here's a patch which addresses the issue, using the technique I described above.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME and", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "Otherwise looks good.", "Thanks a lot for the review, Eli.", "Here's an updated patch which addresses your feedback.", "In the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it.", "The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated.", "This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client.", "To fix this, I made the NN make copies of conf objects provided to it in a few places.", "This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.", "bq.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "OK, done.", "bq.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "I revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.", "bq.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME", "Done.", "bq.", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "This is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.", "I also ran the following tests to verify this change, and they all passed:", "TestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "Otherwise looks great.", "Nice find wrt the mutating configuration issue.", "Thanks a lot, Eli.", "Here's an updated patch which adds the wrapper method as you requested.", "+1", "Thanks a lot for the reviews, Eli.", "I've just committed this to the HA branch.", "This patch seems to have made some tests fail (I noticed with TestFileAppend2).", "It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.", "Thanks Todd.", "I've filed HDFS-3033 to address these failures.", "On second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case.", "The conf mutation issue should be fixed in a more general way.", "Here's an updated patch which addresses the issue and removes the change to the NameNode.", "Instead, to avoid the issue with conf mutation, the relevant tests make a copy of the configuration objects and then set the addresses appropriately after the NNs have started, to get the correct ephemeral ports configured.", "Small nits:", "please rename {{getNameNodeInfo}} to {{getNameNodeInfos}} or {{getNameNodeInfoArray}} or something?", "in {{setFederatedConfiguration}} add assert that info.nameserviceId is not null", "Thanks a lot for the quick review, Todd.", "Here's an updated patch which addresses your feedback.", "+1 lgtm (I reviewed only the delta from the previous patch, not the full patch, but should be good to commit given prior reviews on the main body)", "Thanks a lot for the reviews, Todd.", "I've just committed the latest patch to the HA branch.", "Thanks a lot Aaron for working on this issue and moving it to closure.", "I was out station for the last week, so I could not put my efforts on this issue.", "Integrated in Hadoop-Hdfs-HAbranch-build #93 (See [https://builds.apache.org/job/Hadoop-Hdfs-HAbranch-build/93/])", "HDFS-2979.", "Balancer should use logical uri for creating failover proxy with HA enabled.", "Contributed by Aaron T. Myers.", "(Revision 1295473)", "Revert commit of HDFS-2979.", "(Revision 1295435)", "HDFS-2979.", "Balancer should use logical uri for creating failover proxy with HA enabled.", "Contributed by Aaron T. Myers.", "(Revision 1295340)", "Result = UNSTABLE", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295473", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295435", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295340", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java"], "SplitGT": [" Get a URI for each configured nameservice.", "If a nameservice is HA-enabled, then the logical URI of the nameservice is returned."], "issueString": "HA: Balancer should use logical uri for creating failover proxy with HA enabled.\nPresently Balancer uses real URI for creating the failover proxy.\nSince the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI. Presently will work only with default port.\n\njava.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.\n\tat org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)\n\tat org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)\n\tat org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)\n\tat org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401) \nAttached the patch with addressing the issue.\nHey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.\n\nFurthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.\n\nWhat do you think about the following?\n\n# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.\n# Remove the need to convert from inet address -> URI in NameNodeConnector(...).\n\nAlso, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol. I've filed HDFS-2986 to address this issue.\nHere's a patch which addresses the issue, using the technique I described above.\n- The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n- Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n- getNNUris should use HdfsConstants#HDFS_URI_SCHEME and\n- testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nOtherwise looks good.\nThanks a lot for the review, Eli. Here's an updated patch which addresses your feedback.\n\nIn the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it. The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated. This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client. To fix this, I made the NN make copies of conf objects provided to it in a few places. This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.\n\nbq. The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n\nOK, done.\n\nbq. Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n\nI revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.\n\nbq. getNNUris should use HdfsConstants#HDFS_URI_SCHEME\n\nDone.\n\nbq. testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nThis is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.\n\nI also ran the following tests to verify this change, and they all passed:\n\nTestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes\nLet's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys. Otherwise looks great. Nice find wrt the mutating configuration issue.\nThanks a lot, Eli. Here's an updated patch which adds the wrapper method as you requested.\n+1\nThanks a lot for the reviews, Eli. I've just committed this to the HA branch.\nThis patch seems to have made some tests fail (I noticed with TestFileAppend2). It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.\nThanks Todd. I've filed HDFS-3033 to address these failures.\nOn second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case. The conf mutation issue should be fixed in a more general way.\nHere's an updated patch which addresses the issue and removes the change to the NameNode. Instead, to avoid the issue with conf mutation, the relevant tests make a copy of the configuration objects and then set the addresses appropriately after the NNs have started, to get the correct ephemeral ports configured.\nSmall nits:\n- please rename {{getNameNodeInfo}} to {{getNameNodeInfos}} or {{getNameNodeInfoArray}} or something?\n- in {{setFederatedConfiguration}} add assert that info.nameserviceId is not null\n\nThanks a lot for the quick review, Todd. Here's an updated patch which addresses your feedback.\n+1 lgtm (I reviewed only the delta from the previous patch, not the full patch, but should be good to commit given prior reviews on the main body)\nThanks a lot for the reviews, Todd. I've just committed the latest patch to the HA branch.\nThanks a lot Aaron for working on this issue and moving it to closure. I was out station for the last week, so I could not put my efforts on this issue.\nIntegrated in Hadoop-Hdfs-HAbranch-build #93 (See [https://builds.apache.org/job/Hadoop-Hdfs-HAbranch-build/93/])\n    HDFS-2979. Balancer should use logical uri for creating failover proxy with HA enabled. Contributed by Aaron T. Myers. (Revision 1295473)\nRevert commit of HDFS-2979. (Revision 1295435)\nHDFS-2979. Balancer should use logical uri for creating failover proxy with HA enabled. Contributed by Aaron T. Myers. (Revision 1295340)\n\n     Result = UNSTABLE\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295473\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295435\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295340\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\n", "issueSearchSentences": ["Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys."], "issueSearchScores": [0.7654621601104736, 0.7654621601104736, 0.6461271047592163, 0.57203209400177, 0.57203209400177]}
{"aId": 20, "code": "@InterfaceAudience.LimitedPrivate( { \"HDFS\", \"MapReduce\" })\n  public List<Token<?>> getDelegationTokens(\n      Path p, String renewer) throws IOException {\n    Set<AbstractFileSystem> afsSet = resolveAbstractFileSystems(p);\n    List<Token<?>> tokenList = \n        new ArrayList<Token<?>>();\n    for (AbstractFileSystem afs : afsSet) {\n      List<Token<?>> afsTokens = afs.getDelegationTokens(renewer);\n      tokenList.addAll(afsTokens);\n    }\n    return tokenList;\n  }", "comment": " Get delegation tokens for the file systems accessed for a given path.", "issueId": "HADOOP-6994", "issueStringList": ["Api to get delegation token in AbstractFileSystem", "APIs to get delegation tokens is required in AbstractFileSystem.", "AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed.", "A path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "bq.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "I also have this concern, particularly in regard to HADOOP-6988.", "New patch addressing the comment.", "The InterfaceAudience for the new APIs are annotated to be LimitedPrivate.", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "This patch is based against the latest trunk.", "New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090039.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console", "This message is automatically generated.", "# AbstractFileSystem.java", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem.", "Normally a file system returns a single delegation token.", "A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.", "\"This comment applies to FileSystem also.", "# When a file system returns multiple delegation tokens, how does renew and cancel work?", "Woudl the file system know where to renew/cancel the token?", "2.", "OK", "3.", "For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.", "+1 for the patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console", "This message is automatically generated.", "The latest patch fixes FilterFileSystem and FilterFS.", "The renew and cancel delegation token APIs are removed from the patch.", "These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself.", "Implementation in SecurityUtil will be taken up in a separate jira.", "+1 for the change.", "Updated patch includes a javadoc for the new unit test.", "Rest is all identical with previous patch.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console", "This message is automatically generated.", "I have committed this.", "Integrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])", "HADOOP-6994.", "Api to get delegation token in AbstractFileSystem.", "Contributed by jitendra."], "SplitGT": [" Get delegation tokens for the file systems accessed for a given path."], "issueString": "Api to get delegation token in AbstractFileSystem\nAPIs to get delegation tokens is required in AbstractFileSystem. AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed. \nA path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path. \nThe new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.\nbq. The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent. \nI also have this concern, particularly in regard to HADOOP-6988. \nNew patch addressing the comment.\n\nThe InterfaceAudience for the new APIs are annotated to be LimitedPrivate.\nThe getDelegationToken API in FileSystem is also changed to LimitedPrivate.\n\nThis patch is based against the latest trunk.\nNew patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090039.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console\n\nThis message is automatically generated.\n# AbstractFileSystem.java\n# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\". It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem. Normally a file system returns a single delegation token. A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.\"This comment applies to FileSystem also.\n# When a file system returns multiple delegation tokens, how does renew and cancel work? Woudl the file system know where to renew/cancel the token?\n\n2. OK \n3. For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.\n+1 for the patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console\n\nThis message is automatically generated.\nThe latest patch fixes FilterFileSystem and FilterFS. The renew and cancel delegation token APIs are removed from the patch. These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself. Implementation in SecurityUtil will be taken up in a separate jira. \n+1 for the change.\nUpdated patch includes a javadoc for the new unit test. Rest is all identical with previous patch.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console\n\nThis message is automatically generated.\nI have committed this.\nIntegrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])\n    HADOOP-6994. Api to get delegation token in AbstractFileSystem. Contributed by jitendra.\n\n", "issueSearchSentences": ["New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent."], "issueSearchScores": [0.7264823913574219, 0.6566969752311707, 0.6510539054870605, 0.6245507001876831, 0.5823221802711487]}
{"aId": 22, "code": "@InterfaceAudience.LimitedPrivate( { \"HDFS\", \"MapReduce\" })\n  public List<Token<?>> getDelegationTokens(String renewer) throws IOException {\n    return null;\n  }", "comment": " Get one or more delegation tokens associated with the filesystem. Normally a file system returns a single delegation token. A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.", "issueId": "HADOOP-6994", "issueStringList": ["Api to get delegation token in AbstractFileSystem", "APIs to get delegation tokens is required in AbstractFileSystem.", "AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed.", "A path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "bq.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "I also have this concern, particularly in regard to HADOOP-6988.", "New patch addressing the comment.", "The InterfaceAudience for the new APIs are annotated to be LimitedPrivate.", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "This patch is based against the latest trunk.", "New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090039.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console", "This message is automatically generated.", "# AbstractFileSystem.java", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem.", "Normally a file system returns a single delegation token.", "A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.", "\"This comment applies to FileSystem also.", "# When a file system returns multiple delegation tokens, how does renew and cancel work?", "Woudl the file system know where to renew/cancel the token?", "2.", "OK", "3.", "For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.", "+1 for the patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console", "This message is automatically generated.", "The latest patch fixes FilterFileSystem and FilterFS.", "The renew and cancel delegation token APIs are removed from the patch.", "These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself.", "Implementation in SecurityUtil will be taken up in a separate jira.", "+1 for the change.", "Updated patch includes a javadoc for the new unit test.", "Rest is all identical with previous patch.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console", "This message is automatically generated.", "I have committed this.", "Integrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])", "HADOOP-6994.", "Api to get delegation token in AbstractFileSystem.", "Contributed by jitendra."], "SplitGT": [" Get one or more delegation tokens associated with the filesystem.", "Normally a file system returns a single delegation token.", "A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages."], "issueString": "Api to get delegation token in AbstractFileSystem\nAPIs to get delegation tokens is required in AbstractFileSystem. AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed. \nA path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path. \nThe new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.\nbq. The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent. \nI also have this concern, particularly in regard to HADOOP-6988. \nNew patch addressing the comment.\n\nThe InterfaceAudience for the new APIs are annotated to be LimitedPrivate.\nThe getDelegationToken API in FileSystem is also changed to LimitedPrivate.\n\nThis patch is based against the latest trunk.\nNew patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090039.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console\n\nThis message is automatically generated.\n# AbstractFileSystem.java\n# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\". It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem. Normally a file system returns a single delegation token. A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.\"This comment applies to FileSystem also.\n# When a file system returns multiple delegation tokens, how does renew and cancel work? Woudl the file system know where to renew/cancel the token?\n\n2. OK \n3. For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.\n+1 for the patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console\n\nThis message is automatically generated.\nThe latest patch fixes FilterFileSystem and FilterFS. The renew and cancel delegation token APIs are removed from the patch. These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself. Implementation in SecurityUtil will be taken up in a separate jira. \n+1 for the change.\nUpdated patch includes a javadoc for the new unit test. Rest is all identical with previous patch.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console\n\nThis message is automatically generated.\nI have committed this.\nIntegrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])\n    HADOOP-6994. Api to get delegation token in AbstractFileSystem. Contributed by jitendra.\n\n", "issueSearchSentences": ["New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "The renew and cancel delegation token APIs are removed from the patch.", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem."], "issueSearchScores": [0.6543129682540894, 0.6527262926101685, 0.6516562700271606, 0.6012526750564575, 0.5929394364356995]}
{"aId": 23, "code": "public List<ECSchema> loadSchema(Configuration conf) {\n    File confFile = getSchemaFile(conf);\n    if (confFile == null) {\n      LOG.warn(\"Not found any predefined EC schema file\");\n      return Collections.emptyList();\n    }\n\n    try {\n      return loadSchema(confFile);\n    } catch (ParserConfigurationException e) {\n      throw new RuntimeException(\"Failed to load schema file: \" + confFile);\n    } catch (IOException e) {\n      throw new RuntimeException(\"Failed to load schema file: \" + confFile);\n    } catch (SAXException e) {\n      throw new RuntimeException(\"Failed to load schema file: \" + confFile);\n    }\n  }", "comment": " Load predefined ec schemas from configuration file.", "issueId": "HADOOP-11664", "issueStringList": ["Loading predefined EC schemas from configuration", "System administrator can configure multiple EC codecs in hdfs-site.xml file, and codec instances or schemas in a new configuration file named ec-schema.xml in the conf folder.", "A codec can be referenced by its instance or schema using the codec name, and a schema can be utilized and specified by the schema name for a folder or EC ZONE to enforce EC.", "Once a schema is used to define an EC ZONE, then its associated parameter values will be stored as xattributes and respected thereafter.", "Configuring multiple erasure codecs (*JerasureRS*, *ISARS*, *ISALRC*) in *hdfs-site.xml* file:", "{code}", "<property>", "<name> hadoop.hdfs.ec.erasurecodec.codec.JerasureRS</name>", "<value> org.apache.hadoop.hdfs.ec.codec.JerasureRS </value>", "</property>", "<property>", "<name> hadoop.hdfs.ec.erasurecodec.codec.ISARS</name>", "<value> org.apache.hadoop.hdfs.ec.codec.IsaRS </value>", "</property>", "<property>", "<name> hadoop.hdfs.ec.erasurecodec.codec.ISALRC </name>", "<value> org.apache.hadoop.hdfs.ec.codec.IsaLRC </value>", "</property>", "{code}", "And above configured codec can be referenced in the schemas defined in new configuration file *HADOOP-CONF/ec-schema.xml*, like follows:", "{code}", "<schemas>", "<schema name=\"RSk6m3\">", "<k>6</k>", "<m>3</m>", "<codec>JerasureRS</codec>", "</schema>", "<schema name=\"LRCk6l2r2\">", "<k>6</k>", "<l>3</l>", "<r>2</r>", "<codec>ISALRC</codec>", "</schema>", "</schemas>", "{code}", "In the above defined schemas, the options/parameters like *k*, *m* etc.", "are subject to be changed and they're to be interpreted by the corresponding codec.", "[~zhz] has good idea to store schema parameters as xattributes and specify schema parameters when define an EC ZONE via predefined schema list (discussed in HDFS-7349).", "So it's good to adapt this issue and the work here for implementing predefining EC schemas.", "Thanks Kai.", "It's an interesting and non-trivial question what should be configured on command line and what should go into config XML files.", "We can refer to [encryption design | http://www.cloudera.com/content/cloudera/en/documentation/core/v5-2-x/topics/cdh_sg_hdfs_encryption.html].", "Thanks Zhe.", "Will look at the design and think about it.", "HDFS-7839 will implement the basic XAttr structure for EC policies.", "It's great to have HDFS-7839, so this one will focus on loading predefined schemas from configuration file.", "Opened another HDFS-7859 to handle the persisting of EC schemas in NameNode.", "To ease the sync and collaboration among all of these related issues, HADOOP-11643 is opened to provide the necessary EC schema API.", "Uploaded a patch to load predefined ec schemas from an XML configuration file by SchemaLoader that's to be used by follow on issues.", "Refined the patch based on latest branch.", "Ready for review.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12706464/HADOOP-11664-v2.patch", "against trunk revision 82eda77.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5985//console", "This message is automatically generated.", "How about including a predifined schema xml also in this patch.", "?", "Great idea Vinay!", "\u2014", "Sent from Mailbox", "On Mon, Mar 23, 2015 at 10:44 PM, Vinayakumar B (JIRA) <jira@apache.org>", "Let's have it.", "Will update the patch including the mentioned XML file.", "Updated the patch including an ecschema-def.xml file.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12707159/HADOOP-11664-v3.patch", "against trunk revision 80278a5.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5993//console", "This message is automatically generated.", "The patch is ready for review.", "Cancelled the patch to get rid of the QA building.", "Thanks Kai for the patch!", "The main logic looks good.", "Just 1 minor comment:", "Is it necessary to configure the name of the xml file?", "I suggest we just hard code the file name to simplify code.", "{code}", "+  public static final String IO_ERASURECODE_SCHEMA_FILE_KEY =", "+      \"hadoop.io.erasurecode.", "\";", "+  public static final String IO_ERASURECODE_SCHEMA_FILE_DEFAULT =", "+      \"ecschema-def.xml\";", "{code}", "Hi Zhe, thanks for your review.", "I agree with you it doesn't have to be configured.", "It just follows conventions in Hadoop because some one would prefer to find those configurable items from file like core-default.xml to see what needs to be prepared.", "Hard-coded values sometimes are just not easy to be out.", "If you agree we can keep the configurable item, I will have to change the property key !", "Thanks for the pasting.", "I agree.", "+1 on the patch; I committed it.", "bq.If you agree we can keep the configurable item, I will have to change the property key", "I just did it in the branch, fixing the property key.", "Thanks [~zhz] for the help !", "Thanks Kai for making the additional change.", "I merged it in the weekly rebase so we have a cleaner git history."], "SplitGT": [" Load predefined ec schemas from configuration file."], "issueString": "Loading predefined EC schemas from configuration\nSystem administrator can configure multiple EC codecs in hdfs-site.xml file, and codec instances or schemas in a new configuration file named ec-schema.xml in the conf folder. A codec can be referenced by its instance or schema using the codec name, and a schema can be utilized and specified by the schema name for a folder or EC ZONE to enforce EC. Once a schema is used to define an EC ZONE, then its associated parameter values will be stored as xattributes and respected thereafter.\nConfiguring multiple erasure codecs (*JerasureRS*, *ISARS*, *ISALRC*) in *hdfs-site.xml* file:\n{code}\n  <property>\n    <name> hadoop.hdfs.ec.erasurecodec.codec.JerasureRS</name>\n    <value> org.apache.hadoop.hdfs.ec.codec.JerasureRS </value>\n  </property>\n  <property>\n    <name> hadoop.hdfs.ec.erasurecodec.codec.ISARS</name>\n    <value> org.apache.hadoop.hdfs.ec.codec.IsaRS </value>\n  </property> \n  <property>\n    <name> hadoop.hdfs.ec.erasurecodec.codec.ISALRC </name>\n    <value> org.apache.hadoop.hdfs.ec.codec.IsaLRC </value>\n  </property> \n{code}\n\nAnd above configured codec can be referenced in the schemas defined in new configuration file *HADOOP-CONF/ec-schema.xml*, like follows:\n{code}\n<schemas>\n  <schema name=\"RSk6m3\">\n      <k>6</k>\n     <m>3</m>\n     <codec>JerasureRS</codec>\n  </schema>\n  <schema name=\"LRCk6l2r2\">\n      <k>6</k>\n     <l>3</l>\n     <r>2</r>\n     <codec>ISALRC</codec>\n  </schema>\n</schemas>\n{code}\nIn the above defined schemas, the options/parameters like *k*, *m* etc. are subject to be changed and they're to be interpreted by the corresponding codec.\n[~zhz] has good idea to store schema parameters as xattributes and specify schema parameters when define an EC ZONE via predefined schema list (discussed in HDFS-7349). So it's good to adapt this issue and the work here for implementing predefining EC schemas.\nThanks Kai.\n\nIt's an interesting and non-trivial question what should be configured on command line and what should go into config XML files. We can refer to [encryption design | http://www.cloudera.com/content/cloudera/en/documentation/core/v5-2-x/topics/cdh_sg_hdfs_encryption.html].\nThanks Zhe. Will look at the design and think about it.\nHDFS-7839 will implement the basic XAttr structure for EC policies.\nIt's great to have HDFS-7839, so this one will focus on loading predefined schemas from configuration file.\nOpened another HDFS-7859 to handle the persisting of EC schemas in NameNode.\nTo ease the sync and collaboration among all of these related issues, HADOOP-11643 is opened to provide the necessary EC schema API.\nUploaded a patch to load predefined ec schemas from an XML configuration file by SchemaLoader that's to be used by follow on issues.\nRefined the patch based on latest branch. Ready for review.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12706464/HADOOP-11664-v2.patch\n  against trunk revision 82eda77.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5985//console\n\nThis message is automatically generated.\nHow about including a predifined schema xml also in this patch.?\nGreat idea Vinay!\n\n\n\n\u2014\nSent from Mailbox\n\nOn Mon, Mar 23, 2015 at 10:44 PM, Vinayakumar B (JIRA) <jira@apache.org>\n\n\nLet's have it. Will update the patch including the mentioned XML file.\nUpdated the patch including an ecschema-def.xml file.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12707159/HADOOP-11664-v3.patch\n  against trunk revision 80278a5.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5993//console\n\nThis message is automatically generated.\nThe patch is ready for review. Cancelled the patch to get rid of the QA building.\nThanks Kai for the patch! The main logic looks good. Just 1 minor comment:\n\nIs it necessary to configure the name of the xml file? I suggest we just hard code the file name to simplify code.\n{code}\n+  public static final String IO_ERASURECODE_SCHEMA_FILE_KEY =\n+      \"hadoop.io.erasurecode.\";\n+  public static final String IO_ERASURECODE_SCHEMA_FILE_DEFAULT =\n+      \"ecschema-def.xml\";\n{code}\nHi Zhe, thanks for your review.\nI agree with you it doesn't have to be configured. It just follows conventions in Hadoop because some one would prefer to find those configurable items from file like core-default.xml to see what needs to be prepared. Hard-coded values sometimes are just not easy to be out.\n\nIf you agree we can keep the configurable item, I will have to change the property key ! Thanks for the pasting.\nI agree. +1 on the patch; I committed it.\nbq.If you agree we can keep the configurable item, I will have to change the property key \nI just did it in the branch, fixing the property key.\nThanks [~zhz] for the help !\nThanks Kai for making the additional change. I merged it in the weekly rebase so we have a cleaner git history. \n", "issueSearchSentences": ["It's great to have HDFS-7839, so this one will focus on loading predefined schemas from configuration file.", "Loading predefined EC schemas from configuration", "</schema>", "</schema>", "Uploaded a patch to load predefined ec schemas from an XML configuration file by SchemaLoader that's to be used by follow on issues."], "issueSearchScores": [0.6707421541213989, 0.629399299621582, 0.4941834807395935, 0.4941834807395935, 0.49019038677215576]}
{"aId": 24, "code": "private static void putTimelineEntitiesInJSONFile(String path) {\n    File jsonFile = new File(path);\n    if (!jsonFile.exists()) {\n      System.out.println(\"Error: File [\" + jsonFile.getAbsolutePath()\n          + \"] doesn't exist\");\n      return;\n    }\n    ObjectMapper mapper = new ObjectMapper();\n    YarnJacksonJaxbJsonProvider.configObjectMapper(mapper);\n    TimelineEntities entities = null;\n    try {\n      entities = mapper.readValue(jsonFile, TimelineEntities.class);\n    } catch (Exception e) {\n      System.err.println(\"Error: \" + e.getMessage());\n      e.printStackTrace(System.err);\n      return;\n    }\n    Configuration conf = new YarnConfiguration();\n    TimelineClient client = TimelineClient.createTimelineClient();\n    client.init(conf);\n    client.start();\n    try {\n      if (UserGroupInformation.isSecurityEnabled()\n          && conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, false)) {\n        Token<TimelineDelegationTokenIdentifier> token =\n            client.getDelegationToken(\n                UserGroupInformation.getCurrentUser().getUserName());\n        UserGroupInformation.getCurrentUser().addToken(token);\n      }\n      TimelinePutResponse response = client.putEntities(\n          entities.getEntities().toArray(\n              new TimelineEntity[entities.getEntities().size()]));\n      if (response.getErrors().size() == 0) {\n        System.out.println(\"Timeline data is successfully put\");\n      } else {\n        for (TimelinePutResponse.TimelinePutError error : response.getErrors()) {\n          System.out.println(\"TimelineEntity [\" + error.getEntityType() + \":\" +\n              error.getEntityId() + \"] is not successfully put. Error code: \" +\n              error.getErrorCode());\n        }\n      }\n    } catch (Exception e) {\n      System.err.println(\"Error: \" + e.getMessage());\n      e.printStackTrace(System.err);\n    } finally {\n      client.stop();\n    }\n  }", "comment": " Put timeline data in a JSON file via command line.", "issueId": "YARN-1936", "issueStringList": ["Secured timeline client", "TimelineClient should be able to talk to the timeline server with kerberos authentication or delegation token", "I created a patch:", "1.", "It makes use of the hadoop-auth module and YARN-2049 to talk to the timeline server with either kerberos authentication or delegation token:", "2.", "I creates a main method, which allows users to upload the timeline data in a JSON file from command line.", "3.", "When using YarnClient to submit an application, if the authentication is enabled, YarnClient is going to check whether app submission context has the timeline DT or not.", "If not, it will add the DT to the context, such that when AM uses TimelineClient, it can use the DT for authentication, as it can not use kerberos instead.", "BTW, the patch depends on YARN-2049 for compiling", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12644522/YARN-1936.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:red}-1 javac{color:red}.", "The patch appears to cause the build to fail.", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3742//console", "This message is automatically generated.", "Upload a new patch:", "We shouldn't request the timeline DT when the timeline services is not enabled.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12645288/YARN-1936.2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:red}-1 javac{color:red}.", "The patch appears to cause the build to fail.", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3758//console", "This message is automatically generated.", "Tx for working on this.", "I started reviewing this after applying this on top of YARN-2049.", "Some comments:", "TimelineClient.main():", "Make the event-put as one of the options \"-put\"", "Add delegation token only if timeline-service is enabled.", "Also move this main to TimelineClientImpl", "TimelineClientImpl", "selectToken() can use a TimelineDelegationTokenSelector to find the token?", "Can we add a simple test to validate the addition of the Delegation Token to the client credentials?", "Vinod, thanks for review.", "See my response bellow:", "bq.", "Make the event-put as one of the options \"-put\"", "Good point.", "I make use of CommandLine to do simple CLI.", "bq.", "Add delegation token only if timeline-service is enabled.", "Added the check", "bq.", "Also move this main to TimelineClientImpl", "moved", "bq.", "selectToken() can use a TimelineDelegationTokenSelector to find the token?", "Use selector instead, and do some refactoring required.", "bq.", "Can we add a simple test to validate the addition of the Delegation Token to the client credentials?", "Added a test case", "+1, this looks good.", "Will check this in if Jenkins says okay.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12646445/YARN-1936.3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice:", "org.apache.hadoop.yarn.client.TestRMAdminCLI", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3796//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3796//console", "This message is automatically generated.", "Again, the test failure is not related.", "Also did some validation for the new client patch on my local secured cluster.", "It still works correctly.", "Committed this to trunk and branch-2.", "Thanks Zhijie!", "SUCCESS: Integrated in Hadoop-trunk-Commit #5608 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5608/])", "YARN-1936.", "Added security support for the Timeline Client.", "Contributed by Zhijie Shen.", "(vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597153)", "hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/TimelineClient.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/YarnClientImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestYarnClient.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/timeline/TimelineUtils.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/timeline/security/TimelineDelegationTokenSecretManagerService.java"], "SplitGT": [" Put timeline data in a JSON file via command line."], "issueString": "Secured timeline client\nTimelineClient should be able to talk to the timeline server with kerberos authentication or delegation token\nI created a patch:\n\n1. It makes use of the hadoop-auth module and YARN-2049 to talk to the timeline server with either kerberos authentication or delegation token:\n\n2. I creates a main method, which allows users to upload the timeline data in a JSON file from command line.\n\n3. When using YarnClient to submit an application, if the authentication is enabled, YarnClient is going to check whether app submission context has the timeline DT or not. If not, it will add the DT to the context, such that when AM uses TimelineClient, it can use the DT for authentication, as it can not use kerberos instead.\nBTW, the patch depends on YARN-2049 for compiling\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12644522/YARN-1936.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.\n\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/3742//console\n\nThis message is automatically generated.\nUpload a new patch:\n\nWe shouldn't request the timeline DT when the timeline services is not enabled.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12645288/YARN-1936.2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.\n\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/3758//console\n\nThis message is automatically generated.\nTx for working on this. I started reviewing this after applying this on top of YARN-2049. Some comments:\n\nTimelineClient.main(): \n - Make the event-put as one of the options \"-put\"\n - Add delegation token only if timeline-service is enabled.\n - Also move this main to TimelineClientImpl\n\nTimelineClientImpl\n - selectToken() can use a TimelineDelegationTokenSelector to find the token?\n\nCan we add a simple test to validate the addition of the Delegation Token to the client credentials?\nVinod, thanks for review. See my response bellow:\n\nbq. Make the event-put as one of the options \"-put\"\n\nGood point. I make use of CommandLine to do simple CLI.\n\nbq. Add delegation token only if timeline-service is enabled.\n\nAdded the check\n\nbq. Also move this main to TimelineClientImpl\n\nmoved\n\nbq. selectToken() can use a TimelineDelegationTokenSelector to find the token?\n\nUse selector instead, and do some refactoring required.\n\nbq. Can we add a simple test to validate the addition of the Delegation Token to the client credentials?\n\nAdded a test case\n\n+1, this looks good. Will check this in if Jenkins says okay.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12646445/YARN-1936.3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice:\n\n                  org.apache.hadoop.yarn.client.TestRMAdminCLI\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/3796//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/3796//console\n\nThis message is automatically generated.\nAgain, the test failure is not related.\nAlso did some validation for the new client patch on my local secured cluster. It still works correctly.\nCommitted this to trunk and branch-2. Thanks Zhijie!\nSUCCESS: Integrated in Hadoop-trunk-Commit #5608 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5608/])\nYARN-1936. Added security support for the Timeline Client. Contributed by Zhijie Shen. (vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597153)\n* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/TimelineClient.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/YarnClientImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestYarnClient.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/timeline/TimelineUtils.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/timeline/security/TimelineDelegationTokenSecretManagerService.java\n\n", "issueSearchSentences": ["I creates a main method, which allows users to upload the timeline data in a JSON file from command line.", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/timeline/TimelineUtils.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/TimelineClient.java", "Added security support for the Timeline Client.", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java"], "issueSearchScores": [0.476511687040329, 0.43652966618537903, 0.37481510639190674, 0.3731088638305664, 0.35008877515792847]}
{"aId": 25, "code": "Block getBlockOnStorage(DatanodeStorageInfo storage) {\n    int index = getStorageBlockIndex(storage);\n    if (index < 0) {\n      return null;\n    } else {\n      Block block = new Block(this);\n      block.setBlockId(this.getBlockId() + index);\n      return block;\n    }\n  }", "comment": " Identify the block stored in the given datanode storage.", "issueId": "HDFS-7907", "issueStringList": ["Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode", "With recent changes from HDFS-7411, we need to update DecommissionManager to support striped blocks.", "Initial patch.", "The patch also tracks invalid and corrupt striped blocks.", "Thanks Jing for the patch!", "I haven't finished but have some early comments:", "# We can also have the block ID translation logic on DN.", "When receiving any read or delete commands with a group ID, it should always be able to find the individual block that it stores.", "I haven't read that part of DN code so am not sure how efficient it would be.", "This is like a gatekeeper and might be logically simpler than taking care of the translation in NN whenever talking to a DN.", "# Nits in {{getStorageBlock}}:", "#* A minor error in comment \"Identify the block stored in the given the datanode storage\"", "#* How about {{getBlockOnStorage}}?", "I just finished reviewing the patch.", "It looks good overall.", "I thought more about the block ID translation logic and feel that handling it in DN is not very robust.", "So please ignore that part of comment.", "More detailed review:", "# The signature change of {{blockHasEnoughRacks}}: is it because {{requiredReplication}} has already been calculated before calling the function?", "# Similarly, changing signature of {{removeStoredBlock}} is to avoid calling {{getStoredBlock}} again?", "# It seems {{storage}} is only used to get {{dn}}?", "Should we change the signature?", "{code}", "private void addToExcessReplicate(DatanodeInfo dn, Block block) {", "+  private void addToExcessReplicate(DatanodeStorageInfo storage,", "+      BlockInfo storedBlock) {", "assert namesystem.hasWriteLock();", "LightWeightLinkedSet<Block> excessBlocks = excessReplicateMap.get(dn.getDatanodeUuid());", "+    DatanodeInfo dn = storage.getDatanodeDescriptor();", "{code}", "# The patch seems more about over replication / invalidation than about decomm manager?", "If so maybe we should update the JIRA description?", "None of the above affects the main logic though.", "So my +1 pending some clarification.", "Thanks again for the work Jing!", "Thanks for the review, Zhe!", "Update the patch to address your comments.", "bq.", "The signature change of blockHasEnoughRacks: is it because requiredReplication has already been calculated before calling the function?", "Yes.", "bq.", "Similarly, changing signature of removeStoredBlock is to avoid calling getStoredBlock again?", "Yes.", "Also {{blocksMap.removeNode}} also tries to get the blockInfo from the map.", "I've committed this to the feature branch.", "Thanks Jing!", "I'll post a review on HDFS-8005 soon."], "SplitGT": [" Identify the block stored in the given datanode storage."], "issueString": "Erasure Coding: track invalid, corrupt, and under-recovery striped blocks in NameNode\nWith recent changes from HDFS-7411, we need to update DecommissionManager to support striped blocks.\nInitial patch. The patch also tracks invalid and corrupt striped blocks.\nThanks Jing for the patch! I haven't finished but have some early comments:\n\n# We can also have the block ID translation logic on DN. When receiving any read or delete commands with a group ID, it should always be able to find the individual block that it stores. I haven't read that part of DN code so am not sure how efficient it would be. This is like a gatekeeper and might be logically simpler than taking care of the translation in NN whenever talking to a DN.\n# Nits in {{getStorageBlock}}:\n#* A minor error in comment \"Identify the block stored in the given the datanode storage\"\n#* How about {{getBlockOnStorage}}?\nI just finished reviewing the patch. It looks good overall. I thought more about the block ID translation logic and feel that handling it in DN is not very robust. So please ignore that part of comment. \n\nMore detailed review:\n# The signature change of {{blockHasEnoughRacks}}: is it because {{requiredReplication}} has already been calculated before calling the function?\n# Similarly, changing signature of {{removeStoredBlock}} is to avoid calling {{getStoredBlock}} again?\n# It seems {{storage}} is only used to get {{dn}}? Should we change the signature?\n{code}\n-  private void addToExcessReplicate(DatanodeInfo dn, Block block) {\n+  private void addToExcessReplicate(DatanodeStorageInfo storage,\n+      BlockInfo storedBlock) {\n     assert namesystem.hasWriteLock();\n-    LightWeightLinkedSet<Block> excessBlocks = excessReplicateMap.get(dn.getDatanodeUuid());\n+    DatanodeInfo dn = storage.getDatanodeDescriptor();\n{code}\n# The patch seems more about over replication / invalidation than about decomm manager? If so maybe we should update the JIRA description?\nNone of the above affects the main logic though. So my +1 pending some clarification. Thanks again for the work Jing!\nThanks for the review, Zhe! Update the patch to address your comments.\n\nbq. The signature change of blockHasEnoughRacks: is it because requiredReplication has already been calculated before calling the function?\n\nYes.\n\nbq. Similarly, changing signature of removeStoredBlock is to avoid calling getStoredBlock again?\n\nYes. Also {{blocksMap.removeNode}} also tries to get the blockInfo from the map.\nI've committed this to the feature branch.\nThanks Jing! I'll post a review on HDFS-8005 soon.\n", "issueSearchSentences": ["+      BlockInfo storedBlock) {", "#* A minor error in comment \"Identify the block stored in the given the datanode storage\"", "#* How about {{getBlockOnStorage}}?", "# Nits in {{getStorageBlock}}:", "+    DatanodeInfo dn = storage.getDatanodeDescriptor();"], "issueSearchScores": [0.6967921257019043, 0.6415083408355713, 0.6402157545089722, 0.613547146320343, 0.5694771409034729]}
{"aId": 26, "code": "public Map<String, Metadata> getKeysMetadata() throws IOException {\n    Map<String, Metadata> keysMetadata = new LinkedHashMap<String, Metadata>();\n    for (String key : getKeys()) {\n      Metadata meta = getMetadata(key);\n      if (meta != null) {\n        keysMetadata.put(key, meta);\n      }\n    }\n    return keysMetadata;\n  }", "comment": " Get the key metadata for all keys.", "issueId": "HADOOP-10430", "issueStringList": ["KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys", "Being able to attach an optional description (and show it when displaying metadata) will enable giving some context on the keys.", "Adds an optional label to the Metadata for keys.", "Adds a {{public Map<String, Metadata> getKeysMetadata()}} method to the {{KeyProvider}}.", "Patch is ready but it depends on HADOOP-10429, HADOOP-10428, HADOOP-10427.", "Once they are in I'll set this JIRA as patch-avail.", "The patch looks good to me.", "+1 pending Jenkins.", "Looks good to me as well.", "I would like to see a bit more of a usecase description or example here for clarity.", "Use case example:", "For example, you want to create a key for encrypting files under a directory in HFDS.", "The label would be directory URI, thus giving a context of what/where the key is used for.", "[~tucu00]] - what do you think about making this a comma delimited or hashtag (#thatshowweroll) based set of labels that could be used for any number of purposes?", "I can think of a few purposes relative to classification, etc:", "Governance policy may require varying rules for rolling keys based on data classification level for instance.", "Additional authorization checks could also be made based on group membership relative to the classification of the key, etc.", "This would be similar to the labels used in accumulo for cell level security.", "I would prefer to leave that to the apps using the label to decide how they encode multiple information, else we will have to deal with escaping, how to escape will depend on what are valid values, and different apps may have different rules to define valid values.", "Thoughts?", "Hmmm...", "I don't think that I agree - though I can potentially see this as a separate piece of metadata to be introduced in a follow up jira.", "This way it doesn't interfere with the simple descriptive goal of this label.", "I do think that we should prescribe a simple delimiter for it though - maybe just space - and the values should be arbitrary.", "This allows the apps and tools to uniformly understand how to parse them and potentially have app specific semantics for evaluating them.", "What do you think about separate a jira to introduce a tags metadata field?", "Yes, a separate JIRA for tags sounds good.", "My idea of the label is to be a description.", "That makes sense.", "I'll file one.", "I wonder whether we should consider calling this \"description\" rather than label then.", "Labels would be a better name for a collection of attributes than tags and would align with other projects like accumulo.", "sure, i'll update the patch replacing label with description", "patch that renames label to description.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12639460/HADOOP-10430.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.crypto.key.TestKeyShell", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3774//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3774//console", "This message is automatically generated.", "Please replace getKeysMetadata with getDescription(String key).", "fixing testcase failure and changing a bit the testcase logic, to use a tempdir under target and to capture STDOUT/STDERR (setting it back to original streams afterwards).", "[~owen.omalley], I assume you refer in the KeyShell.java, in the ListCommand subclass, execute() method.", "If so, using getKeysMetadata() is a single KeyProvider call to fetch the metadata for all keys while the old pattern would require 1+N calls to the KeyProvider with N being the number of keys: one to get all keys, and N to get the metadata for all keys.", "If the KeyProvider is local this is not a big issue, but if the key provider is a client/server implementation, this means 1+N network roundtrips instead of just 1.", "Because of this I think the current patch is correct.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12639475/HADOOP-10430.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.crypto.key.TestKeyShell", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3776//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3776//console", "This message is automatically generated.", "Fixing KeyShell testcase, had similar issue as HADOOP-10488, plus the captured stdout/stderr were not being reset before every test.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12639539/HADOOP-10430.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3778//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3778//console", "This message is automatically generated.", "LGTM, +1.", "Owen, is tucu's rationale for getKeysMetadata okay by you?", "[~owen.omalley], do you have any further concerns?", "Else I'll commit latest patch tomorrow.", "committed to trunk.", "SUCCESS: Integrated in Hadoop-trunk-Commit #5504 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5504/])", "HADOOP-10430.", "KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys.", "(tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1586730)", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/UserProvider.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java"], "SplitGT": [" Get the key metadata for all keys."], "issueString": "KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys\nBeing able to attach an optional description (and show it when displaying metadata) will enable giving some context on the keys.\nAdds an optional label to the Metadata for keys.\nAdds a {{public Map<String, Metadata> getKeysMetadata()}} method to the {{KeyProvider}}.\n\nPatch is ready but it depends on HADOOP-10429, HADOOP-10428, HADOOP-10427. Once they are in I'll set this JIRA as patch-avail.\nThe patch looks good to me. +1 pending Jenkins.\nLooks good to me as well.\nI would like to see a bit more of a usecase description or example here for clarity.\nUse case example:\n\nFor example, you want to create a key for encrypting files under a directory in HFDS. The label would be directory URI, thus giving a context of what/where the key is used for.\n[~tucu00]] - what do you think about making this a comma delimited or hashtag (#thatshowweroll) based set of labels that could be used for any number of purposes?\n\nI can think of a few purposes relative to classification, etc:\n\nGovernance policy may require varying rules for rolling keys based on data classification level for instance.\nAdditional authorization checks could also be made based on group membership relative to the classification of the key, etc.\n\nThis would be similar to the labels used in accumulo for cell level security.\nI would prefer to leave that to the apps using the label to decide how they encode multiple information, else we will have to deal with escaping, how to escape will depend on what are valid values, and different apps may have different rules to define valid values. \n\nThoughts?\nHmmm... I don't think that I agree - though I can potentially see this as a separate piece of metadata to be introduced in a follow up jira. This way it doesn't interfere with the simple descriptive goal of this label.\n\nI do think that we should prescribe a simple delimiter for it though - maybe just space - and the values should be arbitrary.\nThis allows the apps and tools to uniformly understand how to parse them and potentially have app specific semantics for evaluating them.\n\nWhat do you think about separate a jira to introduce a tags metadata field?\n\nYes, a separate JIRA for tags sounds good. My idea of the label is to be a description.\nThat makes sense.\nI'll file one.\n\nI wonder whether we should consider calling this \"description\" rather than label then.\nLabels would be a better name for a collection of attributes than tags and would align with other projects like accumulo.\n\nsure, i'll update the patch replacing label with description\npatch that renames label to description.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12639460/HADOOP-10430.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.crypto.key.TestKeyShell\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3774//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3774//console\n\nThis message is automatically generated.\nPlease replace getKeysMetadata with getDescription(String key).\nfixing testcase failure and changing a bit the testcase logic, to use a tempdir under target and to capture STDOUT/STDERR (setting it back to original streams afterwards).\n[~owen.omalley], I assume you refer in the KeyShell.java, in the ListCommand subclass, execute() method. If so, using getKeysMetadata() is a single KeyProvider call to fetch the metadata for all keys while the old pattern would require 1+N calls to the KeyProvider with N being the number of keys: one to get all keys, and N to get the metadata for all keys. If the KeyProvider is local this is not a big issue, but if the key provider is a client/server implementation, this means 1+N network roundtrips instead of just 1. Because of this I think the current patch is correct.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12639475/HADOOP-10430.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.crypto.key.TestKeyShell\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3776//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3776//console\n\nThis message is automatically generated.\nFixing KeyShell testcase, had similar issue as HADOOP-10488, plus the captured stdout/stderr were not being reset before every test.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12639539/HADOOP-10430.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3778//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3778//console\n\nThis message is automatically generated.\nLGTM, +1. Owen, is tucu's rationale for getKeysMetadata okay by you?\n[~owen.omalley], do you have any further concerns? Else I'll commit latest patch tomorrow.\ncommitted to trunk.\nSUCCESS: Integrated in Hadoop-trunk-Commit #5504 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5504/])\nHADOOP-10430. KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys. (tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1586730)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/UserProvider.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n\n", "issueSearchSentences": ["If so, using getKeysMetadata() is a single KeyProvider call to fetch the metadata for all keys while the old pattern would require 1+N calls to the KeyProvider with N being the number of keys: one to get all keys, and N to get the metadata for all keys.", "KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys.", "KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys", "Adds a {{public Map<String, Metadata> getKeysMetadata()}} method to the {{KeyProvider}}.", "Please replace getKeysMetadata with getDescription(String key)."], "issueSearchScores": [0.6872507333755493, 0.6605796217918396, 0.6479816436767578, 0.5995909571647644, 0.5798332691192627]}
{"aId": 27, "code": "public static Collection<URI> getNsServiceRpcUris(Configuration conf) {\n    return getNameServiceUris(conf,\n        DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);\n  }", "comment": " Get a URI for each configured nameservice. If a nameservice is HA-enabled, then the logical URI of the nameservice is returned.", "issueId": "HDFS-2979", "issueStringList": ["HA: Balancer should use logical uri for creating failover proxy with HA enabled.", "Presently Balancer uses real URI for creating the failover proxy.", "Since the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI.", "Presently will work only with default port.", "java.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.", "at org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)", "at org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)", "at org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)", "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401)", "Attached the patch with addressing the issue.", "Hey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.", "Furthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.", "What do you think about the following?", "# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.", "# Remove the need to convert from inet address -> URI in NameNodeConnector(...).", "Also, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol.", "I've filed HDFS-2986 to address this issue.", "Here's a patch which addresses the issue, using the technique I described above.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME and", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "Otherwise looks good.", "Thanks a lot for the review, Eli.", "Here's an updated patch which addresses your feedback.", "In the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it.", "The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated.", "This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client.", "To fix this, I made the NN make copies of conf objects provided to it in a few places.", "This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.", "bq.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "OK, done.", "bq.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "I revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.", "bq.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME", "Done.", "bq.", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "This is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.", "I also ran the following tests to verify this change, and they all passed:", "TestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "Otherwise looks great.", "Nice find wrt the mutating configuration issue.", "Thanks a lot, Eli.", "Here's an updated patch which adds the wrapper method as you requested.", "+1", "Thanks a lot for the reviews, Eli.", "I've just committed this to the HA branch.", "This patch seems to have made some tests fail (I noticed with TestFileAppend2).", "It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.", "Thanks Todd.", "I've filed HDFS-3033 to address these failures.", "On second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case.", "The conf mutation issue should be fixed in a more general way."], "SplitGT": [" Get a URI for each configured nameservice.", "If a nameservice is HA-enabled, then the logical URI of the nameservice is returned."], "issueString": "HA: Balancer should use logical uri for creating failover proxy with HA enabled.\nPresently Balancer uses real URI for creating the failover proxy.\nSince the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI. Presently will work only with default port.\n\njava.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.\n\tat org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)\n\tat org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)\n\tat org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)\n\tat org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401) \nAttached the patch with addressing the issue.\nHey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.\n\nFurthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.\n\nWhat do you think about the following?\n\n# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.\n# Remove the need to convert from inet address -> URI in NameNodeConnector(...).\n\nAlso, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol. I've filed HDFS-2986 to address this issue.\nHere's a patch which addresses the issue, using the technique I described above.\n- The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n- Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n- getNNUris should use HdfsConstants#HDFS_URI_SCHEME and\n- testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nOtherwise looks good.\nThanks a lot for the review, Eli. Here's an updated patch which addresses your feedback.\n\nIn the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it. The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated. This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client. To fix this, I made the NN make copies of conf objects provided to it in a few places. This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.\n\nbq. The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n\nOK, done.\n\nbq. Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n\nI revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.\n\nbq. getNNUris should use HdfsConstants#HDFS_URI_SCHEME\n\nDone.\n\nbq. testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nThis is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.\n\nI also ran the following tests to verify this change, and they all passed:\n\nTestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes\nLet's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys. Otherwise looks great. Nice find wrt the mutating configuration issue.\nThanks a lot, Eli. Here's an updated patch which adds the wrapper method as you requested.\n+1\nThanks a lot for the reviews, Eli. I've just committed this to the HA branch.\nThis patch seems to have made some tests fail (I noticed with TestFileAppend2). It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.\nThanks Todd. I've filed HDFS-3033 to address these failures.\nOn second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case. The conf mutation issue should be fixed in a more general way.\n", "issueSearchSentences": ["Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys."], "issueSearchScores": [0.6856510639190674, 0.6856510639190674, 0.6238977909088135, 0.5397199392318726, 0.5397199392318726]}
{"aId": 28, "code": "@VisibleForTesting\n  static DNSToSwitchMapping getDnsToSwitchMapping(){\n    return dnsToSwitchMapping;\n  }", "comment": " Only used by tests", "issueId": "MAPREDUCE-4105", "issueStringList": ["Yarn RackResolver ignores rack configurations", "Incorrect mappings because the Yarn RackResolver ignores rack configurations.", "This can be verified by inspecting the resource manager web ui that lists all the nodes, all of them show up with /default-rack regardless of the output from the script specified using net.topology.script.file.name configuration property.", "Attaching a patch.", "Basically the problem is that RackResolver.java when initialized, it doesn't set the conf for the created dnsToSwitchMapping object.", "So the ScriptBasedMapping doesn't find the scriptName.", "I have also added a a testcase to the patch and also manually verified that the web ui is now showing the correct mappings.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12521446/MAPREDUCE-4105.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 11 new or modified tests.", "1 patch.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2150//console", "This message is automatically generated.", "Is the bug that setConf() isn't being called on DNSToSwitchMapping?", "The new test doesn't actually test that case.", "(The test could go in TestRackResolver - it doesn't need a new test class - and could pass in a mock DNSToSwitchMapping that checks that its setConf() is called.)", "Rather than adding a new method to the DNSToSwitchMapping interface, can you use ReflectionUtils.newInstance?", "This will take care of calling setConf() if the DNSToSwitchMapping implements Configurable.", "Thanks Tom for reviewing this!", "The test checks that setConf() is called by inspecting the script filename is correct (which is passed through the conf).", "So, this way we check that the conf is passed and also that the script name is correct in the same time.", "The reason for adding a new test class is that RackResolver.init() is a static method, and it checks if it was called before and returns.", "So when I added a new test method to the existing TestRackResolver what happened is that the NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY class get set by the test method that initializes the RackResolver first.", "What we want is a different initialization using ScriptBasedMapping.class.", "DNSToSwitchMapping is not Configurable.", "> The test checks that setConf() is called by inspecting the script filename is correct (which is passed through the conf).", "I see.", "RackResolver.getDnsToSwitchMapping() is a new method that is just used by the test.", "Generally it's not a good idea to add things that are only used by tests, but maybe a getter is harmless.", "You could make it package private though and add a comment saying it is for tests, or add @VisibleForTesting.", "> The reason for adding a new test class is that RackResolver.init() is a static method, and it checks if it was called before and returns.", "The fact that RackResolver has static state is a deeper problem, which putting the test into another class doesn't solve.", "What if TestRackResolver runs before TestRackResolverScriptBasedMapping?", "Can we make RackResolver non-static?", "> DNSToSwitchMapping is not Configurable.", "No, but ScriptBasedMapping is.", "You don't need to change the DNSToSwitchMapping interface.", "Thanks Tom!", "I have updated the patch per your comments.", "Regarding the static state of the RackResolver, I can see that the main hadoop-project/pom.xml has <forkMode>always</forkMode> as part of surefire configuration, which means that each test class will have separate jvm.", "So this shouldn't be an issue, what so you think?", "I think having such stateful RackResolver might be advantageous in prevent changing the mappings, etc within the same jvm, so we can leave it as long as tests are fine.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12521770/MAPREDUCE-4105_rev2.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified test files.", "1 patch.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2169//console", "This message is automatically generated.", "Running each test in a separate JVM solves the problem, so it's not an issue, but in general I'd rather avoid the use of static state in a class like this.", "The latest patch still has a change to DNSToSwitchMapping, which I think is unnecessary.", "I see your point and agree that  ScriptBasedMapping is configurable because it has The AbstractDNSToSwitchMapping as a superclass which implements both DNSToSwitchMapping and Configurable interfaces.", "But the RackResolver uses the DNSToSwitchMapping interface to create the new DNSToSwitchMapping:", "{code}", "Constructor<?", "extends DNSToSwitchMapping> dnsToSwitchMappingConstructor", "= dnsToSwitchMappingClass.getConstructor();", "DNSToSwitchMapping newInstance =", "dnsToSwitchMappingConstructor.newInstance();", "{code}", "This is why I needed to add the setConf() method to the DNSSwitchMapping interface to just keep using the interface instead of tying RackResolver to a specific implementation of this interface.", "Alternatively I can make DNSToSwitchMapping extends Configurable (which will require adding the getConf() method to a couple of places).", "What do you think?", "See my comment above about using ReflectionUtils.newInstance.", "I see, so you want to use ReflectionUtils instead.", "Got it, please see updated patch accordingly.", "Thanks!", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12521782/MAPREDUCE-4105_rev3.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified test files.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService", "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry", "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization", "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2170//testReport/", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2170//console", "This message is automatically generated.", "The test failures reported by jenkins above doesn't seem to be related to this patch.", "Thanks!", "+1 thanks for addressing my comments, Ahmed.", "I've just committed this.", "Thanks Ahmed!", "Integrated in Hadoop-Mapreduce-trunk-Commit #2043 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2043/])", "MAPREDUCE-4105.", "Yarn RackResolver ignores rack configurations.", "Contributed by Ahmed Radwan.", "(Revision 1311520)", "Result = SUCCESS", "tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520", "Files :", "hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java", "Integrated in Hadoop-Common-trunk-Commit #2032 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2032/])", "MAPREDUCE-4105.", "Yarn RackResolver ignores rack configurations.", "Contributed by Ahmed Radwan.", "(Revision 1311520)", "Result = SUCCESS", "tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520", "Files :", "hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2107 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2107/])", "MAPREDUCE-4105.", "Yarn RackResolver ignores rack configurations.", "Contributed by Ahmed Radwan.", "(Revision 1311520)", "Result = SUCCESS", "tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520", "Files :", "hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java"], "SplitGT": [" Only used by tests"], "issueString": "Yarn RackResolver ignores rack configurations\nIncorrect mappings because the Yarn RackResolver ignores rack configurations. This can be verified by inspecting the resource manager web ui that lists all the nodes, all of them show up with /default-rack regardless of the output from the script specified using net.topology.script.file.name configuration property.\nAttaching a patch. Basically the problem is that RackResolver.java when initialized, it doesn't set the conf for the created dnsToSwitchMapping object. So the ScriptBasedMapping doesn't find the scriptName.\n\nI have also added a a testcase to the patch and also manually verified that the web ui is now showing the correct mappings.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12521446/MAPREDUCE-4105.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 11 new or modified tests.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2150//console\n\nThis message is automatically generated.\nIs the bug that setConf() isn't being called on DNSToSwitchMapping? The new test doesn't actually test that case. (The test could go in TestRackResolver - it doesn't need a new test class - and could pass in a mock DNSToSwitchMapping that checks that its setConf() is called.) Rather than adding a new method to the DNSToSwitchMapping interface, can you use ReflectionUtils.newInstance? This will take care of calling setConf() if the DNSToSwitchMapping implements Configurable.\n\nThanks Tom for reviewing this!\n\nThe test checks that setConf() is called by inspecting the script filename is correct (which is passed through the conf). So, this way we check that the conf is passed and also that the script name is correct in the same time.\n\nThe reason for adding a new test class is that RackResolver.init() is a static method, and it checks if it was called before and returns. So when I added a new test method to the existing TestRackResolver what happened is that the NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY class get set by the test method that initializes the RackResolver first. What we want is a different initialization using ScriptBasedMapping.class.\n\nDNSToSwitchMapping is not Configurable.\n> The test checks that setConf() is called by inspecting the script filename is correct (which is passed through the conf).\n\nI see. RackResolver.getDnsToSwitchMapping() is a new method that is just used by the test. Generally it's not a good idea to add things that are only used by tests, but maybe a getter is harmless. You could make it package private though and add a comment saying it is for tests, or add @VisibleForTesting.\n\n> The reason for adding a new test class is that RackResolver.init() is a static method, and it checks if it was called before and returns.\n\nThe fact that RackResolver has static state is a deeper problem, which putting the test into another class doesn't solve. What if TestRackResolver runs before TestRackResolverScriptBasedMapping? Can we make RackResolver non-static?\n\n> DNSToSwitchMapping is not Configurable.\n\nNo, but ScriptBasedMapping is. You don't need to change the DNSToSwitchMapping interface.\n\nThanks Tom!\n\nI have updated the patch per your comments. Regarding the static state of the RackResolver, I can see that the main hadoop-project/pom.xml has <forkMode>always</forkMode> as part of surefire configuration, which means that each test class will have separate jvm. So this shouldn't be an issue, what so you think? I think having such stateful RackResolver might be advantageous in prevent changing the mappings, etc within the same jvm, so we can leave it as long as tests are fine.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12521770/MAPREDUCE-4105_rev2.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified test files.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2169//console\n\nThis message is automatically generated.\nRunning each test in a separate JVM solves the problem, so it's not an issue, but in general I'd rather avoid the use of static state in a class like this.\n\nThe latest patch still has a change to DNSToSwitchMapping, which I think is unnecessary.\n\n\nI see your point and agree that  ScriptBasedMapping is configurable because it has The AbstractDNSToSwitchMapping as a superclass which implements both DNSToSwitchMapping and Configurable interfaces. \n\nBut the RackResolver uses the DNSToSwitchMapping interface to create the new DNSToSwitchMapping:\n\n{code}\nConstructor<? extends DNSToSwitchMapping> dnsToSwitchMappingConstructor\n                       = dnsToSwitchMappingClass.getConstructor();\nDNSToSwitchMapping newInstance =\n    dnsToSwitchMappingConstructor.newInstance();\n{code}\n\nThis is why I needed to add the setConf() method to the DNSSwitchMapping interface to just keep using the interface instead of tying RackResolver to a specific implementation of this interface. Alternatively I can make DNSToSwitchMapping extends Configurable (which will require adding the getConf() method to a couple of places). What do you think?\nSee my comment above about using ReflectionUtils.newInstance.\nI see, so you want to use ReflectionUtils instead. Got it, please see updated patch accordingly. Thanks!\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12521782/MAPREDUCE-4105_rev3.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified test files.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests:\n                  org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService\n                  org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry\n                  org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization\n                  org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2170//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2170//console\n\nThis message is automatically generated.\nThe test failures reported by jenkins above doesn't seem to be related to this patch. Thanks!\n+1 thanks for addressing my comments, Ahmed.\nI've just committed this. Thanks Ahmed!\nIntegrated in Hadoop-Mapreduce-trunk-Commit #2043 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2043/])\n    MAPREDUCE-4105. Yarn RackResolver ignores rack configurations. Contributed by Ahmed Radwan. (Revision 1311520)\n\n     Result = SUCCESS\ntomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520\nFiles : \n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java\n\nIntegrated in Hadoop-Common-trunk-Commit #2032 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2032/])\n    MAPREDUCE-4105. Yarn RackResolver ignores rack configurations. Contributed by Ahmed Radwan. (Revision 1311520)\n\n     Result = SUCCESS\ntomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520\nFiles : \n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java\n\nIntegrated in Hadoop-Hdfs-trunk-Commit #2107 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2107/])\n    MAPREDUCE-4105. Yarn RackResolver ignores rack configurations. Contributed by Ahmed Radwan. (Revision 1311520)\n\n     Result = SUCCESS\ntomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1311520\nFiles : \n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolverScriptBasedMapping.java\n\n", "issueSearchSentences": ["extends DNSToSwitchMapping> dnsToSwitchMappingConstructor", "You don't need to change the DNSToSwitchMapping interface.", "RackResolver.getDnsToSwitchMapping() is a new method that is just used by the test.", "> DNSToSwitchMapping is not Configurable.", "DNSToSwitchMapping is not Configurable."], "issueSearchScores": [0.8703844547271729, 0.8316753506660461, 0.8295999765396118, 0.8259788751602173, 0.8207662105560303]}
{"aId": 29, "code": "synchronized void addBlock(ExtendedBlock block, boolean scanNow) {\n    BlockScanInfo info = blockMap.get(block.getLocalBlock());\n    long lastScanTime = 0;\n    if (info != null) {\n      lastScanTime = info.lastScanTime;\n    }\n    // If the particular block is scanned in last 5 minutes, the  no need to \n    // verify that block again\n    if (scanNow && Time.monotonicNow() - lastScanTime < \n        lastScanTimeDifference) {\n      return;\n    }\n    \n    if ( info != null ) {\n      LOG.warn(\"Adding an already existing block \" + block);\n      delBlockInfo(info);\n    }\n    \n    info = new BlockScanInfo(block.getLocalBlock());    \n    info.lastScanTime = getNewBlockScanTime();\n    if (scanNow) {\n      // Create a new BlockScanInfo object and set the lastScanTime to 0\n      // which will make it the high priority block\n      LOG.info(\"Adding block for immediate verification \" + block);\n      info.nextScanType = ScanType.IMMEDIATE_SCAN;\n    }\n    \n    addBlockInfo(info, true);\n    adjustThrottler();\n  }", "comment": " Adds block to list of blocks", "issueId": "HDFS-7548", "issueStringList": ["Corrupt block reporting delayed until datablock scanner thread detects it", "When there is one datanode holding the block and that block happened to be", "corrupt, namenode would keep on trying to replicate the block repeatedly but it would only report the block as corrupt only when the data block scanner thread of the datanode picks up this bad block.", "Requesting improvement in namenode reporting so that corrupt replica would be reported when there is only 1 replica and the replication of that replica keeps on failing with the checksum error.", "Whenever in write pipeline if the datanode detects any checksum error while transferring the block to target node, that particular block is added to first position in the blockInfoSet with setting the lastScanTime to 0.", "This will make the BlockPoolSliceScanner to pick this block first since that data structure is sorted by lastScanTime.", "In this way, we will scan this corrupt block first and will report it to namenode.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12688062/HDFS-7548.patch", "against trunk revision 389f881.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover", "org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration", "org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9075//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9075//console", "This message is automatically generated.", "The following tests are passing fine on my local setup on  both branch-2 and trunk:", "org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover", "org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA", "There is already a jira filed for org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration test failure :  HDFS-7547", "Requesting all the members to review the patch and comment.", "The patch appears to mostly duplicate existing code just to prevent {{BlockPoolSliceScanner#addBlock}} from updating the {{lastScanTime}}.", "Since this method is only called in one place, I'd suggest adding a \"scanNow\" boolean.", "Update the current caller to pass false, the new try-catch can pass true.", "Regarding the new try-catch, why catch IOE and re-throw if not ChecksumException versus explicitly catching ChecksumException?", "I think we need to handle the java.io.IOException: Input/output error case as well since this is what we'll see if having trouble reading from disk.", "Thanks Daryn for the review.", "Cancelling the current patch to address Daryn's comments.", "Will update the patch shortly.", "@Nathan: What action needs to be taken if java.io.IOException: Input/output error occurs.", "I think we need to prioritize a scan for that block.", "Also, some comments on addBlockToFirstLocation().", "imo, WARN should be INFO.", "If this block has been scanned in the last 5 minutes (or some reasonable time frame), then maybe we shouldn't add it back to the list of blocks to be scanned.", "If all IOExceptions are going to re-prioritize the scan of a block, having a minimum delay between scans would avoid corner cases where a network glitch or badly behaving clients are causing IOExceptions that don't really warrant rescans.", "Attaching a new patch addressing Nathan's and Daryn's comment.", "Please review.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12692629/HDFS-7548-v2.patch", "against trunk revision 780a6bf.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//console", "This message is automatically generated.", "Talked offline with Daryn.", "He uncovered a potential bug.", "So cancelling the patch.", "Attaching  a new patch addressing Daryn's offline comments.", "# In {{BlockPoolSliceScanner#updateBlockInfo}}, you should update the nextScanType else the verification logs will never be able to change the value.", "Probably something like set last to next, update next to normal verification when updating the scanner.", "# In {{BlockSender#sendPacket}}, I think you should only check the block in the else clause.", "If the client socket closed then that doesn't imply a local error.", "# I'd suggest following coding style of adding spaces after \"if\"", "# You should add @VisibleForTesting to {{DataBlockScanner#setLastScanTimeDifference}} or make the delta interval a static and poke it.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12692851/HDFS-7548-v3.patch", "against trunk revision 60cbcff.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//console", "This message is automatically generated.", "Cancelling the patch to address Daryn's comment.", "Attaching a new patch addressing Daryn's comments.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12693402/HDFS-7548-v4.patch", "against trunk revision dd0228b.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//console", "This message is automatically generated.", "If you take care of the findbugs warning, I think it looks ok.", "Cancelling the patch to address findbugs warning", "Attachnig a new patch to address findbugs warning.", "Added synchronized keyword to newly added method.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12693618/HDFS-7548-v5.patch", "against trunk revision 6b17eb9.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.qjournal.TestSecureNNWithQJM", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9293//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9293//console", "This message is automatically generated.", "+1 the patch looks good.", "The test case is failing even without this patch and the patch was not changing anything related to it.", "[~daryn] and [~kihwal]  Thanks for the comments and review.", "Thanks for working on the bug, Rushabh.", "I've committed this to trunk and branch-2.", "FAILURE: Integrated in Hadoop-trunk-Commit #6906 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6906/])", "HDFS-7548.", "Corrupt block reporting delayed until datablock scanner thread detects it.", "Contributed by Rushabh Shah.", "(kihwal: rev c0af72c7f74b6925786e24543cac433b906dd6d3)", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeBlockScanner.java", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java", "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java"], "SplitGT": [" Adds block to list of blocks"], "issueString": "Corrupt block reporting delayed until datablock scanner thread detects it\nWhen there is one datanode holding the block and that block happened to be\ncorrupt, namenode would keep on trying to replicate the block repeatedly but it would only report the block as corrupt only when the data block scanner thread of the datanode picks up this bad block.\nRequesting improvement in namenode reporting so that corrupt replica would be reported when there is only 1 replica and the replication of that replica keeps on failing with the checksum error.\nWhenever in write pipeline if the datanode detects any checksum error while transferring the block to target node, that particular block is added to first position in the blockInfoSet with setting the lastScanTime to 0.\nThis will make the BlockPoolSliceScanner to pick this block first since that data structure is sorted by lastScanTime.\nIn this way, we will scan this corrupt block first and will report it to namenode.\n\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12688062/HDFS-7548.patch\n  against trunk revision 389f881.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\n                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration\n                  org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9075//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9075//console\n\nThis message is automatically generated.\nThe following tests are passing fine on my local setup on  both branch-2 and trunk: \norg.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\norg.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA\n\nThere is already a jira filed for org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration test failure :  HDFS-7547\n\n\n\n\nRequesting all the members to review the patch and comment.\nThe patch appears to mostly duplicate existing code just to prevent {{BlockPoolSliceScanner#addBlock}} from updating the {{lastScanTime}}.  Since this method is only called in one place, I'd suggest adding a \"scanNow\" boolean.  Update the current caller to pass false, the new try-catch can pass true.\n\nRegarding the new try-catch, why catch IOE and re-throw if not ChecksumException versus explicitly catching ChecksumException?\nI think we need to handle the java.io.IOException: Input/output error case as well since this is what we'll see if having trouble reading from disk.\nThanks Daryn for the review.\nCancelling the current patch to address Daryn's comments.\nWill update the patch shortly.\n\n@Nathan: What action needs to be taken if java.io.IOException: Input/output error occurs.\n\n- I think we need to prioritize a scan for that block.\n\n- Also, some comments on addBlockToFirstLocation().\n  - imo, WARN should be INFO. \n  - If this block has been scanned in the last 5 minutes (or some reasonable time frame), then maybe we shouldn't add it back to the list of blocks to be scanned. If all IOExceptions are going to re-prioritize the scan of a block, having a minimum delay between scans would avoid corner cases where a network glitch or badly behaving clients are causing IOExceptions that don't really warrant rescans.\nAttaching a new patch addressing Nathan's and Daryn's comment.\nPlease review.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12692629/HDFS-7548-v2.patch\n  against trunk revision 780a6bf.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9232//console\n\nThis message is automatically generated.\nTalked offline with Daryn. He uncovered a potential bug.\nSo cancelling the patch.\nAttaching  a new patch addressing Daryn's offline comments.\n# In {{BlockPoolSliceScanner#updateBlockInfo}}, you should update the nextScanType else the verification logs will never be able to change the value.  Probably something like set last to next, update next to normal verification when updating the scanner.\n# In {{BlockSender#sendPacket}}, I think you should only check the block in the else clause.  If the client socket closed then that doesn't imply a local error.\n# I'd suggest following coding style of adding spaces after \"if\"\n# You should add @VisibleForTesting to {{DataBlockScanner#setLastScanTimeDifference}} or make the delta interval a static and poke it.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12692851/HDFS-7548-v3.patch\n  against trunk revision 60cbcff.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9256//console\n\nThis message is automatically generated.\nCancelling the patch to address Daryn's comment.\nAttaching a new patch addressing Daryn's comments.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12693402/HDFS-7548-v4.patch\n  against trunk revision dd0228b.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9282//console\n\nThis message is automatically generated.\nIf you take care of the findbugs warning, I think it looks ok.\nCancelling the patch to address findbugs warning\nAttachnig a new patch to address findbugs warning.\nAdded synchronized keyword to newly added method.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12693618/HDFS-7548-v5.patch\n  against trunk revision 6b17eb9.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.qjournal.TestSecureNNWithQJM\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9293//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9293//console\n\nThis message is automatically generated.\n+1 the patch looks good.\nThe test case is failing even without this patch and the patch was not changing anything related to it.\n[~daryn] and [~kihwal]  Thanks for the comments and review.\nThanks for working on the bug, Rushabh. I've committed this to trunk and branch-2.\nFAILURE: Integrated in Hadoop-trunk-Commit #6906 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6906/])\nHDFS-7548. Corrupt block reporting delayed until datablock scanner thread detects it. Contributed by Rushabh Shah. (kihwal: rev c0af72c7f74b6925786e24543cac433b906dd6d3)\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeBlockScanner.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java\n\n", "issueSearchSentences": ["The patch appears to mostly duplicate existing code just to prevent {{BlockPoolSliceScanner#addBlock}} from updating the {{lastScanTime}}.", "If this block has been scanned in the last 5 minutes (or some reasonable time frame), then maybe we shouldn't add it back to the list of blocks to be scanned.", "Whenever in write pipeline if the datanode detects any checksum error while transferring the block to target node, that particular block is added to first position in the blockInfoSet with setting the lastScanTime to 0.", "# In {{BlockPoolSliceScanner#updateBlockInfo}}, you should update the nextScanType else the verification logs will never be able to change the value.", "This will make the BlockPoolSliceScanner to pick this block first since that data structure is sorted by lastScanTime."], "issueSearchScores": [0.7781559228897095, 0.7600223422050476, 0.7234571576118469, 0.6938676834106445, 0.6529640555381775]}
{"aId": 30, "code": "public static INodeFile valueOf(INode inode, String path) throws IOException {\n    if (inode == null) {\n      throw new FileNotFoundException(\"File does not exist: \" + path);\n    }\n    if (!(inode instanceof INodeFile)) {\n      throw new FileNotFoundException(\"Path is not a file: \" + path);\n    }\n    return (INodeFile)inode;\n  }", "comment": " Cast INode to INodeFile.", "issueId": "HDFS-4107", "issueStringList": ["Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction", "In the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.", "Let's add utility methods for such checks.", "h4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550408/h4107_20121022.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer", "org.apache.hadoop.hdfs.TestDistributedFileSystem", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//console", "This message is automatically generated.", "TestDistributedFileSystem compared exception messsages so that it failed.", "TestBalancerWithEncryptedTransfer timed out.", "It did not seem related to the patch.", "h4107_20121023.patch: updates exception messages.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550517/h4107_20121023.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//console", "This message is automatically generated.", "Went through the call stack to ensure the behavior is not changed.", "Only change I see is throwing ClassCastException instead of NPE in one of the cases.", "Can you please add few unit tests for these new methods?", "h4107_20121024.patch: adds a test and rewrites some exception messages.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12550688/h4107_20121024.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//console", "This message is automatically generated.", "+1 for the patch.", "Integrated in Hadoop-trunk-Commit #2928 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2928/])", "HDFS-4107.", "Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.", "(Revision 1402265)", "Result = SUCCESS", "szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402265", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java", "I have committed this."], "SplitGT": [" Cast INode to INodeFile."], "issueString": "Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction\nIn the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.  Let's add utility methods for such checks. \nh4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550408/h4107_20121022.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer\n                  org.apache.hadoop.hdfs.TestDistributedFileSystem\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3385//console\n\nThis message is automatically generated.\nTestDistributedFileSystem compared exception messsages so that it failed.  TestBalancerWithEncryptedTransfer timed out.  It did not seem related to the patch.\n\nh4107_20121023.patch: updates exception messages.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550517/h4107_20121023.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3388//console\n\nThis message is automatically generated.\nWent through the call stack to ensure the behavior is not changed. Only change I see is throwing ClassCastException instead of NPE in one of the cases.\n\nCan you please add few unit tests for these new methods?\nh4107_20121024.patch: adds a test and rewrites some exception messages.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12550688/h4107_20121024.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3396//console\n\nThis message is automatically generated.\n+1 for the patch.\nIntegrated in Hadoop-trunk-Commit #2928 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2928/])\n    HDFS-4107. Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction. (Revision 1402265)\n\n     Result = SUCCESS\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402265\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java\n\nI have committed this.\n", "issueSearchSentences": ["h4107_20121022.patch: adds INodeFile.valueOf(..) and INodeFileUnderConstruction.valueOf(..).", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java", "In the namenode code, there are many individual routines checking whether an inode is null and whether it could be cast to INodeFile/INodeFileUnderConstruction.", "Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.", "Add utility methods to cast INode to INodeFile and INodeFileUnderConstruction"], "issueSearchScores": [0.5841505527496338, 0.5537877082824707, 0.5380287170410156, 0.5086480975151062, 0.48551028966903687]}
{"aId": 32, "code": "public static void writeINodeFile(INodeFile file, DataOutput out,\n      boolean writeUnderConstruction) throws IOException {\n    writeLocalName(file, out);\n    out.writeLong(file.getId());\n    out.writeShort(file.getFileReplication());\n    out.writeLong(file.getModificationTime());\n    out.writeLong(file.getAccessTime());\n    out.writeLong(file.getPreferredBlockSize());\n    // whether the file has striped blocks\n    out.writeBoolean(file.isWithStripedBlocks());\n    writeBlocks(file.getBlocks(), out);\n    SnapshotFSImageFormat.saveFileDiffList(file, out);\n\n    if (writeUnderConstruction) {\n      if (file.isUnderConstruction()) {\n        out.writeBoolean(true);\n        final FileUnderConstructionFeature uc = file.getFileUnderConstructionFeature();\n        writeString(uc.getClientName(), out);\n        writeString(uc.getClientMachine(), out);\n      } else {\n        out.writeBoolean(false);\n      }\n    }\n\n    writePermissionStatus(file, out);\n  }", "comment": " Serialize a INodeFile node", "issueId": "HDFS-7827", "issueStringList": ["Erasure Coding: support striped blocks in non-protobuf fsimage", "HDFS-7749 only adds code to persist striped blocks to protobuf-based fsimage.", "We should also add this support to the non-protobuf fsimage since it is still used for use cases like offline image processing.", "Hi Jing", "I would like to work on this jira.", "Could you assign it to me?", "Sure.", "Assign the jira to you.", "Thanks for working on this, Hui!", "To support striped block in no-protobuf fsimage we only need to persist striped blocks in non-protobuf format.", "The BlockInfo class  uses writable  interface to persist itself in non-protobuf format.", "So we can override the write method to implement it.", "Hi Jing", "I upload the patch which override the write method of BlockInfoStriped class.", "I think we don't need to implement the loading of a non-protobuf fsimage in Namenode,so it is not nessary to implement readFields method of    the Writable inface.", "But we need to support it in tools(ImageLoaderCurrent.java).", "Is that right?", "Yes, you're right.", "Hi Hui, the current patch looks good to me.", "One comment is that we may need to explicitly record the type of the blocks in the fsimage (e.g., using a byte in the fsimage to indicate if the file has striped block).", "Otherwise the offline image viewer or fsimage loader has no way to identify the correct block type when reading the block list.", "As you mentioned, we also need to add support in offline image viewer.", "Besides, though we may not have a use case anymore, it will be more complete to update the corresponding FSImage Loader code (more specifically, FImageFormat#loadINode).", "Note that we need to check the NameNodeLayoutVersion there.", "Hi Jing", "I agree that we need a flag to identify the striped block, but could we use the STORAGE_POLICY_ID in header of INodeFile?", "Hi Jing", "I made a mistake and found that the INodeFile does not serialize its header.So we need to record the type of the blocks in the fsimage as you said.", "Hi Jing", "I updated the patch.", "But how should we test it without EC client?", "I'm not sure whether this is the right thing to do as the old fsimage is quite out-of-dated compared to the features we have today.", "It might make sense to add the information to WebImageViewer though.", "I have add a testcase for saving and loading a INodeFile.", "Thanks [~wheat9]", "Is there any plan to remove the non-protobuf fsimage?", "Currently we still have non-protobuf image for offline processing.", "Thus this change can still be helpful especially for storage usage analysis.", "Some comments on the current patch:", "# After updating {{writeINodeUnderConstruction}}, we also need to update {{readINodeUnderConstruction}} accordingly.", "# We also need a unit test to cover the under construction file with striped blocks.", "# Instead of the following code, we can simply use \"out.writeBoolean(cons.isWithStripedBlocks())\".", "Whether we keep using \"EC_STORAGE_POLICY_ID\" is still under discussion.", "{code}", "+    if (cons.isStriped()){", "+      out.writeByte(HdfsConstants.EC_STORAGE_POLICY_ID);", "+    }else{", "+      out.writeByte(0);", "+    }", "{code}", "# Please correct the coding format according to the project conventions.", "Besides, as [~wheat9] pointed out, we also need to update WebImageViewer for file size calculation.", "Please open a separate jira to do this.", "Hi Jing", "I updated the patch,please review it.", "1) Corrected the {code}writeINodeUnderConstruction/readINodeUnderConstruction{code} method", "2.", "Added a testcase for save/locad a INodeFileUnderConstruction.", "3.", "Corrected the coding format.", "But I'm not sure whether they are all correct.", "If there are still code that is not according to the conventions please tell me the details.", "4.", "About WebImageViewer, I create a new [HDFS-7949|https://issues.apache.org/jira/browse/HDFS-7949]", "Thanks for updating the patch, Hui.", "The new patch looks good to me.", "Some minors:", "# In {{readINodeUnderConstruction}}, you still need to check if ERASURE_CODING is supported in the image", "# When loading an INodeFile with striped blocks, we do not need to pass in the EC policy ID.", "More likely we will have different implementation to mark files/directories as erasure coded.", "# There are still a lot of places not following the coding convention.", "Since these are not critical, I just made these changes on top of your patch.", "Please see if 004 patch looks good to you.", "Hi Jing", "Thank you for your help.The 004 patch looks good to me.", "Also I have known where is not following the coding convention by the patch.", "I've committed this to the feature branch.", "Thanks for the contribution, [~huizane]!"], "SplitGT": [" Serialize a INodeFile node"], "issueString": "Erasure Coding: support striped blocks in non-protobuf fsimage\nHDFS-7749 only adds code to persist striped blocks to protobuf-based fsimage. We should also add this support to the non-protobuf fsimage since it is still used for use cases like offline image processing.\nHi Jing\nI would like to work on this jira. Could you assign it to me?\nSure. Assign the jira to you. Thanks for working on this, Hui!\nTo support striped block in no-protobuf fsimage we only need to persist striped blocks in non-protobuf format.\nThe BlockInfo class  uses writable  interface to persist itself in non-protobuf format. So we can override the write method to implement it.\n\nHi Jing\nI upload the patch which override the write method of BlockInfoStriped class.\nI think we don't need to implement the loading of a non-protobuf fsimage in Namenode,so it is not nessary to implement readFields method of    the Writable inface. But we need to support it in tools(ImageLoaderCurrent.java).\nIs that right?\nYes, you're right.\nHi Hui, the current patch looks good to me. One comment is that we may need to explicitly record the type of the blocks in the fsimage (e.g., using a byte in the fsimage to indicate if the file has striped block). Otherwise the offline image viewer or fsimage loader has no way to identify the correct block type when reading the block list.\n\nAs you mentioned, we also need to add support in offline image viewer. Besides, though we may not have a use case anymore, it will be more complete to update the corresponding FSImage Loader code (more specifically, FImageFormat#loadINode). Note that we need to check the NameNodeLayoutVersion there.\nHi Jing\nI agree that we need a flag to identify the striped block, but could we use the STORAGE_POLICY_ID in header of INodeFile?\nHi Jing \nI made a mistake and found that the INodeFile does not serialize its header.So we need to record the type of the blocks in the fsimage as you said.\nHi Jing\nI updated the patch. But how should we test it without EC client?\nI'm not sure whether this is the right thing to do as the old fsimage is quite out-of-dated compared to the features we have today. It might make sense to add the information to WebImageViewer though.\nI have add a testcase for saving and loading a INodeFile. \nThanks [~wheat9]\nIs there any plan to remove the non-protobuf fsimage?\nCurrently we still have non-protobuf image for offline processing. Thus this change can still be helpful especially for storage usage analysis.\n\nSome comments on the current patch:\n# After updating {{writeINodeUnderConstruction}}, we also need to update {{readINodeUnderConstruction}} accordingly.\n# We also need a unit test to cover the under construction file with striped blocks.\n# Instead of the following code, we can simply use \"out.writeBoolean(cons.isWithStripedBlocks())\". Whether we keep using \"EC_STORAGE_POLICY_ID\" is still under discussion.\n{code}\n+    if (cons.isStriped()){\n+      out.writeByte(HdfsConstants.EC_STORAGE_POLICY_ID);\n+    }else{\n+      out.writeByte(0);\n+    }\n{code}\n# Please correct the coding format according to the project conventions.\n\nBesides, as [~wheat9] pointed out, we also need to update WebImageViewer for file size calculation. Please open a separate jira to do this.\nHi Jing\nI updated the patch,please review it.\n1) Corrected the {code}writeINodeUnderConstruction/readINodeUnderConstruction{code} method\n2. Added a testcase for save/locad a INodeFileUnderConstruction.\n3. Corrected the coding format. But I'm not sure whether they are all correct. If there are still code that is not according to the conventions please tell me the details.\n4. About WebImageViewer, I create a new [HDFS-7949|https://issues.apache.org/jira/browse/HDFS-7949]\n\n  \n\nThanks for updating the patch, Hui.\n\nThe new patch looks good to me. Some minors:\n# In {{readINodeUnderConstruction}}, you still need to check if ERASURE_CODING is supported in the image\n# When loading an INodeFile with striped blocks, we do not need to pass in the EC policy ID. More likely we will have different implementation to mark files/directories as erasure coded.\n# There are still a lot of places not following the coding convention.\n\nSince these are not critical, I just made these changes on top of your patch. Please see if 004 patch looks good to you.\nHi Jing\nThank you for your help.The 004 patch looks good to me.\nAlso I have known where is not following the coding convention by the patch.\nI've committed this to the feature branch. Thanks for the contribution, [~huizane]!\n", "issueSearchSentences": ["1) Corrected the {code}writeINodeUnderConstruction/readINodeUnderConstruction{code} method", "Added a testcase for save/locad a INodeFileUnderConstruction.", "# After updating {{writeINodeUnderConstruction}}, we also need to update {{readINodeUnderConstruction}} accordingly.", "I made a mistake and found that the INodeFile does not serialize its header.So we need to record the type of the blocks in the fsimage as you said.", "+      out.writeByte(0);"], "issueSearchScores": [0.6432074904441833, 0.5987427234649658, 0.5719039440155029, 0.5593901872634888, 0.5432931184768677]}
{"aId": 34, "code": "@Override\n  public String toString() {\n    StringBuilder builder = new StringBuilder();\n    builder.append(\"Execution Summary:-\");\n    builder.append(\"\\nInput trace: \").append(getInputTraceLocation());\n    builder.append(\"\\nInput trace signature: \")\n           .append(getInputTraceSignature());\n    builder.append(\"\\nTotal number of jobs in trace: \")\n           .append(getNumJobsInTrace());\n    builder.append(\"\\nExpected input data size: \")\n           .append(getExpectedDataSize());\n    builder.append(\"\\nInput data statistics: \")\n           .append(getInputDataStatistics());\n    builder.append(\"\\nTotal number of jobs processed: \")\n           .append(getNumSubmittedJobs());\n    builder.append(\"\\nTotal number of successful jobs: \")\n           .append(getNumSuccessfulJobs());\n    builder.append(\"\\nTotal number of failed jobs: \")\n           .append(getNumFailedJobs());\n    builder.append(\"\\nTotal number of map tasks launched: \")\n           .append(getNumMapTasksLaunched());\n    builder.append(\"\\nTotal number of reduce task launched: \")\n           .append(getNumReduceTasksLaunched());\n    builder.append(\"\\nGridmix start time: \")\n           .append(UTIL.format(getStartTime()));\n    builder.append(\"\\nGridmix end time: \").append(UTIL.format(getEndTime()));\n    builder.append(\"\\nGridmix simulation start time: \")\n           .append(UTIL.format(getStartTime()));\n    builder.append(\"\\nGridmix runtime: \")\n           .append(StringUtils.formatTime(getRuntime()));\n    builder.append(\"\\nTime spent in initialization (data-gen etc): \")\n           .append(StringUtils.formatTime(getInitTime()));\n    builder.append(\"\\nTime spent in simulation: \")\n           .append(StringUtils.formatTime(getSimulationTime()));\n    builder.append(\"\\nGridmix configuration parameters: \")\n           .append(getCommandLineArgsString());\n    builder.append(\"\\nGridmix job submission policy: \")\n           .append(getJobSubmissionPolicy());\n    builder.append(\"\\nGridmix resolver: \").append(getUserResolver());\n    builder.append(\"\\n\\n\");\n    return builder.toString();\n  }", "comment": " Summarizes the current Gridmix run.", "issueId": "MAPREDUCE-2596", "issueStringList": ["Gridmix should notify job failures", "Gridmix doesn't warn the user if any of the jobs in the mix fail... it probably should printout a summary of the jobs and other statistics at the end too.", "Attaching a patch that summarizes the gridmix run.", "This is how the summary looks like:", "{noformat}", "Execution Summary:-", "Input trace: file:/home/user/hadoop/mapreduce/job_201106081604_0006.json", "Input trace signature: 621979b3d484a2d824bd5d9a721dc085", "Total number of jobs in trace: 1", "Expected input data size: N/A", "Input data statistics: Input data size: 5.0m, Number of files: 1, Compression: true", "Total number of jobs processed: 1", "Total number of successful jobs: 1", "Total number of failed jobs: 0", "Total number of jobs failed after submission: 0", "Total number of jobs failed before submission: 0", "Total number of map tasks launched: 1", "Total number of reduce task launched: 1", "Gridmix start time: 6/23/11 12:37 PM", "Gridmix end time: 6/23/11 12:40 PM", "Gridmix simulation start time: 6/23/11 12:37 PM", "Gridmix runtime: 2mins, 57sec", "Time spent in initialization (data-gen etc): 2sec", "Time spent in simulation: 2mins, 54sec", "Gridmix configuration parameters: -Dgridmix.output.directory=output -Dmapreduce.job.ubertask.enable=false -Dgridmix.min.file.size=1024 -Dgridmix.compression-emulation.enable=true input file:///home/user/hadoop/mapreduce/job_201106081604_0006.json", "Gridmix job submission policy: STRESS", "Gridmix resolver: org.apache.hadoop.mapred.gridmix.SubmitterUserResolver", "Cluster Summary:-", "JobTracker: localhost:9520", "FileSystem: localhost:9500", "Number of blacklisted trackers: 0", "Number of active trackers: 1", "Max map task capacity: 2", "Max reduce task capacity: 2", "{noformat}", "test-patch and gridmix junit tests passed.", "Todo:", "1. job level summary (current-jobid, original-jobid, runtime characteristics, input output characteristics etc)", "2. ability to record the summary in a specified file and in a specified format (xml/json/csv)", "3.", "Test this on a slightly larger cluster.", "The first cut is intentionally kept simple.", "I have reviewed the patch and few minor comments.", "{noformat}", "+  @SuppressWarnings(\"deprecation\")", "+  public void update(ClusterStats item) {", "+    try {", "+      numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();", "+      numActiveTrackers = item.getStatus().getTaskTrackers();", "+      maxMapTasks = item.getStatus().getMaxMapTasks();", "+      maxReduceTasks = item.getStatus().getMaxReduceTasks();", "+    } catch (Exception e) {", "+      long time = System.currentTimeMillis();", "+      LOG.info(\"Error in processing cluster status at \"", "+               + FastDateFormat.getInstance().format(time));", "+    }", "+  }", "{noformat}", "bq.", "You can define the above method as static method right,because cluster information is same for across the jobs in a gridmix run right.", "bq.", "\\ No newline at end of file", "I think, no newline at end of file will cause an issues while adding the patch.Make sure to give a space at end of the file.", "{noformat}", "+    if (job.isSuccessful()) {", "+      ++totalSuccessfulJobs;", "+    } else {", "+      if (job.isComplete()) {", "+        ++totalFailedJobs;", "+      } else {", "+        //TODO Check", "+        ++totalSubmitFailedJobs;", "+      }", "+    }", "{noformat}", "bq.", "The above instructions will change to if(condition){}elseif(conditon){}else{} format right.", "bq.Add java doc comments at least for public methods.", "Thanks for the review Vinay.", "bq.", "You can define the above method as static method right,because cluster information is same for across the jobs in a gridmix run right.", "I cannot define it as static as the member variables of that class are being set in the method.", "bq.", "I think, no newline at end of file will cause an issues while adding the patch.Make sure to give a space at end of the file.", "The patch cleanly applies and test-patch passed.", "I have verified this.", "bq.", "The above instructions will change to if(condition){}elseif(conditon){}else{} format right.", "+1.", "bq.", "Add java doc comments at least for public methods.", "I have intentionally not added comments for overridden apis.", "I will check again and add the missing ones.", "Attaching a new patch incorporating Vinay's review comments.", "test-patch passed on my box.", "Patch looks good to me.", "+1.", "Attaching a new patch that removes the distinction between jobs that failed before submission and jobs that failed after job submission.", "Also fixed a bug in JobSubmitter and JobMonitor.", "test-patch and gridmix junit tests passed in my box.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12484410/gridmix-summary-v1.5.patch", "against trunk revision 1139400.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "org.apache.hadoop.fs.TestFileSystem", "org.apache.hadoop.mapred.TestNodeRefresh", "1 contrib tests.", "The patch failed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//console", "This message is automatically generated.", "The failed testcases are", "1. org.apache.hadoop.mapred.TestNodeRefresh.testMRRefreshDecommissioning", "2. org.apache.hadoop.raid.TestRaidNode.testPathFilter", "3. org.apache.hadoop.fs.TestFileSystem.testCommandFormat", "4. org.apache.hadoop.cli.TestMRCLI.testAll", "None of them are related to this patch.", "Attaching a new patch that fixes a corner case (only data gen).", "test-patch passed on my box.", "Attaching a new patch with some minor offline comments from Ravi and Vinay.", "test-patch passed.", "The changes are in code comments and log statements.", "Attaching a final version of the patch for commit.", "Includes minor changes w.r.t variable/api naming.", "test-patch passed on my box.", "Patch looks fine to me.", "+1", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12485686/gridmix-summary-v1.8.patch", "against trunk revision 1144097.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "org.apache.hadoop.fs.TestFileSystem", "1 contrib tests.", "The patch failed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//console", "This message is automatically generated.", "None of the failed tests seem related to this patch.", "Failing tests are mostly RAID tests and CLI tests.", "I just committed the latest patch to trunk.", "Thanks Ravi and Vinay for reviewing this.", "Integrated in Hadoop-Mapreduce-trunk-Commit #736 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/736/])", "MAPREDUCE-2596.", "[Gridmix] Summarize Gridmix runs.", "(amarrk)", "amarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144403", "Files :", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java", "hadoop/common/trunk/mapreduce/CHANGES.txt", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java", "hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java"], "SplitGT": [" Summarizes the current Gridmix run."], "issueString": "Gridmix should notify job failures\nGridmix doesn't warn the user if any of the jobs in the mix fail... it probably should printout a summary of the jobs and other statistics at the end too.\nAttaching a patch that summarizes the gridmix run. This is how the summary looks like:\n{noformat}\nExecution Summary:-\nInput trace: file:/home/user/hadoop/mapreduce/job_201106081604_0006.json\nInput trace signature: 621979b3d484a2d824bd5d9a721dc085\nTotal number of jobs in trace: 1\nExpected input data size: N/A\nInput data statistics: Input data size: 5.0m, Number of files: 1, Compression: true\nTotal number of jobs processed: 1\nTotal number of successful jobs: 1\nTotal number of failed jobs: 0\nTotal number of jobs failed after submission: 0\nTotal number of jobs failed before submission: 0\nTotal number of map tasks launched: 1\nTotal number of reduce task launched: 1\nGridmix start time: 6/23/11 12:37 PM\nGridmix end time: 6/23/11 12:40 PM\nGridmix simulation start time: 6/23/11 12:37 PM\nGridmix runtime: 2mins, 57sec\nTime spent in initialization (data-gen etc): 2sec\nTime spent in simulation: 2mins, 54sec\nGridmix configuration parameters: -Dgridmix.output.directory=output -Dmapreduce.job.ubertask.enable=false -Dgridmix.min.file.size=1024 -Dgridmix.compression-emulation.enable=true input file:///home/user/hadoop/mapreduce/job_201106081604_0006.json\nGridmix job submission policy: STRESS\nGridmix resolver: org.apache.hadoop.mapred.gridmix.SubmitterUserResolver\n\nCluster Summary:-\nJobTracker: localhost:9520\nFileSystem: localhost:9500\nNumber of blacklisted trackers: 0\nNumber of active trackers: 1\nMax map task capacity: 2\nMax reduce task capacity: 2\n{noformat}\n\ntest-patch and gridmix junit tests passed.\n\nTodo:\n1. job level summary (current-jobid, original-jobid, runtime characteristics, input output characteristics etc)\n2. ability to record the summary in a specified file and in a specified format (xml/json/csv)\n3. Test this on a slightly larger cluster.\n\nThe first cut is intentionally kept simple.\nI have reviewed the patch and few minor comments.\n\n{noformat} \n+  @SuppressWarnings(\"deprecation\")\n+  public void update(ClusterStats item) {\n+    try {\n+      numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();\n+      numActiveTrackers = item.getStatus().getTaskTrackers();\n+      maxMapTasks = item.getStatus().getMaxMapTasks();\n+      maxReduceTasks = item.getStatus().getMaxReduceTasks();\n+    } catch (Exception e) {\n+      long time = System.currentTimeMillis();\n+      LOG.info(\"Error in processing cluster status at \" \n+               + FastDateFormat.getInstance().format(time));\n+    }\n+  }\n{noformat} \nbq. You can define the above method as static method right,because cluster information is same for across the jobs in a gridmix run right.\n\nbq. \\ No newline at end of file\nI think, no newline at end of file will cause an issues while adding the patch.Make sure to give a space at end of the file.\n\n{noformat} \n+    if (job.isSuccessful()) {\n+      ++totalSuccessfulJobs;\n+    } else {\n+      if (job.isComplete()) {\n+        ++totalFailedJobs;\n+      } else {\n+        //TODO Check\n+        ++totalSubmitFailedJobs;\n+      }\n+    }\n{noformat} \nbq. The above instructions will change to if(condition){}elseif(conditon){}else{} format right.\n\nbq.Add java doc comments at least for public methods.\n \nThanks for the review Vinay. \n\nbq.     You can define the above method as static method right,because cluster information is same for across the jobs in a gridmix run right.\nI cannot define it as static as the member variables of that class are being set in the method.\n\nbq. I think, no newline at end of file will cause an issues while adding the patch.Make sure to give a space at end of the file.\nThe patch cleanly applies and test-patch passed. I have verified this.\n\nbq. The above instructions will change to if(condition){}elseif(conditon){}else{} format right.\n+1.\n\nbq. Add java doc comments at least for public methods.\nI have intentionally not added comments for overridden apis. I will check again and add the missing ones.\nAttaching a new patch incorporating Vinay's review comments. test-patch passed on my box.\nPatch looks good to me.\n+1.\nAttaching a new patch that removes the distinction between jobs that failed before submission and jobs that failed after job submission. Also fixed a bug in JobSubmitter and JobMonitor. \n\ntest-patch and gridmix junit tests passed in my box. \n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12484410/gridmix-summary-v1.5.patch\n  against trunk revision 1139400.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n                  org.apache.hadoop.fs.TestFileSystem\n                  org.apache.hadoop.mapred.TestNodeRefresh\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/430//console\n\nThis message is automatically generated.\nThe failed testcases are\n1. org.apache.hadoop.mapred.TestNodeRefresh.testMRRefreshDecommissioning \n2. org.apache.hadoop.raid.TestRaidNode.testPathFilter \n3. org.apache.hadoop.fs.TestFileSystem.testCommandFormat \n4. org.apache.hadoop.cli.TestMRCLI.testAll \n\nNone of them are related to this patch.\nAttaching a new patch that fixes a corner case (only data gen). test-patch passed on my box. \nAttaching a new patch with some minor offline comments from Ravi and Vinay. test-patch passed. The changes are in code comments and log statements.\nAttaching a final version of the patch for commit. Includes minor changes w.r.t variable/api naming.\n\ntest-patch passed on my box.\nPatch looks fine to me.\n+1\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12485686/gridmix-summary-v1.8.patch\n  against trunk revision 1144097.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n                  org.apache.hadoop.fs.TestFileSystem\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/445//console\n\nThis message is automatically generated.\nNone of the failed tests seem related to this patch. Failing tests are mostly RAID tests and CLI tests.\nI just committed the latest patch to trunk. Thanks Ravi and Vinay for reviewing this.\nIntegrated in Hadoop-Mapreduce-trunk-Commit #736 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/736/])\n    MAPREDUCE-2596. [Gridmix] Summarize Gridmix runs. (amarrk)\n\namarrk : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144403\nFiles : \n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java\n* /hadoop/common/trunk/mapreduce/CHANGES.txt\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Summarizer.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/GenerateData.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobFactory.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java\n* /hadoop/common/trunk/mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java\n\n", "issueSearchSentences": ["1. job level summary (current-jobid, original-jobid, runtime characteristics, input output characteristics etc)", "Input trace: file:/home/user/hadoop/mapreduce/job_201106081604_0006.json", "Execution Summary:-", "This is how the summary looks like:", "Cluster Summary:-"], "issueSearchScores": [0.5224559903144836, 0.5087538957595825, 0.48206835985183716, 0.4721151292324066, 0.45444485545158386]}
{"aId": 35, "code": "public boolean deleteOnExit(Path f) throws IOException {\n    if (!exists(f)) {\n      return false;\n    }\n    synchronized (deleteOnExit) {\n      if (deleteOnExit.isEmpty() && !finalizer.isAlive()) {\n        Runtime.getRuntime().addShutdownHook(finalizer);\n      }\n      \n      Set<Path> set = deleteOnExit.get(this);\n      if (set == null) {\n        set = new TreeSet<Path>();\n        deleteOnExit.put(this, set);\n      }\n      set.add(f);\n    }\n    return true;\n  }", "comment": " Mark a path to be deleted on JVM shutdown.", "issueId": "HADOOP-6270", "issueStringList": ["FileContext needs to provide deleteOnExit functionality", "FileSystem provided an API to the applications {{deleteOnExit(Path f)}} used for registering a path to be deleted on JVM shutdown or when calling {{FileSystem.close()}} methods.", "Equivalent functionality is required in FileContext.", "Attached patch adds deleteOnExit functionality to FileContext", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12419971/deleteOnExit.patch", "against trunk revision 816409.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/console", "This message is automatically generated.", "+1 Looks good.", "+1", "I have committed this.", "Thanks, Suresh!"], "SplitGT": [" Mark a path to be deleted on JVM shutdown."], "issueString": "FileContext needs to provide deleteOnExit functionality\nFileSystem provided an API to the applications {{deleteOnExit(Path f)}} used for registering a path to be deleted on JVM shutdown or when calling {{FileSystem.close()}} methods. Equivalent functionality is required in FileContext.\nAttached patch adds deleteOnExit functionality to FileContext\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12419971/deleteOnExit.patch\n  against trunk revision 816409.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/48/console\n\nThis message is automatically generated.\n+1 Looks good.\n+1\n\nI have committed this.  Thanks, Suresh!\n", "issueSearchSentences": ["FileSystem provided an API to the applications {{deleteOnExit(Path f)}} used for registering a path to be deleted on JVM shutdown or when calling {{FileSystem.close()}} methods.", "FileContext needs to provide deleteOnExit functionality", "http://issues.apache.org/jira/secure/attachment/12419971/deleteOnExit.patch", "Attached patch adds deleteOnExit functionality to FileContext", "I have committed this."], "issueSearchScores": [0.776023268699646, 0.6257274746894836, 0.578856348991394, 0.505874752998352, 0.302639365196228]}
{"aId": 36, "code": "public synchronized \n  void setAuthenticationMethod(AuthenticationMethod authMethod) {\n    for (User p : subject.getPrincipals(User.class)) {\n      p.setAuthenticationMethod(authMethod);\n    }\n  }", "comment": " Sets the authentication method in the subject", "issueId": "HADOOP-6580", "issueStringList": ["UGI should contain authentication method.", "The UserGroupInformation should contain authentication method in its subject.", "This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients.", "Patch submitted for hudson tests.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12436470/HADOOP-6580.1.patch", "against trunk revision 912207.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/console", "This message is automatically generated.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12436627/HADOOP-6580.2.patch", "against trunk revision 912207.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/console", "This message is automatically generated.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12436661/HADOOP-6580.3.patch", "against trunk revision 915097.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/console", "This message is automatically generated.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12436752/HADOOP-6580.4.patch", "against trunk revision 915168.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/console", "This message is automatically generated.", "For the proxy users, you shouldn't set the AuthMethod.", "Other than that looks good.", "New patch submitted addressing the comment.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12437046/HADOOP-6580.5.patch", "against trunk revision 916390.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/console", "This message is automatically generated.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12437128/HADOOP-6580.6.patch", "against trunk revision 916529.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/console", "This message is automatically generated.", "This jira adds support to set and get authenticaton method from the UGI.", "This feature is used by hdfs and mr to get the authentication method to allow only kerberos and web authenticated clients to get and renew delegation tokens.", "It also introduces a new AuthMethod.WEB enum for authenticated web connections.", "I think we need to keep the user facing AuthenticationMethod separate from the SaslRpc server AuthMethod.", "I'd also propose that we keep PROXY as a separate value and the service needs to check the real user's AuthenticationMethod to determine the underlying basis for the authentication.", "I think it is also important to avoid abbreviations in the interfaces.", "Here is a quick modification of your patch that changes the api as suggested.", "New patch, includes the changes suggested by Owen.", "The authentication method is set in the processHeader in Server.java for all rpc authentication scenarios including SIMPLE.", "Same patch HADOOP-6580.7.patch renamed so that it appears on the top of the list", "c6580.1.patch was removed because it still didn't show up on top.", "HADOOP-6580.7.patch submitted for hudson tests.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12437240/HADOOP-6580.7.patch", "against trunk revision 916779.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "1 patch.", "The patch command could not apply the patch.", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/393/console", "This message is automatically generated.", "New patch updated due to recent commits.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12437261/HADOOP-6580.8.patch", "against trunk revision 916779.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/console", "This message is automatically generated.", "+1 (pending hudson)", "patch for hadoop-20, includes the linked jiras in hdfs and mr", "Patch updated to merge with latest trunk.", "HADOOP-6580.9.patch  is submitted for hudson tests.", "test-patch results", "[exec] +1 overall.", "[exec]", "[exec]     +1 @author.", "The patch does not contain any @author tags.", "[exec]", "[exec]     +1 tests included.", "The patch appears to include 6 new or modified tests.", "[exec]", "[exec]     +1 javadoc.", "The javadoc tool did not generate any warning messages.", "[exec]", "[exec]     +1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "[exec]", "[exec]     +1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "[exec]", "[exec]     +1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "ant test also ran successfully.", "committed this.", "Thanks Jitendra.", "Integrated in Hadoop-Common-trunk-Commit #218 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/218/])", "HADOOP-6580.", "UGI should contain authentication method."], "SplitGT": [" Sets the authentication method in the subject"], "issueString": "UGI should contain authentication method.\nThe UserGroupInformation should contain authentication method in its subject. This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients.\nPatch submitted for hudson tests. \n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12436470/HADOOP-6580.1.patch\n  against trunk revision 912207.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/23/console\n\nThis message is automatically generated.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12436627/HADOOP-6580.2.patch\n  against trunk revision 912207.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/366/console\n\nThis message is automatically generated.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12436661/HADOOP-6580.3.patch\n  against trunk revision 915097.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/368/console\n\nThis message is automatically generated.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12436752/HADOOP-6580.4.patch\n  against trunk revision 915168.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/27/console\n\nThis message is automatically generated.\nFor the proxy users, you shouldn't set the AuthMethod. Other than that looks good.\nNew patch submitted addressing the comment.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12437046/HADOOP-6580.5.patch\n  against trunk revision 916390.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/386/console\n\nThis message is automatically generated.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12437128/HADOOP-6580.6.patch\n  against trunk revision 916529.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/389/console\n\nThis message is automatically generated.\nThis jira adds support to set and get authenticaton method from the UGI. \nThis feature is used by hdfs and mr to get the authentication method to allow only kerberos and web authenticated clients to get and renew delegation tokens. It also introduces a new AuthMethod.WEB enum for authenticated web connections.\nI think we need to keep the user facing AuthenticationMethod separate from the SaslRpc server AuthMethod. I'd also propose that we keep PROXY as a separate value and the service needs to check the real user's AuthenticationMethod to determine the underlying basis for the authentication.\n\nI think it is also important to avoid abbreviations in the interfaces.\n\nHere is a quick modification of your patch that changes the api as suggested.\nNew patch, includes the changes suggested by Owen.\nThe authentication method is set in the processHeader in Server.java for all rpc authentication scenarios including SIMPLE. \nSame patch HADOOP-6580.7.patch renamed so that it appears on the top of the list\nc6580.1.patch was removed because it still didn't show up on top.\n\nHADOOP-6580.7.patch submitted for hudson tests.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12437240/HADOOP-6580.7.patch\n  against trunk revision 916779.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/393/console\n\nThis message is automatically generated.\nNew patch updated due to recent commits.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12437261/HADOOP-6580.8.patch\n  against trunk revision 916779.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/396/console\n\nThis message is automatically generated.\n+1 (pending hudson)\npatch for hadoop-20, includes the linked jiras in hdfs and mr\nPatch updated to merge with latest trunk.\nHADOOP-6580.9.patch  is submitted for hudson tests.\ntest-patch results\n\n     [exec] +1 overall.  \n     [exec] \n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec] \n     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.\n     [exec] \n     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.\n     [exec] \n     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n     [exec] \n     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n     [exec] \n     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\nant test also ran successfully.\ncommitted this. Thanks Jitendra.\nIntegrated in Hadoop-Common-trunk-Commit #218 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/218/])\n    HADOOP-6580. UGI should contain authentication method.\n\n", "issueSearchSentences": ["For the proxy users, you shouldn't set the AuthMethod.", "I think we need to keep the user facing AuthenticationMethod separate from the SaslRpc server AuthMethod.", "It also introduces a new AuthMethod.WEB enum for authenticated web connections.", "The UserGroupInformation should contain authentication method in its subject.", "The authentication method is set in the processHeader in Server.java for all rpc authentication scenarios including SIMPLE."], "issueSearchScores": [0.7387692928314209, 0.62374347448349, 0.5500476956367493, 0.5405506491661072, 0.5049607157707214]}
{"aId": 37, "code": "public BytesWritable(byte[] bytes, int length) {\n    this.bytes = bytes;\n    this.size = length;\n  }", "comment": " Create a BytesWritable using the byte array as the initial value and length as the length.", "issueId": "HADOOP-310", "issueStringList": ["Additional constructor requested in BytesWritable", "It would be grand if BytesWritable.java had an additional constructor as below.", "This allows me to use the BytesWritable class without doing a buffer copy, since we have a less-than-fully-utilized byte array holding our key.", "Thanks!", "Create a BytesWritable using the byte array as the initial value.", "@param bytes This array becomes the backing storage for the object.", "public BytesWritable(byte[] bytes, int size) {", "this.bytes = bytes;", "this.size = size;", "}", "Perhaps a more general constructor, similar to the ones used in many other places, would be better:", "Create a BytesWritable using a part of the byte array as the initial value.", "public BytesWritable(byte[] bytes, int offset, int length) {", "....", "}", "This way it would satisfy your requirement, and perhaps help to avoid many other cases of copying ...", "That would be even better... but would require a rewrite and retest of the rest of the class (the class already supports a length, but not an offset).", "I'm ok with either change.", "Obviously introducing an offset is a bigger change.", "Clearly if the BytesWritable needs to grow, the offset should go to 0.", "Equivalently, the offset should not be saved as part of the serialization.", "I think the offset would be ideal, but the extra constructor and a set() method which does the same is a good incremental change.", "That patch is attached.", "Updated one test assert statement with reason for failure.", "Hey Brock, thanks a lot for reviving this issue (after 5 years!)", "I have a few small concerns about this patch:", "# The constructor which only takes a {{byte[]}} can now be implemented in terms of this new constructor you've introduced.", "# We now have {{set(byte[] bytes, int length)}} and {{set(byte[] bytes, int offset, int length)}}, which differ not only in signature, but in their semantics with respect to copying of the input data.", "This seems like it might lead to confusion.", "Perhaps we should name the new method something like {{setDirect(...)}}?", "Or perhaps we should amend the existing {{set(...)}} method to not do a copy?", "The latter is obviously a trickier and backward-incompatible change,  but probably clearer.", "Great points.", "The attached removes the set method which would have been confusing.", "This is what the original JIRA request was for.", "I do not see any need for the set method I had previously written.", "If you compare the cost of creating of an additional object, due to no zero copy set, versus being required to perform a copy to fill the BW, the extra object should be negligible with a byte array of any size.", "+1, the patch looks good to me.", "Marking this as patch available so Hudson runs test-patch.", "I'll commit this pending clean Hudson results.", "Also reassigning this to you, Brock.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12483841/bytes-writable-zero-copy-interface-2.patch", "against trunk revision 1139947.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//console", "This message is automatically generated.", "I've just committed this.", "Thanks a lot for the contribution, Brock!"], "SplitGT": [" Create a BytesWritable using the byte array as the initial value and length as the length."], "issueString": "Additional constructor requested in BytesWritable\n\nIt would be grand if BytesWritable.java had an additional constructor as below. This allows me to use the BytesWritable class without doing a buffer copy, since we have a less-than-fully-utilized byte array holding our key.\n\nThanks!\n \n /**\n   * Create a BytesWritable using the byte array as the initial value.\n   * @param bytes This array becomes the backing storage for the object.\n   */\n  public BytesWritable(byte[] bytes, int size) {\n    this.bytes = bytes;\n    this.size = size;\n  }\n  \n\nPerhaps a more general constructor, similar to the ones used in many other places, would be better:\n\n/**\n   * Create a BytesWritable using a part of the byte array as the initial value.\n   */\n  public BytesWritable(byte[] bytes, int offset, int length) {\n    ....\n  }\n\nThis way it would satisfy your requirement, and perhaps help to avoid many other cases of copying ...\n\n\nThat would be even better... but would require a rewrite and retest of the rest of the class (the class already supports a length, but not an offset). \nI'm ok with either change. Obviously introducing an offset is a bigger change. Clearly if the BytesWritable needs to grow, the offset should go to 0. Equivalently, the offset should not be saved as part of the serialization.\nI think the offset would be ideal, but the extra constructor and a set() method which does the same is a good incremental change. That patch is attached.\n\nUpdated one test assert statement with reason for failure.\nHey Brock, thanks a lot for reviving this issue (after 5 years!) I have a few small concerns about this patch:\n\n# The constructor which only takes a {{byte[]}} can now be implemented in terms of this new constructor you've introduced.\n# We now have {{set(byte[] bytes, int length)}} and {{set(byte[] bytes, int offset, int length)}}, which differ not only in signature, but in their semantics with respect to copying of the input data. This seems like it might lead to confusion. Perhaps we should name the new method something like {{setDirect(...)}}? Or perhaps we should amend the existing {{set(...)}} method to not do a copy? The latter is obviously a trickier and backward-incompatible change,  but probably clearer.\nGreat points. The attached removes the set method which would have been confusing. This is what the original JIRA request was for.\n\nI do not see any need for the set method I had previously written. If you compare the cost of creating of an additional object, due to no zero copy set, versus being required to perform a copy to fill the BW, the extra object should be negligible with a byte array of any size.\n+1, the patch looks good to me.\n\nMarking this as patch available so Hudson runs test-patch. I'll commit this pending clean Hudson results.\n\nAlso reassigning this to you, Brock.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12483841/bytes-writable-zero-copy-interface-2.patch\n  against trunk revision 1139947.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//console\n\nThis message is automatically generated.\nI've just committed this.\n\nThanks a lot for the contribution, Brock!\n", "issueSearchSentences": ["public BytesWritable(byte[] bytes, int offset, int length) {", "public BytesWritable(byte[] bytes, int size) {", "Create a BytesWritable using a part of the byte array as the initial value.", "This allows me to use the BytesWritable class without doing a buffer copy, since we have a less-than-fully-utilized byte array holding our key.", "Create a BytesWritable using the byte array as the initial value."], "issueSearchScores": [0.9457693099975586, 0.9284602403640747, 0.745424747467041, 0.7409491539001465, 0.7375585436820984]}
{"aId": 38, "code": "@VisibleForTesting\n  boolean checkAndDeleteCgroup(File cgf) throws InterruptedException {\n    boolean deleted = false;\n    // FileInputStream in = null;\n    try (FileInputStream in = new FileInputStream(cgf + \"/tasks\")) {\n      if (in.read() == -1) {\n        /*\n         * \"tasks\" file is empty, sleep a bit more and then try to delete the\n         * cgroup. Some versions of linux will occasionally panic due to a race\n         * condition in this area, hence the paranoia.\n         */\n        Thread.sleep(deleteCgroupDelay);\n        deleted = cgf.delete();\n        if (!deleted) {\n          LOG.warn(\"Failed attempt to delete cgroup: \" + cgf);\n        }\n      } else {\n        logLineFromTasksFile(cgf);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Failed to read cgroup tasks file. \", e);\n    }\n    return deleted;\n  }", "comment": " If tasks file is empty, delete the cgroup.", "issueId": "YARN-2809", "issueStringList": ["Implement workaround for linux kernel panic when removing cgroup", "Some older versions of linux have a bug that can cause a kernel panic when the LCE attempts to remove a cgroup.", "It is a race condition so it's a bit rare but on a few thousand node cluster it can result in a couple of panics per day.", "This is the commit that likely (haven't verified) fixes the problem in linux: https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/commit/?h=linux-2.6.39.y&id=068c5cc5ac7414a8e9eb7856b4bf3cc4d4744267", "Details will be added in comments.", "Stack trace:", "{noformat}", "[<ffffffff8150d4a8>] ?", "panic+0xa7/0x16f", "[<ffffffff815116d4>] ?", "oops_end+0xe4/0x100", "[<ffffffff81046bfb>] ?", "no_context+0xfb/0x260", "[<ffffffff81449058>] ?", "dev_hard_start_xmit+0x308/0x530", "[<ffffffff81046e85>] ?", "__bad_area_nosemaphore+0x125/0x1e0", "[<ffffffff812773a9>] ?", "cpumask_next_and+0x29/0x50", "[<ffffffff81046f53>] ?", "bad_area_nosemaphore+0x13/0x20", "[<ffffffff810476b1>] ?", "__do_page_fault+0x321/0x480", "[<ffffffff81056881>] ?", "update_curr+0xe1/0x1f0", "[<ffffffff81065905>] ?", "enqueue_entity+0x125/0x410", "[<ffffffff810524e3>] ?", "set_next_buddy+0x43/0x50", "[<ffffffff810570e0>] ?", "check_preempt_wakeup+0x1c0/0x260", "[<ffffffff81065ceb>] ?", "enqueue_task_fair+0xfb/0x100", "[<ffffffff8105230c>] ?", "check_preempt_curr+0x7c/0x90", "[<ffffffff815135fe>] ?", "do_page_fault+0x3e/0xa0", "[<ffffffff815109b5>] ?", "page_fault+0x25/0x30", "[<ffffffff81056b19>] ?", "update_cfs_shares+0x29/0x170", "[<ffffffff81065363>] ?", "dequeue_entity+0x113/0x2e0", "[<ffffffff810664da>] ?", "dequeue_task_fair+0x6a/0x130", "[<ffffffff81055ebe>] ?", "dequeue_task+0x8e/0xb0", "[<ffffffff81055f03>] ?", "deactivate_task+0x23/0x30", "[<ffffffff8150dc99>] ?", "thread_return+0x127/0x76e", "[<ffffffff810e6e1e>] ?", "call_rcu+0xe/0x10", "[<ffffffff8107196f>] ?", "release_task+0x33f/0x4b0", "[<ffffffff81073837>] ?", "do_exit+0x5b7/0x870", "[<ffffffff81073b48>] ?", "do_group_exit+0x58/0xd0", "[<ffffffff81088e36>] ?", "get_signal_to_deliver+0x1f6/0x460", "[<ffffffff8100a265>] ?", "do_signal+0x75/0x800", "[<ffffffff810dc675>] ?", "__audit_syscall_exit+0x265/0x290", "[<ffffffff8100aa80>] ?", "do_notify_resume+0x90/0xc0", "[<ffffffff8100b341>] ?", "int_signal+0x12/0x17", "{noformat}", "What's happening is that CgroupsLCEResourcesHandler is attempting to delete the cgroup before all the tasks within the cgroup have exited (explained later).", "It tries every 20ms to remove the cgroup until successful, or a timeout (default 1 second) expires.", "Sometimes these attempts hit a race within the kernel where the last task has not completely finished tearing down, yet it is far enough down that the cgroup is able to be removed.", "This leaves a NULL pointer around which results in the panic.", "The kernel has been fixed and most recent distributions will have the fix.", "However, there are older kernel versions out there that would benefit from a simple workaround.", "The proposed workaround is to wait until the \"tasks\" file within the cgroup is empty, and then delay a small amount of time before attempting to delete the cgroup.", "One question is why are there still tasks in the cgroup?", "Don't have a complete answer here and some of the details may be slightly off, but do know the following: The processtree within a mapreduce  cgroup looks like \"bash -c\" -> \"java ...\"", "When map or reduce processing is complete, the AM is informed, who then informs the NM so that the container can be torn down.", "A SIGTERM is sent to the session (bash is session leader).", "bash is much quicker at exiting than everything else so it exits and its parent (container-executor) gets a SIGCHILD and starts cleaning up, this includes removing the cgroup which gets us into the race described above.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12683621/YARN-2809.patch", "against trunk revision 61a2510.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5934//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5934//console", "This message is automatically generated.", "refreshed patch to latest version of trunk", "upmerge to latest trunk", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12697032/YARN-2809-v2.patch", "against trunk revision 1425e3d.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6535//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6535//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6535//console", "This message is automatically generated.", "Fixed findbugs warnings in v3 of patch.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12697110/YARN-2809-v3.patch", "against trunk revision c1957fe.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6538//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6538//console", "This message is automatically generated.", "+1 lgtm.", "Will commit this early next week if there are no objections.", "Thanks, Nathan!", "I committed this to trunk and branch-2.", "FAILURE: Integrated in Hadoop-trunk-Commit #7063 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7063/])", "YARN-2809.", "Implement workaround for linux kernel panic when removing cgroup.", "Contributed by Nathan Roberts (jlowe: rev 3f5431a22fcef7e3eb9aceeefe324e5b7ac84049)", "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/util/CgroupsLCEResourcesHandler.java", "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/util/TestCgroupsLCEResourcesHandler.java", "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java", "hadoop-yarn-project/CHANGES.txt"], "SplitGT": [" If tasks file is empty, delete the cgroup."], "issueString": "Implement workaround for linux kernel panic when removing cgroup\nSome older versions of linux have a bug that can cause a kernel panic when the LCE attempts to remove a cgroup. It is a race condition so it's a bit rare but on a few thousand node cluster it can result in a couple of panics per day.\n\nThis is the commit that likely (haven't verified) fixes the problem in linux: https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/commit/?h=linux-2.6.39.y&id=068c5cc5ac7414a8e9eb7856b4bf3cc4d4744267\n\nDetails will be added in comments.\n\nStack trace:\n {noformat}\n[<ffffffff8150d4a8>] ? panic+0xa7/0x16f\n [<ffffffff815116d4>] ? oops_end+0xe4/0x100\n [<ffffffff81046bfb>] ? no_context+0xfb/0x260\n [<ffffffff81449058>] ? dev_hard_start_xmit+0x308/0x530\n [<ffffffff81046e85>] ? __bad_area_nosemaphore+0x125/0x1e0\n [<ffffffff812773a9>] ? cpumask_next_and+0x29/0x50\n [<ffffffff81046f53>] ? bad_area_nosemaphore+0x13/0x20\n [<ffffffff810476b1>] ? __do_page_fault+0x321/0x480\n [<ffffffff81056881>] ? update_curr+0xe1/0x1f0\n [<ffffffff81065905>] ? enqueue_entity+0x125/0x410\n [<ffffffff810524e3>] ? set_next_buddy+0x43/0x50\n [<ffffffff810570e0>] ? check_preempt_wakeup+0x1c0/0x260\n [<ffffffff81065ceb>] ? enqueue_task_fair+0xfb/0x100\n [<ffffffff8105230c>] ? check_preempt_curr+0x7c/0x90\n [<ffffffff815135fe>] ? do_page_fault+0x3e/0xa0\n [<ffffffff815109b5>] ? page_fault+0x25/0x30\n [<ffffffff81056b19>] ? update_cfs_shares+0x29/0x170\n [<ffffffff81065363>] ? dequeue_entity+0x113/0x2e0\n [<ffffffff810664da>] ? dequeue_task_fair+0x6a/0x130\n [<ffffffff81055ebe>] ? dequeue_task+0x8e/0xb0\n [<ffffffff81055f03>] ? deactivate_task+0x23/0x30\n [<ffffffff8150dc99>] ? thread_return+0x127/0x76e\n [<ffffffff810e6e1e>] ? call_rcu+0xe/0x10\n [<ffffffff8107196f>] ? release_task+0x33f/0x4b0\n [<ffffffff81073837>] ? do_exit+0x5b7/0x870\n [<ffffffff81073b48>] ? do_group_exit+0x58/0xd0\n [<ffffffff81088e36>] ? get_signal_to_deliver+0x1f6/0x460\n [<ffffffff8100a265>] ? do_signal+0x75/0x800\n [<ffffffff810dc675>] ? __audit_syscall_exit+0x265/0x290\n [<ffffffff8100aa80>] ? do_notify_resume+0x90/0xc0\n [<ffffffff8100b341>] ? int_signal+0x12/0x17\n{noformat}\nWhat's happening is that CgroupsLCEResourcesHandler is attempting to delete the cgroup before all the tasks within the cgroup have exited (explained later). It tries every 20ms to remove the cgroup until successful, or a timeout (default 1 second) expires. Sometimes these attempts hit a race within the kernel where the last task has not completely finished tearing down, yet it is far enough down that the cgroup is able to be removed. This leaves a NULL pointer around which results in the panic.\n\nThe kernel has been fixed and most recent distributions will have the fix. However, there are older kernel versions out there that would benefit from a simple workaround. The proposed workaround is to wait until the \"tasks\" file within the cgroup is empty, and then delay a small amount of time before attempting to delete the cgroup. \n\nOne question is why are there still tasks in the cgroup? Don't have a complete answer here and some of the details may be slightly off, but do know the following: The processtree within a mapreduce  cgroup looks like \"bash -c\" -> \"java ...\" \nWhen map or reduce processing is complete, the AM is informed, who then informs the NM so that the container can be torn down. A SIGTERM is sent to the session (bash is session leader). bash is much quicker at exiting than everything else so it exits and its parent (container-executor) gets a SIGCHILD and starts cleaning up, this includes removing the cgroup which gets us into the race described above. \n\n\n\n\n\n\n\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12683621/YARN-2809.patch\n  against trunk revision 61a2510.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/5934//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/5934//console\n\nThis message is automatically generated.\nrefreshed patch to latest version of trunk\nupmerge to latest trunk\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12697032/YARN-2809-v2.patch\n  against trunk revision 1425e3d.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/6535//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6535//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/6535//console\n\nThis message is automatically generated.\nFixed findbugs warnings in v3 of patch.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12697110/YARN-2809-v3.patch\n  against trunk revision c1957fe.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/6538//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/6538//console\n\nThis message is automatically generated.\n+1 lgtm.  Will commit this early next week if there are no objections.\nThanks, Nathan!  I committed this to trunk and branch-2.\nFAILURE: Integrated in Hadoop-trunk-Commit #7063 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7063/])\nYARN-2809. Implement workaround for linux kernel panic when removing cgroup. Contributed by Nathan Roberts (jlowe: rev 3f5431a22fcef7e3eb9aceeefe324e5b7ac84049)\n* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/util/CgroupsLCEResourcesHandler.java\n* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/util/TestCgroupsLCEResourcesHandler.java\n* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\n* hadoop-yarn-project/CHANGES.txt\n\n", "issueSearchSentences": ["The proposed workaround is to wait until the \"tasks\" file within the cgroup is empty, and then delay a small amount of time before attempting to delete the cgroup.", "What's happening is that CgroupsLCEResourcesHandler is attempting to delete the cgroup before all the tasks within the cgroup have exited (explained later).", "Sometimes these attempts hit a race within the kernel where the last task has not completely finished tearing down, yet it is far enough down that the cgroup is able to be removed.", "One question is why are there still tasks in the cgroup?", "Implement workaround for linux kernel panic when removing cgroup"], "issueSearchScores": [0.7528156042098999, 0.6660585403442383, 0.5981735587120056, 0.5838496685028076, 0.5814622640609741]}
{"aId": 39, "code": "private void triggerActiveLogRoll() {\n    LOG.info(\"Triggering log roll on remote NameNode \" + activeAddr);\n    try {\n      getActiveNodeProxy().rollEditLog();\n      lastRollTriggerTxId = lastLoadedTxnId;\n    } catch (IOException ioe) {\n      LOG.warn(\"Unable to trigger a roll of the active NN\", ioe);\n    }\n  }", "comment": " Trigger the active node to roll its logs.", "issueId": "HDFS-2737", "issueStringList": ["HA: Automatically trigger log rolls periodically on the active NN", "Currently, the edit log tailing process can only read finalized log segments.", "So, if the active NN is not rolling its logs periodically, the SBN will lag a lot.", "This also causes many datanode messages to be queued up in the PendingDatanodeMessage structure.", "To combat this, the active NN needs to roll its logs periodically -- perhaps based on a time threshold, or perhaps based on a number of transactions.", "I'm not sure yet whether it's better to have the NN roll on its own or to have the SBN ask the active NN to roll its logs.", "Is it worth considering supporting tailing non-finalized logs?", "A couple options here:", "1) Add a thread to the NN which rolls periodically (based on time or # txns)*", "This would be advantageous if we had some use cases for keeping edit log segments short even absent HA.", "The only case Aaron and I could brainstorm would be for backups, where it's a little easier to backup a finalized file compared to a rolling one.", "But we can satisfy this easily by adding a command line tool to trigger a roll, which a backup script can use.", "So it's not super compelling.", "2) Add a new thread to the SBN which makes an IPC to the active and asks it to roll periodically", "Advantage here is simplicity.", "3) Add some code to the EditLogTailer thread in the SBN which makes a call to the active NN to trigger a roll when necessary (eg when the PendingDatanodeMessage queue is too large, or it's been too long since it has read any edits).", "Advantage here is that the real motivation for the rolls is the EditLogTailer itself.", "We want to keep lag low (for fast recovery) and also keep the pending datanode queue small (to fit within memory bounds).", "By putting the trigger here, we can directly inspect those two variables, and trigger rolls when necessary.", "So I'm thinking option 3 is the best.", "bq.", "Is it worth considering supporting tailing non-finalized logs?", "Worth considering, but as I understand from Ivan and Jitendra, BK doesn't support this functionality yet.", "Since it's simpler to just cause frequent rolls, we may as well do it this way and also solve the problem for BK at the same time, IMO.", "I also support option 3 from above.", "I see little advantage to option 2, as it's only slightly simpler than 3 and we might as well reuse the EditLogTailer thread.", "Option 1 (rolling based on time or # txns) is really only a (potentially poor) proxy for #3, and doesn't seem much simpler.", "My preference is not to do trigger editlog rollover for the purpose of HA.", "When we undertook cleaning up editlog one of the things that was good about the design was, NN could roll editlogs independently of the backup/secondary/checkpointer.", "It became a local decision at the namenode.", "For HA requiring an external trigger for editlog rolling is going back to previous inflexibility.", "My preference is to read the editlog in progress and leave the rolling of editlog as it is today, a local NN decision.", "bq.", "My preference is to read the editlog in progress and leave the rolling of editlog as it is today, a local NN decision", "That's not how it is today -- the SecondaryNameNode is the one that triggers it.", "bq.", "For HA requiring an external trigger for editlog rolling is going back to previous inflexibility", "It's not inflexible, since it's not tightly interlocked.", "The HA SBN may trigger a roll, but it's also fine if the NN rolls more often than this due to other triggers.", "For the time being, it is okay not to read the editlog in progress and trigger editlog rolling using existing JournalProtocol mechanism.", "We should revisit this in the future to read editlog in progress.", "attaching preliminary patch since ATM was wanting to do some cluster testing which depends on this.", "Still need to finish some work on this, I forget if it's actually working or not :)", "Here's an update to Todd's last patch.", "Not much has changed.", "It's rebased on the tip of HDFS-1623, and has some cleanup, including:", "# Changing the config option units from milliseconds to seconds.", "# Improved logging in a few places.", "# s/interval/period/g in the new config options, to be consistent with the checkpoint options.", "# Roll the edit log before trying to tail the logs, which should keep the standby a little hotter.", "In addition to the unit test included in the patch, I also did some manual cluster testing, which worked very well.", "A few small notes:", "Can you add some javadocs to the new member variables of EditLogTailer?", "Not all of them are super obvious by their name", "The new test says \"roll every 300ms\" but the config is every 1 second.", "Maybe the test should also do a \"failover\" to the other node and make sure nn0 can trigger rolls on nn1 (not just nn1 triggering on nn0)?", "Add simple javadoc on {{getConfForOtherNode}}", "Thanks a lot for the review, Todd.", "Here's a patch which should address all your requests.", "The only thing I did a little differently was I added an extra test case for testing that NN1 is capable of rolling the edits of NN0, rather than triggering a failover in the same test case.", "This seemed a little bit easier to implement, and should improve test case isolation.", "Here's an updated patch which is identical to the previous patch, except it fixes the potential deadlock described in HDFS-2823.", "Missed two typos.", "+1", "Thanks a lot for the reviews, Todd.", "I've just committed this to the HA branch."], "SplitGT": [" Trigger the active node to roll its logs."], "issueString": "HA: Automatically trigger log rolls periodically on the active NN\nCurrently, the edit log tailing process can only read finalized log segments. So, if the active NN is not rolling its logs periodically, the SBN will lag a lot. This also causes many datanode messages to be queued up in the PendingDatanodeMessage structure.\n\nTo combat this, the active NN needs to roll its logs periodically -- perhaps based on a time threshold, or perhaps based on a number of transactions. I'm not sure yet whether it's better to have the NN roll on its own or to have the SBN ask the active NN to roll its logs.\nIs it worth considering supporting tailing non-finalized logs?\nA couple options here:\n\n*1) Add a thread to the NN which rolls periodically (based on time or # txns)*\n\nThis would be advantageous if we had some use cases for keeping edit log segments short even absent HA. The only case Aaron and I could brainstorm would be for backups, where it's a little easier to backup a finalized file compared to a rolling one. But we can satisfy this easily by adding a command line tool to trigger a roll, which a backup script can use. So it's not super compelling.\n\n2) Add a new thread to the SBN which makes an IPC to the active and asks it to roll periodically\n\nAdvantage here is simplicity.\n\n3) Add some code to the EditLogTailer thread in the SBN which makes a call to the active NN to trigger a roll when necessary (eg when the PendingDatanodeMessage queue is too large, or it's been too long since it has read any edits).\n\nAdvantage here is that the real motivation for the rolls is the EditLogTailer itself. We want to keep lag low (for fast recovery) and also keep the pending datanode queue small (to fit within memory bounds). By putting the trigger here, we can directly inspect those two variables, and trigger rolls when necessary.\n\nSo I'm thinking option 3 is the best.\nbq. Is it worth considering supporting tailing non-finalized logs?\n\nWorth considering, but as I understand from Ivan and Jitendra, BK doesn't support this functionality yet. Since it's simpler to just cause frequent rolls, we may as well do it this way and also solve the problem for BK at the same time, IMO.\nI also support option 3 from above. I see little advantage to option 2, as it's only slightly simpler than 3 and we might as well reuse the EditLogTailer thread. Option 1 (rolling based on time or # txns) is really only a (potentially poor) proxy for #3, and doesn't seem much simpler.\nMy preference is not to do trigger editlog rollover for the purpose of HA. When we undertook cleaning up editlog one of the things that was good about the design was, NN could roll editlogs independently of the backup/secondary/checkpointer. It became a local decision at the namenode. For HA requiring an external trigger for editlog rolling is going back to previous inflexibility. My preference is to read the editlog in progress and leave the rolling of editlog as it is today, a local NN decision.\nbq. My preference is to read the editlog in progress and leave the rolling of editlog as it is today, a local NN decision\n\nThat's not how it is today -- the SecondaryNameNode is the one that triggers it.\n\nbq. For HA requiring an external trigger for editlog rolling is going back to previous inflexibility\n\nIt's not inflexible, since it's not tightly interlocked. The HA SBN may trigger a roll, but it's also fine if the NN rolls more often than this due to other triggers.\nFor the time being, it is okay not to read the editlog in progress and trigger editlog rolling using existing JournalProtocol mechanism. We should revisit this in the future to read editlog in progress.\nattaching preliminary patch since ATM was wanting to do some cluster testing which depends on this. Still need to finish some work on this, I forget if it's actually working or not :)\nHere's an update to Todd's last patch. Not much has changed. It's rebased on the tip of HDFS-1623, and has some cleanup, including:\n\n# Changing the config option units from milliseconds to seconds.\n# Improved logging in a few places.\n# s/interval/period/g in the new config options, to be consistent with the checkpoint options.\n# Roll the edit log before trying to tail the logs, which should keep the standby a little hotter.\n\nIn addition to the unit test included in the patch, I also did some manual cluster testing, which worked very well.\nA few small notes:\n- Can you add some javadocs to the new member variables of EditLogTailer? Not all of them are super obvious by their name\n- The new test says \"roll every 300ms\" but the config is every 1 second.\n- Maybe the test should also do a \"failover\" to the other node and make sure nn0 can trigger rolls on nn1 (not just nn1 triggering on nn0)?\n- Add simple javadoc on {{getConfForOtherNode}}\nThanks a lot for the review, Todd. Here's a patch which should address all your requests. \n\nThe only thing I did a little differently was I added an extra test case for testing that NN1 is capable of rolling the edits of NN0, rather than triggering a failover in the same test case. This seemed a little bit easier to implement, and should improve test case isolation.\nHere's an updated patch which is identical to the previous patch, except it fixes the potential deadlock described in HDFS-2823.\nMissed two typos.\n+1\nThanks a lot for the reviews, Todd. I've just committed this to the HA branch.\n", "issueSearchSentences": ["HA: Automatically trigger log rolls periodically on the active NN", "I'm not sure yet whether it's better to have the NN roll on its own or to have the SBN ask the active NN to roll its logs.", "3) Add some code to the EditLogTailer thread in the SBN which makes a call to the active NN to trigger a roll when necessary (eg when the PendingDatanodeMessage queue is too large, or it's been too long since it has read any edits).", "To combat this, the active NN needs to roll its logs periodically -- perhaps based on a time threshold, or perhaps based on a number of transactions.", "For HA requiring an external trigger for editlog rolling is going back to previous inflexibility."], "issueSearchScores": [0.7898246049880981, 0.7326992750167847, 0.7150272130966187, 0.6941589117050171, 0.6742486953735352]}
{"aId": 40, "code": "private void checkAccessAcl(INode inode, int snapshotId, FsAction access,\n      FsPermission mode, List<AclEntry> featureEntries)\n      throws AccessControlException {\n    boolean foundMatch = false;\n\n    // Use owner entry from permission bits if user is owner.\n    if (user.equals(inode.getUserName(snapshotId))) {\n      if (mode.getUserAction().implies(access)) {\n        return;\n      }\n      foundMatch = true;\n    }\n\n    // Check named user and group entries if user was not denied by owner entry.\n    if (!foundMatch) {\n      for (AclEntry entry: featureEntries) {\n        if (entry.getScope() == AclEntryScope.DEFAULT) {\n          break;\n        }\n        AclEntryType type = entry.getType();\n        String name = entry.getName();\n        if (type == AclEntryType.USER) {\n          // Use named user entry with mask from permission bits applied if user\n          // matches name.\n          if (user.equals(name)) {\n            FsAction masked = entry.getPermission().and(mode.getGroupAction());\n            if (masked.implies(access)) {\n              return;\n            }\n            foundMatch = true;\n          }\n        } else if (type == AclEntryType.GROUP) {\n          // Use group entry (unnamed or named) with mask from permission bits\n          // applied if user is a member and entry grants access.  If user is a\n          // member of multiple groups that have entries that grant access, then\n          // it doesn't matter which is chosen, so exit early after first match.\n          String group = name == null ? inode.getGroupName(snapshotId) : name;\n          if (groups.contains(group)) {\n            FsAction masked = entry.getPermission().and(mode.getGroupAction());\n            if (masked.implies(access)) {\n              return;\n            }\n            foundMatch = true;\n          }\n        }\n      }\n    }\n\n    // Use other entry if user was not denied by an earlier match.\n    if (!foundMatch && mode.getOtherAction().implies(access)) {\n      return;\n    }\n\n    throw new AccessControlException(\n      toAccessControlString(inode, snapshotId, access, mode, featureEntries));\n  }", "comment": " - There is exactly one each of the unnamed user/group/other entries. - The mask entry must not have a name. - The other entry must not have a name.", "issueId": "HDFS-5612", "issueStringList": ["NameNode: change all permission checks to enforce ACLs in addition to permissions.", "All {{NameNode}} code paths that enforce permissions must be updated so that they also enforce ACLs.", "This patch implements the logic for enforcing permissions defined in ACLs.", "This is a new code path in {{FSPermissionChecker}} to check permissions based on either {{FsPermission}} bits (existing logic, unchanged) or an {{AclEntry}} list, if defined on the inode.", "While I was in here, I also fixed a very minor bug that I noticed.", "The permission enforcement can run against permissions defined on a snapshot inode, but the string in the exception created by {{FSPermissionChecker#toAccessControlString}} wasn't using the snapshot inode.", "This wouldn't break any permission enforcement logic, but it could potentially make the exception messages confusing.", "I've added new tests in {{TestFSPermissionChecker}}.", "I manually validated the behavior asserted by these tests against Linux setfacl.", "The tests cover the new code path at nearly 100%.", "Additionally, I ran a sampling of other HDFS tests related to existing permissions logic, and I didn't see any failures.", "(We do have a problem with {{TestOfflineEditsViewer}} and {{TestOfflineImageViewer}} on the HDFS-4685 branch right now, but it's a known problem and it's unrelated.)", "The test has some helper methods that are duplicated from my HDFS-5673 patch.", "After HDFS-5673 gets +1'd and I commit it, I plan to come back here and refactor those helper methods to a shared {{AclTestHelpers}} class.", "I'm attaching patch version 2 with the following changes:", "# Rebased against current HDFS-4685 branch, which is up to date with trunk since yesterday.", "# Corrected comment about sorting on {{FSPermissionChecker#checkAcl}} in reaction to recent changes on the finalized HDFS-5673 patch.", "# Refactored several common methods to {{AclTestHelpers}}.", "Can you specify the invariants (i.e., the correctness conditions) of a valid list of AclEntry?", "I think it is important to document them as {{checkAcl}} depend on these invariants.", "It seems that the following invariants hold for a valid list of AclEntry:", "# The list has to be sorted.", "# Each entry in the list is unique.", "# Default entries do not have names.", "# There is at least one user / group / other entry does not have a name.", "(Why?)", "I guess it is not immediately clear to me what is the semantic of the name of an entry.", "Can you please explain?", "Sure thing.", "Here is a list of the invariants.", "I'll also fold this list into the comments in a new patch later.", "# The list must be sorted.", "# Each entry in the list is unique.", "# There is exactly one each of the unnamed user / group / other entries.", "These entries are identical to the classic owner / group / other permissions encoded in permission bits today.", "The ACL enforcement algorithm states that owner permissions trump named user permissions.", "This becomes important if the file owner also has a named user entry in the ACL.", "Assume the file owner is haohui, and the owner permissions are rw-, but there is also a named user entry for user:haohui:r--.", "In this case, the owner entry must take precedence over the named user entry so that you get read-write access.", "Additionally, the effective permissions granted to a user through groups must include the permissions of the file's group (if the user is a member).", "# The mask entry, if present, must not have a name.", "(The name would be meaningless.)", "# The owner entry must not have a name.", "(The name would be meaningless.)", "# There may be any number of named user entries.", "These entries are used if the username is a specific match (assuming the user is not the owner as discussed above).", "# There may be any number of named group entries.", "Assuming the user is not the owner, and there is no named user entry matching that user, and the user is a member of at least one named group or the file's group, then the user's effective permissions are the union of permissions for all such groups in which the user is a member.", "# Default entries are ignored during permission enforcement.", "Regarding default entries, these are not used during permission enforcement at all, so there really are no invariants related to the default ACL within the context of {{checkAcl}}.", "However, the default ACL on a directory will be copied to the access ACL of its newly created child inodes.", "Since the default ACL eventually becomes an access ACL for a different inode, we can say that the same set of invariants must hold for the default ACL entries.", "(Otherwise, we'd have a violation of invariants later when it comes time to run {{checkAcl}} on that child inode.)", "I'm going to put this patch on hold for now.", "In a separate patch, I'm exploring the possibility of mapping the ACL owner/mask/other entries into the owner/group/other permission bits.", "(Things like chmod g+w on a file with an ACL result in setting the mask entry.)", "Currently, my approach is to keep all of these entries in the ACL itself and then add some code to keep their values consistent with the permission bits.", "If instead I use the group permission bits as the only location for the mask entry, then we get some potential benefits.", "There is less impact to existing code (likely no need to change {{setPermission}} at all, because the existing implementation will just do the right thing).", "There are fewer opportunities for bugs related to keeping the 2 data sources in sync.", "We also could see a minor reduction of memory utilization by storing fewer elements into the {{AclFeature}}, also resulting in greater likelihood of de-duplication when we do the Global ACL Set patch.", "If we do this, then I expect a bit more complexity in {{FSPermissionChecker}} as a consequence.", "The basic logic would remain the same, but {{checkAcl}} would need to inspect both the permission bits and the ACL entry list instead of just the ACL entry list.", "I want to understand that complexity trade-off better, so while I explore that, let's put this code review on hold.", "I'm attaching version 3 of this patch.", "This version has been updated in reaction to the recent changes on HDFS-5758.", "{{FSPermissionChecker}} has been updated to pull the relevant pieces of the whole logical ACL from either {{FsPermission}} or the list of {{AclEntry}}.", "I've also added comments to document the invariants described earlier.", "Overall, the HDFS-5758 changes didn't add that much complexity to {{FSPermissionChecker}}, so I'm satisfied with the end result.", "The patch looks good.", "nit: you might be able to simplify the code a little bit by consolidating the if branches in {{checkAccessAcl}}.", "Thanks, Haohui.", "Here is patch version 4.", "I was only able to consolidate conditionals for the final check on the other perms.", "For the other checks, we may need to set the {{foundMatch}} flag, so it's easier to express this with multiple nested ifs.", "+1.", "I'll commit it shortly.", "Thanks for the review, Haohui.", "Just to clarify, this patch is dependent on HDFS-5758.", "I expect to post the full patch for HDFS-5758 no later than tomorrow, so Haohui and I are targeting tomorrow to commit both HDFS-5758 and HDFS-5612.", "Uploading patch version 5 for a small rebase.", "I've committed in the branch.", "Thanks Chris for the contribution."], "SplitGT": [" - There is exactly one each of the unnamed user/group/other entries.", "- The mask entry must not have a name.", "- The other entry must not have a name."], "issueString": "NameNode: change all permission checks to enforce ACLs in addition to permissions.\nAll {{NameNode}} code paths that enforce permissions must be updated so that they also enforce ACLs.\nThis patch implements the logic for enforcing permissions defined in ACLs.  This is a new code path in {{FSPermissionChecker}} to check permissions based on either {{FsPermission}} bits (existing logic, unchanged) or an {{AclEntry}} list, if defined on the inode.  While I was in here, I also fixed a very minor bug that I noticed.  The permission enforcement can run against permissions defined on a snapshot inode, but the string in the exception created by {{FSPermissionChecker#toAccessControlString}} wasn't using the snapshot inode.  This wouldn't break any permission enforcement logic, but it could potentially make the exception messages confusing.\n\nI've added new tests in {{TestFSPermissionChecker}}.  I manually validated the behavior asserted by these tests against Linux setfacl.  The tests cover the new code path at nearly 100%.  Additionally, I ran a sampling of other HDFS tests related to existing permissions logic, and I didn't see any failures.  (We do have a problem with {{TestOfflineEditsViewer}} and {{TestOfflineImageViewer}} on the HDFS-4685 branch right now, but it's a known problem and it's unrelated.)\n\nThe test has some helper methods that are duplicated from my HDFS-5673 patch.  After HDFS-5673 gets +1'd and I commit it, I plan to come back here and refactor those helper methods to a shared {{AclTestHelpers}} class.\n\nI'm attaching patch version 2 with the following changes:\n# Rebased against current HDFS-4685 branch, which is up to date with trunk since yesterday.\n# Corrected comment about sorting on {{FSPermissionChecker#checkAcl}} in reaction to recent changes on the finalized HDFS-5673 patch.\n# Refactored several common methods to {{AclTestHelpers}}.\n\nCan you specify the invariants (i.e., the correctness conditions) of a valid list of AclEntry? I think it is important to document them as {{checkAcl}} depend on these invariants.\n\nIt seems that the following invariants hold for a valid list of AclEntry:\n\n# The list has to be sorted. \n# Each entry in the list is unique.\n# Default entries do not have names.\n# There is at least one user / group / other entry does not have a name. (Why?)\n\nI guess it is not immediately clear to me what is the semantic of the name of an entry. Can you please explain?\nSure thing.  Here is a list of the invariants.  I'll also fold this list into the comments in a new patch later.\n# The list must be sorted.\n# Each entry in the list is unique.\n# There is exactly one each of the unnamed user / group / other entries.  These entries are identical to the classic owner / group / other permissions encoded in permission bits today.  The ACL enforcement algorithm states that owner permissions trump named user permissions.  This becomes important if the file owner also has a named user entry in the ACL.  Assume the file owner is haohui, and the owner permissions are rw-, but there is also a named user entry for user:haohui:r--.  In this case, the owner entry must take precedence over the named user entry so that you get read-write access.  Additionally, the effective permissions granted to a user through groups must include the permissions of the file's group (if the user is a member).\n# The mask entry, if present, must not have a name.  (The name would be meaningless.)\n# The owner entry must not have a name.  (The name would be meaningless.)\n# There may be any number of named user entries.  These entries are used if the username is a specific match (assuming the user is not the owner as discussed above).\n# There may be any number of named group entries.  Assuming the user is not the owner, and there is no named user entry matching that user, and the user is a member of at least one named group or the file's group, then the user's effective permissions are the union of permissions for all such groups in which the user is a member.\n# Default entries are ignored during permission enforcement.\n\nRegarding default entries, these are not used during permission enforcement at all, so there really are no invariants related to the default ACL within the context of {{checkAcl}}.  However, the default ACL on a directory will be copied to the access ACL of its newly created child inodes.  Since the default ACL eventually becomes an access ACL for a different inode, we can say that the same set of invariants must hold for the default ACL entries.  (Otherwise, we'd have a violation of invariants later when it comes time to run {{checkAcl}} on that child inode.)\n\nI'm going to put this patch on hold for now.\n\nIn a separate patch, I'm exploring the possibility of mapping the ACL owner/mask/other entries into the owner/group/other permission bits.  (Things like chmod g+w on a file with an ACL result in setting the mask entry.)  Currently, my approach is to keep all of these entries in the ACL itself and then add some code to keep their values consistent with the permission bits.  If instead I use the group permission bits as the only location for the mask entry, then we get some potential benefits.  There is less impact to existing code (likely no need to change {{setPermission}} at all, because the existing implementation will just do the right thing).  There are fewer opportunities for bugs related to keeping the 2 data sources in sync.  We also could see a minor reduction of memory utilization by storing fewer elements into the {{AclFeature}}, also resulting in greater likelihood of de-duplication when we do the Global ACL Set patch.\n\nIf we do this, then I expect a bit more complexity in {{FSPermissionChecker}} as a consequence.  The basic logic would remain the same, but {{checkAcl}} would need to inspect both the permission bits and the ACL entry list instead of just the ACL entry list.  I want to understand that complexity trade-off better, so while I explore that, let's put this code review on hold.\nI'm attaching version 3 of this patch.  This version has been updated in reaction to the recent changes on HDFS-5758.  {{FSPermissionChecker}} has been updated to pull the relevant pieces of the whole logical ACL from either {{FsPermission}} or the list of {{AclEntry}}.  I've also added comments to document the invariants described earlier.\n\nOverall, the HDFS-5758 changes didn't add that much complexity to {{FSPermissionChecker}}, so I'm satisfied with the end result.\nThe patch looks good. nit: you might be able to simplify the code a little bit by consolidating the if branches in {{checkAccessAcl}}.\nThanks, Haohui.  Here is patch version 4.  I was only able to consolidate conditionals for the final check on the other perms.  For the other checks, we may need to set the {{foundMatch}} flag, so it's easier to express this with multiple nested ifs.\n+1. I'll commit it shortly.\nThanks for the review, Haohui.\n\nJust to clarify, this patch is dependent on HDFS-5758.  I expect to post the full patch for HDFS-5758 no later than tomorrow, so Haohui and I are targeting tomorrow to commit both HDFS-5758 and HDFS-5612.\nUploading patch version 5 for a small rebase.\nI've committed in the branch. Thanks Chris for the contribution.\n", "issueSearchSentences": ["This is a new code path in {{FSPermissionChecker}} to check permissions based on either {{FsPermission}} bits (existing logic, unchanged) or an {{AclEntry}} list, if defined on the inode.", "The basic logic would remain the same, but {{checkAcl}} would need to inspect both the permission bits and the ACL entry list instead of just the ACL entry list.", "The ACL enforcement algorithm states that owner permissions trump named user permissions.", "{{FSPermissionChecker}} has been updated to pull the relevant pieces of the whole logical ACL from either {{FsPermission}} or the list of {{AclEntry}}.", "The permission enforcement can run against permissions defined on a snapshot inode, but the string in the exception created by {{FSPermissionChecker#toAccessControlString}} wasn't using the snapshot inode."], "issueSearchScores": [0.6923432946205139, 0.662948489189148, 0.6186299324035645, 0.6037165522575378, 0.6000500321388245]}
{"aId": 41, "code": "public void allowSnapshot(String snapshotRoot)\n      throws IOException {\n    dfs.allowSnapshot(snapshotRoot);\n  }", "comment": " Allow snapshot on a directory.", "issueId": "HDFS-4084", "issueStringList": ["provide CLI support for allow and disallow snapshot on a directory", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "@Brandon", "Can we make the new commands case insensitive?", "We can log a different jira to make existing commands also case insensitive.", "Good point, Arpit.", "Will change it and upload a new patch.", "Re-based the patch and addressed Arpit's comment.", "Comments:", "# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods", "{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "{quote}", "Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "Added javadoc for them.", "{quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "{quote}", "Removed, realized these two methods need to be public anyway.", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote}", "Yes.", "removed.", "{quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote}", "Done.", "Thanks.", "I committed the patch to HDFS-2802 branch.", "Thank you Brandon."], "SplitGT": [" Allow snapshot on a directory."], "issueString": "provide CLI support for allow and disallow snapshot on a directory\nTo provide CLI support to allow snapshot, disallow snapshot on a directory.\n@Brandon\n\nCan we make the new commands case insensitive? We can log a different jira to make existing commands also case insensitive. \nGood point, Arpit. Will change it and upload a new patch. \nRe-based the patch and addressed Arpit's comment.\nComments:\n# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.\n# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.\n# FSNamesystem.java is unnecessarily importing SnapshotInfo?\n# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods\n\n{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.{quote} \nMethods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override. Added javadoc for them.\n   {quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.{quote} \nRemoved, realized these two methods need to be public anyway.\n    {quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?{quote} \nYes. removed.\n    {quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote} \nDone. Thanks.\n\nI committed the patch to HDFS-2802 branch. Thank you Brandon.\n", "issueSearchSentences": ["Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "provide CLI support for allow and disallow snapshot on a directory", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?"], "issueSearchScores": [0.6734956502914429, 0.5849887132644653, 0.5247311592102051, 0.4808589518070221, 0.47358882427215576]}
{"aId": 42, "code": "private void startCheckDiskErrorThread() {\n    checkDiskErrorThread = new Thread(new Runnable() {\n          @Override\n          public void run() {\n            while(shouldRun) {\n              boolean tempFlag ;\n              synchronized(checkDiskErrorMutex) {\n                tempFlag = checkDiskErrorFlag;\n                checkDiskErrorFlag = false;\n              }\n              if(tempFlag) {\n                try {\n                  data.checkDataDir();\n                } catch (DiskErrorException de) {\n                  handleDiskError(de.getMessage());\n                } catch (Exception e) {\n                  LOG.warn(\"Unexpected exception occurred while checking disk error  \" + e);\n                  checkDiskErrorThread = null;\n                  return;\n                }\n                synchronized(checkDiskErrorMutex) {\n                  lastDiskErrorCheck = System.currentTimeMillis();\n                }\n              }\n              try {\n                Thread.sleep(checkDiskErrorInterval);\n              } catch (InterruptedException e) {\n                LOG.debug(\"InterruptedException in check disk error thread\", e);\n                checkDiskErrorThread = null;\n                return;\n              }\n            }\n          }\n    });\n  }", "comment": " Starts a new thread which will check for disk error check request every 5 sec", "issueId": "HDFS-5522", "issueStringList": ["Datanode disk error check may be incorrectly skipped", "After HDFS-4581 and HDFS-4699, {{checkDiskError()}} is not called when network errors occur during processing data node requests.", "This appears to create problems when a disk is having problems, but not failing I/O soon.", "If I/O hangs for a long time, network read/write may timeout first and the peer may close the connection.", "Although the error was caused by a faulty local disk, disk check is not being carried out in this case.", "One option will be to move the check to a new or existing thread and decouple checks from request handling.", "In this patch, I have created new thread which will check for disk errors when there is request fro disk error check every 5 seconds.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643378/HDFS-5522.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6817//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6817//console", "This message is automatically generated.", "Broke testcase:", "Attached a new patch fixing the broken test case", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643801/HDFS-5522-v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.namenode.TestNameNodeRespectsBindHostKeys", "org.apache.hadoop.TestRefreshCallQueue", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6846//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6846//console", "This message is automatically generated.", "It looks like both test failures are due to the flaw in MiniDFSCluster and its usage.", "I will file a jira for it.", "The patch looks okay, except few minor cosmetic issues.", "Move the second {{catch()}} next to  the \"\\}\" in the previous line.", "There is one more like this.", "Typo, \"occured\".", "It will be nice if start and termination are logged.", "For start, log it outside the synchronized block after staring the thread.", "If the disk containing the file system where log is stored is failing, the logging action may hang.", "We want disk check to start before this happens.", "A possible future improvement will be to make the disk check ({{mkdir()}}) to timeout.", "This will require another thread to monitor the disk check activity and interrupt the disk check thread if a check is taking too long.", "Since I/O timeout is usually very long (e.g.", "15-20 minutes), the common I/O hang condition can make a lot of user requests timeout/fail before a faulty disk is marked bad.", "With the timeout/interrupt feature, the fault detection latency can be made shorter.", "This is beyond the scope of this jira, of course.", "Will work on Kihwaal comments and post a new patch", "Thanks Kihwal for your comments.", "I incorporated your first and second comment.", "For the third comment, I agree that start and termination of checkDiskError Thread should be logged.", "But for your suggestion to place outside synchronized block, I think that will not be good place since it will log whenever checkDiskError() is called.", "So I logged inside the synchronized block.", "Let me know if you have more comments.", "Submitting a new patch incorporating Kihwals comments.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12644147/HDFS-5522-v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6869//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6869//console", "This message is automatically generated.", "+ 1 lgtm", "Thanks for working on this, Rushabh.", "I've committed this to branch-2 and trunk.", "This feature makes disk check asynchronous, so handlers(DataXceiver) won't get blocked while checking disks.", "FAILURE: Integrated in Hadoop-trunk-Commit #5604 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5604/])", "HDFS-5522.", "Datanode disk error check may be incorrectly skipped.", "Contributed by Rushabh Shah.", "(kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594055)", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java"], "SplitGT": [" Starts a new thread which will check for disk error check request every 5 sec"], "issueString": "Datanode disk error check may be incorrectly skipped\nAfter HDFS-4581 and HDFS-4699, {{checkDiskError()}} is not called when network errors occur during processing data node requests.  This appears to create problems when a disk is having problems, but not failing I/O soon. \n\nIf I/O hangs for a long time, network read/write may timeout first and the peer may close the connection. Although the error was caused by a faulty local disk, disk check is not being carried out in this case. \nOne option will be to move the check to a new or existing thread and decouple checks from request handling.\nIn this patch, I have created new thread which will check for disk errors when there is request fro disk error check every 5 seconds.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643378/HDFS-5522.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6817//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6817//console\n\nThis message is automatically generated.\nBroke testcase: \nAttached a new patch fixing the broken test case\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643801/HDFS-5522-v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.TestNameNodeRespectsBindHostKeys\n                  org.apache.hadoop.TestRefreshCallQueue\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6846//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6846//console\n\nThis message is automatically generated.\nIt looks like both test failures are due to the flaw in MiniDFSCluster and its usage. I will file a jira for it.\nThe patch looks okay, except few minor cosmetic issues. \n* Move the second {{catch()}} next to  the \"\\}\" in the previous line. There is one more like this.  \n* Typo, \"occured\".\n* It will be nice if start and termination are logged. For start, log it outside the synchronized block after staring the thread. If the disk containing the file system where log is stored is failing, the logging action may hang. We want disk check to start before this happens.\n\nA possible future improvement will be to make the disk check ({{mkdir()}}) to timeout. This will require another thread to monitor the disk check activity and interrupt the disk check thread if a check is taking too long.  Since I/O timeout is usually very long (e.g. 15-20 minutes), the common I/O hang condition can make a lot of user requests timeout/fail before a faulty disk is marked bad.  With the timeout/interrupt feature, the fault detection latency can be made shorter.  This is beyond the scope of this jira, of course.\nWill work on Kihwaal comments and post a new patch\nThanks Kihwal for your comments.\nI incorporated your first and second comment.\nFor the third comment, I agree that start and termination of checkDiskError Thread should be logged. But for your suggestion to place outside synchronized block, I think that will not be good place since it will log whenever checkDiskError() is called.\nSo I logged inside the synchronized block.\nLet me know if you have more comments.\nSubmitting a new patch incorporating Kihwals comments.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12644147/HDFS-5522-v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6869//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6869//console\n\nThis message is automatically generated.\n+ 1 lgtm\nThanks for working on this, Rushabh. I've committed this to branch-2 and trunk.  This feature makes disk check asynchronous, so handlers(DataXceiver) won't get blocked while checking disks.  \nFAILURE: Integrated in Hadoop-trunk-Commit #5604 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5604/])\nHDFS-5522. Datanode disk error check may be incorrectly skipped. Contributed by Rushabh Shah. (kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594055)\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java\n\n", "issueSearchSentences": ["For the third comment, I agree that start and termination of checkDiskError Thread should be logged.", "In this patch, I have created new thread which will check for disk errors when there is request fro disk error check every 5 seconds.", "This will require another thread to monitor the disk check activity and interrupt the disk check thread if a check is taking too long.", "We want disk check to start before this happens.", "But for your suggestion to place outside synchronized block, I think that will not be good place since it will log whenever checkDiskError() is called."], "issueSearchScores": [0.845673143863678, 0.7668768167495728, 0.7489129304885864, 0.7301310300827026, 0.6630809903144836]}
{"aId": 43, "code": "public void addResource(InputStream in, String name) {\n    addResourceObject(new Resource(in, name));\n  }", "comment": " Add a configuration resource.", "issueId": "HADOOP-8525", "issueStringList": ["Provide Improved Traceability for Configuration", "Configuration provides basic traceability to see where a config setting came from, but once the configuration is written out that information is written to a comment in the XML and then lost the next time the configuration is read back in.", "It would really be great to be able to store a complete history of where the config came from in the XML, so that it can then be retrieved later for debugging.", "I think the only big change here is a small addition to the XML format, so that we can add in a list of previous files it was read from and remove the comment that more or less gives the same information.", "Internally the Map storing this into will have to now point to a list instead of a single value.", "It looks fairly straight forward.", "This adds in the traceability.", "I have tested this on a small cluster.", "Canceling patch after trying to use the results as part of MAPREDUCE-4375.", "It is working as designed, but with the history server we are getting a lot of {noformat}org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream@129b073{noformat} so we need to add in a way to include a name along with the InputStream when adding in a resource.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12533538/HADOOP-8525.txt", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 1 new or modified test files.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 javadoc.", "The javadoc tool appears to have generated 2 warning messages.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.fs.viewfs.TestViewFsTrash", "org.apache.hadoop.ha.TestZKFailoverController", "org.apache.hadoop.io.file.tfile.TestTFileByteArrays", "org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1142//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1142//console", "This message is automatically generated.", "Overall this looks pretty good!", "One comment regarding completeness: I noticed Configuration being a Writable.", "I've not really seen a use of Configuration being used as Writable, but perhaps the readFields and write methods need to be updated too, without breaking compatibility?", "Or we may document that serialization via Writable will not include sources of configs.", "Other than this, +1.", "Yes that would make since to update the Writable methods.", "I remember looking and they are not really used, but they are still there.", "I have updated the code so that the Writeable part maintains the sources.", "The source can be provided when setting a config, or when adding in a stream resource.", "I also updated the GenericOptionsParser to also set the source as command line for those configs that are set on the command line.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12533874/HADOOP-8525.txt", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 1 new or modified test files.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.ha.TestZKFailoverController", "org.apache.hadoop.io.file.tfile.TestTFileByteArrays", "org.apache.hadoop.conf.TestConfServlet", "org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1149//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1149//console", "This message is automatically generated.", "Canecling patch to address test failures", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12535438/HADOOP-8525.txt", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified test files.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1179//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1179//console", "This message is automatically generated.", "+1, the patch looks good.", "Just applied this and tried it out, and the results look good too :)", "Integrated in Hadoop-Common-trunk-Commit #2438 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2438/])", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359777)", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359775)", "Result = SUCCESS", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2506 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2506/])", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359777)", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359775)", "Result = SUCCESS", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2456 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2456/])", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359777)", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359775)", "Result = FAILURE", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2508 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2508/])", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359777)", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359775)", "Result = SUCCESS", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Common-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2441/])", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359777)", "HADOOP-8525.", "Provide Improved Traceability for Configuration (bobby) (Revision 1359775)", "Result = SUCCESS", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java"], "SplitGT": [" Add a configuration resource."], "issueString": "Provide Improved Traceability for Configuration\nConfiguration provides basic traceability to see where a config setting came from, but once the configuration is written out that information is written to a comment in the XML and then lost the next time the configuration is read back in.  It would really be great to be able to store a complete history of where the config came from in the XML, so that it can then be retrieved later for debugging.\nI think the only big change here is a small addition to the XML format, so that we can add in a list of previous files it was read from and remove the comment that more or less gives the same information.  Internally the Map storing this into will have to now point to a list instead of a single value.  It looks fairly straight forward.  \nThis adds in the traceability.  I have tested this on a small cluster.\nCanceling patch after trying to use the results as part of MAPREDUCE-4375.  It is working as designed, but with the history server we are getting a lot of {noformat}org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream@129b073{noformat} so we need to add in a way to include a name along with the InputStream when adding in a resource.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12533538/HADOOP-8525.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.fs.viewfs.TestViewFsTrash\n                  org.apache.hadoop.ha.TestZKFailoverController\n                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays\n                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1142//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1142//console\n\nThis message is automatically generated.\nOverall this looks pretty good!\n\nOne comment regarding completeness: I noticed Configuration being a Writable. I've not really seen a use of Configuration being used as Writable, but perhaps the readFields and write methods need to be updated too, without breaking compatibility? Or we may document that serialization via Writable will not include sources of configs.\n\nOther than this, +1.\nYes that would make since to update the Writable methods.  I remember looking and they are not really used, but they are still there.\nI have updated the code so that the Writeable part maintains the sources.  The source can be provided when setting a config, or when adding in a stream resource.  I also updated the GenericOptionsParser to also set the source as command line for those configs that are set on the command line.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12533874/HADOOP-8525.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.ha.TestZKFailoverController\n                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays\n                  org.apache.hadoop.conf.TestConfServlet\n                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1149//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1149//console\n\nThis message is automatically generated.\nCanecling patch to address test failures\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12535438/HADOOP-8525.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1179//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1179//console\n\nThis message is automatically generated.\n+1, the patch looks good. Just applied this and tried it out, and the results look good too :)\nIntegrated in Hadoop-Common-trunk-Commit #2438 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2438/])\n    HADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359777)\nHADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359775)\n\n     Result = SUCCESS\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java\n\nIntegrated in Hadoop-Hdfs-trunk-Commit #2506 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2506/])\n    HADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359777)\nHADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359775)\n\n     Result = SUCCESS\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java\n\nIntegrated in Hadoop-Mapreduce-trunk-Commit #2456 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2456/])\n    HADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359777)\nHADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359775)\n\n     Result = FAILURE\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java\n\nIntegrated in Hadoop-Hdfs-trunk-Commit #2508 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2508/])\n    HADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359777)\nHADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359775)\n\n     Result = SUCCESS\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java\n\nIntegrated in Hadoop-Common-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2441/])\n    HADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359777)\nHADOOP-8525. Provide Improved Traceability for Configuration (bobby) (Revision 1359775)\n\n     Result = SUCCESS\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359777\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n\nbobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359775\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java\n\n", "issueSearchSentences": ["The source can be provided when setting a config, or when adding in a stream resource.", "It is working as designed, but with the history server we are getting a lot of {noformat}org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream@129b073{noformat} so we need to add in a way to include a name along with the InputStream when adding in a resource.", "This adds in the traceability.", "Yes that would make since to update the Writable methods.", "Internally the Map storing this into will have to now point to a list instead of a single value."], "issueSearchScores": [0.5476548075675964, 0.4739944338798523, 0.280784010887146, 0.2663533091545105, 0.23580044507980347]}
{"aId": 44, "code": "public static String getHostFromPrincipal(String principalName) {\n    return new KerberosName(principalName).getHostName();\n  }", "comment": " Get the host name from the principal name of format <service>/host@realm.", "issueId": "HADOOP-7215", "issueStringList": ["RPC clients must connect over a network interface corresponding to the host name in the client's kerberos principal key", "HADOOP-7104 introduced a change where RPC server matches client's hostname with the hostname specified in the client's Kerberos principal name.", "RPC client binds the socket to a random local address, which might not match the hostname specified in the principal name.", "This results authorization failure of the client at the server.", "Given that the server validate's the host name of the client's connection with the host name in the client's prinicipal name of format <service>/_HOST@realm, I propose binding the client's socket to the address specified in the principal name.", "Path for 203 release.", "I don't think this is quite right.", "It works when the IPC client is a server with a principal like hdfs/dn1.foo.com@REALM.", "But two-part principals are also used for other cases, eg tlipcon/admin@REALM.", "In this patch, won't that try to bind the local socket of my IPC client to the non-host \"admin\"?", "Patch for the trunk, with minor modifications and tests.", "HDFS-1704 for some of the protocols such where clientPrincipal is needed, uses the second part as host name for validation.", "For such protocols, you cannot use a principal name that you mentioned.", "This does not affect protocols such as ClientProtocol though.", "I mean HDFS-7104 in my previous comment.", "I see what you mean now.", "We could check if the second part is _HOST in the key and try binding to local address.", "Let me see if that could be done.", "Here is a way to do that at the RPC client:", "# get part2 from principal name of format <part1>/<part2>@realm and ensure part2 is a host name.", "# the address corresponding to this host name belongs to one of the local network interfaces on that host.", "If the above two conditions are satisfied, the bind to the socket to that address, before making rpc calls.", "It should address the following cases:", "# Principal name is not of format <part1>/<part2>@realm", "# part2 is not a valid host name", "# part2 is a valid host name, but not that of the client host", "Does this sound reasonable?", "Hi Suresh.", "This seems like a reasonable solution.", "Maybe we can also check the kerberos info on the protocol interface, and if there's no clientPrincipal set, we can skip all of this?", "My thinking is that we want to avoid the reverse DNS lookups for the common case of user->cluster communications where no clientPrincipal annotation is present.", "In that case, we have two choices:", "# Fail at the client side:", "#* If the client principal name does not have <part1>/<part2>@realm format, fail it at the client with appropriate error.", "#* If the format is right, treat part2 as host name.", "Just try to bind to it and if bind fails, then the failure occurs at the client it self with appropriate error.", "# Fail at the server side:", "#* If the client principal name does not have <part1>/<part2>@realm format, bind to any local address for the request.", "#* If the format is right, treat part2 as host name.", "If host name is a valid local address, bind to it else bind to any local address.", "This request will be rejected by the server.", "I am leaning towards (2) because, server is rightly involved in the decision of rejecting the client.", "It provides a record of this at both the client and the server.", "This will help debugging on the server side, independent of client.", "Attached patch with changes from option(2) from my previous comment.", "I also cleaned up NetUtils.java warnings related to javadoc.", "updated patch", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475153/HADOOP-7215.1.trunk.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.net.TestNetUtils", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//console", "This message is automatically generated.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475153/HADOOP-7215.1.trunk.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.net.TestNetUtils", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//console", "This message is automatically generated.", "For some reason one of the test I am adding fails in hudson test, but works fine on my machine.", "Uploading a debug version of the patch to understand the problem.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475156/HADOOP-7215.debug.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//console", "This message is automatically generated.", "With debug log the test did not fail!", "Trying a new patch again.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475157/HADOOP-7215.2.trunk.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.net.TestNetUtils", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//console", "This message is automatically generated.", "Patch without debug fails.", "Trying patch with debug again...", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475162/HADOOP-7215.debug.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.net.TestNetUtils", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//console", "This message is automatically generated.", "The reason tests are failing is: local host maps to 127.0.1.1 with no interface configured for it.", "Looks like the other tests use 127.0.0.1 for local host.", "Doing the same for the newly added test.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475167/HADOOP-7215.3.trunk.patch", "against trunk revision 1087159.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//console", "This message is automatically generated.", "The patch looks good to me.", "+1", "Patch for 203 and 204 version.", "Integrated in Hadoop-Common-trunk-Commit #541 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/541/])", "HADOOP-7215.", "RPC clients must use network interface corresponding to the host in the client's kerberos principal key.", "Contributed by Suresh Srinivas.", "Looks good to me, too.", "Thanks Suresh.", "+1 for 2xx patch."], "SplitGT": [" Get the host name from the principal name of format <service>/host@realm."], "issueString": "RPC clients must connect over a network interface corresponding to the host name in the client's kerberos principal key\nHADOOP-7104 introduced a change where RPC server matches client's hostname with the hostname specified in the client's Kerberos principal name. RPC client binds the socket to a random local address, which might not match the hostname specified in the principal name. This results authorization failure of the client at the server.\nGiven that the server validate's the host name of the client's connection with the host name in the client's prinicipal name of format <service>/_HOST@realm, I propose binding the client's socket to the address specified in the principal name.\n\n\nPath for 203 release.\nI don't think this is quite right. It works when the IPC client is a server with a principal like hdfs/dn1.foo.com@REALM. But two-part principals are also used for other cases, eg tlipcon/admin@REALM. In this patch, won't that try to bind the local socket of my IPC client to the non-host \"admin\"?\nPatch for the trunk, with minor modifications and tests.\nHDFS-1704 for some of the protocols such where clientPrincipal is needed, uses the second part as host name for validation. For such protocols, you cannot use a principal name that you mentioned. This does not affect protocols such as ClientProtocol though.\nI mean HDFS-7104 in my previous comment.\nI see what you mean now. We could check if the second part is _HOST in the key and try binding to local address. Let me see if that could be done.\nHere is a way to do that at the RPC client:\n# get part2 from principal name of format <part1>/<part2>@realm and ensure part2 is a host name.\n# the address corresponding to this host name belongs to one of the local network interfaces on that host.\n\nIf the above two conditions are satisfied, the bind to the socket to that address, before making rpc calls. It should address the following cases:\n# Principal name is not of format <part1>/<part2>@realm\n# part2 is not a valid host name\n# part2 is a valid host name, but not that of the client host\n\nDoes this sound reasonable?\nHi Suresh. This seems like a reasonable solution. Maybe we can also check the kerberos info on the protocol interface, and if there's no clientPrincipal set, we can skip all of this? My thinking is that we want to avoid the reverse DNS lookups for the common case of user->cluster communications where no clientPrincipal annotation is present.\nIn that case, we have two choices:\n# Fail at the client side:\n#* If the client principal name does not have <part1>/<part2>@realm format, fail it at the client with appropriate error. \n#* If the format is right, treat part2 as host name. Just try to bind to it and if bind fails, then the failure occurs at the client it self with appropriate error. \n\n# Fail at the server side:\n#* If the client principal name does not have <part1>/<part2>@realm format, bind to any local address for the request.\n#* If the format is right, treat part2 as host name. If host name is a valid local address, bind to it else bind to any local address. This request will be rejected by the server.\n\nI am leaning towards (2) because, server is rightly involved in the decision of rejecting the client. It provides a record of this at both the client and the server. This will help debugging on the server side, independent of client.\nAttached patch with changes from option(2) from my previous comment. I also cleaned up NetUtils.java warnings related to javadoc.\nupdated patch\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475153/HADOOP-7215.1.trunk.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.net.TestNetUtils\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/325//console\n\nThis message is automatically generated.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475153/HADOOP-7215.1.trunk.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.net.TestNetUtils\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/326//console\n\nThis message is automatically generated.\nFor some reason one of the test I am adding fails in hudson test, but works fine on my machine.\n\nUploading a debug version of the patch to understand the problem.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475156/HADOOP-7215.debug.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/327//console\n\nThis message is automatically generated.\nWith debug log the test did not fail! Trying a new patch again.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475157/HADOOP-7215.2.trunk.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.net.TestNetUtils\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/328//console\n\nThis message is automatically generated.\nPatch without debug fails. Trying patch with debug again...\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475162/HADOOP-7215.debug.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.net.TestNetUtils\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/329//console\n\nThis message is automatically generated.\nThe reason tests are failing is: local host maps to 127.0.1.1 with no interface configured for it.\n\nLooks like the other tests use 127.0.0.1 for local host. Doing the same for the newly added test.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475167/HADOOP-7215.3.trunk.patch\n  against trunk revision 1087159.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/330//console\n\nThis message is automatically generated.\nThe patch looks good to me. +1\nPatch for 203 and 204 version.\nIntegrated in Hadoop-Common-trunk-Commit #541 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/541/])\n    HADOOP-7215. RPC clients must use network interface corresponding to the host in the client's kerberos principal key. Contributed by Suresh Srinivas.\n\nLooks good to me, too. Thanks Suresh.\n+1 for 2xx patch.\n", "issueSearchSentences": ["HDFS-1704 for some of the protocols such where clientPrincipal is needed, uses the second part as host name for validation.", "HADOOP-7104 introduced a change where RPC server matches client's hostname with the hostname specified in the client's Kerberos principal name.", "For such protocols, you cannot use a principal name that you mentioned.", "# get part2 from principal name of format <part1>/<part2>@realm and ensure part2 is a host name.", "# Principal name is not of format <part1>/<part2>@realm"], "issueSearchScores": [0.6559157371520996, 0.6418219804763794, 0.6293638944625854, 0.5700481534004211, 0.540648877620697]}
{"aId": 45, "code": "private static boolean checkNoChange(DistCpOptions inputOptions,\n      DistributedFileSystem fs, Path path) {\n    try {\n      SnapshotDiffReport targetDiff =\n          fs.getSnapshotDiffReport(path, inputOptions.getFromSnapshot(), \"\");\n      if (!targetDiff.getDiffList().isEmpty()) {\n        DistCp.LOG.warn(\"The target has been modified since snapshot \"\n            + inputOptions.getFromSnapshot());\n        return false;\n      } else {\n        return true;\n      }\n    } catch (IOException e) {\n      DistCp.LOG.warn(\"Failed to compute snapshot diff on \" + path, e);\n    }\n    return false;\n  }", "comment": " Compute the snapshot diff on the given file system.", "issueId": "HDFS-7535", "issueStringList": ["Utilize Snapshot diff report for distcp", "Currently HDFS snapshot diff report can identify file/directory creation, deletion, rename and modification under a snapshottable directory.", "We can use the diff report for distcp between the primary cluster and a backup cluster to avoid unnecessary data copy.", "This is especially useful when there is a big directory rename happening in the primary cluster: the current distcp cannot detect the rename op thus this rename usually leads to large amounts of real data copy.", "More details of the approach will come in the first comment.", "A typical scenario using snapshot for distcp can be like this: every time we start distcp between the primary cluster and the backup cluster, a snapshot is first created in the primary cluster.", "Then the snapshot diff report is computed between the latest snapshot and the snapshot created for the last distcp.", "This snapshot diff report represents the delta that should be applied to the backup cluster.", "For changes like deletion and rename we can directly apply the same operations (following some specific order based on their dependency) in the backup cluster.", "For changes like creation, append, and other metadata modification we keep using the functionality of the current distcp.", "In this approach, we can avoid unnecessary data copy and also guarantee the source data is immutable since our snapshot is read-only.", "We plan to use this jira to provide the basic functionalities in the above approach.", "More specifically, we can first add extra options to the current distcp tool so that it can compute the dalta based on the diff report of two given snapshot names.", "How to manage snapshots in the source/target clusters can be done in separate jiras or through separate tools.", "Upload a very initial patch to demo the functionality.", "The patch adds new options in the current distcp tool to indicate the from/to snapshot names (assuming the same snapshot names are used in primary and backup cluster).", "And before doing the real data copy, the patch identifies the delete/rename operations based on the diff report, and calls the same ops in the backup cluster so that we can avoid unnecessary data copy caused by rename.", "Still need more work to make the whole process more efficient.", "Also need to add tests and handle corner cases.", "Update the patch with new strategies to handle rename operations and also add unit tests.", "Currently for this feature we have the following assumptions:", "# Both the source and target FileSystem must be DistributedFileSystem", "# Two snapshots (e.g., s1 and s2) have been created on the source FS.", "The diff between these two snapshots will be copied to the target FS.", "# The target has the same snapshot s1.", "No changes have been made on the target since s1.", "All the files/directories in the target are the same with source.s1", "We verify these assumptions before the sync and we fallback to the default distcp behavior if the assumptions do not stand.", "Note that for #3 currently we only check the diff before the current target and target.s1 is empty, instead of directly comparing target to source.s1.", "This may be fine since any failure while applying the snapshot diff on the target will cause the distcp to copy all the data.", "The main challenge here is to translate the rename diffs to doable rename ops.", "For example, if we have the following rename ops happening in the source:", "1) /test --> /foo-tmp", "2) /foo --> /test", "3) /bar --> /foo", "4) /foo-tmp --> /bar", "The snapshot diff report now looks like:", "R /foo --> /test", "R /test --> /bar", "R /bar --> /foo", "This diff report cannot be directly applied.", "The current patch thus create a tmp folder and breaks each rename op into two steps: move the source to the tmp folder and then move the data from tmp to target.", "Then we only need to sort all the first-phase renames based on the source paths (to make sure the files and subdirs are moved before their parents/ancestors), and sort all the second-phase renames based on the target paths (to make sure the parent directories are created first).", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12699392/HDFS-7535.001.patch", "against trunk revision 685af8a.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:red}-1 javac{color}.", "The applied patch generated 1156 javac compiler warnings (more than the trunk's current 1155 warnings).", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-tools/hadoop-distcp.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//artifact/patchprocess/newPatchFindbugsWarningshadoop-distcp.html", "Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//artifact/patchprocess/diffJavacWarnings.txt", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//console", "This message is automatically generated.", "Update the patch to fix the javac and findbug warnings.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12699520/HDFS-7535.002.patch", "against trunk revision 1714609.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-tools/hadoop-distcp.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9610//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9610//console", "This message is automatically generated.", "> We verify these assumptions before the sync and we fallback to the default distcp behavior ...", "Is it better to throw an exception instead since the user may not want to fallback?", "Thanks for working on this.", "The idea is great!", "Some comments on the patch:", "In DistCpSync.moveToTmpDir, why move the paths to tmp for the delete operations?", "In DistCpSync.moveToTarget, it mkdir if parent directory does not exist.", "Would it be able to preserve other attributes for the \"-p\" option?", "In DiffInfo.getDiffs(..), no need to create the entries array.", "Just iterate over report.getDiffList().", "Please add javadoc for DiffInfo.", "Thanks for the review, Nicholas!", "Update the patch to address your comments.", "bq.", "In DistCpSync.moveToTmpDir, why move the paths to tmp for the delete operations?", "So I'm thinking about if we can support \"undo\" for this functionality in the future.", "I.e., if the user hits any issue while applying the diff, if we move all the files/dirs to the tmp dir, we can still have a chance to undo all the changes.", "bq.", "Would it be able to preserve other attributes for the \"-p\" option?", "The attributes preservation will be covered later in the CopyMapper, which calls {{DistCpUtils#preserve}}.", "I will do some system tests and maybe add a new unit test to verify.", "bq.", "Is it better to throw an exception instead since the user may not want to fallback?", "My current concern is that if this functionality is used by applications like Falcon and Oozie, it may be more convenient if we can include the fallback logic inside of the distcp.", "If we directly throw exceptions then these applications need to have the capability to change the options to avoid using snapshot diff.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12701171/HDFS-7535.003.patch", "against trunk revision 2214dab.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-tools/hadoop-distcp.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9672//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9672//console", "This message is automatically generated.", "+1 patch looks good.", "Just found a bug in option parsing while testing the patch in a cluster: we need to specify the argument number for the new diff option.", "The new patch fixes the bug, and adds a unit test for this.", "With the fix this new feature works fine in my testing cluster.", "[~szetszwo], could you please take another look at the new patch?", "Thanks.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12702393/HDFS-7535.004.patch", "against trunk revision 29bb689.", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-tools/hadoop-distcp.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9722//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9722//console", "This message is automatically generated.", "+1 the new patch looks good.", "Thanks for testing it and adding the new tests.", "Thanks again for the review, Nicholas!", "I've committed this to trunk and branch-2.", "FAILURE: Integrated in Hadoop-trunk-Commit #7256 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7256/])", "HDFS-7535.", "Utilize Snapshot diff report for distcp.", "Contributed by Jing Zhao.", "(jing9: rev ed70fa142cabdbc1065e4dbbc95e99c8850c4751)", "hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSync.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java", "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DiffInfo.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java", "hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java", "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java"], "SplitGT": [" Compute the snapshot diff on the given file system."], "issueString": "Utilize Snapshot diff report for distcp\nCurrently HDFS snapshot diff report can identify file/directory creation, deletion, rename and modification under a snapshottable directory. We can use the diff report for distcp between the primary cluster and a backup cluster to avoid unnecessary data copy. This is especially useful when there is a big directory rename happening in the primary cluster: the current distcp cannot detect the rename op thus this rename usually leads to large amounts of real data copy.\n\nMore details of the approach will come in the first comment.\nA typical scenario using snapshot for distcp can be like this: every time we start distcp between the primary cluster and the backup cluster, a snapshot is first created in the primary cluster. Then the snapshot diff report is computed between the latest snapshot and the snapshot created for the last distcp. This snapshot diff report represents the delta that should be applied to the backup cluster. For changes like deletion and rename we can directly apply the same operations (following some specific order based on their dependency) in the backup cluster. For changes like creation, append, and other metadata modification we keep using the functionality of the current distcp. In this approach, we can avoid unnecessary data copy and also guarantee the source data is immutable since our snapshot is read-only.\n\nWe plan to use this jira to provide the basic functionalities in the above approach. More specifically, we can first add extra options to the current distcp tool so that it can compute the dalta based on the diff report of two given snapshot names. How to manage snapshots in the source/target clusters can be done in separate jiras or through separate tools.\nUpload a very initial patch to demo the functionality. The patch adds new options in the current distcp tool to indicate the from/to snapshot names (assuming the same snapshot names are used in primary and backup cluster). And before doing the real data copy, the patch identifies the delete/rename operations based on the diff report, and calls the same ops in the backup cluster so that we can avoid unnecessary data copy caused by rename.\n\nStill need more work to make the whole process more efficient. Also need to add tests and handle corner cases.\nUpdate the patch with new strategies to handle rename operations and also add unit tests.\n\nCurrently for this feature we have the following assumptions:\n# Both the source and target FileSystem must be DistributedFileSystem\n# Two snapshots (e.g., s1 and s2) have been created on the source FS. The diff between these two snapshots will be copied to the target FS.\n# The target has the same snapshot s1. No changes have been made on the target since s1. All the files/directories in the target are the same with source.s1\n\nWe verify these assumptions before the sync and we fallback to the default distcp behavior if the assumptions do not stand. Note that for #3 currently we only check the diff before the current target and target.s1 is empty, instead of directly comparing target to source.s1. This may be fine since any failure while applying the snapshot diff on the target will cause the distcp to copy all the data.\n\nThe main challenge here is to translate the rename diffs to doable rename ops. For example, if we have the following rename ops happening in the source:\n1) /test --> /foo-tmp\n2) /foo --> /test\n3) /bar --> /foo\n4) /foo-tmp --> /bar\n\nThe snapshot diff report now looks like:\nR /foo --> /test\nR /test --> /bar\nR /bar --> /foo\n\nThis diff report cannot be directly applied. The current patch thus create a tmp folder and breaks each rename op into two steps: move the source to the tmp folder and then move the data from tmp to target. Then we only need to sort all the first-phase renames based on the source paths (to make sure the files and subdirs are moved before their parents/ancestors), and sort all the second-phase renames based on the target paths (to make sure the parent directories are created first).\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12699392/HDFS-7535.001.patch\n  against trunk revision 685af8a.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n      {color:red}-1 javac{color}.  The applied patch generated 1156 javac compiler warnings (more than the trunk's current 1155 warnings).\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//artifact/patchprocess/newPatchFindbugsWarningshadoop-distcp.html\nJavac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//artifact/patchprocess/diffJavacWarnings.txt\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9606//console\n\nThis message is automatically generated.\nUpdate the patch to fix the javac and findbug warnings.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12699520/HDFS-7535.002.patch\n  against trunk revision 1714609.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9610//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9610//console\n\nThis message is automatically generated.\n> We verify these assumptions before the sync and we fallback to the default distcp behavior ...\n\nIs it better to throw an exception instead since the user may not want to fallback?\nThanks for working on this.  The idea is great!  Some comments on the patch:\n- In DistCpSync.moveToTmpDir, why move the paths to tmp for the delete operations?\n- In DistCpSync.moveToTarget, it mkdir if parent directory does not exist.  Would it be able to preserve other attributes for the \"-p\" option?\n- In DiffInfo.getDiffs(..), no need to create the entries array.  Just iterate over report.getDiffList().\n- Please add javadoc for DiffInfo.\nThanks for the review, Nicholas! Update the patch to address your comments.\n\nbq. In DistCpSync.moveToTmpDir, why move the paths to tmp for the delete operations?\n\nSo I'm thinking about if we can support \"undo\" for this functionality in the future. I.e., if the user hits any issue while applying the diff, if we move all the files/dirs to the tmp dir, we can still have a chance to undo all the changes.\n\nbq. Would it be able to preserve other attributes for the \"-p\" option?\n\nThe attributes preservation will be covered later in the CopyMapper, which calls {{DistCpUtils#preserve}}. I will do some system tests and maybe add a new unit test to verify.\n\nbq. Is it better to throw an exception instead since the user may not want to fallback?\n\nMy current concern is that if this functionality is used by applications like Falcon and Oozie, it may be more convenient if we can include the fallback logic inside of the distcp. If we directly throw exceptions then these applications need to have the capability to change the options to avoid using snapshot diff.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12701171/HDFS-7535.003.patch\n  against trunk revision 2214dab.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9672//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9672//console\n\nThis message is automatically generated.\n+1 patch looks good.\nJust found a bug in option parsing while testing the patch in a cluster: we need to specify the argument number for the new diff option. The new patch fixes the bug, and adds a unit test for this. With the fix this new feature works fine in my testing cluster.\n\n[~szetszwo], could you please take another look at the new patch? Thanks.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12702393/HDFS-7535.004.patch\n  against trunk revision 29bb689.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9722//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9722//console\n\nThis message is automatically generated.\n+1 the new patch looks good.\n\nThanks for testing it and adding the new tests.\nThanks again for the review, Nicholas! I've committed this to trunk and branch-2.\nFAILURE: Integrated in Hadoop-trunk-Commit #7256 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7256/])\nHDFS-7535. Utilize Snapshot diff report for distcp. Contributed by Jing Zhao. (jing9: rev ed70fa142cabdbc1065e4dbbc95e99c8850c4751)\n* hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSync.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptionSwitch.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DiffInfo.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java\n* hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestOptionsParser.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java\n* hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java\n\n", "issueSearchSentences": ["This may be fine since any failure while applying the snapshot diff on the target will cause the distcp to copy all the data.", "Currently HDFS snapshot diff report can identify file/directory creation, deletion, rename and modification under a snapshottable directory.", "Then the snapshot diff report is computed between the latest snapshot and the snapshot created for the last distcp.", "The diff between these two snapshots will be copied to the target FS.", "No changes have been made on the target since s1."], "issueSearchScores": [0.6590253114700317, 0.6145591735839844, 0.6029626131057739, 0.5945900082588196, 0.5911186933517456]}
{"aId": 48, "code": "EncryptionZoneWithId getEZForPath(final String srcArg)\n    throws AccessControlException, UnresolvedLinkException, IOException {\n    String src = srcArg;\n    HdfsFileStatus resultingStat = null;\n    final byte[][] pathComponents =\n        FSDirectory.getPathComponentsForReservedPath(src);\n    boolean success = false;\n    final FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.READ);\n      }\n      checkOperation(OperationCategory.READ);\n      src = resolvePath(src, pathComponents);\n      final INodesInPath iip = dir.getINodesInPath(src, true);\n      final EncryptionZoneWithId ret = dir.getEZForPath(iip);\n      resultingStat = getAuditFileInfo(src, false);\n      success = true;\n      return ret;\n    } finally {\n      readUnlock();\n      logAuditEvent(success, \"getEZForPath\", srcArg, null, resultingStat);\n    }\n  }", "comment": " Get the encryption zone for the specified path.", "issueId": "HDFS-6546", "issueStringList": ["Add non-superuser capability to get the encryption zone for a specific path", "Need to add protocol, api, and CLI that allows a non super user to ask whether a path is part of an EZ, and if so, which one.", "We had a request from Hive for this feature.", "For a table in an encryption zone, they want to put intermediate output inside a per-zone tmp dir, rather than /tmp.", "The easiest way for them to do this is being able to query the EZ for a table.", "Attached is a patch which adds an isEncrypted() method to FileStatus.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12661229/HDFS-6546.001.patch", "against trunk revision .", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7617//console", "This message is automatically generated.", "Rebased.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12661233/HDFS-6546.001.patch", "against trunk revision .", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7618//console", "This message is automatically generated.", "The attached patch adds a new method to HdfsAdmin: getEncryptionZoneRootForPath which accepts a path and returns the path of the EZ root (if the arg is in an ez) or null (if it is not in an ez.", "Nice idea.", "Returning just a path seems a bit inflexible.", "Can we also return an encryption zone id of sorts?", "I think the inode ID of the EZ would work pretty nicely (based on some offline discussion with Andrew).", "That way we can also add more stuff if we want later... we're not locked into just what fields Path has.", "Also, I noticed a few places in the test where you inverted \"expected\" and \"provided\".", "The expected thing should come first in Assert.assert, so if the test fails, you don't get confusing error messages...", "One last thing...", "I modified the test slightly to call this API on something in a snapshot, and it failed with this exception:", "{code}", "Running org.apache.hadoop.hdfs.TestEncryptionZones", "Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.539 sec <<< FAILURE!", "- in org.apache.hadoop.hdfs.TestEncryptionZones", "testGetEZRootAsNonSuperUser(org.apache.hadoop.hdfs.TestEncryptionZones)  Time elapsed: 3.876 sec  <<< ERROR!", "org.apache.hadoop.ipc.RemoteException: Modification on a read-only snapshot is disallowed", "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write(FSDirectory.java:3071)", "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write(FSDirectory.java:1490)", "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZRootForPath(FSNamesystem.java:8598)", "{code}", "This should work on snapshotted files... probably a good idea to add a unit test for that.", "Similarly, we should test what happens when both the file and the EZ have been deleted, but are still in a snapshot.", "Thanks", "[~cmccabe],", "Thanks for the review.", "New rev of the patch is attached as .002.", "{code}", "+   * Returns an EncryptionZoneWithId representing the ez root for a given path.", "+   * Returns null if the path is not in an ez.", "+   *", "+   * \\@param iip The INodesInPath of the path to check", "+   * \\@return the EncryptionZoneWithId representing the ez root for the path.", "+   */", "+  EncryptionZoneWithId getEZRootINodeForPath(INodesInPath iip) {", "+    final EncryptionZoneInt ezi = getEncryptionZoneForPath(iip);", "+    if (ezi == null) {", "+      return new EncryptionZoneWithId(\"\", \"\", -1);", "+    } else {", "+      return new EncryptionZoneWithId(getFullPathName(ezi), ezi.getKeyName(),", "+          ezi.getINodeId());", "+    }", "+  }", "{code}", "The comment says it returns null, but it seems to be returning an empty object instead.", "I guess either update the comment or make it really return null.", "I can see how returning an empty object might be more convenient for PB purposes... if you want to go down that route, perhaps make this a named final constant?", "On the other hand null is more consistent with other getEZRoot methods.", "Your call.", "Also, should we call this RPC getEzForPath?", "It's returning an EncryptionZone object, not an \"EZ root inode\" (do users know what that is?)", "+1 once those comments are addressed.", ".003 addresses these issues.", "I turned the empty EZWId into a final static called NULL_EZ.", "I changed all of the diffs to remove the word \"root\" so it is now called HdfsAdmin.getEncryptionZoneForPath.", "I've committed this to fs-encryption.", "Thanks [~cmccabe].", "Thanks, Charles"], "SplitGT": [" Get the encryption zone for the specified path."], "issueString": "Add non-superuser capability to get the encryption zone for a specific path\nNeed to add protocol, api, and CLI that allows a non super user to ask whether a path is part of an EZ, and if so, which one.\nWe had a request from Hive for this feature. For a table in an encryption zone, they want to put intermediate output inside a per-zone tmp dir, rather than /tmp. The easiest way for them to do this is being able to query the EZ for a table.\nAttached is a patch which adds an isEncrypted() method to FileStatus.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12661229/HDFS-6546.001.patch\n  against trunk revision .\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/7617//console\n\nThis message is automatically generated.\nRebased.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12661233/HDFS-6546.001.patch\n  against trunk revision .\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/7618//console\n\nThis message is automatically generated.\nThe attached patch adds a new method to HdfsAdmin: getEncryptionZoneRootForPath which accepts a path and returns the path of the EZ root (if the arg is in an ez) or null (if it is not in an ez.\n\nNice idea.  \n\nReturning just a path seems a bit inflexible.  Can we also return an encryption zone id of sorts?  I think the inode ID of the EZ would work pretty nicely (based on some offline discussion with Andrew).  That way we can also add more stuff if we want later... we're not locked into just what fields Path has.\n\nAlso, I noticed a few places in the test where you inverted \"expected\" and \"provided\".  The expected thing should come first in Assert.assert, so if the test fails, you don't get confusing error messages...\n\nOne last thing... I modified the test slightly to call this API on something in a snapshot, and it failed with this exception:\n{code}\nRunning org.apache.hadoop.hdfs.TestEncryptionZones\nTests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.539 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestEncryptionZones\ntestGetEZRootAsNonSuperUser(org.apache.hadoop.hdfs.TestEncryptionZones)  Time elapsed: 3.876 sec  <<< ERROR!\norg.apache.hadoop.ipc.RemoteException: Modification on a read-only snapshot is disallowed\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write(FSDirectory.java:3071)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getINodesInPath4Write(FSDirectory.java:1490)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZRootForPath(FSNamesystem.java:8598)\n{code}\n\nThis should work on snapshotted files... probably a good idea to add a unit test for that.  Similarly, we should test what happens when both the file and the EZ have been deleted, but are still in a snapshot.  Thanks\n[~cmccabe],\n\nThanks for the review. New rev of the patch is attached as .002.\n\n{code}\n   /**\n+   * Returns an EncryptionZoneWithId representing the ez root for a given path.\n+   * Returns null if the path is not in an ez.\n+   *\n+   * \\@param iip The INodesInPath of the path to check\n+   * \\@return the EncryptionZoneWithId representing the ez root for the path.\n+   */\n+  EncryptionZoneWithId getEZRootINodeForPath(INodesInPath iip) {\n+    final EncryptionZoneInt ezi = getEncryptionZoneForPath(iip);\n+    if (ezi == null) {\n+      return new EncryptionZoneWithId(\"\", \"\", -1);\n+    } else {\n+      return new EncryptionZoneWithId(getFullPathName(ezi), ezi.getKeyName(),\n+          ezi.getINodeId());\n+    }\n+  }\n{code}\n\nThe comment says it returns null, but it seems to be returning an empty object instead.  I guess either update the comment or make it really return null.  I can see how returning an empty object might be more convenient for PB purposes... if you want to go down that route, perhaps make this a named final constant?  On the other hand null is more consistent with other getEZRoot methods.  Your call.\n\nAlso, should we call this RPC getEzForPath?  It's returning an EncryptionZone object, not an \"EZ root inode\" (do users know what that is?)\n\n+1 once those comments are addressed.\n.003 addresses these issues. I turned the empty EZWId into a final static called NULL_EZ. I changed all of the diffs to remove the word \"root\" so it is now called HdfsAdmin.getEncryptionZoneForPath.\n\nI've committed this to fs-encryption. Thanks [~cmccabe].\nThanks, Charles\n", "issueSearchSentences": ["+   * \\@return the EncryptionZoneWithId representing the ez root for the path.", "The attached patch adds a new method to HdfsAdmin: getEncryptionZoneRootForPath which accepts a path and returns the path of the EZ root (if the arg is in an ez) or null (if it is not in an ez.", "+   * Returns an EncryptionZoneWithId representing the ez root for a given path.", "+      return new EncryptionZoneWithId(getFullPathName(ezi), ezi.getKeyName(),", "+  EncryptionZoneWithId getEZRootINodeForPath(INodesInPath iip) {"], "issueSearchScores": [0.7231730222702026, 0.6888138055801392, 0.6838958859443665, 0.6766602396965027, 0.6599465608596802]}
{"aId": 52, "code": "@Override\n  public Path getTrashRoot(Path path) {\n    boolean useMountPointLocalTrash =\n        config.getBoolean(CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH,\n            CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH_DEFAULT);\n\n    try {\n      InodeTree.ResolveResult<FileSystem> res =\n          fsState.resolve(getUriPath(path), true);\n\n      Path trashRoot = res.targetFileSystem.getTrashRoot(res.remainingPath);\n      if (!useMountPointLocalTrash) {\n        return trashRoot;\n      } else {\n        // Path p is either in a mount point or in the fallback FS\n\n        if (ROOT_PATH.equals(new Path(res.resolvedPath))\n            || trashRoot.toUri().getPath().startsWith(res.resolvedPath)) {\n          // Path p is in the fallback FS or targetFileSystem.trashRoot is in\n          // the same mount point as Path p\n          return trashRoot;\n        } else {\n          // targetFileSystem.trashRoot is in a different mount point from\n          // Path p. Return the trash root for the mount point.\n          Path mountPointRoot =\n              res.targetFileSystem.getFileStatus(new Path(\"/\")).getPath();\n          return new Path(mountPointRoot,\n              TRASH_PREFIX + \"/\" + ugi.getShortUserName());\n        }\n      }\n    } catch (IOException | IllegalArgumentException e) {\n      throw new NotInMountpointException(path, \"getTrashRoot\");\n    }\n  }", "comment": " 2) else, return a trash root in the mounted targetFS (/mntpoint/.Trash/{user)", "issueId": "HADOOP-18110", "issueStringList": ["ViewFileSystem: Add Support for Localized Trash Root", "getTrashRoot() in ViewFileSystem calls getTrashRoot() from underlying filesystem, to return the trash root.", "Most of the time, we get a trash root in user home dir.", "This can lead to problems when an application wants to delete a file in a mounted point using moveToTrash() in TrashPolicyDefault, because we can not rename across multiple filesystems/hdfs namenodes.", "We propose the following extension to getTrashRoot/getTrashRoots in ViewFileSystem: add a flag to return a localized trash root for ViewFileSystem.", "A localized trash root is a trash root which starts from the root of a mount point (e.g., /mountpointRoot/.Trash/\\{user}).", "If CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is not set to true, or", "when the path p is in a snapshot or an encryption zone, return", "the default trash root in user home dir.", "when CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is set to true,", "1) if path p is mounted from the same targetFS as user home dir,", "return a trash root in user home dir.", "2) else, return a trash root in the mounted targetFS"], "SplitGT": [" 2) else, return a trash root in the mounted targetFS (/mntpoint/.Trash/{user)"], "issueString": "ViewFileSystem: Add Support for Localized Trash Root\ngetTrashRoot() in ViewFileSystem calls getTrashRoot() from underlying filesystem, to return the trash root. Most of the time, we get a trash root in user home dir. This can lead to problems when an application wants to delete a file in a mounted point using moveToTrash() in TrashPolicyDefault, because we can not rename across multiple filesystems/hdfs namenodes.\u00a0\r\n\r\n\u00a0\r\n\r\nWe propose the following extension to getTrashRoot/getTrashRoots in ViewFileSystem: add a flag to return a localized trash root for ViewFileSystem. A localized trash root is a trash root which starts from the root of a mount point (e.g., /mountpointRoot/.Trash/\\{user}).\u00a0\r\n\r\n* If CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is not set to true, or\r\n* when the path p is in a snapshot or an encryption zone, return\r\n* the default trash root in user home dir.\r\n*\r\n* when CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is set to true,\r\n* 1) if path p is mounted from the same targetFS as user home dir,\r\n* return a trash root in user home dir.\r\n* 2) else, return a trash root in the mounted targetFS\r\n*\n", "issueSearchSentences": ["getTrashRoot() in ViewFileSystem calls getTrashRoot() from underlying filesystem, to return the trash root.", "We propose the following extension to getTrashRoot/getTrashRoots in ViewFileSystem: add a flag to return a localized trash root for ViewFileSystem.", "A localized trash root is a trash root which starts from the root of a mount point (e.g., /mountpointRoot/.Trash/\\{user}).", "2) else, return a trash root in the mounted targetFS", "ViewFileSystem: Add Support for Localized Trash Root"], "issueSearchScores": [0.7633767127990723, 0.6776573061943054, 0.6520047187805176, 0.6341388821601868, 0.6238280534744263]}
{"aId": 54, "code": "public void calculateChunkedSums(ByteBuffer data, ByteBuffer checksums) {\r\n    if (size == 0) return;\r\n    \r\n    if (data.hasArray() && checksums.hasArray()) {\r\n      calculateChunkedSums(data.array(), data.arrayOffset() + data.position(), data.remaining(),\r\n          checksums.array(), checksums.arrayOffset() + checksums.position());\r\n      return;\r\n    }\r\n    \r\n    data.mark();\r\n    checksums.mark();\r\n    try {\r\n      byte[] buf = new byte[bytesPerChecksum];\r\n      while (data.remaining() > 0) {\r\n        int n = Math.min(data.remaining(), bytesPerChecksum);\r\n        data.get(buf, 0, n);\r\n        summer.reset();\r\n        summer.update(buf, 0, n);\r\n        checksums.putInt((int)summer.getValue());\r\n      }\r\n    } finally {\r\n      data.reset();\r\n      checksums.reset();\r\n    }\r\n  }", "comment": " Calculate checksums for the given data.", "issueId": "HADOOP-7444", "issueStringList": ["Add Checksum API to verify and calculate checksums \"in bulk\"", "Currently, the various checksum types only provide the capability to calculate the checksum of a range of a byte array.", "For HDFS-2080, it's advantageous to provide an API that, given a buffer with some number of \"checksum chunks\", can either calculate or verify the checksums of all of the chunks.", "For example, given a 4KB buffer and a 512-byte chunk size, it would calculate or verify 8 CRC32s in one call.", "This allows efficient JNI-based checksum implementations since the cost of crossing the JNI boundary is amortized across many computations.", "Fairly simple patch implementing this API.", "Unfortunately it wasn't straightforward to use the new APIs from FSInputChecker, since that class only has a Checksum instance and not a DataChecksum instance.", "We can tackle that as an improvement later.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12485374/hadoop-7444.txt", "against trunk revision 1143219.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//console", "This message is automatically generated.", "Improved patch which avoids copies when passed ByteBuffers that are array-backed.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12485453/hadoop-7444.txt", "against trunk revision 1143491.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//console", "This message is automatically generated.", "Looks good.", "Some comments.", "Why cast a long to an int and then put the int in a long?", "{code}", "long calculated = (int)summer.getValue();", "{code}", "It won't work for 64-bit checksums.", "In line 357, {{calculated}} is redundant.", "{code}", "long calculated = (int)summer.getValue();", "checksums.putInt((int)calculated);", "{code}", "Line 382 to 385, {{& 0xff}} are redundant.", "{code}", "sums[sumsOffset++] = (byte) ((calculated >> 24) & 0xff);", "sums[sumsOffset++] = (byte) ((calculated >> 16) & 0xff);", "sums[sumsOffset++] = (byte) ((calculated >> 8) & 0xff);", "sums[sumsOffset++] = (byte) ((calculated) & 0xff);", "{code}", "I would avoid using {{Math.min(..)}}.", "fixed the redundant casts and bitmasks, thanks", "left the Math.min call, since it actually is a VM intrinsic in hotspot: http://google.com/codesearch#NZZBPLyUxqs/src/share/vm/opto/library_call.cpp&q=hotspot%20intrinsics%20min&type=cs&l=1499", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12486254/hadoop-7444.txt", "against trunk revision 1145839.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//console", "This message is automatically generated.", "+1 patch looks good.", "It won't work for 64-bit checksum.", "I think it is fine, provided that the existing code may also not work for 64-bit checksums.", "Committed to trunk, thanks for review, Nicholas.", "I agree that the current code in DataChecksum is already specific to 32-bit checksums.", "Integrated in Hadoop-Common-trunk-Commit #688 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/688/])", "HADOOP-7444.", "Add Checksum API to verify and calculate checksums \"in bulk\".", "Contributed by Todd Lipcon.", "todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146111", "Files :", "hadoop/common/trunk/common/CHANGES.txt", "hadoop/common/trunk/common/src/test/core/org/apache/hadoop/util/TestDataChecksum.java", "hadoop/common/trunk/common/src/java/org/apache/hadoop/util/DataChecksum.java", "It would be great if DFSClient (or some other class) offered an API to verify a file manually.", "Right now DFSClient offers the getFileChecksum method, though there's no way to verify the CRC against the file (eg, I think it's all under the hood right now).", "Copying a file to /dev/null will verify it.", "@Doug Right, basically read the stream until completion.", "I guess that's one way to do it!", "I was thinking a method that returned a boolean, like verifyFile(Path)."], "SplitGT": [" Calculate checksums for the given data."], "issueString": "Add Checksum API to verify and calculate checksums \"in bulk\"\nCurrently, the various checksum types only provide the capability to calculate the checksum of a range of a byte array. For HDFS-2080, it's advantageous to provide an API that, given a buffer with some number of \"checksum chunks\", can either calculate or verify the checksums of all of the chunks. For example, given a 4KB buffer and a 512-byte chunk size, it would calculate or verify 8 CRC32s in one call.\n\nThis allows efficient JNI-based checksum implementations since the cost of crossing the JNI boundary is amortized across many computations.\nFairly simple patch implementing this API.\n\nUnfortunately it wasn't straightforward to use the new APIs from FSInputChecker, since that class only has a Checksum instance and not a DataChecksum instance. We can tackle that as an improvement later.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12485374/hadoop-7444.txt\n  against trunk revision 1143219.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/698//console\n\nThis message is automatically generated.\nImproved patch which avoids copies when passed ByteBuffers that are array-backed.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12485453/hadoop-7444.txt\n  against trunk revision 1143491.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/702//console\n\nThis message is automatically generated.\nLooks good.  Some comments.\n\n- Why cast a long to an int and then put the int in a long?\n{code}\n      long calculated = (int)summer.getValue();\n{code}\nIt won't work for 64-bit checksums.\n\n- In line 357, {{calculated}} is redundant. \n{code}\n        long calculated = (int)summer.getValue();\n        checksums.putInt((int)calculated);\n{code}\n\n- Line 382 to 385, {{& 0xff}} are redundant.\n{code}\n      sums[sumsOffset++] = (byte) ((calculated >> 24) & 0xff);\n      sums[sumsOffset++] = (byte) ((calculated >> 16) & 0xff);\n      sums[sumsOffset++] = (byte) ((calculated >> 8) & 0xff);\n      sums[sumsOffset++] = (byte) ((calculated) & 0xff);\n{code}\n\n- I would avoid using {{Math.min(..)}}.\n- fixed the redundant casts and bitmasks, thanks\n- left the Math.min call, since it actually is a VM intrinsic in hotspot: http://google.com/codesearch#NZZBPLyUxqs/src/share/vm/opto/library_call.cpp&q=hotspot%20intrinsics%20min&type=cs&l=1499\n\n\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12486254/hadoop-7444.txt\n  against trunk revision 1145839.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/721//console\n\nThis message is automatically generated.\n+1 patch looks good.  It won't work for 64-bit checksum.  I think it is fine, provided that the existing code may also not work for 64-bit checksums.\nCommitted to trunk, thanks for review, Nicholas. I agree that the current code in DataChecksum is already specific to 32-bit checksums.\nIntegrated in Hadoop-Common-trunk-Commit #688 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/688/])\n    HADOOP-7444. Add Checksum API to verify and calculate checksums \"in bulk\". Contributed by Todd Lipcon.\n\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146111\nFiles : \n* /hadoop/common/trunk/common/CHANGES.txt\n* /hadoop/common/trunk/common/src/test/core/org/apache/hadoop/util/TestDataChecksum.java\n* /hadoop/common/trunk/common/src/java/org/apache/hadoop/util/DataChecksum.java\n\nIt would be great if DFSClient (or some other class) offered an API to verify a file manually.  Right now DFSClient offers the getFileChecksum method, though there's no way to verify the CRC against the file (eg, I think it's all under the hood right now).\nCopying a file to /dev/null will verify it.\n@Doug Right, basically read the stream until completion.  I guess that's one way to do it!  I was thinking a method that returned a boolean, like verifyFile(Path).\n", "issueSearchSentences": ["I agree that the current code in DataChecksum is already specific to 32-bit checksums.", "For HDFS-2080, it's advantageous to provide an API that, given a buffer with some number of \"checksum chunks\", can either calculate or verify the checksums of all of the chunks.", "Currently, the various checksum types only provide the capability to calculate the checksum of a range of a byte array.", "It won't work for 64-bit checksums.", "It won't work for 64-bit checksum."], "issueSearchScores": [0.6885260343551636, 0.67781001329422, 0.6682850122451782, 0.6234917044639587, 0.5966814756393433]}
{"aId": 55, "code": "public AccessControlList getQueueAcl(String queue, QueueACL operation) {\n    Map<QueueACL, AccessControlList> queueAcls = info.queueAcls.get(queue);\n    if (queueAcls == null || !queueAcls.containsKey(operation)) {\n      return (queue.equals(ROOT_QUEUE)) ? EVERYBODY_ACL : NOBODY_ACL;\n    }\n    return queueAcls.get(operation);\n  }", "comment": " The default for the root queue is everybody (\"\") and the default for all other queues is nobody (\"\")", "issueId": "YARN-1288", "issueStringList": ["Make Fair Scheduler ACLs more user friendly", "The Fair Scheduler currently defaults the root queue's acl to empty and all other queues' acl to \"*\".", "Now that YARN-1258 enables configuring the root queue, we should reverse this.", "This will also bring the Fair Scheduler in line with the Capacity Scheduler.", "We should also not trim the acl strings, which makes it impossible to only specify groups in an acl.", "Attached patch that makes the changes discussed above.", "To avoid allocating and filling a HashMap every time acls are checked, the patch also changes QueueManager#getQueueAcls to QueueManager#getQueueAcl and removes the getQueueAcls method in Queue that is no longer needed because of this.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12607708/YARN-1288.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2158//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2158//console", "This message is automatically generated.", "Queue.java:", "false change line 46", "unused import line 27", "FSLeafQueue.java:", "unused imports lines 27,34", "FSQueue.java:", "unused imports lines 23,24,29", "QueueManager.java:", "false changes line 75,296,414,417", "NOONE_ACL constant should be NO_ONE_ACL", "default getQueueAcl() behavior has not changed, correct?", "The following comment is removed by the patch, I think we should have it in the getQueueAcl() method:", "{code}", "Root queue should have empty ACLs.", "As a queue's ACL is the union of", "its ACL and all its parents' ACLs, setting the roots' to empty will", "neither allow nor prohibit more access to its children.", "{code}", "Uploaded a new patch.", "Addressed false changes and unused imports.", "bq.", "NOONE_ACL constant should be NO_ONE_ACL", "Changed this to EVERYBODY_ACL and NOBODY_ACL.", "bq.", "default getQueueAcl() behavior has not changed, correct?", "The behavior has changed.", "Added a comment to the getQueueACL method that explains the behavior.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12608807/YARN-1288-1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2193//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2193//console", "This message is automatically generated.", "patch LGTM.", "Before committing it ...", "Would this be an incompatible change?", "If so, can the configuration be set to have the previous behavior?", "If so, that should be the default setting.", "Documentation is missing.", "bq.", "Would this be an incompatible change?", "If so, can the configuration be set to have the previous behavior?", "If so, that should be the default setting.", "Yes it is an incompatible change.", "The commit concerns the default settings and thus we can't leave the previous behavior, which is incorrect, as the default.", "bq.", "Documentation is missing.", "Will add some doc", "For the record, the old default behavior was that the root queue would have ACLs that defaulted to nobody and all other queues would default to everybody.", "The patch switches this, so now the root queue defaults to everybody and all other queues default to nobody.", "+1.", "Would be possible in the docs to make it more explicit that the behavior is an OR of all the ACLs from the root to the leaf queue?", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12609055/YARN-1288-2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2210//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2210//console", "This message is automatically generated.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12609237/YARN-1288-3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2224//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2224//console", "This message is automatically generated.", "I just committed this.", "SUCCESS: Integrated in Hadoop-trunk-Commit #4636 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4636/])", "YARN-1288.", "Make Fair Scheduler ACLs more user friendly (Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1534315)", "hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/Queue.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm"], "SplitGT": [" The default for the root queue is everybody (\"\") and the default for all other queues is nobody (\"\")"], "issueString": "Make Fair Scheduler ACLs more user friendly\nThe Fair Scheduler currently defaults the root queue's acl to empty and all other queues' acl to \"*\".  Now that YARN-1258 enables configuring the root queue, we should reverse this.  This will also bring the Fair Scheduler in line with the Capacity Scheduler.\n\nWe should also not trim the acl strings, which makes it impossible to only specify groups in an acl.\nAttached patch that makes the changes discussed above. To avoid allocating and filling a HashMap every time acls are checked, the patch also changes QueueManager#getQueueAcls to QueueManager#getQueueAcl and removes the getQueueAcls method in Queue that is no longer needed because of this.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12607708/YARN-1288.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2158//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2158//console\n\nThis message is automatically generated.\n* Queue.java: \n** false change line 46 \n** unused import line 27\n* FSLeafQueue.java: \n** unused imports lines 27,34\n* FSQueue.java: \n** unused imports lines 23,24,29\n* QueueManager.java: \n** false changes line 75,296,414,417\n** NOONE_ACL constant should be NO_ONE_ACL\n** default getQueueAcl() behavior has not changed, correct? The following comment is removed by the patch, I think we should have it in the getQueueAcl() method:\n\n{code}\n       // Root queue should have empty ACLs.  As a queue's ACL is the union of\n      // its ACL and all its parents' ACLs, setting the roots' to empty will\n      // neither allow nor prohibit more access to its children.\n{code}\n\nUploaded a new patch.\n\nAddressed false changes and unused imports.\n\nbq. NOONE_ACL constant should be NO_ONE_ACL\nChanged this to EVERYBODY_ACL and NOBODY_ACL.\n\nbq. default getQueueAcl() behavior has not changed, correct?\nThe behavior has changed.  Added a comment to the getQueueACL method that explains the behavior.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12608807/YARN-1288-1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2193//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2193//console\n\nThis message is automatically generated.\npatch LGTM. Before committing it ...\n\n* Would this be an incompatible change? If so, can the configuration be set to have the previous behavior? If so, that should be the default setting.\n* Documentation is missing.\nbq. Would this be an incompatible change? If so, can the configuration be set to have the previous behavior? If so, that should be the default setting.\nYes it is an incompatible change.  The commit concerns the default settings and thus we can't leave the previous behavior, which is incorrect, as the default.\n\nbq. Documentation is missing.\nWill add some doc\nFor the record, the old default behavior was that the root queue would have ACLs that defaulted to nobody and all other queues would default to everybody.  The patch switches this, so now the root queue defaults to everybody and all other queues default to nobody.\n+1. Would be possible in the docs to make it more explicit that the behavior is an OR of all the ACLs from the root to the leaf queue?\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12609055/YARN-1288-2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2210//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2210//console\n\nThis message is automatically generated.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12609237/YARN-1288-3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/2224//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/2224//console\n\nThis message is automatically generated.\nI just committed this.\nSUCCESS: Integrated in Hadoop-trunk-Commit #4636 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4636/])\nYARN-1288. Make Fair Scheduler ACLs more user friendly (Sandy Ryza) (sandy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1534315)\n* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/Queue.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/FairScheduler.apt.vm\n\n", "issueSearchSentences": ["default getQueueAcl() behavior has not changed, correct?", "default getQueueAcl() behavior has not changed, correct?", "To avoid allocating and filling a HashMap every time acls are checked, the patch also changes QueueManager#getQueueAcls to QueueManager#getQueueAcl and removes the getQueueAcls method in Queue that is no longer needed because of this.", "The following comment is removed by the patch, I think we should have it in the getQueueAcl() method:", "As a queue's ACL is the union of"], "issueSearchScores": [0.7106945514678955, 0.7106945514678955, 0.6683554649353027, 0.6555068492889404, 0.6385718584060669]}
{"aId": 56, "code": "private static List<String> getUnixGroups(final String user) throws IOException {\n    String result = \"\";\n    try {\n      result = Shell.execCommand(Shell.getGROUPS_FOR_USER_COMMAND(user));\n    } catch (ExitCodeException e) {\n      // if we didn't get the group - just return empty list;\n      LOG.warn(\"got exception trying to get groups for user \" + user, e);\n    }\n    \n    StringTokenizer tokenizer = new StringTokenizer(result);\n    List<String> groups = new LinkedList<String>();\n    while (tokenizer.hasMoreTokens()) {\n      groups.add(tokenizer.nextToken());\n    }\n    return groups;\n  }", "comment": " For non-existing user it will return EMPTY list", "issueId": "HADOOP-4656", "issueStringList": ["Add a user to groups mapping service", "Currently the IPC client sends the UGI which contains the user/group information for the Server.", "However this represents the groups for the user on the client-end.", "The more pertinent mapping from user to groups is actually the one seen by the Server.", "Hence the client should only send the user and we should add a 'group mapping service' so that the Server can query it for the mapping.", "HADOOP-4348 is switching IPC to use the JAAS Subject rather than UGI (which will become an internal artifact).", "While we are adding the user-to-group mapping service, I propose we change the IPC Client to send the JAAS Subject in the header rather than UGI, this will also be compatible with the way we will do Kerberos-based authentication via the GSS API.", "I propose a new abstract class Groups with a single method 'getGroups' as below:", "{code:title=Groups.java}", "public abstract class Groups {", "List<String> getGroups(String username);", "}", "{code}", "with a concrete implementation which gets the unix groups for the given user.", "Preliminary patch while I continue testing.", "> I propose we change the IPC Client to send the JAAS Subject in the header rather than UGI, this will also be compatible with the way we will do Kerberos-based authentication via the GSS API.", "Just want to clarify that application code doesn't send anything when using Kerberos.", "It's all hiding inside the GSS API library.", "After authentication, server can query the established GSS context to get client ID as GSSName which can be converted to a String.", "So for compatibility, IPC Client doesn't have to send JAAS Subject in the header.", "Send a String is fine.", "Groups should definitely come from asking the host OS in some form using the Java equivalent of getgrent() and friends.", "[ Be aware that getgroups() is BSD-specific and may not be available on System V, such as Solaris and HP-UX.]", "Doing this via shell call out is just going to exasperate the memory problems we already see, especially on the secondary name node that requires more memory than the primary due to the fork of whoami/id!", "It also opens up yet another security hole where any random groups command on the name nodes path can be used to override.", "Not Good(tm).", "Privately, someone asked about caching the group content.", "One of the big advantages of talking to the OS is that many systems include a naming services caching daemon that handles caching group and similar content for the entire machine.", "nscd generally includes great support for controlling the size, ttl, negative ttl, etc, for the cache.", "Duplicating that functionality seems like overkill and, worse, will act as a cache against a cache!", "Arun, can we get this one done soon?", "I'm working on 4343, which depends on this.", "Thanks.", "Please consider passing the authentication context to the getGroups() method,", "as it might be easier to retrieve the associated groups using that information,", "then based only on the username.", "For example in POSIX environments it is faster to do a lookup based on the", "numeric UID, than based on the username.", "If you are using Kerberos with PAC, then the authentication context may already", "contain a list of associated groups:", "http://k5wiki.kerberos.org/wiki/Projects/PAC_and_principal_APIs", "There is a similar solution based on X509 authentication, where the associated", "list of groups is embedded into the authentication context.", "AFAIK, Hadoop has no concept of uid, as everything in the HDFS, etc, is stored as a string.", "So the username is about all the context you can probably get.", ":)", "Preliminary patch, with some testing done.", "This patch will create two versions of SecurityUtil.getSubject.", "One that builds list of group principles from UGI group list and another one that builds the list from UNIX id command.", "Do we really need the first one?", "I suggest we remove it.", "left both getSubject in place.", "added test for the new getSubject(user)", "Removed Timer thread for refreshing cache of groups mapping.", "Instead using lazy approach - verifying cacheTimeout on access.", "Also implemented few comments form Jakob:", "1. removed author tag", "2. using CommonConfigurationKeys constants for conf.keys.", "Reviewed patch:", "Nit: Calling an abstract class GroupMappingImpl seems a bit odd, even if it is technically correct for this.", "Service provider, maybe?", "In Groups.java the previous timer-based code is still present, but commented out.", "Needs removed.", "Note: HADOOP-6299, if added as-is from the draft posted, will introduce code duplication in terms of executing the shell.", "When that code is reviewed, we should try to eliminate that.", "In the unit test, principal is spelled as principle.", "In the second-to-last line of the unit test, there is a spelling error of subject.", "The provided unit test is very happy pathy.", "It'd be great if there were more testing of failures.", "Gary suggested testing what happens if we pass a user name that doesn't exist.", "Of note, with the patch applied I'm seeing a time-out for TestIPC and a failure of TestIPCServerResponder:testServerResponder (null value encountered).", "I'm not seeing this without the patch applied.", "We should investigate why this is happening before submitting an updated patch.", "implemented review's comments and fixed the tests", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12427260/HADOOP-4656-4.patch", "against trunk revision 887472.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/console", "This message is automatically generated.", "for fakeUser - do not fail NameNode.", "Return a valid Subject object with 0 lengths groups list.", "Adjusted TestUnixGroupInformation test to this change", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12427961/HADOOP-4656-6.patch", "against trunk revision 890502.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/console", "This message is automatically generated.", "Reviewed v6 of patch:", "It may be a good idea to explicitly document that getGroups (both in GroupMappingServiceProvider and Groups) will return an empty list for a non-existent user.", "This avoids any future null-related confusion.", "Comment on RefreshUserToGroupMappingProtocol seems incomplete", "\"just return the emptly list\" - empty spelled incorrectly", "The change to the exception message in setUserGroupNames no longer reflects the fact that userName shouldn't be a zero-length string", "In TestUnixUserGroupInformation, {code}testConstructorFailures(USER_NAME, new String[0]); // valid case now{code} is commented out, and should be removed.", "In the negative test, an extra assert that getPrincipals is zero-length wouldn't hurt.", "{code}fail(\"fakeUser should have no grups\");{code} - groups is spelled incorrectly", "Almost forgot: is there any reason GroupMappingServiceProvider is an abstract class rather than an interface?", "It doesn't have any implemented methods...", "implemented comments from Jakob.", "Looks good.", "+1 pending Hudson.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12428248/HADOOP-4656-7.patch", "against trunk revision 891511.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/console", "This message is automatically generated.", "Integrated in Hadoop-Common-trunk-Commit #121 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/121/])", ".", "Add a user to groups mapping service (boryas and acmurthy_)", "Marking the issue as resolved."], "SplitGT": [" For non-existing user it will return EMPTY list"], "issueString": "Add a user to groups mapping service \nCurrently the IPC client sends the UGI which contains the user/group information for the Server. However this represents the groups for the user on the client-end. The more pertinent mapping from user to groups is actually the one seen by the Server. Hence the client should only send the user and we should add a 'group mapping service' so that the Server can query it for the mapping.\nHADOOP-4348 is switching IPC to use the JAAS Subject rather than UGI (which will become an internal artifact). While we are adding the user-to-group mapping service, I propose we change the IPC Client to send the JAAS Subject in the header rather than UGI, this will also be compatible with the way we will do Kerberos-based authentication via the GSS API.\nI propose a new abstract class Groups with a single method 'getGroups' as below:\n\n{code:title=Groups.java}\npublic abstract class Groups {\n  List<String> getGroups(String username);\n}\n{code}\n\nwith a concrete implementation which gets the unix groups for the given user.\n\nPreliminary patch while I continue testing.\n\n> I propose we change the IPC Client to send the JAAS Subject in the header rather than UGI, this will also be compatible with the way we will do Kerberos-based authentication via the GSS API.\n\nJust want to clarify that application code doesn't send anything when using Kerberos. It's all hiding inside the GSS API library. After authentication, server can query the established GSS context to get client ID as GSSName which can be converted to a String. So for compatibility, IPC Client doesn't have to send JAAS Subject in the header. Send a String is fine.\nGroups should definitely come from asking the host OS in some form using the Java equivalent of getgrent() and friends. [ Be aware that getgroups() is BSD-specific and may not be available on System V, such as Solaris and HP-UX.]  Doing this via shell call out is just going to exasperate the memory problems we already see, especially on the secondary name node that requires more memory than the primary due to the fork of whoami/id! \n\nIt also opens up yet another security hole where any random groups command on the name nodes path can be used to override.  Not Good(tm).\n\nPrivately, someone asked about caching the group content.\n\nOne of the big advantages of talking to the OS is that many systems include a naming services caching daemon that handles caching group and similar content for the entire machine.  nscd generally includes great support for controlling the size, ttl, negative ttl, etc, for the cache.  Duplicating that functionality seems like overkill and, worse, will act as a cache against a cache!\n\nArun, can we get this one done soon? I'm working on 4343, which depends on this. Thanks.\nPlease consider passing the authentication context to the getGroups() method,\nas it might be easier to retrieve the associated groups using that information, \nthen based only on the username.\n\nFor example in POSIX environments it is faster to do a lookup based on the \nnumeric UID, than based on the username.\n\nIf you are using Kerberos with PAC, then the authentication context may already\ncontain a list of associated groups:\nhttp://k5wiki.kerberos.org/wiki/Projects/PAC_and_principal_APIs\n\nThere is a similar solution based on X509 authentication, where the associated\nlist of groups is embedded into the authentication context.\nAFAIK, Hadoop has no concept of uid, as everything in the HDFS, etc, is stored as a string. So the username is about all the context you can probably get. :)\nPreliminary patch, with some testing done.\nThis patch will create two versions of SecurityUtil.getSubject. One that builds list of group principles from UGI group list and another one that builds the list from UNIX id command. Do we really need the first one? I suggest we remove it.\nleft both getSubject in place.\nadded test for the new getSubject(user) \nRemoved Timer thread for refreshing cache of groups mapping. Instead using lazy approach - verifying cacheTimeout on access.\n\nAlso implemented few comments form Jakob:\n1. removed author tag\n2. using CommonConfigurationKeys constants for conf.keys.\nReviewed patch:\n* Nit: Calling an abstract class GroupMappingImpl seems a bit odd, even if it is technically correct for this.  Service provider, maybe?\n* In Groups.java the previous timer-based code is still present, but commented out.  Needs removed.\n* Note: HADOOP-6299, if added as-is from the draft posted, will introduce code duplication in terms of executing the shell.  When that code is reviewed, we should try to eliminate that.\n* In the unit test, principal is spelled as principle.\n* In the second-to-last line of the unit test, there is a spelling error of subject.\n* The provided unit test is very happy pathy.  It'd be great if there were more testing of failures. Gary suggested testing what happens if we pass a user name that doesn't exist.  \n\nOf note, with the patch applied I'm seeing a time-out for TestIPC and a failure of TestIPCServerResponder:testServerResponder (null value encountered).  I'm not seeing this without the patch applied.  We should investigate why this is happening before submitting an updated patch. \nimplemented review's comments and fixed the tests\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12427260/HADOOP-4656-4.patch\n  against trunk revision 887472.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/172/console\n\nThis message is automatically generated.\nfor fakeUser - do not fail NameNode. Return a valid Subject object with 0 lengths groups list.\nAdjusted TestUnixGroupInformation test to this change\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12427961/HADOOP-4656-6.patch\n  against trunk revision 890502.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/203/console\n\nThis message is automatically generated.\nReviewed v6 of patch:\n* It may be a good idea to explicitly document that getGroups (both in GroupMappingServiceProvider and Groups) will return an empty list for a non-existent user. This avoids any future null-related confusion.\n* Comment on RefreshUserToGroupMappingProtocol seems incomplete\n* \"just return the emptly list\" - empty spelled incorrectly\n* The change to the exception message in setUserGroupNames no longer reflects the fact that userName shouldn't be a zero-length string\n* In TestUnixUserGroupInformation, {code}testConstructorFailures(USER_NAME, new String[0]); // valid case now{code} is commented out, and should be removed.\n* In the negative test, an extra assert that getPrincipals is zero-length wouldn't hurt.\n* {code}fail(\"fakeUser should have no grups\");{code} - groups is spelled incorrectly\n\nAlmost forgot: is there any reason GroupMappingServiceProvider is an abstract class rather than an interface? It doesn't have any implemented methods...\nimplemented comments from Jakob. \nLooks good.  +1 pending Hudson.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12428248/HADOOP-4656-7.patch\n  against trunk revision 891511.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/31/console\n\nThis message is automatically generated.\nIntegrated in Hadoop-Common-trunk-Commit #121 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/121/])\n    . Add a user to groups mapping service (boryas and acmurthy_)\n\nMarking the issue as resolved.\n", "issueSearchSentences": ["List<String> getGroups(String username);", "with a concrete implementation which gets the unix groups for the given user.", "Groups should definitely come from asking the host OS in some form using the Java equivalent of getgrent() and friends.", "[ Be aware that getgroups() is BSD-specific and may not be available on System V, such as Solaris and HP-UX.]", "It may be a good idea to explicitly document that getGroups (both in GroupMappingServiceProvider and Groups) will return an empty list for a non-existent user."], "issueSearchScores": [0.6284836530685425, 0.6283391714096069, 0.5896857976913452, 0.5765361189842224, 0.5587708950042725]}
{"aId": 57, "code": "public void removeMap(String mapId) {\n    IndexInformation info = cache.get(mapId);\n    if ((info != null) && (info.getSize() == 0)) {\n      return;\n    }\n    info = cache.remove(mapId);\n    if (info != null) {\n      totalMemoryUsed.addAndGet(-info.getSize());\n      if (!queue.remove(mapId)) {\n        LOG.warn(\"Map ID\" + mapId + \" not found in queue!!\");\n      }\n    } else {\n      LOG.info(\"Map ID \" + mapId + \" not found in cache\");\n    }\n  }", "comment": " This method removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed.", "issueId": "MAPREDUCE-2541", "issueStringList": ["Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception", "The race condition goes like this:", "Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())", "Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());", "When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).", "When this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.", "A quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.", "patch to 0.21 branch.", "Make IndexCache.removeMap() do nothing", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12480750/MAPREDUCE-2541.patch", "against trunk revision 1128394.", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "org.apache.hadoop.tools.TestHadoopArchives", "org.apache.hadoop.tools.TestHarFileSystem", "1 contrib tests.", "The patch failed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//testReport/", "Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//console", "This message is automatically generated.", "It seems that there is something wrong with current trunk, recent PreCommit builds from #303~#320 all failed.", "https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/", "Changes:", "1. removeMap()removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed", "2. add checkTotalMemoryUsed() in IndexCache to check consistency, this is only used in unit test.", "3. add a unit test to construct the race condition, the test failed against current trunk code, and patched version passed the case on my computer.", "The failed test(TestMRCLI) posted by HadoopQA was not caused by this patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12481461/MAPREDUCE-2541.v2.patch", "against trunk revision 1131265.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//testReport/", "Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//console", "This message is automatically generated.", "I just committed this.", "Thanks Binglin!"], "SplitGT": [" This method removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed."], "issueString": "Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception\nThe race condition goes like this:\nThread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())\nThread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());\nWhen SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).\nWhen this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.\n\nA quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.\n\npatch to 0.21 branch. Make IndexCache.removeMap() do nothing\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12480750/MAPREDUCE-2541.patch\n  against trunk revision 1128394.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n                  org.apache.hadoop.tools.TestHadoopArchives\n                  org.apache.hadoop.tools.TestHarFileSystem\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//console\n\nThis message is automatically generated.\nIt seems that there is something wrong with current trunk, recent PreCommit builds from #303~#320 all failed.\nhttps://builds.apache.org/job/PreCommit-MAPREDUCE-Build/\nChanges:\n1. removeMap()removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed\n2. add checkTotalMemoryUsed() in IndexCache to check consistency, this is only used in unit test.\n3. add a unit test to construct the race condition, the test failed against current trunk code, and patched version passed the case on my computer.\n\nThe failed test(TestMRCLI) posted by HadoopQA was not caused by this patch. \n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12481461/MAPREDUCE-2541.v2.patch\n  against trunk revision 1131265.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//console\n\nThis message is automatically generated.\nI just committed this. Thanks Binglin!\n", "issueSearchSentences": ["1. removeMap()removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed", "Make IndexCache.removeMap() do nothing", "Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());", "A quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.", "Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception"], "issueSearchScores": [0.7362629175186157, 0.6690465807914734, 0.6520203351974487, 0.6417917013168335, 0.46051841974258423]}
{"aId": 58, "code": "public DatanodeStateMachine(Configuration conf) throws IOException {\n    this.conf = conf;\n    executorService = HadoopExecutors.newScheduledThreadPool(\n        this.conf.getInt(OzoneConfigKeys.OZONE_SCM_CONTAINER_THREADS,\n            OzoneConfigKeys.OZONE_SCM_CONTAINER_THREADS_DEFAULT),\n        new ThreadFactoryBuilder().setDaemon(true)\n            .setNameFormat(\"Datanode State Machine Thread - %d\").build());\n    connectionManager = new SCMConnectionManager(conf);\n    context = new StateContext(this.conf, DatanodeStates.getInitState(), this);\n    heartbeatFrequency = OzoneClientUtils.getScmHeartbeatInterval(conf);\n    container = new OzoneContainer(conf);\n  }", "comment": " Constructs a a datanode state machine.", "issueId": "HDFS-11108", "issueStringList": ["Ozone: use containers with the state machine", "Use containers via the new added state machine.", "Adding patch for early code review.", "This patch depends on HDFS-11081 and HDFS-11103", "Thanks [~anu] for working on this.", "The patch looks good to me.", "I just have few minor issues.", "Regarding rename the OzoneContainer to SCMContainer, it can be done with a separate JIRA.", "Datanode.java*", "Line 399: NIT: can we name containerStateMachine to datanodeStateMachine?", "DatanodeStateMachine.java*", "Line 49 NIT: \u201cOzoneContainer\u201d should be \u201cSCMContainer\u201d but this can be addressed later.", "Line 51 NIT:  \u201c  * Constructs a container state machine.\u201d should be \u201c  * Constructs a datanode state machine.", "Line 55 The change of exception declaration is not needed, the InterruptedException | ExecutionException | TimeoutException", "has been caught and IOException is good enough.", "This also reduces exception declaration changes in some of the tests to match it.", "TestDatanodeStateMachine.java*", "Line 117, 122, 169: changes are not needed.", "Updated patch to address review comments.", "The change related to exception is still needed because the ozoneContainer class throws exception.", "Addressed all other comments.", "Thanks [~anu] for the update.", "I think we need to update the exception declaration of methods from XceiverServer to OzoneContainer to DatanodeStateMachine as they throw IOException instead of Exception.", "Also found that XceiverServer#stop() misses an exception declaration even though the Javadoc said so.", "[~xyao] Thanks for the comment about the exception.", "I have fixed that issue in this patch.", "+1 pending Jenkins and the dependency on HDFS-11081 .", "| (x) *{color:red}-1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 16m 14s{color} | {color:blue} Docker mode activated.", "{color} |", "| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags.", "{color} |", "| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files.", "{color} |", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 10m 40s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 46s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 30s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 52s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in HDFS-7240 has 9 extant Findbugs warnings.", "{color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} HDFS-7240 passed {color} |", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |", "| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 44s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs generated 3 new + 102 unchanged - 3 fixed = 105 total (was 105) {color} |", "| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 26s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 164 unchanged - 4 fixed = 165 total (was 168) {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 54s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |", "| {color:red}-1{color} | {color:red} unit {color} | {color:red} 59m 19s{color} | {color:red} hadoop-hdfs in the patch failed.", "{color} |", "| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings.", "{color} |", "| {color:black}{color} | {color:black} {color} | {color:black} 98m 28s{color} | {color:black} {color} |", "\\\\", "\\\\", "|| Reason || Tests ||", "| Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |", "|   | hadoop.hdfs.TestDFSShell |", "|   | hadoop.hdfs.tools.TestDelegationTokenFetcher |", "|   | hadoop.ozone.container.common.TestDatanodeStateMachine |", "|   | hadoop.hdfs.server.datanode.TestFsDatasetCache |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| Docker |  Image:yetus/hadoop:e809691 |", "| JIRA Issue | HDFS-11108 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12839055/HDFS-11108-HDFS-7240.003.patch |", "| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |", "| uname | Linux 08ae6231292d 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |", "| Build tool | maven |", "| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |", "| git revision | HDFS-7240 / d10f39e |", "| Default Java | 1.8.0_111 |", "| findbugs | v3.0.0 |", "| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |", "| javac | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/diff-compile-javac-hadoop-hdfs-project_hadoop-hdfs.txt |", "| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |", "| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |", "|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/testReport/ |", "| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |", "| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/console |", "| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |", "This message was automatically generated.", "most test failures are not related to this patch.", "One test failure is not reproducible locally.", "In short, test failures seem to be infrastructure issues.", "I will commit this patch shortly.", "[~xyao] Thanks for the reviews.", "I have committed this to the feature branch.", "SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14057 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14057/])", "HDFS-11108.", "Ozone: use containers with the state machine.", "Contributed by (aengineer: rev 52925ef824910c7509daa4e43cef7278283ea431)", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/XceiverServer.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainer.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/common/TestDatanodeStateMachine.java"], "SplitGT": [" Constructs a a datanode state machine."], "issueString": "Ozone: use containers with the state machine\nUse containers via the new added state machine.\nAdding patch for early code review. This patch depends on HDFS-11081 and HDFS-11103\nThanks [~anu] for working on this. The patch looks good to me. I just have few minor issues. Regarding rename the OzoneContainer to SCMContainer, it can be done with a separate JIRA.\n\n\n*Datanode.java*\nLine 399: NIT: can we name containerStateMachine to datanodeStateMachine?\n\n*DatanodeStateMachine.java*\nLine 49 NIT: \u201cOzoneContainer\u201d should be \u201cSCMContainer\u201d but this can be addressed later. \nLine 51 NIT:  \u201c  * Constructs a container state machine.\u201d should be \u201c  * Constructs a datanode state machine.\nLine 55 The change of exception declaration is not needed, the InterruptedException | ExecutionException | TimeoutException\nhas been caught and IOException is good enough. This also reduces exception declaration changes in some of the tests to match it.\n\n*TestDatanodeStateMachine.java*\nLine 117, 122, 169: changes are not needed.\n\n\nUpdated patch to address review comments.\n\nThe change related to exception is still needed because the ozoneContainer class throws exception. Addressed all other comments.\nThanks [~anu] for the update. I think we need to update the exception declaration of methods from XceiverServer to OzoneContainer to DatanodeStateMachine as they throw IOException instead of Exception. Also found that XceiverServer#stop() misses an exception declaration even though the Javadoc said so.\n[~xyao] Thanks for the comment about the exception. I have fixed that issue in this patch.\n+1 pending Jenkins and the dependency on HDFS-11081 .\n| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 16m 14s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 10m 40s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 46s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 30s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 52s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in HDFS-7240 has 9 extant Findbugs warnings. {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} HDFS-7240 passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 44s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs generated 3 new + 102 unchanged - 3 fixed = 105 total (was 105) {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 26s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 164 unchanged - 4 fixed = 165 total (was 168) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 54s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 59m 19s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 98m 28s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\n|   | hadoop.hdfs.TestDFSShell |\n|   | hadoop.hdfs.tools.TestDelegationTokenFetcher |\n|   | hadoop.ozone.container.common.TestDatanodeStateMachine |\n|   | hadoop.hdfs.server.datanode.TestFsDatasetCache |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:e809691 |\n| JIRA Issue | HDFS-11108 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12839055/HDFS-11108-HDFS-7240.003.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 08ae6231292d 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | HDFS-7240 / d10f39e |\n| Default Java | 1.8.0_111 |\n| findbugs | v3.0.0 |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |\n| javac | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/diff-compile-javac-hadoop-hdfs-project_hadoop-hdfs.txt |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17594/console |\n| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n\nmost test failures are not related to this patch. One test failure is not reproducible locally.  In short, test failures seem to be infrastructure issues. I will commit this patch shortly.\n[~xyao] Thanks for the reviews. I have committed this to the feature branch.\nSUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14057 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14057/])\nHDFS-11108. Ozone: use containers with the state machine. Contributed by (aengineer: rev 52925ef824910c7509daa4e43cef7278283ea431)\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/XceiverServer.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainer.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/common/TestDatanodeStateMachine.java\n\n", "issueSearchSentences": ["(edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java", "(edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/common/TestDatanodeStateMachine.java", "DatanodeStateMachine.java*", "|   | hadoop.ozone.container.common.TestDatanodeStateMachine |", "TestDatanodeStateMachine.java*"], "issueSearchScores": [0.6454209685325623, 0.6174826622009277, 0.5916438102722168, 0.5687556862831116, 0.5445871353149414]}
{"aId": 59, "code": "@Override\n  public boolean useLogicalURI() {\n    return true;\n  }", "comment": " Logical URI is required for this failover proxy provider.", "issueId": "HDFS-6334", "issueStringList": ["Client failover proxy provider for IP failover based NN HA", "With RPCv9 and improvements in the SPNEGO auth handling, it is possible to set up a pair of HA namenodes utilizing IP failover as client-request fencing mechanism.", "This jira will make it possible for HA to be configured without requiring use of logical URI and provide a simple IP failover proxy provider.", "The change will allow any old implementation of {{FailoverProxyProvider}} to continue to work.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643379/HDFS-6334.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:red}-1 javac{color}.", "The applied patch generated 1281 javac compiler warnings (more than the trunk's current 1275 warnings).", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 1 warning messages.", "See https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//testReport/", "Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavacWarnings.txt", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//console", "This message is automatically generated.", "The new patch fixes the warnings.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643875/HDFS-6334.v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//console", "This message is automatically generated.", "[~tlipcon], [~atm]: can one of you review this?", "Patch looks great to me, Kihwal.", "Thanks for doing it.", "Two little nits:", "# bq.", "+   * used, a special token handling mat be needed to make sure a token acquired", "s/mat/may/g", "# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\"", "While strictly true, I'm guessing you meant to put a different comment here.", "+1 once these are addressed.", "This is the diff between v2 and v3 of the patch.", "All changes are in comment.", "So I won't wait for precommit.", "{panel}", "293c293", "< +   * used, a special token handling mat be needed to make sure a token acquired", "> +   * used, a special token handling may be needed to make sure a token acquired", "338c338", "< +   * Logical URI is not used for IP failover.", "> +   * Logical URI is required for this failover proxy provider.", "{panel}", "Thanks for the review, Aaron.", "I've committed this to trunk and branch-2."], "SplitGT": [" Logical URI is required for this failover proxy provider."], "issueString": "Client failover proxy provider for IP failover based NN HA\nWith RPCv9 and improvements in the SPNEGO auth handling, it is possible to set up a pair of HA namenodes utilizing IP failover as client-request fencing mechanism.\n\nThis jira will make it possible for HA to be configured without requiring use of logical URI and provide a simple IP failover proxy provider.  The change will allow any old implementation of {{FailoverProxyProvider}} to continue to work.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643379/HDFS-6334.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n      {color:red}-1 javac{color}.  The applied patch generated 1281 javac compiler warnings (more than the trunk's current 1275 warnings).\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.\n        See https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//testReport/\nJavac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavacWarnings.txt\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//console\n\nThis message is automatically generated.\nThe new patch fixes the warnings.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643875/HDFS-6334.v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//console\n\nThis message is automatically generated.\n[~tlipcon], [~atm]: can one of you review this?\nPatch looks great to me, Kihwal. Thanks for doing it. Two little nits:\n\n# bq. +   * used, a special token handling mat be needed to make sure a token acquired \ns/mat/may/g\n# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\" While strictly true, I'm guessing you meant to put a different comment here.\n\n+1 once these are addressed.\nThis is the diff between v2 and v3 of the patch. All changes are in comment. So I won't wait for precommit.\n\n{panel}\n293c293\n< +   * used, a special token handling mat be needed to make sure a token acquired \n---\n> +   * used, a special token handling may be needed to make sure a token acquired \n338c338\n< +   * Logical URI is not used for IP failover.\n---\n> +   * Logical URI is required for this failover proxy provider.\n{panel}\nThanks for the review, Aaron. I've committed this to trunk and branch-2.\n", "issueSearchSentences": ["# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\"", "< +   * Logical URI is not used for IP failover.", "> +   * Logical URI is required for this failover proxy provider.", "293c293", "338c338"], "issueSearchScores": [0.5677535533905029, 0.5638187527656555, 0.5057244896888733, 0.37096256017684937, 0.3647368550300598]}
{"aId": 60, "code": "@InterfaceAudience.LimitedPrivate( { \"HDFS\", \"MapReduce\" })\n  public List<Token<?>> getDelegationTokens(String renewer) throws IOException {\n    return null;\n  }", "comment": " Get one or more delegation tokens associated with the filesystem. Normally a file system returns a single delegation token. A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages", "issueId": "HADOOP-6994", "issueStringList": ["Api to get delegation token in AbstractFileSystem", "APIs to get delegation tokens is required in AbstractFileSystem.", "AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed.", "A path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "bq.", "The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.", "I also have this concern, particularly in regard to HADOOP-6988.", "New patch addressing the comment.", "The InterfaceAudience for the new APIs are annotated to be LimitedPrivate.", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "This patch is based against the latest trunk.", "New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090039.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console", "This message is automatically generated.", "# AbstractFileSystem.java", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem.", "Normally a file system returns a single delegation token.", "A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.", "\"This comment applies to FileSystem also.", "# When a file system returns multiple delegation tokens, how does renew and cancel work?", "Woudl the file system know where to renew/cancel the token?", "2.", "OK", "3.", "For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.", "+1 for the patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.fs.TestFilterFileSystem", "org.apache.hadoop.fs.TestFilterFs", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console", "This message is automatically generated.", "The latest patch fixes FilterFileSystem and FilterFS.", "The renew and cancel delegation token APIs are removed from the patch.", "These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself.", "Implementation in SecurityUtil will be taken up in a separate jira.", "+1 for the change.", "Updated patch includes a javadoc for the new unit test.", "Rest is all identical with previous patch.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch", "against trunk revision 1090485.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 4 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console", "This message is automatically generated.", "I have committed this.", "Integrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])", "HADOOP-6994.", "Api to get delegation token in AbstractFileSystem.", "Contributed by jitendra."], "SplitGT": [" Get one or more delegation tokens associated with the filesystem.", "Normally a file system returns a single delegation token.", "A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages"], "issueString": "Api to get delegation token in AbstractFileSystem\nAPIs to get delegation tokens is required in AbstractFileSystem. AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed. \nA path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path. \nThe new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent.\nbq. The new methods on AbstractFileSystem are public, yet Token and AbstractDelegationTokenIdentifier are LimitedPrivate, which seems inconsistent. \nI also have this concern, particularly in regard to HADOOP-6988. \nNew patch addressing the comment.\n\nThe InterfaceAudience for the new APIs are annotated to be LimitedPrivate.\nThe getDelegationToken API in FileSystem is also changed to LimitedPrivate.\n\nThis patch is based against the latest trunk.\nNew patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090039.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/335//console\n\nThis message is automatically generated.\n# AbstractFileSystem.java\n# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\". It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem. Normally a file system returns a single delegation token. A file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages.\"This comment applies to FileSystem also.\n# When a file system returns multiple delegation tokens, how does renew and cancel work? Woudl the file system know where to renew/cancel the token?\n\n2. OK \n3. For renew and cancel, a file system that manages multiple filesystems underneath should pick the relevant filesystem by matching the service set in the token.\n+1 for the patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12475754/HADOOP-6994.5.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.fs.TestFilterFileSystem\n                  org.apache.hadoop.fs.TestFilterFs\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/341//console\n\nThis message is automatically generated.\nThe latest patch fixes FilterFileSystem and FilterFS. The renew and cancel delegation token APIs are removed from the patch. These APIs should be implemented as a util methods in SecurityUtil and the filesystem to renew or cancel can be figured out from the service and kind set in the token itself. Implementation in SecurityUtil will be taken up in a separate jira. \n+1 for the change.\nUpdated patch includes a javadoc for the new unit test. Rest is all identical with previous patch.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12476162/HADOOP-6994.9.patch\n  against trunk revision 1090485.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 4 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/343//console\n\nThis message is automatically generated.\nI have committed this.\nIntegrated in Hadoop-Common-trunk-Commit #549 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/549/])\n    HADOOP-6994. Api to get delegation token in AbstractFileSystem. Contributed by jitendra.\n\n", "issueSearchSentences": ["New patch adds the getDelegationTokens API in FileContext and the API in AFS is changed to return a list of tokens.", "The getDelegationToken API in FileSystem is also changed to LimitedPrivate.", "# #getDelegationTokens() - Javadoc says \"Get a new delegation token for this file system.\".", "The renew and cancel delegation token APIs are removed from the patch.", "It would be good to explain it as \"Get one or more delegations tokens associated with the filesystem."], "issueSearchScores": [0.6543129682540894, 0.6527262926101685, 0.6516562700271606, 0.6012526750564575, 0.5929394364356995]}
{"aId": 61, "code": "public static void determineTimestampsAndCacheVisibilities(Configuration job)\n  throws IOException {\n    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();\n    determineTimestamps(job, statCache);\n    determineCacheVisibilities(job, statCache);\n  }", "comment": " Determines the visibilities of the distributed cache files and archives.", "issueId": "MAPREDUCE-4907", "issueStringList": ["TrackerDistributedCacheManager issues too many getFileStatus calls", "TrackerDistributedCacheManager issues a number of redundant getFileStatus calls when determining the timestamps and visibilities of files in the distributed cache.", "300 distributed cache files deep in the directory structure can hammer HDFS with a couple thousand requests.", "A couple optimizations can reduce this load:", "1. determineTimestamps and determineCacheVisibilities both call getFileStatus on every file.", "We could cache the results of the former and use them for the latter.", "2. determineCacheVisibilities needs to check that all ancestor directories of each file have execute permissions for everyone.", "This currently entails a getFileStatus on each ancestor directory for each file.", "The results of these getFileStatus calls could be cached as well.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562625/MAPREDUCE-4907.patch", "against trunk revision .", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3182//console", "This message is automatically generated.", "Sandy, patch looks good, I was looking at trunk/branch-2 and I think we should have the same optimization for trunk/branch-2 (in ClientDistributedCacheManager).", "Attached patch for trunk", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12564046/MAPREDUCE-4907-trunk.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:red}-1 javac{color}.", "The applied patch generated 2019 javac compiler warnings (more than the trunk's current 2013 warnings).", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//testReport/", "Javac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//artifact/trunk/patchprocess/diffJavacWarnings.txt", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//console", "This message is automatically generated.", "Sandy, looks good, a couple of minor things:", "the patch for trunk has 2 getFileStatus() with duplicate logic for adding an entry to the cache, we should have one delegating to the other so the cache addition logic is not dup.", "For branch-1 it is OK because the filesystem access is done in another class for one of the getFileStatus() (not to change the behavior I assume).", "javac warnings, are they related?", "The compiler warnings are because DistributedCache in mapred.filecache is deprecated.", "I'll change it to use the one in org.apache.hadoop.filecache and remove the 2nd trunk getFileStatus.", "LGTM, +1 pending jenkins.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12564053/MAPREDUCE-4907-trunk-1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:", "org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3223//testReport/", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3223//console", "This message is automatically generated.", "Attached a patch that fixes test", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12564059/MAPREDUCE-4907-trunk-1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:", "org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3224//testReport/", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3224//console", "This message is automatically generated.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12564062/MAPREDUCE-4907-trunk-1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3225//testReport/", "Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3225//console", "This message is automatically generated.", "+1", "Thanks Sandy.", "Committed to trunk, branch-2 and branch-1.", "Integrated in Hadoop-trunk-Commit #3210 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3210/])", "MAPREDUCE-4907.", "TrackerDistributedCacheManager issues too many getFileStatus calls.", "(sandyr via tucu) (Revision 1431166)", "Result = SUCCESS", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1431166", "Files :", "hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java", "hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/ClientDistributedCacheManager.java"], "SplitGT": [" Determines the visibilities of the distributed cache files and archives."], "issueString": "TrackerDistributedCacheManager issues too many getFileStatus calls\nTrackerDistributedCacheManager issues a number of redundant getFileStatus calls when determining the timestamps and visibilities of files in the distributed cache.  300 distributed cache files deep in the directory structure can hammer HDFS with a couple thousand requests.\n\nA couple optimizations can reduce this load:\n1. determineTimestamps and determineCacheVisibilities both call getFileStatus on every file.  We could cache the results of the former and use them for the latter.\n2. determineCacheVisibilities needs to check that all ancestor directories of each file have execute permissions for everyone.  This currently entails a getFileStatus on each ancestor directory for each file.  The results of these getFileStatus calls could be cached as well.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562625/MAPREDUCE-4907.patch\n  against trunk revision .\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3182//console\n\nThis message is automatically generated.\nSandy, patch looks good, I was looking at trunk/branch-2 and I think we should have the same optimization for trunk/branch-2 (in ClientDistributedCacheManager).\n\n\n\n\nAttached patch for trunk\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12564046/MAPREDUCE-4907-trunk.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n      {color:red}-1 javac{color}.  The applied patch generated 2019 javac compiler warnings (more than the trunk's current 2013 warnings).\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//testReport/\nJavac warnings: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//artifact/trunk/patchprocess/diffJavacWarnings.txt\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3222//console\n\nThis message is automatically generated.\nSandy, looks good, a couple of minor things:\n\n* the patch for trunk has 2 getFileStatus() with duplicate logic for adding an entry to the cache, we should have one delegating to the other so the cache addition logic is not dup. For branch-1 it is OK because the filesystem access is done in another class for one of the getFileStatus() (not to change the behavior I assume).\n* javac warnings, are they related?\nThe compiler warnings are because DistributedCache in mapred.filecache is deprecated.  I'll change it to use the one in org.apache.hadoop.filecache and remove the 2nd trunk getFileStatus.\nLGTM, +1 pending jenkins.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12564053/MAPREDUCE-4907-trunk-1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:\n\n                  org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3223//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3223//console\n\nThis message is automatically generated.\nAttached a patch that fixes test\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12564059/MAPREDUCE-4907-trunk-1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:\n\n                  org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3224//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3224//console\n\nThis message is automatically generated.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12564062/MAPREDUCE-4907-trunk-1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3225//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3225//console\n\nThis message is automatically generated.\n+1\nThanks Sandy. Committed to trunk, branch-2 and branch-1.\nIntegrated in Hadoop-trunk-Commit #3210 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3210/])\n    MAPREDUCE-4907. TrackerDistributedCacheManager issues too many getFileStatus calls. (sandyr via tucu) (Revision 1431166)\n\n     Result = SUCCESS\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1431166\nFiles : \n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/ClientDistributedCacheManager.java\n\n", "issueSearchSentences": ["1. determineTimestamps and determineCacheVisibilities both call getFileStatus on every file.", "TrackerDistributedCacheManager issues a number of redundant getFileStatus calls when determining the timestamps and visibilities of files in the distributed cache.", "TrackerDistributedCacheManager issues too many getFileStatus calls.", "TrackerDistributedCacheManager issues too many getFileStatus calls", "2. determineCacheVisibilities needs to check that all ancestor directories of each file have execute permissions for everyone."], "issueSearchScores": [0.6856916546821594, 0.5735432505607605, 0.4402613937854767, 0.43104270100593567, 0.42807745933532715]}
{"aId": 64, "code": "public void disallowSnapshot(String snapshotRoot) throws IOException {\n    namenode.disallowSnapshot(snapshotRoot);\n  }", "comment": " Disallow snapshot on a directory.", "issueId": "HDFS-4084", "issueStringList": ["provide CLI support for allow and disallow snapshot on a directory", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "@Brandon", "Can we make the new commands case insensitive?", "We can log a different jira to make existing commands also case insensitive.", "Good point, Arpit.", "Will change it and upload a new patch.", "Re-based the patch and addressed Arpit's comment.", "Comments:", "# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods", "{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "{quote}", "Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "Added javadoc for them.", "{quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "{quote}", "Removed, realized these two methods need to be public anyway.", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote}", "Yes.", "removed.", "{quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote}", "Done.", "Thanks.", "I committed the patch to HDFS-2802 branch.", "Thank you Brandon."], "SplitGT": [" Disallow snapshot on a directory."], "issueString": "provide CLI support for allow and disallow snapshot on a directory\nTo provide CLI support to allow snapshot, disallow snapshot on a directory.\n@Brandon\n\nCan we make the new commands case insensitive? We can log a different jira to make existing commands also case insensitive. \nGood point, Arpit. Will change it and upload a new patch. \nRe-based the patch and addressed Arpit's comment.\nComments:\n# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.\n# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.\n# FSNamesystem.java is unnecessarily importing SnapshotInfo?\n# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods\n\n{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.{quote} \nMethods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override. Added javadoc for them.\n   {quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.{quote} \nRemoved, realized these two methods need to be public anyway.\n    {quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?{quote} \nYes. removed.\n    {quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote} \nDone. Thanks.\n\nI committed the patch to HDFS-2802 branch. Thank you Brandon.\n", "issueSearchSentences": ["Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "provide CLI support for allow and disallow snapshot on a directory", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?"], "issueSearchScores": [0.582577109336853, 0.5776619911193848, 0.4721028804779053, 0.40601247549057007, 0.40511661767959595]}
{"aId": 65, "code": "public static AuthenticationMethod getRealAuthenticationMethod(\n      UserGroupInformation ugi) {\n    AuthenticationMethod authMethod = ugi.getAuthenticationMethod();\n    if (authMethod == AuthenticationMethod.PROXY) {\n      authMethod = ugi.getRealUser().getAuthenticationMethod();\n    }\n    return authMethod;\n  }", "comment": " Returns the authentication method of a ugi. If the authentication method is PROXY, returns the authentication method of the real user.", "issueId": "HADOOP-6814", "issueStringList": ["Method in UGI to get the authentication method of the real user.", "UGI should have a method to return the authentication method of the real user for a proxy-user scenario.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12446545/HADOOP-6814.1.patch", "against trunk revision 952471.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/console", "This message is automatically generated.", "Minor nit - should we rename getAuthenticationMethod in the patch to getRealAuthenticationMethod or some such (just to stress that this method returns the actual underlying authentication method).", "New patch addressing the comment.", "The name of the method is changed to getRealAuthenticationMethod.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12446884/HADOOP-6814.2.patch", "against trunk revision 953517.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/console", "This message is automatically generated.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12446884/HADOOP-6814.2.patch", "against trunk revision 953517.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/console", "This message is automatically generated.", "Javadoc warnings are unrelated to this patch.", "Following warnings are showing up in console output", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:29: warning: sun.security.krb5.Config is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc] import sun.security.krb5.Config;", "[exec]   [javadoc]                         ^", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:30: warning: sun.security.krb5.KrbException is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc] import sun.security.krb5.KrbException;", "[exec]   [javadoc]                         ^", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:77: warning: sun.security.krb5.Config is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc]   private static Config kerbConf;", "[exec]   [javadoc]                  ^", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:31: warning: sun.security.jgss.krb5.Krb5Util is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc] import sun.security.jgss.krb5.Krb5Util;", "[exec]   [javadoc]                              ^", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:32: warning: sun.security.krb5.Credentials is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc] import sun.security.krb5.Credentials;", "[exec]   [javadoc]                         ^", "[exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:33: warning: sun.security.krb5.PrincipalName is Sun proprietary API and may be removed in a future release", "[exec]   [javadoc] import sun.security.krb5.PrincipalName;", "I just committed this.", "Thanks, Jitendra!", "Integrated in Hadoop-Common-trunk-Commit #296 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/296/])", "HADOOP-6814.", "Adds an API in UserGroupInformation to get the real authentication method of a passed UGI.", "Contributed by Jitendra Pandey."], "SplitGT": [" Returns the authentication method of a ugi.", "If the authentication method is PROXY, returns the authentication method of the real user."], "issueString": "Method in UGI to get the authentication method of the real user.\nUGI should have a method to return the authentication method of the real user for a proxy-user scenario.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12446545/HADOOP-6814.1.patch\n  against trunk revision 952471.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/572/console\n\nThis message is automatically generated.\nMinor nit - should we rename getAuthenticationMethod in the patch to getRealAuthenticationMethod or some such (just to stress that this method returns the actual underlying authentication method).\nNew patch addressing the comment. The name of the method is changed to getRealAuthenticationMethod.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12446884/HADOOP-6814.2.patch\n  against trunk revision 953517.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/78/console\n\nThis message is automatically generated.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12446884/HADOOP-6814.2.patch\n  against trunk revision 953517.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/580/console\n\nThis message is automatically generated.\nJavadoc warnings are unrelated to this patch.\n\nFollowing warnings are showing up in console output\n\n  [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:29: warning: sun.security.krb5.Config is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc] import sun.security.krb5.Config;\n     [exec]   [javadoc]                         ^\n     [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:30: warning: sun.security.krb5.KrbException is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc] import sun.security.krb5.KrbException;\n     [exec]   [javadoc]                         ^\n     [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/KerberosName.java:77: warning: sun.security.krb5.Config is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc]   private static Config kerbConf;\n     [exec]   [javadoc]                  ^\n     [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:31: warning: sun.security.jgss.krb5.Krb5Util is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc] import sun.security.jgss.krb5.Krb5Util;\n     [exec]   [javadoc]                              ^\n     [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:32: warning: sun.security.krb5.Credentials is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc] import sun.security.krb5.Credentials;\n     [exec]   [javadoc]                         ^\n     [exec]   [javadoc] /grid/0/hudson/hudson-slave/workspace/Hadoop-Patch-h4.grid.sp2.yahoo.net/trunk/src/java/org/apache/hadoop/security/SecurityUtil.java:33: warning: sun.security.krb5.PrincipalName is Sun proprietary API and may be removed in a future release\n     [exec]   [javadoc] import sun.security.krb5.PrincipalName;\n\nI just committed this. Thanks, Jitendra!\nIntegrated in Hadoop-Common-trunk-Commit #296 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/296/])\n    HADOOP-6814. Adds an API in UserGroupInformation to get the real authentication method of a passed UGI. Contributed by Jitendra Pandey.\n\n", "issueSearchSentences": ["Adds an API in UserGroupInformation to get the real authentication method of a passed UGI.", "Method in UGI to get the authentication method of the real user.", "UGI should have a method to return the authentication method of the real user for a proxy-user scenario.", "The name of the method is changed to getRealAuthenticationMethod.", "Minor nit - should we rename getAuthenticationMethod in the patch to getRealAuthenticationMethod or some such (just to stress that this method returns the actual underlying authentication method)."], "issueSearchScores": [0.7684311866760254, 0.7417391538619995, 0.681578516960144, 0.6209185123443604, 0.5825525522232056]}
{"aId": 66, "code": "public static ByteString mergeRouterFederatedState(ByteString state1, ByteString state2) {\n    Map<String, Long> mapping1 = new HashMap<>(getRouterFederatedStateMap(state1));\n    Map<String, Long> mapping2 = getRouterFederatedStateMap(state2);\n    mapping2.forEach((k, v) -> {\n      long localValue = mapping1.getOrDefault(k, 0L);\n      mapping1.put(k, Math.max(v, localValue));\n    });\n    RouterFederatedStateProto.Builder federatedBuilder = RouterFederatedStateProto.newBuilder();\n    mapping1.forEach(federatedBuilder::putNamespaceStateIds);\n    return federatedBuilder.build().toByteString();\n  }", "comment": " Merge state1 and state2 to get the max value for each namespace.", "issueId": "HDFS-16837", "issueStringList": ["[RBF SBN] ClientGSIContext should merge RouterFederatedStates to get the max state id for each namespace", "ClientGSIContext should merge local and remote\u00a0RouterFederatedState to get the max state id for each namespace.", "And the related code as bellows:", "{code:java}", "@Override", "public synchronized void receiveResponseState(RpcResponseHeaderProto header) {", "if (header.hasRouterFederatedState()) {", "BUG here", "routerFederatedState = header.getRouterFederatedState();", "} else {", "lastSeenStateId.accumulate(header.getStateId());", "}", "} {code}"], "SplitGT": [" Merge state1 and state2 to get the max value for each namespace."], "issueString": "[RBF SBN] ClientGSIContext should merge RouterFederatedStates to get the max state id for each namespace\nClientGSIContext should merge local and remote\u00a0RouterFederatedState to get the max state id for each namespace.\r\n\r\nAnd the related code as bellows:\r\n{code:java}\r\n@Override\r\npublic synchronized void receiveResponseState(RpcResponseHeaderProto header) {\r\n  if (header.hasRouterFederatedState()) {\r\n    // BUG here\r\n    routerFederatedState = header.getRouterFederatedState();\r\n  } else {\r\n    lastSeenStateId.accumulate(header.getStateId());\r\n  }\r\n} {code}\n", "issueSearchSentences": ["routerFederatedState = header.getRouterFederatedState();", "if (header.hasRouterFederatedState()) {", "[RBF SBN] ClientGSIContext should merge RouterFederatedStates to get the max state id for each namespace", "ClientGSIContext should merge local and remote\u00a0RouterFederatedState to get the max state id for each namespace.", "lastSeenStateId.accumulate(header.getStateId());"], "issueSearchScores": [0.5386030673980713, 0.517905592918396, 0.4730890691280365, 0.4729766845703125, 0.4454652667045593]}
{"aId": 67, "code": "public static boolean fullyDelete(File dir) throws IOException {\n    if (dir.delete()) {\n      // dir is (a) normal file, (b) symlink to a file, (c) empty directory or\n      // (d) symlink to a directory\n      return true;\n    }\n\n    // handle nonempty directory deletion\n    if (!fullyDeleteContents(dir)) {\n      return false;\n    }\n    return dir.delete();\n  }", "comment": " The file pointed to by the symlink is not deleted. The directory pointed to by symlink is not deleted.", "issueId": "HADOOP-6536", "issueStringList": ["FileUtil.fullyDelete(dir) behavior is not defined when we pass a symlink as the argument", "FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink.", "If this is the behavior, it should be documented as so.", "Or it should be changed not to delete the contents of the sym-linked directory.", "Since when symlink to a file is sent as param to fullyDelete(), it deletes the symlink only(and not the file pointed to by symlink), I think fullyDelete() should only delete the symlink even when symlink to a dir is passed as param(and should not delete the dir pointed to by symlink).", "Based on a simple test I did on Linux, it appears that removal of a symlink (whether to file or directory) only removes the link.", "If I treat FileUtil.fullyDelete as a Java equivalent for \"rm -r\", then it seems to me that there's a bug in the implementation of FileUtil.fullyDelete.", "It should only delete the link and not contents of it.", "IOW, I am +1 for Ravi's proposal.", "+1 Ravi's proposal.", "I would also add that fullyDelete should delete dangling links (it currently does but we should add a test).", "The current behavior is due to java making it difficult to identify symlinks.", "One more issue.", "Currently FileUtil.fullyDelete(myDir) comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir.", "This is because we return from method if the recursive call \"if(!fullyDelete()) {return false;}\" fails at any level of recursion.", "Shouldn't it continue with deletion of other files/dirs continuing in the for loop instead of returning false here ?", "We can just set a boolean to false in this if() and can continue the for loop sothat return value of fullyDelete() is correct always.", "For example, if we have myDir/subdir1/file1, myDir/subdir2/file2 and if fullyDelete() tries to delete file1 first and could not  delete it(because subDir1 has nonwritable permissions), then fullyDelete() currently comes out without deleting file2 and subdir2.", "I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').", "Thoughts ?", "bq.", "I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').", "+1 for this.", "But this is a separate issue, so I am creating a new ticket for this.", "bq.", "I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').", "Filed HADOOP-6631 for this.", "Attaching patch for trunk.", "This patch is on top of HADOOP-6631.", "Attaching new patch to apply to the latest trunk as HADOOP-6631 has gone in.", "Took a look at the latest patch, the changes seem fine.", "Allowing the patch to go through Hudson...", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12443923/HADOOP-6536.v1.patch", "against trunk revision 941662.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/console", "This message is automatically generated.", "Changes look good.", "bq.", "I would also add that fullyDelete should delete dangling links (it currently does but we should add a test).", "Can you add a test for deleting dangling links also?", "Attaching patch adding more testcases.", "Patch looks good.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12449821/HADOOP-6536.v1.1.patch", "against trunk revision 964993.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/console", "This message is automatically generated.", "The failed test TestTrash passes on my local machine.", "Allowing to go through Hudson again...", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12449821/HADOOP-6536.v1.1.patch", "against trunk revision 965696.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/console", "This message is automatically generated.", ">> -1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "No javadoc warning is introduced in this patch.", "I just committed this.", "Thanks Ravi!", "Integrated in Hadoop-Common-trunk-Commit #332 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/332/])", "HADOOP-6536.", "Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory.", "Contributed by Ravi Gummadi", "Integrated in Hadoop-Common-trunk #398 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/398/])", "HADOOP-6536.", "Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory.", "Contributed by Ravi Gummadi"], "SplitGT": [" The file pointed to by the symlink is not deleted.", "The directory pointed to by symlink is not deleted."], "issueString": "FileUtil.fullyDelete(dir) behavior is not defined when we pass a symlink as the argument\nFileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. \nOr it should be changed not to delete the contents of the sym-linked directory.\nSince when symlink to a file is sent as param to fullyDelete(), it deletes the symlink only(and not the file pointed to by symlink), I think fullyDelete() should only delete the symlink even when symlink to a dir is passed as param(and should not delete the dir pointed to by symlink).\nBased on a simple test I did on Linux, it appears that removal of a symlink (whether to file or directory) only removes the link. If I treat FileUtil.fullyDelete as a Java equivalent for \"rm -r\", then it seems to me that there's a bug in the implementation of FileUtil.fullyDelete. It should only delete the link and not contents of it. IOW, I am +1 for Ravi's proposal.\n+1 Ravi's proposal. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). The current behavior is due to java making it difficult to identify symlinks.\nOne more issue.\n\nCurrently FileUtil.fullyDelete(myDir) comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir. This is because we return from method if the recursive call \"if(!fullyDelete()) {return false;}\" fails at any level of recursion.\n\nShouldn't it continue with deletion of other files/dirs continuing in the for loop instead of returning false here ? We can just set a boolean to false in this if() and can continue the for loop sothat return value of fullyDelete() is correct always.\n\nFor example, if we have myDir/subdir1/file1, myDir/subdir2/file2 and if fullyDelete() tries to delete file1 first and could not  delete it(because subDir1 has nonwritable permissions), then fullyDelete() currently comes out without deleting file2 and subdir2.\n\nI guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').\n\nThoughts ?\nbq. I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').\n+1 for this. But this is a separate issue, so I am creating a new ticket for this.\nbq. I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').\nFiled HADOOP-6631 for this.\nAttaching patch for trunk. This patch is on top of HADOOP-6631.\nAttaching new patch to apply to the latest trunk as HADOOP-6631 has gone in.\nTook a look at the latest patch, the changes seem fine.\nAllowing the patch to go through Hudson...\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12443923/HADOOP-6536.v1.patch\n  against trunk revision 941662.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/510/console\n\nThis message is automatically generated.\nChanges look good.\n\nbq. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). \nCan you add a test for deleting dangling links also?\nAttaching patch adding more testcases.\nPatch looks good.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12449821/HADOOP-6536.v1.1.patch\n  against trunk revision 964993.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/623/console\n\nThis message is automatically generated.\nThe failed test TestTrash passes on my local machine. Allowing to go through Hudson again...\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12449821/HADOOP-6536.v1.1.patch\n  against trunk revision 965696.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/625/console\n\nThis message is automatically generated.\n>> -1 javadoc. The javadoc tool appears to have generated 1 warning messages.\nNo javadoc warning is introduced in this patch.\nI just committed this. Thanks Ravi!\nIntegrated in Hadoop-Common-trunk-Commit #332 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/332/])\n    HADOOP-6536. Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory. Contributed by Ravi Gummadi\n\nIntegrated in Hadoop-Common-trunk #398 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/398/])\n    HADOOP-6536. Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory. Contributed by Ravi Gummadi\n\n", "issueSearchSentences": ["FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink.", "Currently FileUtil.fullyDelete(myDir) comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir.", "Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory.", "Fixes FileUtil.fullyDelete() not to delete the contents of the sym-linked directory.", "FileUtil.fullyDelete(dir) behavior is not defined when we pass a symlink as the argument"], "issueSearchScores": [0.7486476898193359, 0.7425826191902161, 0.7313708662986755, 0.7313708662986755, 0.7292311787605286]}
{"aId": 68, "code": "protected synchronized void cleanup() {\n    try {\n      if (metaFolder != null) {\n        synchronized (this) {\n          if (jobFS != null) {\n            jobFS.delete(metaFolder, true);\n          }\n          metaFolder = null;\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unable to cleanup meta folder: \" + metaFolder, e);\n    }\n  }", "comment": " Clean the staging folder created by distcp.", "issueId": "HADOOP-15789", "issueStringList": ["DistCp does not clean staging folder if class extends DistCp", "My code extends Distcp class and for some reason if distcp fails staging folder is not delete and this staging folder piles up occupying space on hdfs.", "After checking the code i found that cleanup() function is private.", "Making the cleanup() method as public, user should be able to invoke the cleanup if job fails.", "This works fine with command line argument.", "But fails only if we extend Distcp class.", "Following is the fix", "{noformat}", "git diff", "diff --git a/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java", "index 06e08e428c1..f69de6285c6 100644", "a/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java", "+++ b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java", "@@ -487,12 +487,17 @@ private static Configuration getDefaultConf() {", "return config;", "}", "private synchronized void cleanup() {", "+  /**", "+   * Clean the staging folder created by distcp", "+   */", "+  public void cleanup() {", "try {", "if (metaFolder == null) return;", "jobFS.delete(metaFolder, true);", "metaFolder = null;", "+      synchronized (this) {", "+        jobFS.delete(metaFolder, true);", "+        metaFolder = null;", "+      }", "} catch (IOException e) {", "LOG.error(\"Unable to cleanup meta folder: \" + metaFolder, e);", "}", "{noformat}", "sorry, I'd missed this.", "if you can attach it as a .patch file I'll get this in", "saw this, the diff was there in the comment, tried to apply but it doesn't apply now.", ":-(", "I have attached the patch for you [~lawpremkumar].", "I suppose he isn't added as contributor to Hadoop-Common, so the option to attach patch itself might not be visible to him then.", "the change makes sense to me, but would need a +1 from some other committer before I can push this.", "| (x) *{color:red}-1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime ||  Logfile || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 13m 16s{color} | {color:blue}{color} | {color:blue} Docker mode activated.", "{color} |", "|| || || || {color:brown} Prechecks {color} || ||", "| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} No case conflicting files found.", "{color} |", "| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} The patch does not contain any @author tags.", "{color} |", "| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red}{color} | {color:red} The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color} |", "|| || || || {color:brown} trunk Compile Tests {color} || ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 32m 12s{color} | {color:green}{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 35s{color} | {color:green}{color} | {color:green} trunk passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green}{color} | {color:green} trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green}{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 31s{color} | {color:green}{color} | {color:green} branch has no errors when building and testing our client artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 30s{color} | {color:green}{color} | {color:green} trunk passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |", "| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue} 17m 18s{color} | {color:blue}{color} | {color:blue} Both FindBugs and SpotBugs are enabled, using SpotBugs.", "{color} |", "| {color:green}+1{color} | {color:green} spotbugs {color} | {color:green}  0m 50s{color} | {color:green}{color} | {color:green} trunk passed {color} |", "|| || || || {color:brown} Patch Compile Tests {color} || ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 25s{color} | {color:green}{color} | {color:green} the patch passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |", "| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 25s{color} | {color:green}{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |", "| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed {color} |", "| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 17s{color} | {color:orange}https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/artifact/out/diff-checkstyle-hadoop-tools_hadoop-distcp.txt{color} | {color:orange} hadoop-tools/hadoop-distcp: The patch generated 1 new + 13 unchanged - 0 fixed = 14 total (was 13) {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 26s{color} | {color:green}{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} The patch has no whitespace issues.", "{color} |", "| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m  5s{color} | {color:green}{color} | {color:green} patch has no errors when building and testing our client artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |", "| {color:green}+1{color} | {color:green} spotbugs {color} | {color:green}  0m 54s{color} | {color:green}{color} | {color:green} the patch passed {color} |", "|| || || || {color:brown} Other Tests {color} || ||", "| {color:green}+1{color} | {color:green} unit {color} | {color:green} 20m 23s{color} | {color:green}{color} | {color:green} hadoop-distcp in the patch passed.", "{color} |", "| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green}{color} | {color:green} The patch does not generate ASF License warnings.", "{color} |", "| {color:black}{color} | {color:black} {color} | {color:black}104m 48s{color} | {color:black}{color} | {color:black}{color} |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/artifact/out/Dockerfile |", "| JIRA Issue | HADOOP-15789 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13039578/HADOOP-15789-01.patch |", "| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle spotbugs |", "| uname | Linux 2ffcaa410faf 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |", "| Build tool | maven |", "| Personality | personality/hadoop.sh |", "| git revision | trunk / aeae5716cc2 |", "| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |", "| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |", "|  Test Results | https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/testReport/ |", "| Max.", "process+thread count | 545 (vs. ulimit of 5500) |", "| modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |", "| Console output | https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/console |", "| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |", "| Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |", "This message was automatically generated.", "Thanks for converting the comment to patch\u00a0[~ayushtkn]\u00a0!", "[~lawpremkumar] marked you as constributor & assigned JIRA", "can we have this as a github pr for the merge process?", "and [~ayushtkn] given the patch was someone else's code, you can +1 it yourself", "lookiing at the code, checkstyle unhappy about javadocs", "and unless it is critical to be made public, probably better to make cleanup() protected", "[~lawpremkumar] Will you be able to address the above comments.", "If you have issues raising PR, you can update it as patch as well.", "may be just editing the patch in a text editor to add a period \".\"", "in this line:", "Clean the staging folder created by distcp", "will solve the checkstyle and you can change public to protected.", ":)", "If not, and you have issues there as well, just let me know I will do it for you", "I have raised the PR with the code changes from [~lawpremkumar], Considering he isn't active, will merge the code if the build comes clean tomorrow.", "Committed to trunk.", "Thanx [~lawpremkumar] for the contribution!!", "!"], "SplitGT": [" Clean the staging folder created by distcp."], "issueString": "DistCp does not clean staging folder if class extends DistCp\nMy code extends Distcp class and for some reason if distcp fails staging folder is not delete and this staging folder piles up occupying space on hdfs.\n\nAfter checking the code i found that cleanup() function is private. Making the cleanup() method as public, user should be able to invoke the cleanup if job fails.\n\nThis works fine with command line argument. But fails only if we extend Distcp class.\n\nFollowing is the fix\n\n{noformat}\ngit diff\ndiff --git a/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java\nindex 06e08e428c1..f69de6285c6 100644\n--- a/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java\n+++ b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java\n@@ -487,12 +487,17 @@ private static Configuration getDefaultConf() {\n     return config;\n   }\n \n-  private synchronized void cleanup() {\n+  /**\n+   * Clean the staging folder created by distcp\n+   */\n+  public void cleanup() {\n     try {\n       if (metaFolder == null) return;\n \n-      jobFS.delete(metaFolder, true);\n-      metaFolder = null;\n+      synchronized (this) {\n+        jobFS.delete(metaFolder, true);\n+        metaFolder = null;\n+      }\n     } catch (IOException e) {\n       LOG.error(\"Unable to cleanup meta folder: \" + metaFolder, e);\n     }\n{noformat}\nsorry, I'd missed this.\r\n\r\nif you can attach it as a .patch file I'll get this in\nsaw this, the diff was there in the comment, tried to apply but it doesn't apply now. :-( \r\nI have attached the patch for you [~lawpremkumar]. \r\n\r\nI suppose he isn't added as contributor to Hadoop-Common, so the option to attach patch itself might not be visible to him then.\r\nthe change makes sense to me, but would need a +1 from some other committer before I can push this.\n| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime ||  Logfile || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 13m 16s{color} | {color:blue}{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} || ||\r\n| {color:green}+1{color} | {color:green} dupname {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} No case conflicting files found. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red}{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} || ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 32m 12s{color} | {color:green}{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 35s{color} | {color:green}{color} | {color:green} trunk passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green}{color} | {color:green} trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green}{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 31s{color} | {color:green}{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 30s{color} | {color:green}{color} | {color:green} trunk passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |\r\n| {color:blue}0{color} | {color:blue} spotbugs {color} | {color:blue} 17m 18s{color} | {color:blue}{color} | {color:blue} Both FindBugs and SpotBugs are enabled, using SpotBugs. {color} |\r\n| {color:green}+1{color} | {color:green} spotbugs {color} | {color:green}  0m 50s{color} | {color:green}{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} || ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 27s{color} | {color:green}{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 25s{color} | {color:green}{color} | {color:green} the patch passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 25s{color} | {color:green}{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 17s{color} | {color:orange}https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/artifact/out/diff-checkstyle-hadoop-tools_hadoop-distcp.txt{color} | {color:orange} hadoop-tools/hadoop-distcp: The patch generated 1 new + 13 unchanged - 0 fixed = 14 total (was 13) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 26s{color} | {color:green}{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green}{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m  5s{color} | {color:green}{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green}{color} | {color:green} the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 {color} |\r\n| {color:green}+1{color} | {color:green} spotbugs {color} | {color:green}  0m 54s{color} | {color:green}{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} || ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 20m 23s{color} | {color:green}{color} | {color:green} hadoop-distcp in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green}{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}104m 48s{color} | {color:black}{color} | {color:black}{color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/artifact/out/Dockerfile |\r\n| JIRA Issue | HADOOP-15789 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/13039578/HADOOP-15789-01.patch |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle spotbugs |\r\n| uname | Linux 2ffcaa410faf 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | personality/hadoop.sh |\r\n| git revision | trunk / aeae5716cc2 |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.13+8-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/testReport/ |\r\n| Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n| Console output | https://ci-hadoop.apache.org/job/PreCommit-HADOOP-Build/256/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n\nThanks for converting the comment to patch\u00a0[~ayushtkn]\u00a0!\n* [~lawpremkumar] marked you as constributor & assigned JIRA\r\n* can we have this as a github pr for the merge process?\r\n* and [~ayushtkn] given the patch was someone else's code, you can +1 it yourself\r\n\r\nlookiing at the code, checkstyle unhappy about javadocs\r\n\r\nand unless it is critical to be made public, probably better to make cleanup() protected\r\n\r\n\n[~lawpremkumar] Will you be able to address the above comments.\r\nIf you have issues raising PR, you can update it as patch as well. may be just editing the patch in a text editor to add a period \".\" in this line:\r\nClean the staging folder created by distcp\r\nwill solve the checkstyle and you can change public to protected. :)\r\n\r\nIf not, and you have issues there as well, just let me know I will do it for you\nI have raised the PR with the code changes from [~lawpremkumar], Considering he isn't active, will merge the code if the build comes clean tomorrow. \nCommitted to trunk.\r\n\r\nThanx [~lawpremkumar] for the contribution!!!\n", "issueSearchSentences": ["LOG.error(\"Unable to cleanup meta folder: \" + metaFolder, e);", "+        jobFS.delete(metaFolder, true);", "jobFS.delete(metaFolder, true);", "private synchronized void cleanup() {", "+  public void cleanup() {"], "issueSearchScores": [0.7641041278839111, 0.7410391569137573, 0.7381566762924194, 0.7010097503662109, 0.6886582970619202]}
{"aId": 69, "code": "@Override\n  public void reinit(Configuration conf) {\n    reset();\n    if (conf == null) {\n      return;\n    }\n    setLevel(ZlibFactory.getCompressionLevel(conf).compressionLevel());\n    final ZlibCompressor.CompressionStrategy strategy =\n      ZlibFactory.getCompressionStrategy(conf);\n    try {\n      setStrategy(strategy.compressionStrategy());\n    } catch (IllegalArgumentException ill) {\n      Log.warn(strategy + \" not supported by BuiltInZlibDeflater.\");\n      setStrategy(DEFAULT_STRATEGY);\n    }\n    Log.debug(\"Reinit compressor with new compression configuration\");\n  }", "comment": " reinit the compressor with the given configuration.", "issueId": "HADOOP-5879", "issueStringList": ["GzipCodec should read compression level etc from configuration", "GzipCodec currently uses the default compression level.", "We should allow overriding the default value from Configuration.", "{code}", "static final class GzipZlibCompressor extends ZlibCompressor {", "public GzipZlibCompressor() {", "super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,", "ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,", "ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);", "}", "}", "{code}", "A first try.", "Only tested with TestCodec testcase in my local linux box.", "Both SequenceFile and IFile use o.a.h.io.compress.CodecPool to pool codecs.", "This is unlikely to work as expected, there.", "I checked the code, it will work with SequenceFile, for example:", "{noformat}", "public static void testSequenceFileWithGZipCodec() throws IOException{", "Configuration confWithParam = new Configuration();", "confWithParam.set(\"io.compress.level\", \"1\");", "confWithParam.set(\"io.compress.strategy\", \"0\");", "confWithParam.set(\"io.compress.buffer.size\", Integer.toString(128 * 1024));", "FileSystem fs =FileSystem.get(confWithParam);", "GzipCodec codec= new GzipCodec();", "codec.setConf(confWithParam);// this line is not needed for creating SequenceFile.Writer", "SequenceFile.Writer writer =SequenceFile.createWriter(fs, confWithParam, new Path(\"/test/path\"),", "NullWritable.class, NullWritable.class,", "CompressionType.BLOCK, codec);", "writer.close();", "}", "{noformat}", "The call trace for getting a compressor is :", "CodecPool.getCompressor(CompressionCodec)-->GzipCodec.createCompressor()-->new GzipZlibCompressor(conf).", "I have not checked IFile, but i think it should work in the same way.", "Sorry, I didn't mean that it would fail or not change the setting.", "I meant that, since these parameters are specified in the constructor, there's no place where these parameters are reset when a compressor is pulled from the pool.", "HADOOP-5281 is an instance of a bug following this pattern.", "If the intent is to create all Gzip codecs with the same settings, then OK.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12408699/hadoop-5879-5-21.patch", "against trunk revision 778921.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "+1 Eclipse classpath.", "The patch retains Eclipse classpath integrity.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed core unit tests.", "1 contrib tests.", "The patch failed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/console", "This message is automatically generated.", "test failed test is not related this patch.", "However, what Chris commented is right.", "The current patch can not guarantee the Gzip Compressor got from CodecPool is of the settings what users expect.", "This is kind of global settings, but if we change settings but the CodecPool does not clean its buffered codecs which of old settings.", "So it may make things work in a wrong way.", "So one possible way is to let CodecPool do special for Gzip codec, and does either", "1) keeps a map for holding gzip codec of different settings.", "or", "2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.", "Does the changes for CodecPool sound reasonable/acceptable?", "{quote}", "So one possible way is to let CodecPool do special for Gzip codec, and does either", "1) keeps a map for holding gzip codec of different settings.", "or", "2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.", "Does the changes for CodecPool sound reasonable/acceptable?", "{quote}", "I'm not sure the \"clean\" semantics have clear triggers (or they're not clear to me).", "I'd suggest an analog to {{end}} in the {{(Dec|C)ompressor}} interface that reinitializes a (de)compressor, then use those interfaces in the {{CodecPool}}.", "This would be a better fix for HADOOP-5281, but it requires updates to other implementors of {{Compressor}}.", "Something like {{reinit}} that destroys (with {{end}}) and recreates (with {{init}}) the underlying stream.", "Overloading {{CodecPool::getCompressor}} to take a {{Configuration}} and... well, tracing the implications through the rest of the Codec classes makes it easy to trace where compressors are recycled.", "Calling {{reinit}} with parameters matching the current ones should be a noop and calling {{CodecPool::getCompressor}} without any arguments should use default params.", "Since this is a fair amount of work, if you wanted to narrow the issue to be global settings for GzipCodec, then an approach like that in the current patch is probably sufficient for many applications.", "Quick asides on the current patch: {{ZlibCompressor::construct}} should be final; if overridden in a subclass, the partially created object would call the subclass instance from the base cstr.", "Also, since the parameters are specific to GzipCodc, they should not have generic names like \"io.compress.level\".", "upload a new patch integrated Chris' suggestions (Thanks, Chris).", "hadoop-5879-7-13-3.patch changed the conf string io.compress.", "* to zlib.compress.", "*", "Supported conf strings are:", "zlib.compress.level         values can be: NO_COMPRESSION / BEST_SPEED / BEST_COMPRESSION / DEFAULT_COMPRESSION", "zlib.compress.strategy     values can be:  FILTERED / HUFFMAN_ONLY / RLE / FIXED / DEFAULT_STRATEGY", "zlib.compress.buffer.size    values can be any positive integer", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12413291/hadoop-5879-7-13-3.patch", "against trunk revision 793162.", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "1 release audit.", "The applied patch generated 266 release audit warnings (more than the trunk's current 260 warnings).", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/testReport/", "Release audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/current/releaseAuditDiffWarnings.txt", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/console", "This message is automatically generated.", "The latest patch looks good.", "A few notes:", "BuiltInZlibDeflater can use the setLevel and setStrategy methods on java.util.zip.Deflater to implement reset", "This should definitely have a unit test, particularly involving CodecPool", "The javadoc for reinit should describe the contract for that method; the implementations should also describe what happens for that particular codec.", "In ZlibCompression::reinit, shouldn't end() be called before reconfiguring the codec?", "The direct buffer size should not be reconfigured.", "Codecs are pooled because recreating direct buffers is very expensive; allocating new direct buffers on a cache hit defeats the purpose of pooling them.", "The buffer size doesn't affect correctness of zlib compression.", "This should add methods like ZlibCompression::setCompressionLevel(Configuration, <value>) to set the {{zlib.compress.", "*}} properties", "Whoops; ZlibCompression isn't a class.", "Maybe setCompressionLevel(conf, value) should be added to GzipCodec instead.", "A new patch incorporates Chris' comments (Thanks, Chirs!", ").", "Thanks for updating the patch", "Rather than having special semantics for construct, I'd suggest removing the directBufferSize formal from construct and returning the allocation to the constructor", "Sorry, my last comment was not clear.", "By \"implementations should also describe...\" I meant that the classes implementing reinit should include a description of what it effects in its javadoc for reinit, as you did in BuiltInZlibDeflater.", "I didn't mean that more logging was required.", "Compressor::reinit should also describe the contract for future implementers, \"Prepare the compressor to be used in a new stream with settings defined in the given Configuration\" or something like that", "I think it's appropriate to fail if the configuration is invalid rather than taking the default in ZlibFactory::getCompression\\* (why ZlibFactory?).", "I don't think the getDefault\\* methods are necessary.", "setCompression\\* should take the appropriate enum, rather than String.", "Filed HADOOP-6161 for get/setEnum to simplify some of the conf-related code", "In ZlibCompressor::reinit, reset is not called if the Configuration is null.", "For users calling these methods not via CodecPool, reset() should probably be called", "CompressionLevel::compressionLevel() and CompressionStrategy::compressionStrategy() should remain package-private; the integers are implementation details", "The unit test doesn't really verify the functionality added by this patch, save that it doesn't throw exceptions.", "That said, it would be hard to verify that this is working as expected without adding get\\* methods to ZlibCompressor.", "Can you describe how you verified the patch's correctness, both for the native and non-native codecs?", "hadoop-5879-7-26.patch incorporates all Chris's suggestions (Thanks for the detailed comments, Chris!", ").", "I have to admit that the testcase can not fully verify the functionality.", "And to know that it works, what i did was by looking the log and debugging in my IDE.", "Please let me know if you know how we can test it automatically.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12414549/hadoop-5879-7-26.patch", "against trunk revision 807753.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/console", "This message is automatically generated.", "Merged with trunk", "Calls end() in reinit, avoiding leak", "Removes construct and resetStrategyAndCompressionLevel, which caused findbugs warnings.", "Relies on invariant windowBits set in cstr for Default/ZlibCompressors to avoid reemergence of HADOOP-5281", "Updated unit test to verify that compression level is reset", "Hudson isn't picking up the patch, so I ran it on my machine:", "{noformat}", "[exec] +1 overall.", "[exec]", "[exec]     +1 @author.", "The patch does not contain any @author tags.", "[exec]", "[exec]     +1 tests included.", "The patch appears to include 3 new or modified tests.", "[exec]", "[exec]     +1 javadoc.", "The javadoc tool did not generate any warning messages.", "[exec]", "[exec]     +1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "[exec]", "[exec]     +1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "[exec]", "[exec]     +1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "{noformat}", "All unit tests passed with and without native libs installed.", "+1", "I committed this.", "Thanks, He!", "Integrated in Hadoop-Common-trunk-Commit #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/29/])", ".", "Read compression level and strategy from Configuration for", "gzip compression.", "Contributed by He Yongqiang"], "SplitGT": [" reinit the compressor with the given configuration."], "issueString": "GzipCodec should read compression level etc from configuration\nGzipCodec currently uses the default compression level. We should allow overriding the default value from Configuration.\n\n{code}\n  static final class GzipZlibCompressor extends ZlibCompressor {\n    public GzipZlibCompressor() {\n      super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,\n          ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,\n          ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);\n    }\n  }\n{code}\n\nA first try. Only tested with TestCodec testcase in my local linux box.\nBoth SequenceFile and IFile use o.a.h.io.compress.CodecPool to pool codecs. This is unlikely to work as expected, there.\nI checked the code, it will work with SequenceFile, for example:\n{noformat}\n  public static void testSequenceFileWithGZipCodec() throws IOException{\n    Configuration confWithParam = new Configuration();\n    confWithParam.set(\"io.compress.level\", \"1\");\n    confWithParam.set(\"io.compress.strategy\", \"0\");\n    confWithParam.set(\"io.compress.buffer.size\", Integer.toString(128 * 1024));\n    FileSystem fs =FileSystem.get(confWithParam);\n    GzipCodec codec= new GzipCodec();\n    codec.setConf(confWithParam);// this line is not needed for creating SequenceFile.Writer\n    SequenceFile.Writer writer =SequenceFile.createWriter(fs, confWithParam, new Path(\"/test/path\"), \n       NullWritable.class, NullWritable.class, \n        CompressionType.BLOCK, codec);\n    writer.close();\n  }\n{noformat}\nThe call trace for getting a compressor is :\nCodecPool.getCompressor(CompressionCodec)-->GzipCodec.createCompressor()-->new GzipZlibCompressor(conf).\nI have not checked IFile, but i think it should work in the same way.\nSorry, I didn't mean that it would fail or not change the setting. I meant that, since these parameters are specified in the constructor, there's no place where these parameters are reset when a compressor is pulled from the pool. HADOOP-5281 is an instance of a bug following this pattern. If the intent is to create all Gzip codecs with the same settings, then OK.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12408699/hadoop-5879-5-21.patch\n  against trunk revision 778921.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/console\n\nThis message is automatically generated.\ntest failed test is not related this patch.\nHowever, what Chris commented is right. The current patch can not guarantee the Gzip Compressor got from CodecPool is of the settings what users expect.  This is kind of global settings, but if we change settings but the CodecPool does not clean its buffered codecs which of old settings. So it may make things work in a wrong way. \nSo one possible way is to let CodecPool do special for Gzip codec, and does either \n1) keeps a map for holding gzip codec of different settings.\nor\n2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.\n\nDoes the changes for CodecPool sound reasonable/acceptable?\n{quote}\nSo one possible way is to let CodecPool do special for Gzip codec, and does either\n1) keeps a map for holding gzip codec of different settings.\nor\n2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.\n\nDoes the changes for CodecPool sound reasonable/acceptable?\n{quote}\n\nI'm not sure the \"clean\" semantics have clear triggers (or they're not clear to me). I'd suggest an analog to {{end}} in the {{(Dec|C)ompressor}} interface that reinitializes a (de)compressor, then use those interfaces in the {{CodecPool}}. This would be a better fix for HADOOP-5281, but it requires updates to other implementors of {{Compressor}}. Something like {{reinit}} that destroys (with {{end}}) and recreates (with {{init}}) the underlying stream. Overloading {{CodecPool::getCompressor}} to take a {{Configuration}} and... well, tracing the implications through the rest of the Codec classes makes it easy to trace where compressors are recycled. Calling {{reinit}} with parameters matching the current ones should be a noop and calling {{CodecPool::getCompressor}} without any arguments should use default params.\n\nSince this is a fair amount of work, if you wanted to narrow the issue to be global settings for GzipCodec, then an approach like that in the current patch is probably sufficient for many applications.\n\nQuick asides on the current patch: {{ZlibCompressor::construct}} should be final; if overridden in a subclass, the partially created object would call the subclass instance from the base cstr. Also, since the parameters are specific to GzipCodc, they should not have generic names like \"io.compress.level\".\nupload a new patch integrated Chris' suggestions (Thanks, Chris).\nhadoop-5879-7-13-3.patch changed the conf string io.compress.* to zlib.compress.*\n\nSupported conf strings are: \nzlib.compress.level         values can be: NO_COMPRESSION / BEST_SPEED / BEST_COMPRESSION / DEFAULT_COMPRESSION\nzlib.compress.strategy     values can be:  FILTERED / HUFFMAN_ONLY / RLE / FIXED / DEFAULT_STRATEGY\nzlib.compress.buffer.size    values can be any positive integer\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12413291/hadoop-5879-7-13-3.patch\n  against trunk revision 793162.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    -1 release audit.  The applied patch generated 266 release audit warnings (more than the trunk's current 260 warnings).\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/testReport/\nRelease audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/current/releaseAuditDiffWarnings.txt\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/console\n\nThis message is automatically generated.\nThe latest patch looks good. A few notes:\n* BuiltInZlibDeflater can use the setLevel and setStrategy methods on java.util.zip.Deflater to implement reset\n* This should definitely have a unit test, particularly involving CodecPool\n* The javadoc for reinit should describe the contract for that method; the implementations should also describe what happens for that particular codec.\n* In ZlibCompression::reinit, shouldn't end() be called before reconfiguring the codec?\n* The direct buffer size should not be reconfigured. Codecs are pooled because recreating direct buffers is very expensive; allocating new direct buffers on a cache hit defeats the purpose of pooling them. The buffer size doesn't affect correctness of zlib compression.\n* This should add methods like ZlibCompression::setCompressionLevel(Configuration, <value>) to set the {{zlib.compress.*}} properties\nWhoops; ZlibCompression isn't a class. Maybe setCompressionLevel(conf, value) should be added to GzipCodec instead.\nA new patch incorporates Chris' comments (Thanks, Chirs!).\nThanks for updating the patch\n* Rather than having special semantics for construct, I'd suggest removing the directBufferSize formal from construct and returning the allocation to the constructor\n* Sorry, my last comment was not clear. By \"implementations should also describe...\" I meant that the classes implementing reinit should include a description of what it effects in its javadoc for reinit, as you did in BuiltInZlibDeflater. I didn't mean that more logging was required. Compressor::reinit should also describe the contract for future implementers, \"Prepare the compressor to be used in a new stream with settings defined in the given Configuration\" or something like that\n* I think it's appropriate to fail if the configuration is invalid rather than taking the default in ZlibFactory::getCompression\\* (why ZlibFactory?). I don't think the getDefault\\* methods are necessary. setCompression\\* should take the appropriate enum, rather than String. Filed HADOOP-6161 for get/setEnum to simplify some of the conf-related code\n* In ZlibCompressor::reinit, reset is not called if the Configuration is null. For users calling these methods not via CodecPool, reset() should probably be called\n* CompressionLevel::compressionLevel() and CompressionStrategy::compressionStrategy() should remain package-private; the integers are implementation details\n* The unit test doesn't really verify the functionality added by this patch, save that it doesn't throw exceptions. That said, it would be hard to verify that this is working as expected without adding get\\* methods to ZlibCompressor. Can you describe how you verified the patch's correctness, both for the native and non-native codecs?\nhadoop-5879-7-26.patch incorporates all Chris's suggestions (Thanks for the detailed comments, Chris!). \nI have to admit that the testcase can not fully verify the functionality. And to know that it works, what i did was by looking the log and debugging in my IDE. Please let me know if you know how we can test it automatically.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12414549/hadoop-5879-7-26.patch\n  against trunk revision 807753.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/console\n\nThis message is automatically generated.\n* Merged with trunk\n* Calls end() in reinit, avoiding leak\n* Removes construct and resetStrategyAndCompressionLevel, which caused findbugs warnings. Relies on invariant windowBits set in cstr for Default/ZlibCompressors to avoid reemergence of HADOOP-5281\n* Updated unit test to verify that compression level is reset\nHudson isn't picking up the patch, so I ran it on my machine:\n\n{noformat}\n     [exec] +1 overall.  \n     [exec] \n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec] \n     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.\n     [exec] \n     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.\n     [exec] \n     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n     [exec] \n     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n     [exec] \n     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n{noformat}\n\nAll unit tests passed with and without native libs installed.\n+1\n\nI committed this. Thanks, He!\nIntegrated in Hadoop-Common-trunk-Commit #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/29/])\n    . Read compression level and strategy from Configuration for\ngzip compression. Contributed by He Yongqiang\n\n", "issueSearchSentences": ["In ZlibCompressor::reinit, reset is not called if the Configuration is null.", "For users calling these methods not via CodecPool, reset() should probably be called", "BuiltInZlibDeflater can use the setLevel and setStrategy methods on java.util.zip.Deflater to implement reset", "By \"implementations should also describe...\" I meant that the classes implementing reinit should include a description of what it effects in its javadoc for reinit, as you did in BuiltInZlibDeflater.", "Removes construct and resetStrategyAndCompressionLevel, which caused findbugs warnings."], "issueSearchScores": [0.7148021459579468, 0.5972485542297363, 0.581575870513916, 0.5479221940040588, 0.5358383655548096]}
{"aId": 71, "code": "public List<ComparePair> checkDdbInternalConsistency(Path basePath)\n      throws IOException {\n    Preconditions.checkArgument(basePath.isAbsolute(), \"path must be absolute\");\n\n    List<ComparePair> comparePairs = new ArrayList<>();\n    String rootStr = basePath.toString();\n    LOG.info(\"Root for internal consistency check: {}\", rootStr);\n    StopWatch stopwatch = new StopWatch();\n    stopwatch.start();\n\n    final Table table = metadataStore.getTable();\n    final String username = metadataStore.getUsername();\n    DDBTree ddbTree = new DDBTree();\n\n    /*\n     * I. Root node construction\n     * - If the root node is the real bucket root, a node is constructed instead of\n     *   doing a query to the ddb because the bucket root is not stored.\n     * - If the root node is not a real bucket root then the entry is queried from\n     *   the ddb and constructed from the result.\n     */\n\n    DDBPathMetadata baseMeta;\n\n    if (!basePath.isRoot()) {\n      PrimaryKey rootKey = pathToKey(basePath);\n      final GetItemSpec spec = new GetItemSpec()\n          .withPrimaryKey(rootKey)\n          .withConsistentRead(true);\n      final Item baseItem = table.getItem(spec);\n      baseMeta = itemToPathMetadata(baseItem, username);\n\n      if (baseMeta == null) {\n        throw new FileNotFoundException(\n            \"Base element metadata is null. \" +\n                \"This means the base path element is missing, or wrong path was \" +\n                \"passed as base path to the internal ddb consistency checker.\");\n      }\n    } else {\n      baseMeta = new DDBPathMetadata(\n          new S3AFileStatus(Tristate.UNKNOWN, basePath, username)\n      );\n    }\n\n    DDBTreeNode root = new DDBTreeNode(baseMeta);\n    ddbTree.addNode(root);\n    ddbTree.setRoot(root);\n\n    /*\n     * II. Build and check the descendant tree:\n     * 1. query all nodes where the prefix is the given root, and put it in the tree\n     * 2. Check connectivity: check if each parent is in the hashmap\n     *    - This is done in O(n): we only need to find the parent based on the\n     *      path with a hashmap lookup.\n     *    - Do a test if the graph is connected - if the parent is not in the\n     *      hashmap, we found an orphan entry.\n     *\n     * 3. Do test the elements for errors:\n     *    - File is a parent of a file.\n     *    - Entries where the parent is tombstoned but the entries are not.\n     *    - Warn on no lastUpdated field.\n     *\n     */\n    ExpressionSpecBuilder builder = new ExpressionSpecBuilder();\n    builder.withCondition(\n        ExpressionSpecBuilder.S(\"parent\")\n            .beginsWith(pathToParentKey(basePath))\n    );\n    final IteratorSupport<Item, ScanOutcome> resultIterator = table.scan(\n        builder.buildForScan()).iterator();\n    resultIterator.forEachRemaining(item -> {\n      final DDBPathMetadata pmd = itemToPathMetadata(item, username);\n      DDBTreeNode ddbTreeNode = new DDBTreeNode(pmd);\n      ddbTree.addNode(ddbTreeNode);\n    });\n\n    LOG.debug(\"Root: {}\", ddbTree.getRoot());\n\n    for (Map.Entry<Path, DDBTreeNode> entry : ddbTree.getContentMap().entrySet()) {\n      final DDBTreeNode node = entry.getValue();\n      final ComparePair pair = new ComparePair(null, node.val);\n      // let's skip the root node when checking.\n      if (node.getVal().getFileStatus().getPath().isRoot()) {\n        continue;\n      }\n\n      if(node.getVal().getLastUpdated() == 0) {\n        pair.violations.add(Violation.NO_LASTUPDATED_FIELD);\n      }\n\n      // skip further checking the basenode which is not the actual bucket root.\n      if (node.equals(ddbTree.getRoot())) {\n        continue;\n      }\n\n      final Path parent = node.getFileStatus().getPath().getParent();\n      final DDBTreeNode parentNode = ddbTree.getContentMap().get(parent);\n      if (parentNode == null) {\n        pair.violations.add(Violation.ORPHAN_DDB_ENTRY);\n      } else {\n        if (!node.isTombstoned() && !parentNode.isDirectory()) {\n          pair.violations.add(Violation.PARENT_IS_A_FILE);\n        }\n        if(!node.isTombstoned() && parentNode.isTombstoned()) {\n          pair.violations.add(Violation.PARENT_TOMBSTONED);\n        }\n      }\n\n      if (!pair.violations.isEmpty()) {\n        comparePairs.add(pair);\n      }\n\n      node.setParent(parentNode);\n    }\n\n    // Create a handler and handle each violated pairs\n    S3GuardFsckViolationHandler handler =\n        new S3GuardFsckViolationHandler(rawFS, metadataStore);\n    comparePairs.forEach(handler::handle);\n\n    stopwatch.stop();\n    LOG.info(\"Total scan time: {}s\", stopwatch.now(TimeUnit.SECONDS));\n    LOG.info(\"Scanned entries: {}\", ddbTree.contentMap.size());\n\n    return comparePairs;\n  }", "comment": " <pre> Tasks to do here: - find orphan entries (entries without a parent). - find if a file's parent is not a directory (so the parent is a file). - find entries where the parent is a tombstone.", "issueId": "HADOOP-16424", "issueStringList": ["S3Guard fsck: Check internal consistency of the MetadataStore", "The internal consistency should be checked e.g for orphaned entries which can cause trouble in runtime and testing.", "Tasks to do here:", "find orphan entries (entries without a parent)", "find if a file's parent is not a directory (so the parent is a file)", "warn: no lastUpdated field", "entries where the parent is a tombstone", "Our code should not be creating orphan entries.", "If we have an orphan entry than it's a bug in the production code.", "Got +1 on #1691 from [~stevel@apache.org].", "Committing.", "SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #17748 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/17748/])", "HADOOP-16424.", "S3Guard fsck: Check internal consistency of the (github: rev 875a3e97dd4a26fe224a1858c54d1b4512db6be3)", "(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java", "(edit) hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3guard.md", "(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java", "(edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolDynamoDB.java", "(edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardFsck.java", "(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsck.java"], "SplitGT": [" <pre> Tasks to do here: - find orphan entries (entries without a parent).", "- find if a file's parent is not a directory (so the parent is a file).", "- find entries where the parent is a tombstone."], "issueString": "S3Guard fsck: Check internal consistency of the MetadataStore\nThe internal consistency should be checked e.g for orphaned entries which can cause trouble in runtime and testing.\nTasks to do here: \r\n* find orphan entries (entries without a parent)\r\n* find if a file's parent is not a directory (so the parent is a file)\r\n* warn: no lastUpdated field\r\n* entries where the parent is a tombstone\r\n\nOur code should not be creating orphan entries. If we have an orphan entry than it's a bug in the production code.\r\n\nGot +1 on #1691 from [~stevel@apache.org]. Committing.\nSUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #17748 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/17748/])\nHADOOP-16424. S3Guard fsck: Check internal consistency of the (github: rev 875a3e97dd4a26fe224a1858c54d1b4512db6be3)\n* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java\n* (edit) hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3guard.md\n* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java\n* (edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolDynamoDB.java\n* (edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardFsck.java\n* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsck.java\n\n", "issueSearchSentences": ["The internal consistency should be checked e.g for orphaned entries which can cause trouble in runtime and testing.", "S3Guard fsck: Check internal consistency of the MetadataStore", "S3Guard fsck: Check internal consistency of the (github: rev 875a3e97dd4a26fe224a1858c54d1b4512db6be3)", "(edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolDynamoDB.java", "Our code should not be creating orphan entries."], "issueSearchScores": [0.574849009513855, 0.46495646238327026, 0.4586414694786072, 0.36360445618629456, 0.33345872163772583]}
{"aId": 72, "code": "@Override\n  public boolean useLogicalURI() {\n    return false;\n  }", "comment": " Logical URI is not used for IP failover.", "issueId": "HDFS-6334", "issueStringList": ["Client failover proxy provider for IP failover based NN HA", "With RPCv9 and improvements in the SPNEGO auth handling, it is possible to set up a pair of HA namenodes utilizing IP failover as client-request fencing mechanism.", "This jira will make it possible for HA to be configured without requiring use of logical URI and provide a simple IP failover proxy provider.", "The change will allow any old implementation of {{FailoverProxyProvider}} to continue to work.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643379/HDFS-6334.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:red}-1 javac{color}.", "The applied patch generated 1281 javac compiler warnings (more than the trunk's current 1275 warnings).", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 1 warning messages.", "See https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//testReport/", "Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavacWarnings.txt", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//console", "This message is automatically generated.", "The new patch fixes the warnings.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12643875/HDFS-6334.v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//console", "This message is automatically generated.", "[~tlipcon], [~atm]: can one of you review this?", "Patch looks great to me, Kihwal.", "Thanks for doing it.", "Two little nits:", "# bq.", "+   * used, a special token handling mat be needed to make sure a token acquired", "s/mat/may/g", "# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\"", "While strictly true, I'm guessing you meant to put a different comment here.", "+1 once these are addressed.", "This is the diff between v2 and v3 of the patch.", "All changes are in comment.", "So I won't wait for precommit.", "{panel}", "293c293", "< +   * used, a special token handling mat be needed to make sure a token acquired", "> +   * used, a special token handling may be needed to make sure a token acquired", "338c338", "< +   * Logical URI is not used for IP failover.", "> +   * Logical URI is required for this failover proxy provider.", "{panel}", "Thanks for the review, Aaron.", "I've committed this to trunk and branch-2."], "SplitGT": [" Logical URI is not used for IP failover."], "issueString": "Client failover proxy provider for IP failover based NN HA\nWith RPCv9 and improvements in the SPNEGO auth handling, it is possible to set up a pair of HA namenodes utilizing IP failover as client-request fencing mechanism.\n\nThis jira will make it possible for HA to be configured without requiring use of logical URI and provide a simple IP failover proxy provider.  The change will allow any old implementation of {{FailoverProxyProvider}} to continue to work.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643379/HDFS-6334.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n      {color:red}-1 javac{color}.  The applied patch generated 1281 javac compiler warnings (more than the trunk's current 1275 warnings).\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.\n        See https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//testReport/\nJavac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//artifact/trunk/patchprocess/diffJavacWarnings.txt\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6847//console\n\nThis message is automatically generated.\nThe new patch fixes the warnings.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12643875/HDFS-6334.v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6857//console\n\nThis message is automatically generated.\n[~tlipcon], [~atm]: can one of you review this?\nPatch looks great to me, Kihwal. Thanks for doing it. Two little nits:\n\n# bq. +   * used, a special token handling mat be needed to make sure a token acquired \ns/mat/may/g\n# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\" While strictly true, I'm guessing you meant to put a different comment here.\n\n+1 once these are addressed.\nThis is the diff between v2 and v3 of the patch. All changes are in comment. So I won't wait for precommit.\n\n{panel}\n293c293\n< +   * used, a special token handling mat be needed to make sure a token acquired \n---\n> +   * used, a special token handling may be needed to make sure a token acquired \n338c338\n< +   * Logical URI is not used for IP failover.\n---\n> +   * Logical URI is required for this failover proxy provider.\n{panel}\nThanks for the review, Aaron. I've committed this to trunk and branch-2.\n", "issueSearchSentences": ["# The method comment for {{ConfiguredFailoverProxyProvider#useLogicalURI}} says \"Logical URI is not used for IP failover.\"", "< +   * Logical URI is not used for IP failover.", "> +   * Logical URI is required for this failover proxy provider.", "293c293", "338c338"], "issueSearchScores": [0.5755400657653809, 0.5678063631057739, 0.515318751335144, 0.36542564630508423, 0.36516493558883667]}
{"aId": 73, "code": "@Override\n  public synchronized void reinit(Configuration conf) {\n    reset();\n    if (conf == null) {\n      return;\n    }\n    end(stream);\n    level = ZlibFactory.getCompressionLevel(conf);\n    strategy = ZlibFactory.getCompressionStrategy(conf);\n    stream = init(level.compressionLevel(), \n                  strategy.compressionStrategy(), \n                  windowBits.windowBits());\n    Log.debug(\"Reinit compressor with new compression configuration\");\n  }", "comment": " Prepare the compressor to be used in a new stream with settings defined in the given Configuration.", "issueId": "HADOOP-5879", "issueStringList": ["GzipCodec should read compression level etc from configuration", "GzipCodec currently uses the default compression level.", "We should allow overriding the default value from Configuration.", "{code}", "static final class GzipZlibCompressor extends ZlibCompressor {", "public GzipZlibCompressor() {", "super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,", "ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,", "ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);", "}", "}", "{code}", "A first try.", "Only tested with TestCodec testcase in my local linux box.", "Both SequenceFile and IFile use o.a.h.io.compress.CodecPool to pool codecs.", "This is unlikely to work as expected, there.", "I checked the code, it will work with SequenceFile, for example:", "{noformat}", "public static void testSequenceFileWithGZipCodec() throws IOException{", "Configuration confWithParam = new Configuration();", "confWithParam.set(\"io.compress.level\", \"1\");", "confWithParam.set(\"io.compress.strategy\", \"0\");", "confWithParam.set(\"io.compress.buffer.size\", Integer.toString(128 * 1024));", "FileSystem fs =FileSystem.get(confWithParam);", "GzipCodec codec= new GzipCodec();", "codec.setConf(confWithParam);// this line is not needed for creating SequenceFile.Writer", "SequenceFile.Writer writer =SequenceFile.createWriter(fs, confWithParam, new Path(\"/test/path\"),", "NullWritable.class, NullWritable.class,", "CompressionType.BLOCK, codec);", "writer.close();", "}", "{noformat}", "The call trace for getting a compressor is :", "CodecPool.getCompressor(CompressionCodec)-->GzipCodec.createCompressor()-->new GzipZlibCompressor(conf).", "I have not checked IFile, but i think it should work in the same way.", "Sorry, I didn't mean that it would fail or not change the setting.", "I meant that, since these parameters are specified in the constructor, there's no place where these parameters are reset when a compressor is pulled from the pool.", "HADOOP-5281 is an instance of a bug following this pattern.", "If the intent is to create all Gzip codecs with the same settings, then OK.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12408699/hadoop-5879-5-21.patch", "against trunk revision 778921.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "+1 Eclipse classpath.", "The patch retains Eclipse classpath integrity.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed core unit tests.", "1 contrib tests.", "The patch failed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/console", "This message is automatically generated.", "test failed test is not related this patch.", "However, what Chris commented is right.", "The current patch can not guarantee the Gzip Compressor got from CodecPool is of the settings what users expect.", "This is kind of global settings, but if we change settings but the CodecPool does not clean its buffered codecs which of old settings.", "So it may make things work in a wrong way.", "So one possible way is to let CodecPool do special for Gzip codec, and does either", "1) keeps a map for holding gzip codec of different settings.", "or", "2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.", "Does the changes for CodecPool sound reasonable/acceptable?", "{quote}", "So one possible way is to let CodecPool do special for Gzip codec, and does either", "1) keeps a map for holding gzip codec of different settings.", "or", "2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.", "Does the changes for CodecPool sound reasonable/acceptable?", "{quote}", "I'm not sure the \"clean\" semantics have clear triggers (or they're not clear to me).", "I'd suggest an analog to {{end}} in the {{(Dec|C)ompressor}} interface that reinitializes a (de)compressor, then use those interfaces in the {{CodecPool}}.", "This would be a better fix for HADOOP-5281, but it requires updates to other implementors of {{Compressor}}.", "Something like {{reinit}} that destroys (with {{end}}) and recreates (with {{init}}) the underlying stream.", "Overloading {{CodecPool::getCompressor}} to take a {{Configuration}} and... well, tracing the implications through the rest of the Codec classes makes it easy to trace where compressors are recycled.", "Calling {{reinit}} with parameters matching the current ones should be a noop and calling {{CodecPool::getCompressor}} without any arguments should use default params.", "Since this is a fair amount of work, if you wanted to narrow the issue to be global settings for GzipCodec, then an approach like that in the current patch is probably sufficient for many applications.", "Quick asides on the current patch: {{ZlibCompressor::construct}} should be final; if overridden in a subclass, the partially created object would call the subclass instance from the base cstr.", "Also, since the parameters are specific to GzipCodc, they should not have generic names like \"io.compress.level\".", "upload a new patch integrated Chris' suggestions (Thanks, Chris).", "hadoop-5879-7-13-3.patch changed the conf string io.compress.", "* to zlib.compress.", "*", "Supported conf strings are:", "zlib.compress.level         values can be: NO_COMPRESSION / BEST_SPEED / BEST_COMPRESSION / DEFAULT_COMPRESSION", "zlib.compress.strategy     values can be:  FILTERED / HUFFMAN_ONLY / RLE / FIXED / DEFAULT_STRATEGY", "zlib.compress.buffer.size    values can be any positive integer", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12413291/hadoop-5879-7-13-3.patch", "against trunk revision 793162.", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "1 release audit.", "The applied patch generated 266 release audit warnings (more than the trunk's current 260 warnings).", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/testReport/", "Release audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/current/releaseAuditDiffWarnings.txt", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/console", "This message is automatically generated.", "The latest patch looks good.", "A few notes:", "BuiltInZlibDeflater can use the setLevel and setStrategy methods on java.util.zip.Deflater to implement reset", "This should definitely have a unit test, particularly involving CodecPool", "The javadoc for reinit should describe the contract for that method; the implementations should also describe what happens for that particular codec.", "In ZlibCompression::reinit, shouldn't end() be called before reconfiguring the codec?", "The direct buffer size should not be reconfigured.", "Codecs are pooled because recreating direct buffers is very expensive; allocating new direct buffers on a cache hit defeats the purpose of pooling them.", "The buffer size doesn't affect correctness of zlib compression.", "This should add methods like ZlibCompression::setCompressionLevel(Configuration, <value>) to set the {{zlib.compress.", "*}} properties", "Whoops; ZlibCompression isn't a class.", "Maybe setCompressionLevel(conf, value) should be added to GzipCodec instead.", "A new patch incorporates Chris' comments (Thanks, Chirs!", ").", "Thanks for updating the patch", "Rather than having special semantics for construct, I'd suggest removing the directBufferSize formal from construct and returning the allocation to the constructor", "Sorry, my last comment was not clear.", "By \"implementations should also describe...\" I meant that the classes implementing reinit should include a description of what it effects in its javadoc for reinit, as you did in BuiltInZlibDeflater.", "I didn't mean that more logging was required.", "Compressor::reinit should also describe the contract for future implementers, \"Prepare the compressor to be used in a new stream with settings defined in the given Configuration\" or something like that", "I think it's appropriate to fail if the configuration is invalid rather than taking the default in ZlibFactory::getCompression\\* (why ZlibFactory?).", "I don't think the getDefault\\* methods are necessary.", "setCompression\\* should take the appropriate enum, rather than String.", "Filed HADOOP-6161 for get/setEnum to simplify some of the conf-related code", "In ZlibCompressor::reinit, reset is not called if the Configuration is null.", "For users calling these methods not via CodecPool, reset() should probably be called", "CompressionLevel::compressionLevel() and CompressionStrategy::compressionStrategy() should remain package-private; the integers are implementation details", "The unit test doesn't really verify the functionality added by this patch, save that it doesn't throw exceptions.", "That said, it would be hard to verify that this is working as expected without adding get\\* methods to ZlibCompressor.", "Can you describe how you verified the patch's correctness, both for the native and non-native codecs?", "hadoop-5879-7-26.patch incorporates all Chris's suggestions (Thanks for the detailed comments, Chris!", ").", "I have to admit that the testcase can not fully verify the functionality.", "And to know that it works, what i did was by looking the log and debugging in my IDE.", "Please let me know if you know how we can test it automatically.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12414549/hadoop-5879-7-26.patch", "against trunk revision 807753.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 4 new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/console", "This message is automatically generated.", "Merged with trunk", "Calls end() in reinit, avoiding leak", "Removes construct and resetStrategyAndCompressionLevel, which caused findbugs warnings.", "Relies on invariant windowBits set in cstr for Default/ZlibCompressors to avoid reemergence of HADOOP-5281", "Updated unit test to verify that compression level is reset", "Hudson isn't picking up the patch, so I ran it on my machine:", "{noformat}", "[exec] +1 overall.", "[exec]", "[exec]     +1 @author.", "The patch does not contain any @author tags.", "[exec]", "[exec]     +1 tests included.", "The patch appears to include 3 new or modified tests.", "[exec]", "[exec]     +1 javadoc.", "The javadoc tool did not generate any warning messages.", "[exec]", "[exec]     +1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "[exec]", "[exec]     +1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "[exec]", "[exec]     +1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "{noformat}", "All unit tests passed with and without native libs installed.", "+1", "I committed this.", "Thanks, He!", "Integrated in Hadoop-Common-trunk-Commit #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/29/])", ".", "Read compression level and strategy from Configuration for", "gzip compression.", "Contributed by He Yongqiang"], "SplitGT": [" Prepare the compressor to be used in a new stream with settings defined in the given Configuration."], "issueString": "GzipCodec should read compression level etc from configuration\nGzipCodec currently uses the default compression level. We should allow overriding the default value from Configuration.\n\n{code}\n  static final class GzipZlibCompressor extends ZlibCompressor {\n    public GzipZlibCompressor() {\n      super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,\n          ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,\n          ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);\n    }\n  }\n{code}\n\nA first try. Only tested with TestCodec testcase in my local linux box.\nBoth SequenceFile and IFile use o.a.h.io.compress.CodecPool to pool codecs. This is unlikely to work as expected, there.\nI checked the code, it will work with SequenceFile, for example:\n{noformat}\n  public static void testSequenceFileWithGZipCodec() throws IOException{\n    Configuration confWithParam = new Configuration();\n    confWithParam.set(\"io.compress.level\", \"1\");\n    confWithParam.set(\"io.compress.strategy\", \"0\");\n    confWithParam.set(\"io.compress.buffer.size\", Integer.toString(128 * 1024));\n    FileSystem fs =FileSystem.get(confWithParam);\n    GzipCodec codec= new GzipCodec();\n    codec.setConf(confWithParam);// this line is not needed for creating SequenceFile.Writer\n    SequenceFile.Writer writer =SequenceFile.createWriter(fs, confWithParam, new Path(\"/test/path\"), \n       NullWritable.class, NullWritable.class, \n        CompressionType.BLOCK, codec);\n    writer.close();\n  }\n{noformat}\nThe call trace for getting a compressor is :\nCodecPool.getCompressor(CompressionCodec)-->GzipCodec.createCompressor()-->new GzipZlibCompressor(conf).\nI have not checked IFile, but i think it should work in the same way.\nSorry, I didn't mean that it would fail or not change the setting. I meant that, since these parameters are specified in the constructor, there's no place where these parameters are reset when a compressor is pulled from the pool. HADOOP-5281 is an instance of a bug following this pattern. If the intent is to create all Gzip codecs with the same settings, then OK.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12408699/hadoop-5879-5-21.patch\n  against trunk revision 778921.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/406/console\n\nThis message is automatically generated.\ntest failed test is not related this patch.\nHowever, what Chris commented is right. The current patch can not guarantee the Gzip Compressor got from CodecPool is of the settings what users expect.  This is kind of global settings, but if we change settings but the CodecPool does not clean its buffered codecs which of old settings. So it may make things work in a wrong way. \nSo one possible way is to let CodecPool do special for Gzip codec, and does either \n1) keeps a map for holding gzip codec of different settings.\nor\n2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.\n\nDoes the changes for CodecPool sound reasonable/acceptable?\n{quote}\nSo one possible way is to let CodecPool do special for Gzip codec, and does either\n1) keeps a map for holding gzip codec of different settings.\nor\n2) treats the setting as a global setting, and when the setting is changed, clean all gzip codecs cached in CodecPool.\n\nDoes the changes for CodecPool sound reasonable/acceptable?\n{quote}\n\nI'm not sure the \"clean\" semantics have clear triggers (or they're not clear to me). I'd suggest an analog to {{end}} in the {{(Dec|C)ompressor}} interface that reinitializes a (de)compressor, then use those interfaces in the {{CodecPool}}. This would be a better fix for HADOOP-5281, but it requires updates to other implementors of {{Compressor}}. Something like {{reinit}} that destroys (with {{end}}) and recreates (with {{init}}) the underlying stream. Overloading {{CodecPool::getCompressor}} to take a {{Configuration}} and... well, tracing the implications through the rest of the Codec classes makes it easy to trace where compressors are recycled. Calling {{reinit}} with parameters matching the current ones should be a noop and calling {{CodecPool::getCompressor}} without any arguments should use default params.\n\nSince this is a fair amount of work, if you wanted to narrow the issue to be global settings for GzipCodec, then an approach like that in the current patch is probably sufficient for many applications.\n\nQuick asides on the current patch: {{ZlibCompressor::construct}} should be final; if overridden in a subclass, the partially created object would call the subclass instance from the base cstr. Also, since the parameters are specific to GzipCodc, they should not have generic names like \"io.compress.level\".\nupload a new patch integrated Chris' suggestions (Thanks, Chris).\nhadoop-5879-7-13-3.patch changed the conf string io.compress.* to zlib.compress.*\n\nSupported conf strings are: \nzlib.compress.level         values can be: NO_COMPRESSION / BEST_SPEED / BEST_COMPRESSION / DEFAULT_COMPRESSION\nzlib.compress.strategy     values can be:  FILTERED / HUFFMAN_ONLY / RLE / FIXED / DEFAULT_STRATEGY\nzlib.compress.buffer.size    values can be any positive integer\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12413291/hadoop-5879-7-13-3.patch\n  against trunk revision 793162.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    -1 release audit.  The applied patch generated 266 release audit warnings (more than the trunk's current 260 warnings).\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/testReport/\nRelease audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/current/releaseAuditDiffWarnings.txt\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/572/console\n\nThis message is automatically generated.\nThe latest patch looks good. A few notes:\n* BuiltInZlibDeflater can use the setLevel and setStrategy methods on java.util.zip.Deflater to implement reset\n* This should definitely have a unit test, particularly involving CodecPool\n* The javadoc for reinit should describe the contract for that method; the implementations should also describe what happens for that particular codec.\n* In ZlibCompression::reinit, shouldn't end() be called before reconfiguring the codec?\n* The direct buffer size should not be reconfigured. Codecs are pooled because recreating direct buffers is very expensive; allocating new direct buffers on a cache hit defeats the purpose of pooling them. The buffer size doesn't affect correctness of zlib compression.\n* This should add methods like ZlibCompression::setCompressionLevel(Configuration, <value>) to set the {{zlib.compress.*}} properties\nWhoops; ZlibCompression isn't a class. Maybe setCompressionLevel(conf, value) should be added to GzipCodec instead.\nA new patch incorporates Chris' comments (Thanks, Chirs!).\nThanks for updating the patch\n* Rather than having special semantics for construct, I'd suggest removing the directBufferSize formal from construct and returning the allocation to the constructor\n* Sorry, my last comment was not clear. By \"implementations should also describe...\" I meant that the classes implementing reinit should include a description of what it effects in its javadoc for reinit, as you did in BuiltInZlibDeflater. I didn't mean that more logging was required. Compressor::reinit should also describe the contract for future implementers, \"Prepare the compressor to be used in a new stream with settings defined in the given Configuration\" or something like that\n* I think it's appropriate to fail if the configuration is invalid rather than taking the default in ZlibFactory::getCompression\\* (why ZlibFactory?). I don't think the getDefault\\* methods are necessary. setCompression\\* should take the appropriate enum, rather than String. Filed HADOOP-6161 for get/setEnum to simplify some of the conf-related code\n* In ZlibCompressor::reinit, reset is not called if the Configuration is null. For users calling these methods not via CodecPool, reset() should probably be called\n* CompressionLevel::compressionLevel() and CompressionStrategy::compressionStrategy() should remain package-private; the integers are implementation details\n* The unit test doesn't really verify the functionality added by this patch, save that it doesn't throw exceptions. That said, it would be hard to verify that this is working as expected without adding get\\* methods to ZlibCompressor. Can you describe how you verified the patch's correctness, both for the native and non-native codecs?\nhadoop-5879-7-26.patch incorporates all Chris's suggestions (Thanks for the detailed comments, Chris!). \nI have to admit that the testcase can not fully verify the functionality. And to know that it works, what i did was by looking the log and debugging in my IDE. Please let me know if you know how we can test it automatically.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12414549/hadoop-5879-7-26.patch\n  against trunk revision 807753.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 4 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/629/console\n\nThis message is automatically generated.\n* Merged with trunk\n* Calls end() in reinit, avoiding leak\n* Removes construct and resetStrategyAndCompressionLevel, which caused findbugs warnings. Relies on invariant windowBits set in cstr for Default/ZlibCompressors to avoid reemergence of HADOOP-5281\n* Updated unit test to verify that compression level is reset\nHudson isn't picking up the patch, so I ran it on my machine:\n\n{noformat}\n     [exec] +1 overall.  \n     [exec] \n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec] \n     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.\n     [exec] \n     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.\n     [exec] \n     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n     [exec] \n     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n     [exec] \n     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n{noformat}\n\nAll unit tests passed with and without native libs installed.\n+1\n\nI committed this. Thanks, He!\nIntegrated in Hadoop-Common-trunk-Commit #29 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/29/])\n    . Read compression level and strategy from Configuration for\ngzip compression. Contributed by He Yongqiang\n\n", "issueSearchSentences": ["In ZlibCompressor::reinit, reset is not called if the Configuration is null.", "For users calling these methods not via CodecPool, reset() should probably be called", "The javadoc for reinit should describe the contract for that method; the implementations should also describe what happens for that particular codec.", "In ZlibCompression::reinit, shouldn't end() be called before reconfiguring the codec?", "I meant that, since these parameters are specified in the constructor, there's no place where these parameters are reset when a compressor is pulled from the pool."], "issueSearchScores": [0.6799091100692749, 0.6162390112876892, 0.5349917411804199, 0.5317764282226562, 0.5267407298088074]}
{"aId": 74, "code": "private ReplicaInfo resolveDuplicateReplicas(\n      final ReplicaInfo replica1, final ReplicaInfo replica2,\n      final ReplicaMap volumeMap) throws IOException {\n\n    ReplicaInfo replicaToKeep;\n    ReplicaInfo replicaToDelete;\n\n    if (replica1.getGenerationStamp() != replica2.getGenerationStamp()) {\n      replicaToKeep = replica1.getGenerationStamp() > replica2.getGenerationStamp()\n          ? replica1 : replica2;\n    } else if (replica1.getNumBytes() != replica2.getNumBytes()) {\n      replicaToKeep = replica1.getNumBytes() > replica2.getNumBytes() ?\n          replica1 : replica2;\n    } else if (replica1.getVolume().isTransientStorage() &&\n               !replica2.getVolume().isTransientStorage()) {\n      replicaToKeep = replica2;\n    } else {\n      replicaToKeep = replica1;\n    }\n\n    replicaToDelete = (replicaToKeep == replica1) ? replica2 : replica1;\n\n    // Update volumeMap.\n    volumeMap.add(bpid, replicaToKeep);\n\n    // Delete the files on disk. Failure here is okay.\n    replicaToDelete.getBlockFile().delete();\n    replicaToDelete.getMetaFile().delete();\n\n    FsDatasetImpl.LOG.info(\n        \"resolveDuplicateReplicas keeping \" + replicaToKeep.getBlockFile() +\n        \", deleting \" + replicaToDelete.getBlockFile());\n\n    return replicaToKeep;\n  }", "comment": " Prefer the replica with the higher generation stamp. If generation stamps are equal, prefer the replica with the larger on-disk length. If on-disk length is the same, prefer the replica on persistent storage volume. All other factors being equal, keep replica1. The other replica is removed from the volumeMap and is deleted from its storage volume.", "issueId": "HDFS-6931", "issueStringList": ["Move lazily persisted replicas to finalized directory on DN startup", "On restart the DN should move replicas from the {{current/lazyPersist/}} directory to {{current/finalized}}.", "Duplicate replicas of the same block should be deleted from RAM disk.", "On restart each volume will be scanned and replicas under {{lazyPersist/}} will be moved to their corresponding locations under {{finalized/}}.", "We may end up with two replicas of the same block on different volumes, so we use the following scheme to decide which replica to keep.", "# Prefer the replica with the higher generation stamp.", "# If generation stamps are equal, prefer the replica with the larger on-disk length.", "# If on-disk length is the same, prefer the replica on persistent storage volume.", "# All other factors being equal, keep replica1.", "The other replica is removed from the volumeMap and is deleted from its storage volume.", "See {{BlockPoolSlice.resolveDuplicateReplicas}}.", "Thus:", "# If a replica is found on both RAM disk and in lazyPersist/, delete the copy on RAM disk and move the lazyPersist/ copy to finalized/.", "This will be common when a DN is restarted.", "# If a replica is found on RAM disk but not in lazyPersist/, keep the copy on RAM disk and schedule a copy to disk via LazyWriter.", "This can occur if the DN process restarted before the replica could be saved to disk but RAM disk contents are not lost.", "# If a replica is found in lazyPersist/ but not on RAM disk, save the lazyPersist/ copy to finalized.", "This can occur on node restart when RAM disk contents are lost.", "Also added test cases.", "+1", "+1", "Thanks Jitendra!", "Committed to the feature branch."], "SplitGT": [" Prefer the replica with the higher generation stamp.", "If generation stamps are equal, prefer the replica with the larger on-disk length.", "If on-disk length is the same, prefer the replica on persistent storage volume.", "All other factors being equal, keep replica1.", "The other replica is removed from the volumeMap and is deleted from its storage volume."], "issueString": "Move lazily persisted replicas to finalized directory on DN startup\nOn restart the DN should move replicas from the {{current/lazyPersist/}} directory to {{current/finalized}}. Duplicate replicas of the same block should be deleted from RAM disk.\nOn restart each volume will be scanned and replicas under {{lazyPersist/}} will be moved to their corresponding locations under {{finalized/}}.\n\nWe may end up with two replicas of the same block on different volumes, so we use the following scheme to decide which replica to keep.\n\n# Prefer the replica with the higher generation stamp.\n# If generation stamps are equal, prefer the replica with the larger on-disk length.\n# If on-disk length is the same, prefer the replica on persistent storage volume.\n# All other factors being equal, keep replica1.\n\nThe other replica is removed from the volumeMap and is deleted from its storage volume. See {{BlockPoolSlice.resolveDuplicateReplicas}}.\n\nThus:\n# If a replica is found on both RAM disk and in lazyPersist/, delete the copy on RAM disk and move the lazyPersist/ copy to finalized/. This will be common when a DN is restarted.\n# If a replica is found on RAM disk but not in lazyPersist/, keep the copy on RAM disk and schedule a copy to disk via LazyWriter. This can occur if the DN process restarted before the replica could be saved to disk but RAM disk contents are not lost.\n# If a replica is found in lazyPersist/ but not on RAM disk, save the lazyPersist/ copy to finalized. This can occur on node restart when RAM disk contents are lost.\n\n\nAlso added test cases.\n\n\n+1\n+1\nThanks Jitendra! Committed to the feature branch.\n", "issueSearchSentences": ["The other replica is removed from the volumeMap and is deleted from its storage volume.", "See {{BlockPoolSlice.resolveDuplicateReplicas}}.", "We may end up with two replicas of the same block on different volumes, so we use the following scheme to decide which replica to keep.", "Duplicate replicas of the same block should be deleted from RAM disk.", "# If a replica is found on both RAM disk and in lazyPersist/, delete the copy on RAM disk and move the lazyPersist/ copy to finalized/."], "issueSearchScores": [0.7005681991577148, 0.6306880712509155, 0.6157653331756592, 0.6030480265617371, 0.5799017548561096]}
{"aId": 75, "code": "public static String getBaseName(String versionName) throws IOException {\n    Objects.requireNonNull(versionName, \"VersionName cannot be null\");\n    int div = versionName.lastIndexOf('@');\n    if (div == -1) {\n      throw new IOException(\"No version in key path \" + versionName);\n    }\n    return versionName.substring(0, div);\n  }", "comment": " Split the versionName in to a base name.", "issueId": "HADOOP-18504", "issueStringList": ["An unhandled NullPointerException in class KeyProvider", "The code throws an unhandled NullPointerException when the method *getBaseName* of KeyProvider.java is called with a null as input.", "The aforementioned exception originates in the method *getBaseName* when the code tries to find the index of the last occurrence of '@' in the null input string {*}versionName{*}.", "{*}Code snippet of where the NPE throws:{*}{*}{*}", "{code:java}", "public static String getBaseName(String versionName) throws IOException {", "int div = versionName.lastIndexOf('@');                               // <--- thrown here", "if (div == -1) {", "throw new IOException(\"No version in key path \" + versionName);", "}", "return versionName.substring(0, div);", "}{code}", "Solution:*", "We propose adding a null check for the input string and throwing an IOException with a proper error message.", "We also propose fixing the Javadoc comment for this method which demonstrates an incorrect example.", "Code snippet of the comment with buggy example:*", "{code:java}", "Split the versionName in to a base name.", "Converts \"/aaa/bbb/3\" to", "\"/aaa/bbb\".", "..."], "SplitGT": [" Split the versionName in to a base name."], "issueString": " An unhandled NullPointerException in class KeyProvider\nThe code throws an unhandled NullPointerException when the method *getBaseName* of KeyProvider.java is called with a null as input.\nThe aforementioned exception originates in the method *getBaseName* when the code tries to find the index of the last occurrence of '@' in the null input string {*}versionName{*}.\r\n\r\n\u00a0\r\n\r\n{*}Code snippet of where the NPE throws:{*}{*}{*}\r\n{code:java}\r\npublic static String getBaseName(String versionName) throws IOException {\r\n  int div = versionName.lastIndexOf('@');                               // <--- thrown here\r\n  if (div == -1) {\r\n    throw new IOException(\"No version in key path \" + versionName);\r\n  }\r\n  return versionName.substring(0, div);\r\n}{code}\r\n*Solution:*\r\n\r\nWe propose adding a null check for the input string and throwing an IOException with a proper error message.\r\n\r\nWe also propose fixing the Javadoc comment for this method which demonstrates an incorrect example.\r\n\r\n\u00a0\r\n\r\n*Code snippet of the comment with buggy example:*\r\n{code:java}\r\n /**\r\n * Split the versionName in to a base name. Converts \"/aaa/bbb/3\" to\r\n * \"/aaa/bbb\".\r\n ...\r\n */ {code}\n", "issueSearchSentences": ["public static String getBaseName(String versionName) throws IOException {", "Split the versionName in to a base name.", "throw new IOException(\"No version in key path \" + versionName);", "return versionName.substring(0, div);", "The aforementioned exception originates in the method *getBaseName* when the code tries to find the index of the last occurrence of '@' in the null input string {*}versionName{*}."], "issueSearchScores": [0.8788454532623291, 0.6862914562225342, 0.6511784791946411, 0.6425281763076782, 0.6035882830619812]}
{"aId": 76, "code": "public static void main(String[] args) {\n    String usage = \"NativeLibraryChecker [-a|-h]\\n\"\n        + \"  -a  use -a to check all libraries are available\\n\"\n        + \"      by default just check hadoop library is available\\n\"\n        + \"      exit with error code if check failed\\n\"\n        + \"  -h  print this message\\n\";\n    if (args.length > 1 ||\n        (args.length == 1 &&\n            !(args[0].equals(\"-a\") || args[0].equals(\"-h\")))) {\n      System.err.println(usage);\n      ExitUtil.terminate(1);\n    }\n    boolean checkAll = false;\n    if (args.length == 1) {\n      if (args[0].equals(\"-h\")) {\n        System.out.println(usage);\n        return;\n      }\n      checkAll = true;\n    }\n    boolean nativeHadoopLoaded = NativeCodeLoader.isNativeCodeLoaded();\n    boolean zlibLoaded = false;\n    boolean snappyLoaded = false;\n    // lz4 is linked within libhadoop\n    boolean lz4Loaded = nativeHadoopLoaded;\n    if (nativeHadoopLoaded) {\n      zlibLoaded = ZlibFactory.isNativeZlibLoaded(new Configuration());\n      snappyLoaded = NativeCodeLoader.buildSupportsSnappy() &&\n          SnappyCodec.isNativeCodeLoaded();\n    }\n    System.out.println(\"Native library checking:\");\n    System.out.printf(\"hadoop: %b\\n\", nativeHadoopLoaded);\n    System.out.printf(\"zlib:   %b\\n\", zlibLoaded);\n    System.out.printf(\"snappy: %b\\n\", snappyLoaded);\n    System.out.printf(\"lz4:    %b\\n\", lz4Loaded);\n    if ((!nativeHadoopLoaded) ||\n        (checkAll && !(zlibLoaded && snappyLoaded && lz4Loaded))) {\n      // return 1 to indicated check failed\n      ExitUtil.terminate(1);\n    }\n  }", "comment": " A tool to test native library availability,", "issueId": "HADOOP-9162", "issueStringList": ["Add utility to check native library availability", "Many times, after deploy hadoop or when trouble shooting, we need to check whether native library(along with native compression libraries) can work properly, and I just want to use one command to check that, like this:", "hadoop org.apache.hadoop.util.NativeCodeLoader", "and it shows:", "Native library loading test:", "hadoop: false", "zlib:   false", "snappy: false", "lz4:    false", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12561917/HADOOP-9162.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1914//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1914//console", "This message is automatically generated.", "This is a useful utility.", "Comments:", "# Please add this functionality in a separate class instead of NativeCodeLoader.", "# Given that you are providing the availability status as output, exitcode of 1 only when native code is not available seems strange.", "Perhaps you should just always return 0.", "# Please add this a command into src/main/bin/hadoop", "Because the snappy.jar file may not even be on the classpath, depending on the artifacts installed you need to", "# have the snappy test in a different class from NativeCodeLoader, so it wouldn't fail to load hadoop.lib just as snappy.jar was missing.", "# split the zlib test from the snappy test", "It'll be more useful, if it can print the versions of the native libs as well.", "I've seen in the past it can pick up the wrong libs.", "bq.", "Please add this functionality in a separate class instead of NativeCodeLoader.", "bq.", "Please add this a command into src/main/bin/hadoop", "OK\uff0cI will put it in a new class, and add command to bin/hadoop", "bq.", "Given that you are providing the availability status as output, exitcode of 1 only when native code is not available seems strange.", "I'm assuming people can use this in deploy scripts(tools) to check Native library availability, return -1 just a convenience for scripts, I can add a argument -a to check all native libraries(all available return 0) and default just check libhadoop(libhadoop available return 0).", "bq.", "Because the snappy.jar file may not even be on the classpath, depending on the artifacts installed you need to", "I was not aware that I imported org.xerial.snappy.Snappy; it must be eclipse who mistakenly add it, actually it is not useful at all, I will remove it.", "bq.", "It'll be more useful, if it can print the versions of the native libs as well.", "I've seen in the past it can pick up the wrong libs.", "I don't know how to achieve that, it seams a problem to introspect System.loadLibrary(\"hadoop\"); to find out which file it actually load.", "new version addressing my previous comments.", "As a single main class util, it's hard to add unit test.", "But I have done some simple manual test, here is the result.", "{code}", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop", "Usage: hadoop [--config confdir] COMMAND", "where COMMAND is one of:", "fs                   run a generic filesystem user client", "version              print the version", "jar <jar>            run a jar file", "checknative [-a|-h]  check native hadoop and compression libraries availability", "distcp <srcurl> <desturl> copy file or directories recursively", "archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive", "classpath            prints the class path needed to get the", "Hadoop jar and the required libraries", "daemonlog            get/set the log level for each daemon", "or", "CLASSNAME            run the class named CLASSNAME", "Most commands print help when invoked w/o parameters.", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative -h", "NativeLibraryChecker [-a|-h]", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative", "12/12/21 15:07:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable", "Native library checking:", "hadoop: false", "zlib:   false", "snappy: false", "lz4:    false", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> echo $?", "1", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative -a", "12/12/21 15:07:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable", "Native library checking:", "hadoop: false", "zlib:   false", "snappy: false", "lz4:    false", "decster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> echo $?", "1", "{code}", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562050/HADOOP-9162.v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1917//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1917//console", "This message is automatically generated.", "bq.", "I don't know how to achieve that, it seams a problem to introspect System.loadLibrary(\"hadoop\"); to find out which file it actually load.", "I don't see an easy cross platform way either -unless the libraries themselves return a version number.", "We could add that to the Hadoop-specific codecs, but not to Snappy.", "Regarding tests, if you use {{ExitUtils.terminate()}} instead of {{System.exit()}}, you can write a test which calls the {{main()}} function with different args, -after disabling exit with {{ExitUtils.disableSystemExit()}} -this will cause an {{ExitException}} to be thrown instead of the process exiting.", "That would let you write some unit tests that check the class behaviour.", "Using ExitUtil and add unittest.", "bq.", "I don't see an easy cross platform way either -unless the libraries themselves return a version number.", "We could add that to the Hadoop-specific codecs, but not to Snappy.", "Add library version number and/or library file to native libraries are not in the scope of this JIRA, create HADOOP-9164 for this.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562104/HADOOP-9162.v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1920//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1920//console", "This message is automatically generated.", "The v3 patch lgtm.", "+1.", "Minor Comments:", "# Please add annotation to NativeLibraryChecker - InterfaceAudience.Private", "# Please add description of -a to the usage.", "# Do you think it is a good idea to have checknative [hadoop|zlib|snappy|lz4] and absence of specific option prints for all?", "I am okay without this change, if you choose not to make this change.", "What would be the semantics of InterfaceAudience.Private on a executable main class?", "The class name can only be referred in a Hadoop script and not some other scripts?", "We add those annotations to all the public classes.", "Nothing prevents people from adding methods to that class in the future and then it gets used by user apps.", "So it is better to have the annotation for all the public classes.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562192/HADOOP-9162.v4.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1923//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1923//console", "This message is automatically generated.", "Integrated in Hadoop-trunk-Commit #3157 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3157/])", "HADOOP-9162.", "Add utility to check native library availability.", "Contributed by Binglin Chang.", "(Revision 1425390)", "Result = SUCCESS", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1425390", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeLibraryChecker.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeLibraryChecker.java", "I committed the patch trunk and branch-2.", "Thank you Binglin."], "SplitGT": [" A tool to test native library availability,"], "issueString": "Add utility to check native library availability\nMany times, after deploy hadoop or when trouble shooting, we need to check whether native library(along with native compression libraries) can work properly, and I just want to use one command to check that, like this:\n\nhadoop org.apache.hadoop.util.NativeCodeLoader\n\nand it shows:\n\nNative library loading test:\nhadoop: false\nzlib:   false\nsnappy: false\nlz4:    false\n\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12561917/HADOOP-9162.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1914//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1914//console\n\nThis message is automatically generated.\nThis is a useful utility. Comments:\n# Please add this functionality in a separate class instead of NativeCodeLoader.\n# Given that you are providing the availability status as output, exitcode of 1 only when native code is not available seems strange. Perhaps you should just always return 0.\n# Please add this a command into src/main/bin/hadoop\nBecause the snappy.jar file may not even be on the classpath, depending on the artifacts installed you need to\n# have the snappy test in a different class from NativeCodeLoader, so it wouldn't fail to load hadoop.lib just as snappy.jar was missing.\n# split the zlib test from the snappy test\n\n\nIt'll be more useful, if it can print the versions of the native libs as well. I've seen in the past it can pick up the wrong libs.\nbq. Please add this functionality in a separate class instead of NativeCodeLoader.\nbq. Please add this a command into src/main/bin/hadoop\nOK\uff0cI will put it in a new class, and add command to bin/hadoop\n\nbq. Given that you are providing the availability status as output, exitcode of 1 only when native code is not available seems strange. \nI'm assuming people can use this in deploy scripts(tools) to check Native library availability, return -1 just a convenience for scripts, I can add a argument -a to check all native libraries(all available return 0) and default just check libhadoop(libhadoop available return 0).\n\nbq. Because the snappy.jar file may not even be on the classpath, depending on the artifacts installed you need to\nI was not aware that I imported org.xerial.snappy.Snappy; it must be eclipse who mistakenly add it, actually it is not useful at all, I will remove it.\n\nbq. It'll be more useful, if it can print the versions of the native libs as well. I've seen in the past it can pick up the wrong libs.\nI don't know how to achieve that, it seams a problem to introspect System.loadLibrary(\"hadoop\"); to find out which file it actually load.\nnew version addressing my previous comments.\nAs a single main class util, it's hard to add unit test. But I have done some simple manual test, here is the result.\n{code}\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop \nUsage: hadoop [--config confdir] COMMAND\n       where COMMAND is one of:\n  fs                   run a generic filesystem user client\n  version              print the version\n  jar <jar>            run a jar file\n  checknative [-a|-h]  check native hadoop and compression libraries availability\n  distcp <srcurl> <desturl> copy file or directories recursively\n  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n  classpath            prints the class path needed to get the\n                       Hadoop jar and the required libraries\n  daemonlog            get/set the log level for each daemon\n or\n  CLASSNAME            run the class named CLASSNAME\n\nMost commands print help when invoked w/o parameters.\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative -h\nNativeLibraryChecker [-a|-h]\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative\n12/12/21 15:07:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nNative library checking:\nhadoop: false\nzlib:   false\nsnappy: false\nlz4:    false\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> echo $?\n1\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> bin/hadoop checknative -a\n12/12/21 15:07:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nNative library checking:\nhadoop: false\nzlib:   false\nsnappy: false\nlz4:    false\ndecster:~/projects/hadoop-trunk/hadoop-dist/target/hadoop-3.0.0-SNAPSHOT> echo $?\n1\n{code}\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562050/HADOOP-9162.v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1917//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1917//console\n\nThis message is automatically generated.\nbq. I don't know how to achieve that, it seams a problem to introspect System.loadLibrary(\"hadoop\"); to find out which file it actually load.\n\nI don't see an easy cross platform way either -unless the libraries themselves return a version number. We could add that to the Hadoop-specific codecs, but not to Snappy. \n\nRegarding tests, if you use {{ExitUtils.terminate()}} instead of {{System.exit()}}, you can write a test which calls the {{main()}} function with different args, -after disabling exit with {{ExitUtils.disableSystemExit()}} -this will cause an {{ExitException}} to be thrown instead of the process exiting. That would let you write some unit tests that check the class behaviour.\nUsing ExitUtil and add unittest.\nbq. I don't see an easy cross platform way either -unless the libraries themselves return a version number. We could add that to the Hadoop-specific codecs, but not to Snappy.\nAdd library version number and/or library file to native libraries are not in the scope of this JIRA, create HADOOP-9164 for this.\n\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562104/HADOOP-9162.v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1920//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1920//console\n\nThis message is automatically generated.\nThe v3 patch lgtm. +1.\nMinor Comments:\n# Please add annotation to NativeLibraryChecker - InterfaceAudience.Private\n# Please add description of -a to the usage.\n# Do you think it is a good idea to have checknative [hadoop|zlib|snappy|lz4] and absence of specific option prints for all? I am okay without this change, if you choose not to make this change.\n\n\nWhat would be the semantics of InterfaceAudience.Private on a executable main class? The class name can only be referred in a Hadoop script and not some other scripts? \nWe add those annotations to all the public classes. Nothing prevents people from adding methods to that class in the future and then it gets used by user apps. So it is better to have the annotation for all the public classes. \n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562192/HADOOP-9162.v4.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1923//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1923//console\n\nThis message is automatically generated.\nIntegrated in Hadoop-trunk-Commit #3157 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3157/])\n    HADOOP-9162. Add utility to check native library availability. Contributed by Binglin Chang. (Revision 1425390)\n\n     Result = SUCCESS\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1425390\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeLibraryChecker.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeLibraryChecker.java\n\nI committed the patch trunk and branch-2. Thank you Binglin.\n", "issueSearchSentences": ["Native library checking:", "Native library checking:", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeLibraryChecker.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeLibraryChecker.java", "I'm assuming people can use this in deploy scripts(tools) to check Native library availability, return -1 just a convenience for scripts, I can add a argument -a to check all native libraries(all available return 0) and default just check libhadoop(libhadoop available return 0)."], "issueSearchScores": [0.5740073323249817, 0.5740073323249817, 0.570960283279419, 0.5520482659339905, 0.5401889085769653]}
{"aId": 77, "code": "public long getLastLogin() {\n    return lastLogin;\n  }", "comment": " Get the time of the last login.", "issueId": "HADOOP-6656", "issueStringList": ["Security framework needs to renew Kerberos tickets while the process is running", "While a client process is running, there should be a thread that periodically renews the Kerberos credentials to ensure they don't expire.", "This patch is for yahoo 20s and uses an external kinit -R to renew the tickets.", "This patch is for y20s and shouldn't be committed.", "This seems like it it should work, but doesn't for me.", "I get a stream modified exception during the renewal.", "Any suggestions?", "+1 (on the refresh.patch)", "Attaching the trunk patch.", "Unfortunately, I realized towards the end that the patch requires the \"common\" part of MAPREDUCE-1566 to be committed first.", "So this won't compile now but should merge cleanly once the common part of MAPREDUCE-1566 patch is committed.", "Attaching a patch w.r.t the current trunk.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450347/6656-trunk-2.patch", "against trunk revision 967220.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/console", "This message is automatically generated.", "getTGT() method should be replaced by or merged with SecurityUtil.getTgtFromSubject().", "I don't think getTGT() handles cross-realm case.", "Shouldn't User.setLastLogin() and User.getLastLogin() be synchronized methods?", "In current code, only synchronized methods in UGI use them, which is fine.", "But it's safer to synchronize at User class, and not relying on users of User class to synchronize.", "Same for other getters and setters in User.", "hasSufficientTimeElapsed() has the side-effect of setting the last login time to now if it returns true, which is not intuitive to me.", "> I don't think getTGT() handles cross-realm case.", "Sorry, it does.", "But I still recommend it to be merged with SecurityUtil.getTgtFromSubject().", "Addressing Kan's comments..", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450614/6656-trunk-3.patch", "against trunk revision 979785.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/console", "This message is automatically generated.", "Sorry, I now see why you didn't use SecurityUtil.getTgtFromSubject() in the first place.", "You already have a \"subject\" to work with.", "For this reason I think it's better to use your original code, since I'm not sure what your AccessControlContext is when you call getTgtFromSubject().", "But I like your way of figuring out whether a ticket is an original TGT.", "Is it possible to share that logic with SecurityUtil.isOriginalTgt()?", "Otherwise, +1 for the patch.", "Addressed Kan's comments..", "This patch has some improved javadocs.", "+1.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450781/6656-trunk-4.patch", "against trunk revision 980271.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/console", "This message is automatically generated.", "Attaching a patch fixing the findbugs warning.", "The javadoc warning is unrelated.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450783/6656-trunk-4.patch", "against trunk revision 980271.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/console", "This message is automatically generated.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450785/6656-trunk-4.patch", "against trunk revision 980271.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/console", "This message is automatically generated.", "This should take care of findbugs.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12450843/6656-trunk-4.patch", "against trunk revision 980271.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/console", "This message is automatically generated.", "+1 on the latest patch.", "I just committed this.", "Thanks, Owen for the early patches on this."], "SplitGT": [" Get the time of the last login."], "issueString": "Security framework needs to renew Kerberos tickets while the process is running\nWhile a client process is running, there should be a thread that periodically renews the Kerberos credentials to ensure they don't expire.\nThis patch is for yahoo 20s and uses an external kinit -R to renew the tickets.\nThis patch is for y20s and shouldn't be committed.\n\nThis seems like it it should work, but doesn't for me. I get a stream modified exception during the renewal. Any suggestions?\n\n+1 (on the refresh.patch)\nAttaching the trunk patch. Unfortunately, I realized towards the end that the patch requires the \"common\" part of MAPREDUCE-1566 to be committed first. So this won't compile now but should merge cleanly once the common part of MAPREDUCE-1566 patch is committed.\nAttaching a patch w.r.t the current trunk.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450347/6656-trunk-2.patch\n  against trunk revision 967220.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/630/console\n\nThis message is automatically generated.\ngetTGT() method should be replaced by or merged with SecurityUtil.getTgtFromSubject(). I don't think getTGT() handles cross-realm case.\n\nShouldn't User.setLastLogin() and User.getLastLogin() be synchronized methods? In current code, only synchronized methods in UGI use them, which is fine. But it's safer to synchronize at User class, and not relying on users of User class to synchronize. Same for other getters and setters in User.\n\nhasSufficientTimeElapsed() has the side-effect of setting the last login time to now if it returns true, which is not intuitive to me.\n> I don't think getTGT() handles cross-realm case.\nSorry, it does. But I still recommend it to be merged with SecurityUtil.getTgtFromSubject().\nAddressing Kan's comments..\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450614/6656-trunk-3.patch\n  against trunk revision 979785.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/641/console\n\nThis message is automatically generated.\nSorry, I now see why you didn't use SecurityUtil.getTgtFromSubject() in the first place. You already have a \"subject\" to work with. For this reason I think it's better to use your original code, since I'm not sure what your AccessControlContext is when you call getTgtFromSubject(). But I like your way of figuring out whether a ticket is an original TGT. Is it possible to share that logic with SecurityUtil.isOriginalTgt()?\n\nOtherwise, +1 for the patch.\nAddressed Kan's comments..\nThis patch has some improved javadocs.\n+1.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450781/6656-trunk-4.patch\n  against trunk revision 980271.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/646/console\n\nThis message is automatically generated.\nAttaching a patch fixing the findbugs warning. The javadoc warning is unrelated.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450783/6656-trunk-4.patch\n  against trunk revision 980271.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/647/console\n\nThis message is automatically generated.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450785/6656-trunk-4.patch\n  against trunk revision 980271.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/648/console\n\nThis message is automatically generated.\nThis should take care of findbugs.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12450843/6656-trunk-4.patch\n  against trunk revision 980271.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/649/console\n\nThis message is automatically generated.\n+1 on the latest patch.\nI just committed this. Thanks, Owen for the early patches on this.\n", "issueSearchSentences": ["Shouldn't User.setLastLogin() and User.getLastLogin() be synchronized methods?", "hasSufficientTimeElapsed() has the side-effect of setting the last login time to now if it returns true, which is not intuitive to me.", "1 overall.", "1 overall.", "1 overall."], "issueSearchScores": [0.5432538986206055, 0.46158790588378906, 0.2564091086387634, 0.2564091086387634, 0.2564091086387634]}
{"aId": 78, "code": "public static boolean isInstrumentationAccessAllowed(\n    ServletContext servletContext, HttpServletRequest request,\n    HttpServletResponse response) throws IOException {\n    Configuration conf =\n      (Configuration) servletContext.getAttribute(CONF_CONTEXT_ATTRIBUTE);\n\n    boolean access = true;\n    boolean adminAccess = conf.getBoolean(\n      CommonConfigurationKeys.HADOOP_SECURITY_INSTRUMENTATION_REQUIRES_ADMIN,\n      false);\n    if (adminAccess) {\n      access = hasAdministratorAccess(servletContext, request, response);\n    }\n    return access;\n  }", "comment": " If hadoop.security.instrumentation.requires.admin is set to FALSE (default value) it always returns TRUE.", "issueId": "HADOOP-8343", "issueStringList": ["Allow configuration of authorization for JmxJsonServlet and MetricsServlet", "When using authorization for the daemons' web server, it would be useful to specifically control the authorization requirements for accessing /jmx and /metrics.", "Currently, they require administrative access.", "This JIRA would propose that whether or not they are available to administrators only or to all users be controlled by \"hadoop.instrumentation.requires.administrator\" (or similar).", "The default would be that administrator access is required.", "attached is a patch that adds a \"hadoop.security.anonymous.instrumentation.access\" configuration property which is TRUE by default and if set to TRUE enables anonymous access (without ACLs enforcement).", "This works because (as it seems intended) in HttpServer, the JMX, METRICS & CONF servlets are added without requiring authentication.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12525245/HADOOP-8343.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified test files.", "1 javadoc.", "The javadoc tool appears to have generated 2 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/918//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/918//console", "This message is automatically generated.", "the javadoc warnings seem unrelated to this patch", "+1, the patch looks good to me.", "After some investigation on how the HttpServer binds the JMX and METRICS servlets (hardcoded not to add the SPNEGO filter) it seems to me that the correct approach would be:", "have a 'hadoop.security.require.authentication.for.instrumentation' config property set to FALSE by default.", "HttpServer addition of JMX, METRICS and CONF servlets should register the servlets to require authentication or not based on the above property.", "remove the hasAdminAccess check for the JMX, METRICS and CONF servlets.", "the scope of this auth requirement should be extended to the stacks/ and logLevel/ servlets", "After further digging I think figured out how things are supposed to work:", "# the instrumentation servlets (stacks/, logLevel/, conf/, metrics/, jmx/) are not to be authentication protected by the built-in SPNEGO filter.", "# the instrumentation servlets are authentication protected if an custom filter (via FilterInitializer) is added.", "# the instrumentation servlets had a check hasAdminAccess() that guards it access restricting access to admin users if security/authorization is ON.", "This check was incorrect and was fixed by HADOOP-8314", "HADOOP-8314 fix had a side effect of disabling access to instrumentation if the user is not in an ACL.", "While that may be desirable in certain deployments, it is quite common (and reasonable) to have instrumentation access without requiring authentication or authorization.", "The attached patch then introduces (as the original approach suggested) a property *hadoop.security.authorization.for.instrumentation* to enforce or not authorization on the instrumentation servlets.", "The patch does not do any changes related to authentication requirements (which can still be done adding a filter via a filter initializer).", "The patch modifies the 5 instrumentation servlets to use the new logic (encapsulated in the *checkInstrumentationAccess()* method)", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12525350/HADOOP-8343.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified test files.", "1 javadoc.", "The javadoc tool appears to have generated 2 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/922//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/922//console", "This message is automatically generated.", "javadoc warnings seem unrelated", "Patch looks pretty good to me.", "Just a few small comments.", "+1 once these are addressed:", "# I think we should take /logLevel out of the set of servlets which this new config allows anon access to.", "Since its writable, it seems like requiring admin access in all cases is reasonable.", "# Recommend renaming \"hadoop.security.authorization.for.instrumentation\" to \"hadoop.security.instrumentation.requires.admin\".", "# Recommend renaming \"checkInstrumentationAccess\" to \"isInstrumentationAccessAllowed\".", "# The method comment of checkInstrumentationAccess is a little misleading.", "Instead of \"Returns if anonymous authentication access to instrumentation servlets is allowed or not\" it should be something like \"Return true if admin privileges are not required to access instrumentation, or this user is authenticated and an administrator.", "Return false otherwise.\"", "# The method checkInstrumentationAccess can be simplified a little, e.g.", "\"return !adminAccessRequired || hasAdministratorAccess(...)\"", "# The entry for this new config in core-default.xml only lists /jmx, /metrics, and /conf.", "/stacks should also be added (and /logLevel if you object to comment #1 above.)", "# There's a few spots in the patch where you use 4-space indentation instead of Hadoop's standard 2.", "# The test should probably also include the case where admin access is required and the user _is_ listed as an admin.", "@atm, thx.", "integrated all your comments except #5 as boolean expression of a NEG and an OR is more difficult to follow than a the current linear code (the compiler/jvm will take care of optimizing this anyway).", "+1 pending Jenkins.", "One tiny nit:", "{noformat}", "+   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE", "+   * (default value) it returns always returns TRUE.", "{noformat}", "One too many \"returns\" in the sentence above.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12525545/HADOOP-8343.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 2 new or modified test files.", "1 javadoc.", "The javadoc tool appears to have generated 2 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 eclipse:eclipse.", "The patch built with eclipse:eclipse.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.fs.viewfs.TestViewFsTrash", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/936//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/936//console", "This message is automatically generated.", "javadocs warning and testcase failure seem unrelated.", "committed to trunk and branch-2", "Integrated in Hadoop-Common-trunk-Commit #2186 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2186/])", "HADOOP-8343.", "Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)", "Result = SUCCESS", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2260 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2260/])", "HADOOP-8343.", "Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)", "Result = SUCCESS", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2204 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2204/])", "HADOOP-8343.", "Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)", "Result = ABORTED", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java"], "SplitGT": [" If hadoop.security.instrumentation.requires.admin is set to FALSE (default value) it always returns TRUE."], "issueString": "Allow configuration of authorization for JmxJsonServlet and MetricsServlet\nWhen using authorization for the daemons' web server, it would be useful to specifically control the authorization requirements for accessing /jmx and /metrics.  Currently, they require administrative access.  This JIRA would propose that whether or not they are available to administrators only or to all users be controlled by \"hadoop.instrumentation.requires.administrator\" (or similar).  The default would be that administrator access is required.\nattached is a patch that adds a \"hadoop.security.anonymous.instrumentation.access\" configuration property which is TRUE by default and if set to TRUE enables anonymous access (without ACLs enforcement).\n\nThis works because (as it seems intended) in HttpServer, the JMX, METRICS & CONF servlets are added without requiring authentication.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12525245/HADOOP-8343.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified test files.\n\n    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/918//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/918//console\n\nThis message is automatically generated.\nthe javadoc warnings seem unrelated to this patch\n+1, the patch looks good to me.\nAfter some investigation on how the HttpServer binds the JMX and METRICS servlets (hardcoded not to add the SPNEGO filter) it seems to me that the correct approach would be:\n\n* have a 'hadoop.security.require.authentication.for.instrumentation' config property set to FALSE by default.\n* HttpServer addition of JMX, METRICS and CONF servlets should register the servlets to require authentication or not based on the above property.\n* remove the hasAdminAccess check for the JMX, METRICS and CONF servlets.\n\nthe scope of this auth requirement should be extended to the stacks/ and logLevel/ servlets\nAfter further digging I think figured out how things are supposed to work:\n\n# the instrumentation servlets (stacks/, logLevel/, conf/, metrics/, jmx/) are not to be authentication protected by the built-in SPNEGO filter.\n# the instrumentation servlets are authentication protected if an custom filter (via FilterInitializer) is added.\n# the instrumentation servlets had a check hasAdminAccess() that guards it access restricting access to admin users if security/authorization is ON. This check was incorrect and was fixed by HADOOP-8314\n\nHADOOP-8314 fix had a side effect of disabling access to instrumentation if the user is not in an ACL.\n\nWhile that may be desirable in certain deployments, it is quite common (and reasonable) to have instrumentation access without requiring authentication or authorization.\n\nThe attached patch then introduces (as the original approach suggested) a property *hadoop.security.authorization.for.instrumentation* to enforce or not authorization on the instrumentation servlets. The patch does not do any changes related to authentication requirements (which can still be done adding a filter via a filter initializer). The patch modifies the 5 instrumentation servlets to use the new logic (encapsulated in the *checkInstrumentationAccess()* method)\n\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12525350/HADOOP-8343.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified test files.\n\n    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/922//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/922//console\n\nThis message is automatically generated.\njavadoc warnings seem unrelated\nPatch looks pretty good to me. Just a few small comments. +1 once these are addressed:\n\n# I think we should take /logLevel out of the set of servlets which this new config allows anon access to. Since its writable, it seems like requiring admin access in all cases is reasonable.\n# Recommend renaming \"hadoop.security.authorization.for.instrumentation\" to \"hadoop.security.instrumentation.requires.admin\".\n# Recommend renaming \"checkInstrumentationAccess\" to \"isInstrumentationAccessAllowed\".\n# The method comment of checkInstrumentationAccess is a little misleading. Instead of \"Returns if anonymous authentication access to instrumentation servlets is allowed or not\" it should be something like \"Return true if admin privileges are not required to access instrumentation, or this user is authenticated and an administrator. Return false otherwise.\"\n# The method checkInstrumentationAccess can be simplified a little, e.g. \"return !adminAccessRequired || hasAdministratorAccess(...)\"\n# The entry for this new config in core-default.xml only lists /jmx, /metrics, and /conf. /stacks should also be added (and /logLevel if you object to comment #1 above.)\n# There's a few spots in the patch where you use 4-space indentation instead of Hadoop's standard 2.\n# The test should probably also include the case where admin access is required and the user _is_ listed as an admin.\n@atm, thx. integrated all your comments except #5 as boolean expression of a NEG and an OR is more difficult to follow than a the current linear code (the compiler/jvm will take care of optimizing this anyway).\n+1 pending Jenkins. One tiny nit:\n\n{noformat}\n+   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE\n+   * (default value) it returns always returns TRUE.\n{noformat}\n\nOne too many \"returns\" in the sentence above.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12525545/HADOOP-8343.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 2 new or modified test files.\n\n    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.fs.viewfs.TestViewFsTrash\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/936//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/936//console\n\nThis message is automatically generated.\njavadocs warning and testcase failure seem unrelated.\ncommitted to trunk and branch-2\nIntegrated in Hadoop-Common-trunk-Commit #2186 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2186/])\n    HADOOP-8343. Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)\n\n     Result = SUCCESS\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java\n\nIntegrated in Hadoop-Hdfs-trunk-Commit #2260 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2260/])\n    HADOOP-8343. Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)\n\n     Result = SUCCESS\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java\n\nIntegrated in Hadoop-Mapreduce-trunk-Commit #2204 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2204/])\n    HADOOP-8343. Allow configuration of authorization for JmxJsonServlet and MetricsServlet (tucu) (Revision 1333750)\n\n     Result = ABORTED\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1333750\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java\n\n", "issueSearchSentences": ["Instead of \"Returns if anonymous authentication access to instrumentation servlets is allowed or not\" it should be something like \"Return true if admin privileges are not required to access instrumentation, or this user is authenticated and an administrator.", "# the instrumentation servlets had a check hasAdminAccess() that guards it access restricting access to admin users if security/authorization is ON.", "While that may be desirable in certain deployments, it is quite common (and reasonable) to have instrumentation access without requiring authentication or authorization.", "The default would be that administrator access is required.", "This JIRA would propose that whether or not they are available to administrators only or to all users be controlled by \"hadoop.instrumentation.requires.administrator\" (or similar)."], "issueSearchScores": [0.7264885902404785, 0.6582877039909363, 0.6320298910140991, 0.6131688356399536, 0.5997510552406311]}
{"aId": 80, "code": "public static File[] listFiles(File dir) throws IOException {\n    File[] files = dir.listFiles();\n    if(files == null) {\n      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n                + dir.toString());\n    }\n    return files;\n  }", "comment": " A wrapper for File#listFiles. This java.io API returns null when a dir is not a directory or for any I/O error. Instead of having null check everywhere File#listFiles is used, we will add utility API to get around this problem. For the majority of cases where we prefer an IOException to be thrown.", "issueId": "HADOOP-7322", "issueStringList": ["Adding a util method in FileUtil for JDK File.listFiles", "While testing Disk Fail Inplace, we encountered lots of NPE from Dir.listFiles API.", "This API can return null when Dir is not directory or disk is bad.", "I am proposing to have a File Util which can be used consistently across to deal with disk issues.", "This util api will do the following:", "1.", "When error happens it will throw IOException", "2.", "Else it will return empty list or list of files.", "Signature:", "File[] FileUtil.listFiles(File dir) throws IOException {}", "This way we no need to write wrapper code every where.", "Also, API is consistent with the signature.", "Hi Bharath,", "1. suggest javadoc comment for FileUtil.listFiles() to read:", "+   * A wrapper for {@link File#listFiles()}.", "This java.io API returns null", "+   * when dir is not directory or for any I/O error.", "Instead of having to", "+   * null check everywhere File#listFiles() is used, we will add this utility", "+   * api to get around this problem, for the majority of cases where we prefer", "+   * an IOException to be thrown.", "2.", "Not sure about the change to RunJar.main().", "Isn't jar extraction usually pretty forgiving?", "It is currently written to skip any directory it can't read.", "Can you please give an argument for why that's wrong?", "3.", "In the last line of testlistFiles(), to assure that the referenced directory can't possibly exist, why not use the same name as the directory you just deleted in the previous line?", "Maybe after asserting:", "Assert.assertTrue(\"Failed to delete test dir\", !newDir.exists());", "Thanks for the comments, Matt.", "Regarding 2: Yes, we can be forgiving on this case.", "I added this because of eliminating null check.", "Attaching a patch which addresses all these comments.", "Thanks, Bharath.", "Unfortunately I have one more item.", "Looking more at testlistFiles(), there are many calls that could cause an IOException, but only the last one should be an allowed IOException.", "Therefore, I think we have to remove the \"expected=IOException.class\" from the @Test annotation, and instead use the idiom:", "{code}", "try {", "files = FileUtil.listFiles(newDir);", "fail(\"IOException expected on listFiles() for non-existent dir \" + newDir.getString());", "} catch (IOException ioe) {", "expected", "}", "{code}", "Not a problem.", "I have updated the testcase.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12480373/HADOOP-7322-3.patch", "against trunk revision 1127215.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 6 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//testReport/", "Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//console", "This message is automatically generated.", "+1.", "Committed to trunk.", "Thanks, Bharath!", "Integrated in Hadoop-Common-trunk-Commit #622 (See [https://builds.apache.org/hudson/job/Hadoop-Common-trunk-Commit/622/])", "HADOOP-7322.", "Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles().", "Contributed by Bharath Mundlapudi.", "mattf : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127697", "Files :", "hadoop/common/trunk/CHANGES.txt", "hadoop/common/trunk/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java", "hadoop/common/trunk/src/java/org/apache/hadoop/fs/FileUtil.java", "hadoop/common/trunk/src/test/core/org/apache/hadoop/fs/TestFileUtil.java"], "SplitGT": [" A wrapper for File#listFiles.", "This java.io API returns null when a dir is not a directory or for any I/O error.", "Instead of having null check everywhere File#listFiles is used, we will add utility API to get around this problem.", "For the majority of cases where we prefer an IOException to be thrown."], "issueString": "Adding a util method in FileUtil for JDK File.listFiles\nWhile testing Disk Fail Inplace, we encountered lots of NPE from Dir.listFiles API. This API can return null when Dir is not directory or disk is bad. I am proposing to have a File Util which can be used consistently across to deal with disk issues. This util api will do the following:\n\n1. When error happens it will throw IOException\n2. Else it will return empty list or list of files.\n\nSignature:\nFile[] FileUtil.listFiles(File dir) throws IOException {}\n\nThis way we no need to write wrapper code every where. Also, API is consistent with the signature.\n \nHi Bharath,\n1. suggest javadoc comment for FileUtil.listFiles() to read:\n+   * A wrapper for {@link File#listFiles()}. This java.io API returns null \n+   * when dir is not directory or for any I/O error. Instead of having to\n+   * null check everywhere File#listFiles() is used, we will add this utility \n+   * api to get around this problem, for the majority of cases where we prefer\n+   * an IOException to be thrown.\n\n2. Not sure about the change to RunJar.main().  Isn't jar extraction usually pretty forgiving?  It is currently written to skip any directory it can't read.  Can you please give an argument for why that's wrong?\n\n3. In the last line of testlistFiles(), to assure that the referenced directory can't possibly exist, why not use the same name as the directory you just deleted in the previous line?  Maybe after asserting:\nAssert.assertTrue(\"Failed to delete test dir\", !newDir.exists());\nThanks for the comments, Matt. \n\nRegarding 2: Yes, we can be forgiving on this case. I added this because of eliminating null check. \n\nAttaching a patch which addresses all these comments. \nThanks, Bharath.  Unfortunately I have one more item.  Looking more at testlistFiles(), there are many calls that could cause an IOException, but only the last one should be an allowed IOException.  Therefore, I think we have to remove the \"expected=IOException.class\" from the @Test annotation, and instead use the idiom:\n{code}\ntry {\n  files = FileUtil.listFiles(newDir);\n  fail(\"IOException expected on listFiles() for non-existent dir \" + newDir.getString());\n} catch (IOException ioe) {\n  //expected\n}\n{code}\n\nNot a problem. I have updated the testcase.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12480373/HADOOP-7322-3.patch\n  against trunk revision 1127215.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/523//console\n\nThis message is automatically generated.\n+1.  Committed to trunk.  Thanks, Bharath!\nIntegrated in Hadoop-Common-trunk-Commit #622 (See [https://builds.apache.org/hudson/job/Hadoop-Common-trunk-Commit/622/])\n    HADOOP-7322. Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles(). Contributed by Bharath Mundlapudi.\n\nmattf : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127697\nFiles : \n* /hadoop/common/trunk/CHANGES.txt\n* /hadoop/common/trunk/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java\n* /hadoop/common/trunk/src/java/org/apache/hadoop/fs/FileUtil.java\n* /hadoop/common/trunk/src/test/core/org/apache/hadoop/fs/TestFileUtil.java\n\n", "issueSearchSentences": ["File[] FileUtil.listFiles(File dir) throws IOException {}", "fail(\"IOException expected on listFiles() for non-existent dir \" + newDir.getString());", "+   * A wrapper for {@link File#listFiles()}.", "files = FileUtil.listFiles(newDir);", "+   * null check everywhere File#listFiles() is used, we will add this utility"], "issueSearchScores": [0.8556551933288574, 0.7369576096534729, 0.6875072121620178, 0.6465458869934082, 0.6306946277618408]}
{"aId": 81, "code": "public synchronized boolean isValidStartContainerRequest(\n      ContainerTokenIdentifier containerTokenIdentifier) {\n\n    removeAnyContainerTokenIfExpired();\n\n    Long expTime = containerTokenIdentifier.getExpiryTimeStamp();\n    List<ContainerId> containers =\n        this.recentlyStartedContainerTracker.get(expTime);\n    if (containers == null\n        || !containers.contains(containerTokenIdentifier.getContainerID())) {\n      return true;\n    } else {\n      return false;\n    }\n  }", "comment": " It is safe to use expiration time as there is one to many mapping between expiration time and containerId.", "issueId": "YARN-62", "issueStringList": ["AM should not be able to abuse container tokens for repetitive container launches", "Clone of YARN-51.", "ApplicationMaster should not be able to store container tokens and use the same set of tokens for repetitive container launches.", "The possibility of such abuse is there in the current code, for a duration of 1d+10mins, we need to fix this.", "For legally generated containers, such requests are possible only for a period of container-expiry interval (10 mins by default).", "Assuming synced clocks between NM and AM, NM only needs to remember container-ids of containers that came in during the last 10 mins.", "If container-ids don't match the sent token, RPC layer itself will reject the request.", "Thoughts?", "Fixing this as a part of YARN-870.", "attaching patch here.", "Right now on NM we start container only when we receive valid container token (which also contains expiration time).", "Using this expiration time to uniquely identify container as it is tightly coupled to container-id with one to many mapping between expiration time and container-ids.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12589198/YARN-62-20130621.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:", "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown", "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/1380//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/1380//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/1380//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12589212/YARN-62-20130621.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:", "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync", "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown", "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/1405//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/1405//console", "This message is automatically generated.", "fixing test issues...", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12590108/YARN-62-20130628.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/1408//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/1408//console", "This message is automatically generated.", "Patch looks mostly good.", "Some comments:", "Though it works in most cases, it isn't logically correct to expire old token *only* if a new container comes in or succeeds.", "We should perform the expiry in a thread.", "Can you also write a specific test which launches a container that very quickly exits, turns around and launches another container with same ID and token and gets rejected?", "Also, please write a test which makes sure that old tokens are expired after 10 mins.", "thanks vinod..", "bq.", "Though it works in most cases, it isn't logically correct to expire old token only if a new container comes in or succeeds.", "We should perform the expiry in a thread.", "I thought about threads earlier but that is like starting an additional one to maintain this and seems like an overhead.", "thoughts?", "bq.", "Can you also write a specific test which launches a container that very quickly exits, turns around and launches another container with same ID and token and gets rejected?", "bq.", "Also, please write a test which makes sure that old tokens are expired after 10 mins.", "yeah will add one...", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12592699/YARN-62-20130715.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-YARN-Build/1509//testReport/", "Console output: https://builds.apache.org/job/PreCommit-YARN-Build/1509//console", "This message is automatically generated.", "bq.", "I thought about threads earlier but that is like starting an additional one to maintain this and seems like an overhead.", "Yeah, just looked back again.", "Seems like removing on the next startContainer seems fine.", "Fundamentally because we always remember only 10mins worth of containers.", "+1 for the latest patch.", "Checking it in.", "Committed this to trunk, branch-2 and branch-2.1.", "Thanks Omkar!", "SUCCESS: Integrated in Hadoop-trunk-Commit #4102 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4102/])", "YARN-62.", "Modified NodeManagers to avoid AMs from abusing container tokens for repetitive container launches.", "Contributed by Omkar Vinit Joshi.", "(vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1503986)", "hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/TestApplication.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java", "SUCCESS: Integrated in Hadoop-Yarn-trunk #273 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/273/])", "YARN-62.", "Modified NodeManagers to avoid AMs from abusing container tokens for repetitive container launches.", "Contributed by Omkar Vinit Joshi.", "(vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1503986)", "hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/TestApplication.java", "hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java"], "SplitGT": [" It is safe to use expiration time as there is one to many mapping between expiration time and containerId."], "issueString": "AM should not be able to abuse container tokens for repetitive container launches\nClone of YARN-51.\n\nApplicationMaster should not be able to store container tokens and use the same set of tokens for repetitive container launches. The possibility of such abuse is there in the current code, for a duration of 1d+10mins, we need to fix this.\nFor legally generated containers, such requests are possible only for a period of container-expiry interval (10 mins by default). Assuming synced clocks between NM and AM, NM only needs to remember container-ids of containers that came in during the last 10 mins.\n\nIf container-ids don't match the sent token, RPC layer itself will reject the request.\n\nThoughts?\nFixing this as a part of YARN-870. attaching patch here.\nRight now on NM we start container only when we receive valid container token (which also contains expiration time). Using this expiration time to uniquely identify container as it is tightly coupled to container-id with one to many mapping between expiration time and container-ids.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12589198/YARN-62-20130621.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:\n\n                  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown\n                  org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/1380//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/1380//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/1380//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12589212/YARN-62-20130621.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:\n\n                  org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync\n                  org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown\n                  org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/1405//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/1405//console\n\nThis message is automatically generated.\nfixing test issues...\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12590108/YARN-62-20130628.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/1408//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/1408//console\n\nThis message is automatically generated.\nPatch looks mostly good. Some comments:\n - Though it works in most cases, it isn't logically correct to expire old token *only* if a new container comes in or succeeds. We should perform the expiry in a thread.\n - Can you also write a specific test which launches a container that very quickly exits, turns around and launches another container with same ID and token and gets rejected?\n - Also, please write a test which makes sure that old tokens are expired after 10 mins.\nthanks vinod..\nbq. Though it works in most cases, it isn't logically correct to expire old token only if a new container comes in or succeeds. We should perform the expiry in a thread.\n\nI thought about threads earlier but that is like starting an additional one to maintain this and seems like an overhead. thoughts?\n\nbq. Can you also write a specific test which launches a container that very quickly exits, turns around and launches another container with same ID and token and gets rejected?\nbq. Also, please write a test which makes sure that old tokens are expired after 10 mins.\nyeah will add one...\n\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12592699/YARN-62-20130715.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-YARN-Build/1509//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-YARN-Build/1509//console\n\nThis message is automatically generated.\nbq. I thought about threads earlier but that is like starting an additional one to maintain this and seems like an overhead.\nYeah, just looked back again. Seems like removing on the next startContainer seems fine. Fundamentally because we always remember only 10mins worth of containers.\n\n+1 for the latest patch. Checking it in.\nCommitted this to trunk, branch-2 and branch-2.1. Thanks Omkar!\nSUCCESS: Integrated in Hadoop-trunk-Commit #4102 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4102/])\nYARN-62. Modified NodeManagers to avoid AMs from abusing container tokens for repetitive container launches. Contributed by Omkar Vinit Joshi. (vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1503986)\n* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/TestApplication.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java\n\nSUCCESS: Integrated in Hadoop-Yarn-trunk #273 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/273/])\nYARN-62. Modified NodeManagers to avoid AMs from abusing container tokens for repetitive container launches. Contributed by Omkar Vinit Joshi. (vinodkv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1503986)\n* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/TestApplication.java\n* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java\n\n", "issueSearchSentences": ["Right now on NM we start container only when we receive valid container token (which also contains expiration time).", "Using this expiration time to uniquely identify container as it is tightly coupled to container-id with one to many mapping between expiration time and container-ids.", "Seems like removing on the next startContainer seems fine.", "For legally generated containers, such requests are possible only for a period of container-expiry interval (10 mins by default).", "Though it works in most cases, it isn't logically correct to expire old token *only* if a new container comes in or succeeds."], "issueSearchScores": [0.6965272426605225, 0.6700718998908997, 0.6457984447479248, 0.6054810881614685, 0.5972753763198853]}
{"aId": 82, "code": "public static boolean hasRulesBeenSet() {\n    return rules != null;\n  }", "comment": " Indicates if the name rules have been set.", "issueId": "HADOOP-7887", "issueStringList": ["KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration", "While the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName.", "The attached patch configures the name rules as part of the configuration of the KerberosAuthenticatorHandler.", "Verified Kerberos testcases pass and added a new test to test this setting happens.", "The UGI on initialization (first call to *isSecurityEnabled()* ) reads the name rules for Hadoop configuration files (-site.xml).", "It is not possible to se a different name rule directly.", "So, the trick is, if using hadoop-auth outside of Hadoop (ie by Oozie) set the name rules in the hadoop-auth configuration properties and call UGI.isSecurityEnabled() before the KerberosAuthenticationHandler is initialized.", "If using it from Hadoop no need to set it in the hadoop-auth configuration, it will use the one set by UGI", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12506372/HADOOP-7887.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 patch.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/440//console", "This message is automatically generated.", "As a side comment, I'm not that happy with this fix.", "The main issue here is that hadoop-auth does not depend on hadoop-common and because of that UGI cannot be initialized from here.", "A more correct way of fixing this would be to decouple the setRules() from UGI initialization.", "Or ... if a namerules has been explicitly set, UGI should not override it.", "The later approach seems cleaner, thoughts?", "I could add that to the patch and then the init first UGI caveat would disappear.", "Updated patch that does not requires UGI initialization in advance.", "The UGI initialization will set the name rules only if they have not been already set.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12506402/HADOOP-7887.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 9 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in .", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/442//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/442//console", "This message is automatically generated.", "+1, the latest patch looks good to me.", "I like this solution *a lot* better than the original.", "Thanks a lot for fixing this issue, Alejandro.", "The patch works great on Oozie.", "Thanks a lot for fixing this.", "Committed to trunk and branch-0.23", "Integrated in Hadoop-Hdfs-trunk-Commit #1449 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1449/])", "HADOOP-7887.", "KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration.", "(tucu)", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java", "Integrated in Hadoop-Common-trunk-Commit #1377 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1377/])", "HADOOP-7887.", "KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration.", "(tucu)", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java", "Integrated in Hadoop-Hdfs-0.23-Commit #246 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/246/])", "Merge -r 1211672:1211673 from trunk to branch.", "FIXES: HADOOP-7887", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675", "Files :", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java", "Integrated in Hadoop-Common-0.23-Commit #256 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/256/])", "Merge -r 1211672:1211673 from trunk to branch.", "FIXES: HADOOP-7887", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675", "Files :", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #1401 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1401/])", "HADOOP-7887.", "KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration.", "(tucu)", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java", "Integrated in Hadoop-Mapreduce-0.23-Commit #267 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/267/])", "Merge -r 1211672:1211673 from trunk to branch.", "FIXES: HADOOP-7887", "tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675", "Files :", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java"], "SplitGT": [" Indicates if the name rules have been set."], "issueString": "KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration\nWhile the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName.\nThe attached patch configures the name rules as part of the configuration of the KerberosAuthenticatorHandler.\n\nVerified Kerberos testcases pass and added a new test to test this setting happens.\n\nThe UGI on initialization (first call to *isSecurityEnabled()* ) reads the name rules for Hadoop configuration files (-site.xml). It is not possible to se a different name rule directly. \n\nSo, the trick is, if using hadoop-auth outside of Hadoop (ie by Oozie) set the name rules in the hadoop-auth configuration properties and call UGI.isSecurityEnabled() before the KerberosAuthenticationHandler is initialized. If using it from Hadoop no need to set it in the hadoop-auth configuration, it will use the one set by UGI\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12506372/HADOOP-7887.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/440//console\n\nThis message is automatically generated.\nAs a side comment, I'm not that happy with this fix. The main issue here is that hadoop-auth does not depend on hadoop-common and because of that UGI cannot be initialized from here. A more correct way of fixing this would be to decouple the setRules() from UGI initialization. Or ... if a namerules has been explicitly set, UGI should not override it.\n\nThe later approach seems cleaner, thoughts? I could add that to the patch and then the init first UGI caveat would disappear.\nUpdated patch that does not requires UGI initialization in advance.\n\nThe UGI initialization will set the name rules only if they have not been already set.\n\n\n\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12506402/HADOOP-7887.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 9 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in .\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/442//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/442//console\n\nThis message is automatically generated.\n+1, the latest patch looks good to me. I like this solution *a lot* better than the original.\n\nThanks a lot for fixing this issue, Alejandro.\nThe patch works great on Oozie. Thanks a lot for fixing this.\nCommitted to trunk and branch-0.23\nIntegrated in Hadoop-Hdfs-trunk-Commit #1449 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1449/])\n    HADOOP-7887. KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration. (tucu)\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\nIntegrated in Hadoop-Common-trunk-Commit #1377 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1377/])\n    HADOOP-7887. KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration. (tucu)\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\nIntegrated in Hadoop-Hdfs-0.23-Commit #246 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Commit/246/])\n    Merge -r 1211672:1211673 from trunk to branch. FIXES: HADOOP-7887\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675\nFiles : \n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\nIntegrated in Hadoop-Common-0.23-Commit #256 (See [https://builds.apache.org/job/Hadoop-Common-0.23-Commit/256/])\n    Merge -r 1211672:1211673 from trunk to branch. FIXES: HADOOP-7887\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675\nFiles : \n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\nIntegrated in Hadoop-Mapreduce-trunk-Commit #1401 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1401/])\n    HADOOP-7887. KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration. (tucu)\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211673\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\nIntegrated in Hadoop-Mapreduce-0.23-Commit #267 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/267/])\n    Merge -r 1211672:1211673 from trunk to branch. FIXES: HADOOP-7887\n\ntucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211675\nFiles : \n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java\n\n", "issueSearchSentences": ["A more correct way of fixing this would be to decouple the setRules() from UGI initialization.", "The UGI initialization will set the name rules only if they have not been already set.", "While the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName.", "Verified Kerberos testcases pass and added a new test to test this setting happens.", "The attached patch configures the name rules as part of the configuration of the KerberosAuthenticatorHandler."], "issueSearchScores": [0.4491651654243469, 0.35317009687423706, 0.3424944281578064, 0.30680355429649353, 0.29530781507492065]}
{"aId": 84, "code": "public static final void setConnectTimeout(Configuration conf, int timeout) {\n    conf.setInt(CommonConfigurationKeys.IPC_CLIENT_CONNECT_TIMEOUT_KEY, timeout);\n  }", "comment": " set the connection timeout value in configuration", "issueId": "HADOOP-9106", "issueStringList": ["Allow configuration of IPC connect timeout", "Currently the connection timeout in Client.setupConnection() is hard coded to 20seconds.", "This is unreasonable in some scenarios, such as HA failover, if we want a faster failover time.", "We should allow this to be configured per-client.", "Made the timeout parameter configurable and set-able.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12563296/HADOOP-9106v1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.ipc.TestIPC", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1944//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1944//console", "This message is automatically generated.", "Need to correct the unit test", "Quick comments:", "# Client.java - please make the newly added method setConnectionTimeout into static", "# The getConnectionTimeout() method is not needed.", "Please save the connection timeout in a final member varilable in {{Client}} instead of getting it from the configuration every time.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12563371/HADOOP-9106v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1945//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1945//console", "This message is automatically generated.", "Suresh,", "I think making setConnectionTimeout static will not allow configuring 'per-client' which is desired.", "Concur with replacing the getConnectionTimeout.", "Maybe we can change the default 20s to a smaller value as well,  i think 20s is too large, let's tweak it to 1s or some other values ?", "bq.", "think making setConnectionTimeout static will not allow configuring 'per-client' which is desired", "When a Client is created conf is passed to it in constructor.", "Given that the connection timeout in this patch is reading the timeout off of this conf, the timeout should be set in the conf object before it is used in the Client constructor and it can be done using a static method.", "bq.", "Maybe we can change the default 20s to a smaller value as well,", "I think 20s should be fine.", "With this configurability you can change it.", "Lets do it in another jira.", "Made the set and get static and did not use a member variable to match the same pattern in Client for setPingInterval and getPingInterval functions.", "Added the default to the core-default.xml", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12563763/HADOOP-9106v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-common-project/hadoop-common:", "org.apache.hadoop.ha.TestZKFailoverController", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2013//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2013//console", "This message is automatically generated.", "Robert, can you please address my previous comment, either with code change or why that change is not necessary.", "bq.", "Please save the connection timeout in a final member varilable in Client instead of getting it from the configuration every time.", "Other comments:", "# final modifier for static methods seems unnecessary.", "I know there are arguments for using it.", "But not sure if it is required in this case.", "# Change \"final public static\" to  \"public static final\"", "BTW changing \"IPC_CLIENT_CONNECTION_TIMEOUT_KEY\" t0 \"IPC_CLIENT_CONNECT_TIMEOUT_KEY\" is a good change.", "I should have spotted it.", "Suresh, using a class member variable would require it to be set only once in the constructor unless we make the setter function non-static.", "The passing of the conf in the setter makes the use of static desirable but the setting of a member variable makes it undesirable, I chose the static option since the connections are cached and so there is not a lot of time spent trying to establish the connection.", "We could set the member variable in the constructor but that dilutes the meaning of setTimeoutConnection (even if it may not actual use case to set it more than once).", "Bottom line I thought this way was the cleanest implementation.", "If you still think the member variable is required please let me know if you want to set it once in the constructor or remove the static qualifier from the function.", "Thanks for reviewing the patch,", "Rob", "Suresh with resect to", "| Change \"final public static\" to \"public static final\"", "I would like to leave \"final public static\" for consistency and file a separate ticket to change all the uses of \"final public static\" to \"public static final\".", "bq.", "I would like to leave \"final public static\" for consistency and file a separate ticket to change all the uses of \"final public static\" to \"public static final\".", "The code has mix of both public static final and other non standard variants.", "So the new code could just use the right convention.", "But I will leave it up to you.", "+1 for making the code consistent in a separate jira.", "bq.", "We could set the member variable in the constructor but that dilutes the meaning of setTimeoutConnection (even if it may not actual use case to set it more than once).", "The setConnectionTimeout() is setting a parameter in Configuration object and has nothing to with {{Client}} class, right?", "So, I fail to understand the above point.", "The way I see it, {{Client}} gets {{Configuration}} in the constructor.", "That is the only point in time, the connection timeout for a client is decided.", "This is formalized also with member variable {{conf}} declared as final.", "Given that I do not understand why the timeout cannot be set in a final member variable of Client, to clearly show that it is only set once during creation/construction time.", "Suresh,", "Thanks for your input.", "This patch uses the correct convention and has the final member variable.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12565027/HADOOP-9106v4.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-common-project/hadoop-common.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2051//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2051//console", "This message is automatically generated.", "I committed the patch to branch-2 and trunk.", "Thanks you Robert!", "Integrated in Hadoop-trunk-Commit #3246 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3246/])", "HADOOP-9106.", "Allow configuration of IPC connect timeout.", "Contributed by Rober Parker.", "(Revision 1433747)", "Result = SUCCESS", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433747", "Files :", "hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java"], "SplitGT": [" set the connection timeout value in configuration"], "issueString": "Allow configuration of IPC connect timeout\nCurrently the connection timeout in Client.setupConnection() is hard coded to 20seconds. This is unreasonable in some scenarios, such as HA failover, if we want a faster failover time. We should allow this to be configured per-client.\nMade the timeout parameter configurable and set-able. \n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12563296/HADOOP-9106v1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.ipc.TestIPC\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1944//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1944//console\n\nThis message is automatically generated.\nNeed to correct the unit test\nQuick comments:\n# Client.java - please make the newly added method setConnectionTimeout into static\n# The getConnectionTimeout() method is not needed. Please save the connection timeout in a final member varilable in {{Client}} instead of getting it from the configuration every time.\n\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12563371/HADOOP-9106v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1945//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1945//console\n\nThis message is automatically generated.\nSuresh,\nI think making setConnectionTimeout static will not allow configuring 'per-client' which is desired.\nConcur with replacing the getConnectionTimeout.\nMaybe we can change the default 20s to a smaller value as well,  i think 20s is too large, let's tweak it to 1s or some other values ?\nbq.  think making setConnectionTimeout static will not allow configuring 'per-client' which is desired\nWhen a Client is created conf is passed to it in constructor. Given that the connection timeout in this patch is reading the timeout off of this conf, the timeout should be set in the conf object before it is used in the Client constructor and it can be done using a static method.\n \nbq. Maybe we can change the default 20s to a smaller value as well, \nI think 20s should be fine. With this configurability you can change it. Lets do it in another jira.\nMade the set and get static and did not use a member variable to match the same pattern in Client for setPingInterval and getPingInterval functions.  Added the default to the core-default.xml\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12563763/HADOOP-9106v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.ha.TestZKFailoverController\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2013//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2013//console\n\nThis message is automatically generated.\nRobert, can you please address my previous comment, either with code change or why that change is not necessary.\nbq. Please save the connection timeout in a final member varilable in Client instead of getting it from the configuration every time.\n\nOther comments:\n# final modifier for static methods seems unnecessary. I know there are arguments for using it. But not sure if it is required in this case.\n# Change \"final public static\" to  \"public static final\"\n\nBTW changing \"IPC_CLIENT_CONNECTION_TIMEOUT_KEY\" t0 \"IPC_CLIENT_CONNECT_TIMEOUT_KEY\" is a good change. I should have spotted it.\nSuresh, using a class member variable would require it to be set only once in the constructor unless we make the setter function non-static. The passing of the conf in the setter makes the use of static desirable but the setting of a member variable makes it undesirable, I chose the static option since the connections are cached and so there is not a lot of time spent trying to establish the connection.  We could set the member variable in the constructor but that dilutes the meaning of setTimeoutConnection (even if it may not actual use case to set it more than once). Bottom line I thought this way was the cleanest implementation.\nIf you still think the member variable is required please let me know if you want to set it once in the constructor or remove the static qualifier from the function.\nThanks for reviewing the patch,\nRob  \nSuresh with resect to \n| Change \"final public static\" to \"public static final\"\n\nI would like to leave \"final public static\" for consistency and file a separate ticket to change all the uses of \"final public static\" to \"public static final\".\nbq. I would like to leave \"final public static\" for consistency and file a separate ticket to change all the uses of \"final public static\" to \"public static final\".\nThe code has mix of both public static final and other non standard variants. So the new code could just use the right convention. But I will leave it up to you. +1 for making the code consistent in a separate jira.\n\nbq. We could set the member variable in the constructor but that dilutes the meaning of setTimeoutConnection (even if it may not actual use case to set it more than once).\nThe setConnectionTimeout() is setting a parameter in Configuration object and has nothing to with {{Client}} class, right? So, I fail to understand the above point.\n\nThe way I see it, {{Client}} gets {{Configuration}} in the constructor. That is the only point in time, the connection timeout for a client is decided. This is formalized also with member variable {{conf}} declared as final. Given that I do not understand why the timeout cannot be set in a final member variable of Client, to clearly show that it is only set once during creation/construction time.\nSuresh,\nThanks for your input.  This patch uses the correct convention and has the final member variable.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12565027/HADOOP-9106v4.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2051//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2051//console\n\nThis message is automatically generated.\nI committed the patch to branch-2 and trunk.\n\nThanks you Robert!\nIntegrated in Hadoop-trunk-Commit #3246 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3246/])\n    HADOOP-9106. Allow configuration of IPC connect timeout. Contributed by Rober Parker. (Revision 1433747)\n\n     Result = SUCCESS\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433747\nFiles : \n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java\n\n", "issueSearchSentences": ["Given that the connection timeout in this patch is reading the timeout off of this conf, the timeout should be set in the conf object before it is used in the Client constructor and it can be done using a static method.", "I think making setConnectionTimeout static will not allow configuring 'per-client' which is desired.", "Allow configuration of IPC connect timeout", "think making setConnectionTimeout static will not allow configuring 'per-client' which is desired", "Allow configuration of IPC connect timeout."], "issueSearchScores": [0.6993566751480103, 0.6694532632827759, 0.6647480726242065, 0.6614855527877808, 0.6572319269180298]}
{"aId": 85, "code": "boolean checkTotalMemoryUsed() {\n    int totalSize = 0;\n    for (IndexInformation info : cache.values()) {\n      totalSize += info.getSize();\n    }\n    return totalSize == totalMemoryUsed.get();\n  }", "comment": " It is only used for unit test.", "issueId": "MAPREDUCE-2541", "issueStringList": ["Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception", "The race condition goes like this:", "Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())", "Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());", "When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).", "When this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.", "A quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.", "patch to 0.21 branch.", "Make IndexCache.removeMap() do nothing", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12480750/MAPREDUCE-2541.patch", "against trunk revision 1128394.", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "org.apache.hadoop.tools.TestHadoopArchives", "org.apache.hadoop.tools.TestHarFileSystem", "1 contrib tests.", "The patch failed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//testReport/", "Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//console", "This message is automatically generated.", "It seems that there is something wrong with current trunk, recent PreCommit builds from #303~#320 all failed.", "https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/", "Changes:", "1. removeMap()removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed", "2. add checkTotalMemoryUsed() in IndexCache to check consistency, this is only used in unit test.", "3. add a unit test to construct the race condition, the test failed against current trunk code, and patched version passed the case on my computer.", "The failed test(TestMRCLI) posted by HadoopQA was not caused by this patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12481461/MAPREDUCE-2541.v2.patch", "against trunk revision 1131265.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these core unit tests:", "org.apache.hadoop.cli.TestMRCLI", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//testReport/", "Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//console", "This message is automatically generated.", "I just committed this.", "Thanks Binglin!"], "SplitGT": [" It is only used for unit test."], "issueString": "Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception\nThe race condition goes like this:\nThread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())\nThread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());\nWhen SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).\nWhen this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.\n\nA quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.\n\npatch to 0.21 branch. Make IndexCache.removeMap() do nothing\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12480750/MAPREDUCE-2541.patch\n  against trunk revision 1128394.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n                  org.apache.hadoop.tools.TestHadoopArchives\n                  org.apache.hadoop.tools.TestHarFileSystem\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/319//console\n\nThis message is automatically generated.\nIt seems that there is something wrong with current trunk, recent PreCommit builds from #303~#320 all failed.\nhttps://builds.apache.org/job/PreCommit-MAPREDUCE-Build/\nChanges:\n1. removeMap()removes the map from the cache if index information for this map is loaded(size>0), index information entry in cache will not be removed if it is in the loading phrase(size=0), this prevents corruption of totalMemoryUsed\n2. add checkTotalMemoryUsed() in IndexCache to check consistency, this is only used in unit test.\n3. add a unit test to construct the race condition, the test failed against current trunk code, and patched version passed the case on my computer.\n\nThe failed test(TestMRCLI) posted by HadoopQA was not caused by this patch. \n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12481461/MAPREDUCE-2541.v2.patch\n  against trunk revision 1131265.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestMRCLI\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/349//console\n\nThis message is automatically generated.\nI just committed this. Thanks Binglin!\n", "issueSearchSentences": ["2. add checkTotalMemoryUsed() in IndexCache to check consistency, this is only used in unit test.", "When this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.", "Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());", "Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())", "When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger)."], "issueSearchScores": [0.8032589554786682, 0.7258867621421814, 0.6220012903213501, 0.6030486226081848, 0.6020894050598145]}
{"aId": 86, "code": "@Override\n  protected int collectSubtreeBlocksAndClear(List<Block> v) {\n    if (next == this) {\n      //this is the only remaining inode.\n      super.collectSubtreeBlocksAndClear(v);\n    } else {\n      //There are other inode(s) using the blocks.\n      //Compute max file size excluding this and find the last inode. \n      long max = next.computeFileSize(true);\n      INodeFileWithLink last = next;\n      for(INodeFileWithLink i = next.getNext(); i != this; i = i.getNext()) {\n        final long size = i.computeFileSize(true);\n        if (size > max) {\n          max = size;\n        }\n        last = i;\n      }\n\n      collectBlocksBeyondMaxAndClear(max, v);\n      \n      //remove this from the circular linked list.\n      last.next = this.next;\n      this.next = null;\n      //clear parent\n      parent = null;\n    }\n    return 1;\n  }", "comment": " Remove the current inode from the circular linked list. If some blocks at the end of the block list no longer belongs to any other inode, collect them and update the block list.", "issueId": "HDFS-4092", "issueStringList": ["Update file deletion logic to support snapshot files", "When snapshots are created, the blocks are shared among the snapshot files and the original file.", "Deletion of the snapshot file or the original file does not necessarily lead to deletion of the blocks.", "The deletion logic needs to be updated.", "h4092_20121023.patch:", "If there are more than one inodes in teh circular linked list,", "Remove the current inode from the circular linked list; and", "if some blocks at the end of the block list no longer belongs to any other inode, collect them and update the block list.", "+1 for the patch.", "This also has some dependency on HDFS-4107 right and that needs to be committed first?", "HDFS-4107 is now committed.", "h4092_20121025.patch: synced with the branch.", "h4092_20121025b.patch: updates comments", "I have committed this."], "SplitGT": [" Remove the current inode from the circular linked list.", "If some blocks at the end of the block list no longer belongs to any other inode, collect them and update the block list."], "issueString": "Update file deletion logic to support snapshot files\nWhen snapshots are created, the blocks are shared among the snapshot files and the original file.  Deletion of the snapshot file or the original file does not necessarily lead to deletion of the blocks.  The deletion logic needs to be updated.\nh4092_20121023.patch:\n\nIf there are more than one inodes in teh circular linked list,\n   * Remove the current inode from the circular linked list; and\n   * if some blocks at the end of the block list no longer belongs to any other inode, collect them and update the block list.\n\n+1 for the patch. This also has some dependency on HDFS-4107 right and that needs to be committed first?\nHDFS-4107 is now committed.\n\nh4092_20121025.patch: synced with the branch.\nh4092_20121025b.patch: updates comments\nI have committed this.\n", "issueSearchSentences": ["if some blocks at the end of the block list no longer belongs to any other inode, collect them and update the block list.", "When snapshots are created, the blocks are shared among the snapshot files and the original file.", "Deletion of the snapshot file or the original file does not necessarily lead to deletion of the blocks.", "Remove the current inode from the circular linked list; and", "h4092_20121025.patch: synced with the branch."], "issueSearchScores": [0.6383520364761353, 0.519568681716919, 0.5151879787445068, 0.5086320042610168, 0.3565693497657776]}
{"aId": 89, "code": "public FsServerDefaults getServerDefaults() throws IOException {\n    Configuration conf = getConf();\n    return new FsServerDefaults(getDefaultBlockSize(), \n        conf.getInt(\"io.bytes.per.checksum\", 512), \n        64 * 1024, \n        getDefaultReplication(), \n        conf.getInt(\"io.file.buffer.size\", 4096));\n  }", "comment": " Return a set of server default configuration values", "issueId": "HADOOP-6235", "issueStringList": ["Adding a new method for getting server default values from a FileSystem", "This is the changes made to Common to support file creation using server default values for a number of configuration params.", "See HDFS-578 for details.", "attaching a patch that adds a new method getServerDefaults() to FileSystem.", "Any filesystem that supports this method needs to overwrite the default implementation.", "Test for this new method is included in HDFS-578.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12418458/6235-02.patch", "against trunk revision 810756.", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/console", "This message is automatically generated.", "+1", "Integrated in Hadoop-Common-trunk-Commit #21 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/21/])", ".", "Adds new method to FileSystem for clients to get server defaults.", "Contributed by Kan Zhang."], "SplitGT": [" Return a set of server default configuration values"], "issueString": "Adding a new method for getting server default values from a FileSystem\nThis is the changes made to Common to support file creation using server default values for a number of configuration params. See HDFS-578 for details.\nattaching a patch that adds a new method getServerDefaults() to FileSystem. Any filesystem that supports this method needs to overwrite the default implementation.\nTest for this new method is included in HDFS-578.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12418458/6235-02.patch\n  against trunk revision 810756.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/12/console\n\nThis message is automatically generated.\n+1\nIntegrated in Hadoop-Common-trunk-Commit #21 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/21/])\n    . Adds new method to FileSystem for clients to get server defaults. Contributed by Kan Zhang.\n\n", "issueSearchSentences": ["attaching a patch that adds a new method getServerDefaults() to FileSystem.", "This is the changes made to Common to support file creation using server default values for a number of configuration params.", "Adds new method to FileSystem for clients to get server defaults.", "Adding a new method for getting server default values from a FileSystem", "Test for this new method is included in HDFS-578."], "issueSearchScores": [0.6093164682388306, 0.5522101521492004, 0.5339809656143188, 0.5132846832275391, 0.4134216904640198]}
{"aId": 90, "code": "public void disallowSnapshot(String snapshotRoot)\n      throws IOException {\n    dfs.disallowSnapshot(snapshotRoot);\n  }", "comment": " Disallow snapshot on a directory.", "issueId": "HDFS-4084", "issueStringList": ["provide CLI support for allow and disallow snapshot on a directory", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "@Brandon", "Can we make the new commands case insensitive?", "We can log a different jira to make existing commands also case insensitive.", "Good point, Arpit.", "Will change it and upload a new patch.", "Re-based the patch and addressed Arpit's comment.", "Comments:", "# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods", "{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "{quote}", "Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "Added javadoc for them.", "{quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "{quote}", "Removed, realized these two methods need to be public anyway.", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote}", "Yes.", "removed.", "{quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote}", "Done.", "Thanks.", "I committed the patch to HDFS-2802 branch.", "Thank you Brandon."], "SplitGT": [" Disallow snapshot on a directory."], "issueString": "provide CLI support for allow and disallow snapshot on a directory\nTo provide CLI support to allow snapshot, disallow snapshot on a directory.\n@Brandon\n\nCan we make the new commands case insensitive? We can log a different jira to make existing commands also case insensitive. \nGood point, Arpit. Will change it and upload a new patch. \nRe-based the patch and addressed Arpit's comment.\nComments:\n# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.\n# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.\n# FSNamesystem.java is unnecessarily importing SnapshotInfo?\n# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods\n\n{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.{quote} \nMethods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override. Added javadoc for them.\n   {quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.{quote} \nRemoved, realized these two methods need to be public anyway.\n    {quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?{quote} \nYes. removed.\n    {quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote} \nDone. Thanks.\n\nI committed the patch to HDFS-2802 branch. Thank you Brandon.\n", "issueSearchSentences": ["Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "provide CLI support for allow and disallow snapshot on a directory", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "So javadoc for DistributedFileSystem methods you have added can be deleted."], "issueSearchScores": [0.6238968372344971, 0.5988458395004272, 0.4934333860874176, 0.46000412106513977, 0.46000412106513977]}
{"aId": 94, "code": "synchronized void removeVolumes(Collection<StorageLocation> locations) {\n    if (locations.isEmpty()) {\n      return;\n    }\n\n    Set<File> dataDirs = new HashSet<File>();\n    for (StorageLocation sl : locations) {\n      dataDirs.add(sl.getFile());\n    }\n\n    for (BlockPoolSliceStorage bpsStorage : this.bpStorageMap.values()) {\n      bpsStorage.removeVolumes(dataDirs);\n    }\n\n    for (Iterator<StorageDirectory> it = this.storageDirs.iterator();\n         it.hasNext(); ) {\n      StorageDirectory sd = it.next();\n      if (dataDirs.contains(sd.getRoot())) {\n        it.remove();\n      }\n    }\n  }", "comment": " Remove volumes from DataStorage.", "issueId": "HDFS-6774", "issueStringList": ["Make FsDataset and DataStore support removing volumes.", "Managing volumes on DataNode includes decommissioning an active volume without restarting DataNode.", "This task adds support to remove volumes from {{DataStorage}} and {{BlockPoolSliceStorage}} dynamically.", "This patch enables {{FsDataset}} and {{DataStorage}} to remove data volumes dynamically.", "The {{replicaInfos}} that are on the deleted volume will also be removed from {{FsDataset#volumeMap}}.", "The race condition that removing a volume that is being written is not addressed in this patch.", "I will open a new JIRA for that case and potential other race conditions.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12660509/HDFS-6774.000.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover", "org.apache.hadoop.hdfs.web.TestWebHDFS", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7588//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7588//console", "This message is automatically generated.", "The failures are not related.", "Refactor the patch and fix inconsistent comments.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12661102/HDFS-6774.001.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7612//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7612//console", "This message is automatically generated.", "Hey Eddy, patch looks pretty good to me.", "A few questions:", "# The change in {{BlockPoolSlice}} - was that just a separate bug?", "Or why was that necessary?", "# I see the code where we remove the replica info from the replica map, but do we not also need to do something similar in the event that the replica is currently referenced in the BlockScanner or DirectoryScanner data structures?", "It could be that we don't, but I wanted to check with you to see if you've considered this case.", "Hi, [~atm] Thanks for the review.", "bq.", "The change in BlockPoolSlice - was that just a separate bug?", "Or why was that necessary?", "It is indeed a separate bug.", "Should I open a new JIRA for this?", "bq.", "I see the code where we remove the replica info from the replica map, but do we not also need to do something similar in the event that the replica is currently referenced in the BlockScanner or DirectoryScanner data structures?", "It could be that we don't, but I wanted to check with you to see if you've considered this case.", "Thanks for finding this bug.", "{{BlockScanner}} needs to remove the blocks and the corresponding patch is attached.", "I think that {{DirectoryScanner}} does not need to change, since it uses {{FsDatasetImpl}} as input source for each scan and performs {{DirectoryScanner#isValid()}}, which verifies whether the volume is still available, when generate disk reports.", "This logic is the same as handling disk failures.", "Therefore, {{DirectoryScanner}}'s consistency can be provided by {{FsDatasetImpl}}.", "What is your opinion about this?", "bq.", "It is indeed a separate bug.", "Should I open a new JIRA for this?", "Yes, I'd recommend creating a separate JIRA for this bug.", "bq.", "I think that DirectoryScanner does not need to change <snip>", "Yes, I think you're right.", "Thanks for looking into this.", "I just kicked Jenkins to get a test-patch run.", "Not sure why that didn't run when you initially posted the patch.", "I'll be +1 pending Jenkins and pending breaking out that other bug fix mentioned above.", "Thanks a lot, Eddy.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12662354/HDFS-6774.002.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "There were no new javadoc warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.security.TestRefreshUserMappings", "org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover", "org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7796//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7796//console", "This message is automatically generated.", "[~atm], {{TestRefreshUserMappings}} is unrelated (see HADOOP-11020).", "Other two have failed in other patches, so I think they are not related as well.", "Great, thanks for looking into that.", "+1, the latest patch looks good to me.", "I'm going to commit this momentarily.", "I've just committed this to trunk and branch-2.", "Thanks a lot for the contribution, Eddy."], "SplitGT": [" Remove volumes from DataStorage."], "issueString": "Make FsDataset and DataStore support removing volumes.\nManaging volumes on DataNode includes decommissioning an active volume without restarting DataNode. \n\nThis task adds support to remove volumes from {{DataStorage}} and {{BlockPoolSliceStorage}} dynamically.\nThis patch enables {{FsDataset}} and {{DataStorage}} to remove data volumes dynamically. The {{replicaInfos}} that are on the deleted volume will also be removed from {{FsDataset#volumeMap}}. \n\nThe race condition that removing a volume that is being written is not addressed in this patch. I will open a new JIRA for that case and potential other race conditions. \n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12660509/HDFS-6774.000.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\n                  org.apache.hadoop.hdfs.web.TestWebHDFS\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/7588//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/7588//console\n\nThis message is automatically generated.\nThe failures are not related. \nRefactor the patch and fix inconsistent comments. \n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12661102/HDFS-6774.001.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/7612//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/7612//console\n\nThis message is automatically generated.\nHey Eddy, patch looks pretty good to me. A few questions:\n\n# The change in {{BlockPoolSlice}} - was that just a separate bug? Or why was that necessary?\n# I see the code where we remove the replica info from the replica map, but do we not also need to do something similar in the event that the replica is currently referenced in the BlockScanner or DirectoryScanner data structures? It could be that we don't, but I wanted to check with you to see if you've considered this case.\nHi, [~atm] Thanks for the review. \n\nbq. The change in BlockPoolSlice - was that just a separate bug? Or why was that necessary?\n\nIt is indeed a separate bug. Should I open a new JIRA for this?\n\nbq. I see the code where we remove the replica info from the replica map, but do we not also need to do something similar in the event that the replica is currently referenced in the BlockScanner or DirectoryScanner data structures? It could be that we don't, but I wanted to check with you to see if you've considered this case.\n\nThanks for finding this bug. {{BlockScanner}} needs to remove the blocks and the corresponding patch is attached. \n\nI think that {{DirectoryScanner}} does not need to change, since it uses {{FsDatasetImpl}} as input source for each scan and performs {{DirectoryScanner#isValid()}}, which verifies whether the volume is still available, when generate disk reports. This logic is the same as handling disk failures. Therefore, {{DirectoryScanner}}'s consistency can be provided by {{FsDatasetImpl}}. What is your opinion about this?\n\n\nbq. It is indeed a separate bug. Should I open a new JIRA for this?\n\nYes, I'd recommend creating a separate JIRA for this bug.\n\nbq. I think that DirectoryScanner does not need to change <snip>\n\nYes, I think you're right. Thanks for looking into this.\n\nI just kicked Jenkins to get a test-patch run. Not sure why that didn't run when you initially posted the patch. I'll be +1 pending Jenkins and pending breaking out that other bug fix mentioned above.\n\nThanks a lot, Eddy.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12662354/HDFS-6774.002.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.security.TestRefreshUserMappings\n                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\n                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/7796//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/7796//console\n\nThis message is automatically generated.\n[~atm], {{TestRefreshUserMappings}} is unrelated (see HADOOP-11020). Other two have failed in other patches, so I think they are not related as well.\nGreat, thanks for looking into that.\n\n+1, the latest patch looks good to me. I'm going to commit this momentarily.\nI've just committed this to trunk and branch-2.\n\nThanks a lot for the contribution, Eddy.\n", "issueSearchSentences": ["This task adds support to remove volumes from {{DataStorage}} and {{BlockPoolSliceStorage}} dynamically.", "This patch enables {{FsDataset}} and {{DataStorage}} to remove data volumes dynamically.", "The {{replicaInfos}} that are on the deleted volume will also be removed from {{FsDataset#volumeMap}}.", "Managing volumes on DataNode includes decommissioning an active volume without restarting DataNode.", "Make FsDataset and DataStore support removing volumes."], "issueSearchScores": [0.6481555700302124, 0.532396137714386, 0.5133055448532104, 0.43483975529670715, 0.4287383556365967]}
{"aId": 96, "code": "public void processReport(final DatanodeID nodeID,\n      final DatanodeStorage storage, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    namesystem.writeLock();\n    final long startTime = Time.now(); //after acquiring write lock\n    final long endTime;\n    try {\n      final DatanodeDescriptor node = datanodeManager.getDatanode(nodeID);\n      if (node == null || !node.isAlive) {\n        throw new IOException(\n            \"ProcessReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // To minimize startup time, we discard any second (or later) block reports\n      // that we receive while still in startup phase.\n      if (namesystem.isInStartupSafeMode() && !node.isFirstBlockReport()) {\n        blockLog.info(\"BLOCK* processReport: \"\n            + \"discarded non-initial block report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return;\n      }\n\n      if (node.numBlocks() == 0) {\n        // The first block report can be processed a lot more efficiently than\n        // ordinary block reports.  This shortens restart times.\n        processFirstBlockReport(node, storage.getStorageID(), newReport);\n      } else {\n        processReport(node, storage, newReport);\n      }\n      \n      // Now that we have an up-to-date block report, we know that any\n      // deletions from a previous NN iteration have been accounted for.\n      boolean staleBefore = node.areBlockContentsStale();\n      node.receivedBlockReport();\n      if (staleBefore && !node.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processReport: Received first block report from \"\n            + node + \" after starting up or becoming active. Its block \"\n            + \"contents are no longer considered stale\");\n        rescanPostponedMisreplicatedBlocks();\n      }\n      \n    } finally {\n      endTime = Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics = NameNode.getNameNodeMetrics();\n    if (metrics != null) {\n      metrics.addBlockReport((int) (endTime - startTime));\n    }\n    blockLog.info(\"BLOCK* processReport: from \"\n        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }", "comment": " The given storage is reporting all its blocks.", "issueId": "HDFS-4987", "issueStringList": ["Namenode changes to track multiple storages", "Currently namenode track in BlockInfo, the corresponding DataNodeDescriptor.", "I propose changing this with new abstraction StorageDescriptor.", "This change will also make DatanodeDescriptor a collection of StroageDescriptors.", "This will allow given a block, identify its storage and also Datanode.", "h4987_20130822.patch: first patch.", "Hi Nicholas,", "Couple of comments.", "# {{blockContentsStale}} should be per storage since DataNode may want to send separate block reports per storage.", "# {{DatanodeStorageInfo}} should have the Storage type.", "Thanks.", "Arpit, thanks for the comments.", "For #1, I tried to move blockContentsStale to be per storage but it is quite involved.", "How about we move it when we change block report to be per storage?", "For #2, I added storageType to DatanodeStorageInfo.", "I also found another problem that LocatedBlock was used in reportBadBlocks(..) but it did not have storage ID.", "I think we need to add storage ID to LocatedBlock.", "I will file a JIRA.", "h4987_20130827.patch:", "adds storageType to DatanodeStorageInfo;", "adds storageID to ReportedBlockInfo;", "updates storage when processing block reports.", "> ...", "I will file a JIRA.", "Since we already have HDFS-5009, I am not going to file a JIRA but assign HDFS-5009 to myself.", "Nicholas, nice code refactoring work on reportDiff(...).", "Previous delimiter seems to be non-necessary.", "My only question is if we remove TestBlockInfo.java which is only unit test and consumer of moveBlockToHead() in DataNodeDescriptor/BlockInfo, shall we consider to remove this method rather than leaving there without using and testing?", "One additional comment is we can do iteration blocks on DN for specific StorageID (we may add API for it later).", "Now, for each StorageBlockReport, it will iterate the whole block list which seems not the most efficient way.", "Good catch Junping, I will remove moveBlockToHead(..) from BlockInfo.", "For your second comment, I actually don't understand.", "We already have individual StorageBlockReports in the NN-side.", "Do you mean the DN side?", "I have not checked it yet.", "h4987_20130827b.patch: removes moveBlockToHead(..) from BlockInfo.", "bq.", "For your second comment, I actually don't understand.", "We already have individual StorageBlockReports in the NN-side.", "Do you mean the DN side?", "I have not checked it yet.", "Yes.", "I mean in your reportDiff(), it firstly add all blocks of DN to remove list.", "In future, adding blocks belongs to specific storageID may make more sense in this case?", "This is really a good point.", "I should iterate only the blocks in the particular storage.", "h4987_20130827c.patch: fixes reportDiff(..).", "Hi Nicholas,", "Couple of comment related nitpicks we can fix later.", "# BlockInfo.java:44 - Comment should be updated to say {{\"block belongs to triplets[3*i] is the reference to the StorageInfo\"}}", "# BlockInfo#findStorageInfo - Javadoc needs to be updated to {{\"   * @param dn\"}}", "I hit submit too soon.", "{quote}", "For #1, I tried to move blockContentsStale to be per storage but it is quite involved.", "How about we move it when we change block report to be per storage?", "{quote}", "This sounds good.", "I filed HDFS-5134 to track this and couple of other items that can be done per-storage now.", "Javadoc nitpicks but I am +1 on the patch basically.", "Please feel free to commit this today.", "# {{BlockManager#processReport}} - Javadoc should be updated e.g.", "{{\"   * The given datanode is reporting all its blocks on the given Storage.}}", "# BlockInfo.java:44 - Comment should be updated to say {{\"block belongs to triplets[3*i] is the reference to the StorageInfo\"}}", "# {{BlockInfo#findStorageInfo}} - Parameter should be \" {{* @param dn\"}}", "# {{BlockManager#addStoredBlockImmediate}} - Link in header comment needs to be updated.", "Thanks!", "Arpit", "h4987_20130828.patch: updates javadoc.", "Thanks Nicholas!", "+1 for the updated patch.", "Thanks for addressing comments in new patch.", "Patch looks good to me.", "+1", "Thanks Arpit and Junping for reviewing the patches.", "I have committed this.", "Note that some unit tests in the branch may fail.", "We will fix them when feature is getting ready."], "SplitGT": [" The given storage is reporting all its blocks."], "issueString": "Namenode changes to track multiple storages\nCurrently namenode track in BlockInfo, the corresponding DataNodeDescriptor. I propose changing this with new abstraction StorageDescriptor. This change will also make DatanodeDescriptor a collection of StroageDescriptors. This will allow given a block, identify its storage and also Datanode.\nh4987_20130822.patch: first patch.\nHi Nicholas,\n\nCouple of comments.\n\n# {{blockContentsStale}} should be per storage since DataNode may want to send separate block reports per storage.\n# {{DatanodeStorageInfo}} should have the Storage type.\n\nThanks.\nArpit, thanks for the comments.\n\nFor #1, I tried to move blockContentsStale to be per storage but it is quite involved.  How about we move it when we change block report to be per storage?\n\nFor #2, I added storageType to DatanodeStorageInfo.\n\nI also found another problem that LocatedBlock was used in reportBadBlocks(..) but it did not have storage ID.  I think we need to add storage ID to LocatedBlock.  I will file a JIRA.\nh4987_20130827.patch:\n- adds storageType to DatanodeStorageInfo;\n- adds storageID to ReportedBlockInfo;\n- updates storage when processing block reports.\n> ... I will file a JIRA.\n\nSince we already have HDFS-5009, I am not going to file a JIRA but assign HDFS-5009 to myself.\nNicholas, nice code refactoring work on reportDiff(...). Previous delimiter seems to be non-necessary. My only question is if we remove TestBlockInfo.java which is only unit test and consumer of moveBlockToHead() in DataNodeDescriptor/BlockInfo, shall we consider to remove this method rather than leaving there without using and testing?\nOne additional comment is we can do iteration blocks on DN for specific StorageID (we may add API for it later). Now, for each StorageBlockReport, it will iterate the whole block list which seems not the most efficient way.\nGood catch Junping, I will remove moveBlockToHead(..) from BlockInfo.\n\nFor your second comment, I actually don't understand.  We already have individual StorageBlockReports in the NN-side.  Do you mean the DN side?  I have not checked it yet. \nh4987_20130827b.patch: removes moveBlockToHead(..) from BlockInfo.\nbq. For your second comment, I actually don't understand. We already have individual StorageBlockReports in the NN-side. Do you mean the DN side? I have not checked it yet.\nYes. I mean in your reportDiff(), it firstly add all blocks of DN to remove list. In future, adding blocks belongs to specific storageID may make more sense in this case?\n\nThis is really a good point.  I should iterate only the blocks in the particular storage.\n\nh4987_20130827c.patch: fixes reportDiff(..).\nHi Nicholas,\n\nCouple of comment related nitpicks we can fix later.\n\n# BlockInfo.java:44 - Comment should be updated to say {{\"block belongs to triplets[3*i] is the reference to the StorageInfo\"}}\n# BlockInfo#findStorageInfo - Javadoc needs to be updated to {{\"   * @param dn\"}}\n\nI hit submit too soon.\n\n{quote}\nFor #1, I tried to move blockContentsStale to be per storage but it is quite involved. How about we move it when we change block report to be per storage?\n{quote}\nThis sounds good. I filed HDFS-5134 to track this and couple of other items that can be done per-storage now.\n\nJavadoc nitpicks but I am +1 on the patch basically. Please feel free to commit this today. \n# {{BlockManager#processReport}} - Javadoc should be updated e.g. {{\"   * The given datanode is reporting all its blocks on the given Storage.}}\n# BlockInfo.java:44 - Comment should be updated to say {{\"block belongs to triplets[3*i] is the reference to the StorageInfo\"}}\n# {{BlockInfo#findStorageInfo}} - Parameter should be \" {{* @param dn\"}}\n# {{BlockManager#addStoredBlockImmediate}} - Link in header comment needs to be updated.\n\nThanks!\nArpit\n\n\nh4987_20130828.patch: updates javadoc.\nThanks Nicholas! +1 for the updated patch.\nThanks for addressing comments in new patch. Patch looks good to me. +1\nThanks Arpit and Junping for reviewing the patches.\n\nI have committed this.  Note that some unit tests in the branch may fail.  We will fix them when feature is getting ready.\n", "issueSearchSentences": ["Now, for each StorageBlockReport, it will iterate the whole block list which seems not the most efficient way.", "# {{BlockManager#processReport}} - Javadoc should be updated e.g.", "{{\"   * The given datanode is reporting all its blocks on the given Storage.}}", "How about we move it when we change block report to be per storage?", "How about we move it when we change block report to be per storage?"], "issueSearchScores": [0.6674677133560181, 0.6559553742408752, 0.6467255353927612, 0.6399010419845581, 0.6399010419845581]}
{"aId": 98, "code": "public static boolean fullyDeleteContents(File dir) throws IOException {\n    File contents[] = dir.listFiles();\n    if (contents != null) {\n      for (int i = 0; i < contents.length; i++) {\n        if (contents[i].isFile()) {\n          if (!contents[i].delete()) {\n            return false;\n          }\n        } else {\n          //try deleting the directory\n          // this might be a symlink\n          boolean b = false;\n          b = contents[i].delete();\n          if (b){\n            //this was indeed a symlink or an empty directory\n            continue;\n          }\n          // if not an empty directory or symlink let\n          // fullydelete handle it.\n          if (!fullyDelete(contents[i])) {\n            return false;\n          }\n        }\n      }\n    }\n    return true;\n  }", "comment": " Delete the contents of a directory, not the directory itself.", "issueId": "HADOOP-6531", "issueStringList": ["add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory", "Add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory, not directory itself.", "This will be useful if we want to clear the contents of cwd.", "Patch adding the api.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12434506/patch-6531.txt", "against trunk revision 904975.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/console", "This message is automatically generated.", "Changed the unit test to use Junit v4 style", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12434508/patch-6531-1.txt", "against trunk revision 904975.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/console", "This message is automatically generated.", "We are not checking the return value of fullyDeleteContents() in fullyDelete() method.", "But if fullyDeleteContents() returns false, next line \"dir.delete()\" will be executed extra for all the directories in the heirarchy that we traversed.", "Though it is harmless with this existing patch, it looks better to have the check here and return false if fullyDeleteContents() returns false.", "Minor nit in testcase: Last 3 assertions in both testcases in TestFileUtil.java are same and can be moved to a separate method (say validateTmpDir() ) to avoid code-duplication.", "Thanks for the quick review Ravi.", "Patch with comments incorporated.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12434518/patch-6531-2.txt", "against trunk revision 904975.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/console", "This message is automatically generated.", "I would improve the test cases just a little bit:", "Please check for return values of FileUtil.fullyDeleteContents and FileUtil.fullyDelete in test case.", "Also add a failure test case for both fullyDelete and fullyDeleteContents.", "We can create a new directory for which we turn off write permissions.", "Then fullyDelete and fullyDeleteContents should both fail and we should get a false return value that can be checked.", "Code changes seem fine to me.", "Can we also fix the javadoc of each of the fullyDelete*() methods to describe their behavior with directories, files and symlinks?", "Patch modifying the testcase as suggested.", "bq.", "Can we also fix the javadoc of each of the fullyDelete*() methods to describe their behavior with directories, files and symlinks?", "Created HADOOP-6536 for this.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12434646/patch-6531-3.txt", "against trunk revision 905860.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/console", "This message is automatically generated.", "Seems fine to me.", "+1.", "I think HADOOP-6536 will result in a fix to code, not just to documentation.", "Hence, I think it is OK to address it in a separate JIRA.", "I just committed this to trunk.", "Thanks, Amareshwari !", "Can you please update the release note with the relevant API added.", "Integrated in Hadoop-Common-trunk-Commit #156 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/156/])", ".", "Enhance FileUtil with an API to delete all contents of a directory.", "Contributed by Amareshwari Sriramadasu."], "SplitGT": [" Delete the contents of a directory, not the directory itself."], "issueString": "add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory\nAdd FileUtil.fullyDeleteContents(dir) api to delete contents of a directory, not directory itself. This will be useful if we want to clear the contents of cwd.\nPatch adding the api.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12434506/patch-6531.txt\n  against trunk revision 904975.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/312/console\n\nThis message is automatically generated.\nChanged the unit test to use Junit v4 style\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12434508/patch-6531-1.txt\n  against trunk revision 904975.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/313/console\n\nThis message is automatically generated.\nWe are not checking the return value of fullyDeleteContents() in fullyDelete() method. But if fullyDeleteContents() returns false, next line \"dir.delete()\" will be executed extra for all the directories in the heirarchy that we traversed. Though it is harmless with this existing patch, it looks better to have the check here and return false if fullyDeleteContents() returns false.\n\nMinor nit in testcase: Last 3 assertions in both testcases in TestFileUtil.java are same and can be moved to a separate method (say validateTmpDir() ) to avoid code-duplication.\nThanks for the quick review Ravi.\nPatch with comments incorporated.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12434518/patch-6531-2.txt\n  against trunk revision 904975.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/314/console\n\nThis message is automatically generated.\nI would improve the test cases just a little bit:\n\n- Please check for return values of FileUtil.fullyDeleteContents and FileUtil.fullyDelete in test case.\n- Also add a failure test case for both fullyDelete and fullyDeleteContents. We can create a new directory for which we turn off write permissions. Then fullyDelete and fullyDeleteContents should both fail and we should get a false return value that can be checked.\n\nCode changes seem fine to me.\nCan we also fix the javadoc of each of the fullyDelete*() methods to describe their behavior with directories, files and symlinks?\nPatch modifying the testcase as suggested.\n\nbq. Can we also fix the javadoc of each of the fullyDelete*() methods to describe their behavior with directories, files and symlinks?\nCreated HADOOP-6536 for this.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12434646/patch-6531-3.txt\n  against trunk revision 905860.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/322/console\n\nThis message is automatically generated.\nSeems fine to me. +1.\n\nI think HADOOP-6536 will result in a fix to code, not just to documentation. Hence, I think it is OK to address it in a separate JIRA.\n\nI just committed this to trunk. Thanks, Amareshwari !\n\nCan you please update the release note with the relevant API added.\nIntegrated in Hadoop-Common-trunk-Commit #156 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/156/])\n    . Enhance FileUtil with an API to delete all contents of a directory. Contributed by Amareshwari Sriramadasu.\n\n", "issueSearchSentences": ["Add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory, not directory itself.", "add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory", "Enhance FileUtil with an API to delete all contents of a directory.", "But if fullyDeleteContents() returns false, next line \"dir.delete()\" will be executed extra for all the directories in the heirarchy that we traversed.", "We are not checking the return value of fullyDeleteContents() in fullyDelete() method."], "issueSearchScores": [0.7516716718673706, 0.7512553334236145, 0.7159969806671143, 0.699177622795105, 0.6443773508071899]}
{"aId": 99, "code": "public static Collection<URI> getNsServiceRpcUris(Configuration conf) {\n    return getNameServiceUris(conf,\n        DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,\n        DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);\n  }", "comment": " Get a URI for each configured nameservice. If a nameservice is HA-enabled, then the logical URI of the nameservice is returned.", "issueId": "HDFS-2979", "issueStringList": ["HA: Balancer should use logical uri for creating failover proxy with HA enabled.", "Presently Balancer uses real URI for creating the failover proxy.", "Since the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI.", "Presently will work only with default port.", "java.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.", "at org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)", "at org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)", "at org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)", "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401)", "Attached the patch with addressing the issue.", "Hey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.", "Furthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.", "What do you think about the following?", "# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.", "# Remove the need to convert from inet address -> URI in NameNodeConnector(...).", "Also, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol.", "I've filed HDFS-2986 to address this issue.", "Here's a patch which addresses the issue, using the technique I described above.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME and", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "Otherwise looks good.", "Thanks a lot for the review, Eli.", "Here's an updated patch which addresses your feedback.", "In the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it.", "The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated.", "This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client.", "To fix this, I made the NN make copies of conf objects provided to it in a few places.", "This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.", "bq.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "People who want both (can't think of any who should) can add the default URI to the set.", "OK, done.", "bq.", "Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.", "I revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.", "bq.", "getNNUris should use HdfsConstants#HDFS_URI_SCHEME", "Done.", "bq.", "testGetNNUris needs HA-enabled and non-HA-enabled cases", "This is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.", "I also ran the following tests to verify this change, and they all passed:", "TestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "Otherwise looks great.", "Nice find wrt the mutating configuration issue.", "Thanks a lot, Eli.", "Here's an updated patch which adds the wrapper method as you requested.", "+1", "Thanks a lot for the reviews, Eli.", "I've just committed this to the HA branch.", "This patch seems to have made some tests fail (I noticed with TestFileAppend2).", "It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.", "Thanks Todd.", "I've filed HDFS-3033 to address these failures.", "On second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case.", "The conf mutation issue should be fixed in a more general way.", "Here's an updated patch which addresses the issue and removes the change to the NameNode.", "Instead, to avoid the issue with conf mutation, the relevant tests make a copy of the configuration objects and then set the addresses appropriately after the NNs have started, to get the correct ephemeral ports configured.", "Small nits:", "please rename {{getNameNodeInfo}} to {{getNameNodeInfos}} or {{getNameNodeInfoArray}} or something?", "in {{setFederatedConfiguration}} add assert that info.nameserviceId is not null", "Thanks a lot for the quick review, Todd.", "Here's an updated patch which addresses your feedback.", "+1 lgtm (I reviewed only the delta from the previous patch, not the full patch, but should be good to commit given prior reviews on the main body)", "Thanks a lot for the reviews, Todd.", "I've just committed the latest patch to the HA branch.", "Thanks a lot Aaron for working on this issue and moving it to closure.", "I was out station for the last week, so I could not put my efforts on this issue.", "Integrated in Hadoop-Hdfs-HAbranch-build #93 (See [https://builds.apache.org/job/Hadoop-Hdfs-HAbranch-build/93/])", "HDFS-2979.", "Balancer should use logical uri for creating failover proxy with HA enabled.", "Contributed by Aaron T. Myers.", "(Revision 1295473)", "Revert commit of HDFS-2979.", "(Revision 1295435)", "HDFS-2979.", "Balancer should use logical uri for creating failover proxy with HA enabled.", "Contributed by Aaron T. Myers.", "(Revision 1295340)", "Result = UNSTABLE", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295473", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295435", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java", "atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295340", "Files :", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java", "hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java"], "SplitGT": [" Get a URI for each configured nameservice.", "If a nameservice is HA-enabled, then the logical URI of the nameservice is returned."], "issueString": "HA: Balancer should use logical uri for creating failover proxy with HA enabled.\nPresently Balancer uses real URI for creating the failover proxy.\nSince the failover proxy checks for uri consistency, we should pass logical uri for creating failover proxy instead of instead of real URI. Presently will work only with default port.\n\njava.io.IOException: Port 49832 specified in URI hdfs://127.0.0.1:49832 but host '127.0.0.1' is a logical (HA) namenode and does not use port information.\n\tat org.apache.hadoop.hdfs.HAUtil.getFailoverProxyProviderClass(HAUtil.java:224)\n\tat org.apache.hadoop.hdfs.HAUtil.createFailoverProxy(HAUtil.java:247)\n\tat org.apache.hadoop.hdfs.server.balancer.NameNodeConnector.<init>(NameNodeConnector.java:80)\n\tat org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1401) \nAttached the patch with addressing the issue.\nHey Uma, I don't think it's reasonable to assume that if HA is enabled for a given nameservice ID, that its URI can therefore be determined by getting the default URI of the provided conf.\n\nFurthermore, I find it somewhat odd that in Balancer.Cli#run we get the InetSocketAddresses of all the NNs, only to convert those back to URIs in the NameNodeConnector constructor so we can create RPC proxy objects.\n\nWhat do you think about the following?\n\n# Instead of enumerating all service RPC addresses of all NNs, enumerate all URIs (logical or normal) of all name services.\n# Remove the need to convert from inet address -> URI in NameNodeConnector(...).\n\nAlso, while looking into this issue, I realized that the ConfiguredFailoverProxyProvider won't use the NN RPC service address if it's set, even when creating a proxy object for the NameNodeProtocol. I've filed HDFS-2986 to address this issue.\nHere's a patch which addresses the issue, using the technique I described above.\n- The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n- Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n- getNNUris should use HdfsConstants#HDFS_URI_SCHEME and\n- testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nOtherwise looks good.\nThanks a lot for the review, Eli. Here's an updated patch which addresses your feedback.\n\nIn the course of creating this patch, I also discovered a problem with the NN's mutation of conf objects passed to it. The NN calls initializeGenericKeys in a few places, which causes conf objects passed to it to be mutated. This causes a problem if a client provides a configuration object to an NN that it doesn't expect to be mutated, and basically results in a misconfigured conf object from the POV of the client. To fix this, I made the NN make copies of conf objects provided to it in a few places. This could obviously be broken out into a separate JIRA, but it's pretty small so I figured I'd just do it here.\n\nbq. The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys. But always returns the URI of the default fs, which may be an alias for one of the NN URIs already in the set. Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys. People who want both (can't think of any who should) can add the default URI to the set.\n\nOK, done.\n\nbq. Worth mentioning in the javadoc that for HA NNs, unlike federated NNs, we're explicitly skipping the specific NNs and just using the logical NNs.\n\nI revised the comment and also changed the name of the method to getNameServiceUris, since I think that's generally more clear.\n\nbq. getNNUris should use HdfsConstants#HDFS_URI_SCHEME\n\nDone.\n\nbq. testGetNNUris needs HA-enabled and non-HA-enabled cases\n\nThis is already tested, since the configuration which is tested includes multiple name services, one HA-enabled and the other not.\n\nI also ran the following tests to verify this change, and they all passed:\n\nTestDNFencing,TestDNFencingWithReplication,TestEditLogsDuringFailover,TestEditLogTailer,TestFailureOfSharedDir,TestFailureToReadEdits,TestHAConfiguration,TestHAFsck,TestHAMetrics,TestHASafeMode,TestHAStateTransitions,TestHAWebUI,TestPipelinesFailover,TestQuotasWithHA,TestStandbyCheckpoints,TestStandbyIsHot,TestDFSClientFailover,TestBalancerWithHANameNodes,TestDFSUtil,TestBalancer,TestBalancerWithMultipleNameNodes\nLet's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys. Otherwise looks great. Nice find wrt the mutating configuration issue.\nThanks a lot, Eli. Here's an updated patch which adds the wrapper method as you requested.\n+1\nThanks a lot for the reviews, Eli. I've just committed this to the HA branch.\nThis patch seems to have made some tests fail (I noticed with TestFileAppend2). It seems to be trying to connect to 127.0.0.1:0 instead of the correct port.\nThanks Todd. I've filed HDFS-3033 to address these failures.\nOn second thought, I'm going to revert this change and fix the issue with NN conf mutation by modifying the test TestBalancerWithHANameNodes test case. The conf mutation issue should be fixed in a more general way.\nHere's an updated patch which addresses the issue and removes the change to the NameNode. Instead, to avoid the issue with conf mutation, the relevant tests make a copy of the configuration objects and then set the addresses appropriately after the NNs have started, to get the correct ephemeral ports configured.\nSmall nits:\n- please rename {{getNameNodeInfo}} to {{getNameNodeInfos}} or {{getNameNodeInfoArray}} or something?\n- in {{setFederatedConfiguration}} add assert that info.nameserviceId is not null\n\nThanks a lot for the quick review, Todd. Here's an updated patch which addresses your feedback.\n+1 lgtm (I reviewed only the delta from the previous patch, not the full patch, but should be good to commit given prior reviews on the main body)\nThanks a lot for the reviews, Todd. I've just committed the latest patch to the HA branch.\nThanks a lot Aaron for working on this issue and moving it to closure. I was out station for the last week, so I could not put my efforts on this issue.\nIntegrated in Hadoop-Hdfs-HAbranch-build #93 (See [https://builds.apache.org/job/Hadoop-Hdfs-HAbranch-build/93/])\n    HDFS-2979. Balancer should use logical uri for creating failover proxy with HA enabled. Contributed by Aaron T. Myers. (Revision 1295473)\nRevert commit of HDFS-2979. (Revision 1295435)\nHDFS-2979. Balancer should use logical uri for creating failover proxy with HA enabled. Contributed by Aaron T. Myers. (Revision 1295340)\n\n     Result = UNSTABLE\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295473\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295435\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\natm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1295340\nFiles : \n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java\n* /hadoop/common/branches/HDFS-1623/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java\n\n", "issueSearchSentences": ["Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's make this just return the NN URIs for the keys like getNNServiceRpcAddresses and people who just want the default URI from the conf can use getDefaultUri instead of calling this w/o keys.", "Let's add a wrapper for getNameServiceUris, something like getNSServiceRpcUris which is akin to getNNServiceRpcAddresses so all the callers don't have to pass the two keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys.", "The new getNNUris method seems overloaded, eg it only gets a URI for each configured NN or a logical URI for an HA NS if if you pass it keys."], "issueSearchScores": [0.6856510639190674, 0.6856510639190674, 0.6238977909088135, 0.5397199392318726, 0.5397199392318726]}
{"aId": 100, "code": "int moveBlockToHead(BlockInfo b, int curIndex, int headIndex) {\n    blockList = b.moveBlockToHead(blockList, this, curIndex, headIndex);\n    return curIndex;\n  }", "comment": " Move block to the head of the list of blocks belonging to the data-node.", "issueId": "HDFS-2477", "issueStringList": ["Optimize computing the diff between a block report and the namenode state.", "When a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.", "With HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.", "Currently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it.", "In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call).", "findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.", "With this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls.", "Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%.", "Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.", "This is an automatically generated e-mail.", "To reply, visit:", "https://reviews.apache.org/r/2516/", "Review request for Hairong Kuang.", "Summary", "When a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.", "With HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.", "Currently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it.", "In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call).", "findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.", "With this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls.", "Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%.", "Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.", "This addresses bug HDFS-2477.", "https://issues.apache.org/jira/browse/HDFS-2477", "Diffs", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java 1187125", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java 1187125", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java 1187125", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfo.java PRE-CREATION", "Diff: https://reviews.apache.org/r/2516/diff", "Testing", "Additional JUnit tests.", "Thanks,", "Tomasz", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12500379/reportDiff.patch-3", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hdfs.server.namenode.TestBackupNode", "org.apache.hadoop.hdfs.TestFileAppend2", "org.apache.hadoop.hdfs.server.datanode.TestMulitipleNNDataBlockScanner", "org.apache.hadoop.hdfs.TestBalancerBandwidth", "org.apache.hadoop.hdfs.TestRestartDFS", "org.apache.hadoop.hdfs.TestDistributedFileSystem", "org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/1423//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1423//console", "This message is automatically generated.", "Tomasz, as I understand your intentions you want to optimize {{DatanodeDescriptor.moveBlockToHead()}} by replacing the combination of calls of {{listRemove()}} and {{listInsert()}} with a single call to a method that directly moves the list element to the head, because it avoids redundant {{findDatanode()}} calls.", "This sounds like a good idea.", "Implementation wise I'd propose to restructure it a bit", "Instead of introducing helper class DatanodeIndex, which is somewhat confusing I'd rather implement it with extra parameters and return values by changing the signature of {{DatanodeDescriptor.moveBlockToHead()}}, like", "{code}", "DatanodeDescriptor.moveBlockToHead()", "@return the index of the head of the blockList", "int moveBlockToHead(BlockInfo b, int curIndex, int headIndex) {", "blockList = b.listMoveToHead(blockList, this, curIndex, headIndex);", "return curIndex; // new headIndex", "}", "{code}", "where {{BlockInfo.listMoveToHead()}} is the implementation of your moveBlockToHeadFast().", "I think this implementation belongs to BlockInfo rather than DatanodeDescriptor as I tried to confine all logic related to triplets and related list operations inside BlockInfo.", "Then the BlockManager code will look something like this", "{code}", "private void reportDiff() {", "....", "int headIndex = 0;", "while(itBR.hasNext()) {", "....", "move block to the head of the list", "in curIndex = storedBlock.findDatanode(dn);", "if(storedBlock != null && curIndex >= 0){", "headIndex = dn.moveBlockToHead(storedBlock, curIndex, headIndex);", "}", "....", "}", "{code}", "I probably didn't get all the details right, but hope the idea is clear and makes sense.", "Also please", "remove white space changes,", "make sure tabs are replaced with spaces (need to correctly setup your Eclipse environment), and", "add JavaDoc for {{getSetPrevious()}} and {{getSetNext()}}, if you decide to keep these methods in your implementation.", "This is an automatically generated e-mail.", "To reply, visit:", "https://reviews.apache.org/r/2516/", "(Updated 2011-10-25 20:19:10.635353)", "Review request for Hairong Kuang.", "Changes", "Applied Konstantin's comments.", "Summary", "When a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.", "With HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.", "Currently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it.", "In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call).", "findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.", "With this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls.", "Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%.", "Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.", "This addresses bug HDFS-2477.", "https://issues.apache.org/jira/browse/HDFS-2477", "Diffs (updated)", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java 1188885", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java 1188885", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java 1188885", "trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfo.java PRE-CREATION", "Diff: https://reviews.apache.org/r/2516/diff", "Testing", "Additional JUnit tests.", "Thanks,", "Tomasz", "Thanks Konstantin for your comments, you got it exactly right, so I incorporated all your suggestions.", "The changes aren't huge code-wise, but the moveToHead() call is happening for (almost) every block in every block report.", "With this changes I noticed quite substantial speed-up of reportDiff itself.", "I updated the patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12500747/reportDiff.patch-4", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS", "org.apache.hadoop.hdfs.server.namenode.TestParallelImageWrite", "org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks", "org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement", "org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace", "org.apache.hadoop.hdfs.server.datanode.TestBlockReport", "org.apache.hadoop.hdfs.server.namenode.TestEditLogJournalFailures", "org.apache.hadoop.hdfs.TestFileCreationClient", "org.apache.hadoop.hdfs.TestSetrepIncreasing", "org.apache.hadoop.hdfs.TestLeaseRecovery2", "org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure", "org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure", "org.apache.hadoop.hdfs.TestDistributedFileSystem", "org.apache.hadoop.hdfs.TestBlocksScheduledCounter", "org.apache.hadoop.hdfs.TestWriteConfigurationToDFS", "org.apache.hadoop.hdfs.server.namenode.TestEditLog", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/1442//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1442//console", "This message is automatically generated.", "Looks very good.", "+1", "I just committed this.", "Thanks Tomasz!", "Integrated in Hadoop-Hdfs-trunk-Commit #1310 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1310/])", "HDFS-2477.", "Optimize computing the diff between a block report and the namenode state.", "Contributed by Tomasz Nykiel.", "hairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java", "Integrated in Hadoop-Common-trunk-Commit #1235 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1235/])", "HDFS-2477.", "Optimize computing the diff between a block report and the namenode state.", "Contributed by Tomasz Nykiel.", "hairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #1257 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1257/])", "HDFS-2477.", "Optimize computing the diff between a block report and the namenode state.", "Contributed by Tomasz Nykiel.", "hairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java", "Thanks!", "!"], "SplitGT": [" Move block to the head of the list of blocks belonging to the data-node."], "issueString": "Optimize computing the diff between a block report and the namenode state.\nWhen a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.\n\nWith HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.\n\nCurrently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it. In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call). findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.\n\nWith this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls. Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%. Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttps://reviews.apache.org/r/2516/\n-----------------------------------------------------------\n\nReview request for Hairong Kuang.\n\n\nSummary\n-------\n\nWhen a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.\n\nWith HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.\n\nCurrently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it. In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call). findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.\n\nWith this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls. Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%. Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.\n\n\nThis addresses bug HDFS-2477.\n    https://issues.apache.org/jira/browse/HDFS-2477\n\n\nDiffs\n-----\n\n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java 1187125 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java 1187125 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java 1187125 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfo.java PRE-CREATION \n\nDiff: https://reviews.apache.org/r/2516/diff\n\n\nTesting\n-------\n\nAdditional JUnit tests.\n\n\nThanks,\n\nTomasz\n\n\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12500379/reportDiff.patch-3\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests:\n                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode\n                  org.apache.hadoop.hdfs.TestFileAppend2\n                  org.apache.hadoop.hdfs.server.datanode.TestMulitipleNNDataBlockScanner\n                  org.apache.hadoop.hdfs.TestBalancerBandwidth\n                  org.apache.hadoop.hdfs.TestRestartDFS\n                  org.apache.hadoop.hdfs.TestDistributedFileSystem\n                  org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/1423//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/1423//console\n\nThis message is automatically generated.\nTomasz, as I understand your intentions you want to optimize {{DatanodeDescriptor.moveBlockToHead()}} by replacing the combination of calls of {{listRemove()}} and {{listInsert()}} with a single call to a method that directly moves the list element to the head, because it avoids redundant {{findDatanode()}} calls. This sounds like a good idea. Implementation wise I'd propose to restructure it a bit\n\nInstead of introducing helper class DatanodeIndex, which is somewhat confusing I'd rather implement it with extra parameters and return values by changing the signature of {{DatanodeDescriptor.moveBlockToHead()}}, like\n\n{code}\n/** \n * DatanodeDescriptor.moveBlockToHead()\n * @return the index of the head of the blockList\n */\nint moveBlockToHead(BlockInfo b, int curIndex, int headIndex) {\n  blockList = b.listMoveToHead(blockList, this, curIndex, headIndex);\n  return curIndex; // new headIndex\n}\n{code}\nwhere {{BlockInfo.listMoveToHead()}} is the implementation of your moveBlockToHeadFast(). I think this implementation belongs to BlockInfo rather than DatanodeDescriptor as I tried to confine all logic related to triplets and related list operations inside BlockInfo. Then the BlockManager code will look something like this\n{code}\nprivate void reportDiff() {\n  ....\n  int headIndex = 0;\n  while(itBR.hasNext()) {\n    ....\n    // move block to the head of the list\n    in curIndex = storedBlock.findDatanode(dn);\n    if(storedBlock != null && curIndex >= 0){\n      headIndex = dn.moveBlockToHead(storedBlock, curIndex, headIndex);\n  }\n  ....\n}\n{code}\nI probably didn't get all the details right, but hope the idea is clear and makes sense.\nAlso please \n- remove white space changes, \n- make sure tabs are replaced with spaces (need to correctly setup your Eclipse environment), and \n- add JavaDoc for {{getSetPrevious()}} and {{getSetNext()}}, if you decide to keep these methods in your implementation.\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttps://reviews.apache.org/r/2516/\n-----------------------------------------------------------\n\n(Updated 2011-10-25 20:19:10.635353)\n\n\nReview request for Hairong Kuang.\n\n\nChanges\n-------\n\nApplied Konstantin's comments.\n\n\nSummary\n-------\n\nWhen a block report is processed at the NN, the BlockManager.reportDiff traverses all blocks contained in the report, and for each one block, which is also present in the corresponding datanode descriptor, the block is moved to the head of the list of the blocks in this datanode descriptor.\n\nWith HDFS-395 the huge majority of the blocks in the report, are also present in the datanode descriptor, which means that almost every block in the report will have to be moved to the head of the list.\n\nCurrently this operation is performed by DatanodeDescriptor.moveBlockToHead, which removes a block from a list and then inserts it. In this process, we call findDatanode several times (afair 6 times for each moveBlockToHead call). findDatanode is relatively expensive, since it linearly goes through the triplets to locate the given datanode.\n\nWith this patch, we do some memoization of findDatanode, so we can reclaim 2 findDatanode calls. Our experiments show that this can improve the reportDiff (which is executed under write lock) by around 15%. Currently with HDFS-395, reportDiff is responsible for almost 100% of the block report processing time.\n\n\nThis addresses bug HDFS-2477.\n    https://issues.apache.org/jira/browse/HDFS-2477\n\n\nDiffs (updated)\n-----\n\n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java 1188885 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java 1188885 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java 1188885 \n  trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfo.java PRE-CREATION \n\nDiff: https://reviews.apache.org/r/2516/diff\n\n\nTesting\n-------\n\nAdditional JUnit tests.\n\n\nThanks,\n\nTomasz\n\n\nThanks Konstantin for your comments, you got it exactly right, so I incorporated all your suggestions.\n\nThe changes aren't huge code-wise, but the moveToHead() call is happening for (almost) every block in every block report. \nWith this changes I noticed quite substantial speed-up of reportDiff itself.\n\nI updated the patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12500747/reportDiff.patch-4\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests:\n                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS\n                  org.apache.hadoop.hdfs.server.namenode.TestParallelImageWrite\n                  org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks\n                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement\n                  org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace\n                  org.apache.hadoop.hdfs.server.datanode.TestBlockReport\n                  org.apache.hadoop.hdfs.server.namenode.TestEditLogJournalFailures\n                  org.apache.hadoop.hdfs.TestFileCreationClient\n                  org.apache.hadoop.hdfs.TestSetrepIncreasing\n                  org.apache.hadoop.hdfs.TestLeaseRecovery2\n                  org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure\n                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure\n                  org.apache.hadoop.hdfs.TestDistributedFileSystem\n                  org.apache.hadoop.hdfs.TestBlocksScheduledCounter\n                  org.apache.hadoop.hdfs.TestWriteConfigurationToDFS\n                  org.apache.hadoop.hdfs.server.namenode.TestEditLog\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/1442//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/1442//console\n\nThis message is automatically generated.\nLooks very good. +1\nI just committed this. Thanks Tomasz!\nIntegrated in Hadoop-Hdfs-trunk-Commit #1310 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1310/])\n    HDFS-2477. Optimize computing the diff between a block report and the namenode state. Contributed by Tomasz Nykiel.\n\nhairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java\n\nIntegrated in Hadoop-Common-trunk-Commit #1235 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/1235/])\n    HDFS-2477. Optimize computing the diff between a block report and the namenode state. Contributed by Tomasz Nykiel.\n\nhairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java\n\nIntegrated in Hadoop-Mapreduce-trunk-Commit #1257 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1257/])\n    HDFS-2477. Optimize computing the diff between a block report and the namenode state. Contributed by Tomasz Nykiel.\n\nhairong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196676\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java\n\nThanks!!\n", "issueSearchSentences": ["int moveBlockToHead(BlockInfo b, int curIndex, int headIndex) {", "headIndex = dn.moveBlockToHead(storedBlock, curIndex, headIndex);", "blockList = b.listMoveToHead(blockList, this, curIndex, headIndex);", "where {{BlockInfo.listMoveToHead()}} is the implementation of your moveBlockToHeadFast().", "move block to the head of the list"], "issueSearchScores": [0.9505369663238525, 0.8875960111618042, 0.8874086737632751, 0.8000543117523193, 0.7899115681648254]}
{"aId": 102, "code": "public void allowSnapshot(String[] argv) throws IOException {   \n    DistributedFileSystem dfs = getDFS();\n    dfs.allowSnapshot(argv[1]);\n    System.out.println(\"Allowing snaphot on \" + argv[1] + \" succeeded\");\n  }", "comment": " Allow snapshot on a directory.", "issueId": "HDFS-4084", "issueStringList": ["provide CLI support for allow and disallow snapshot on a directory", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "@Brandon", "Can we make the new commands case insensitive?", "We can log a different jira to make existing commands also case insensitive.", "Good point, Arpit.", "Will change it and upload a new patch.", "Re-based the patch and addressed Arpit's comment.", "Comments:", "# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods", "{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class.", "So javadoc for DistributedFileSystem methods you have added can be deleted.", "{quote}", "Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "Added javadoc for them.", "{quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem.", "Also please proper javadoc to the methods.", "{quote}", "Removed, realized these two methods need to be public anyway.", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote}", "Yes.", "removed.", "{quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote}", "Done.", "Thanks.", "I committed the patch to HDFS-2802 branch.", "Thank you Brandon."], "SplitGT": [" Allow snapshot on a directory."], "issueString": "provide CLI support for allow and disallow snapshot on a directory\nTo provide CLI support to allow snapshot, disallow snapshot on a directory.\n@Brandon\n\nCan we make the new commands case insensitive? We can log a different jira to make existing commands also case insensitive. \nGood point, Arpit. Will change it and upload a new patch. \nRe-based the patch and addressed Arpit's comment.\nComments:\n# When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.\n# Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.\n# FSNamesystem.java is unnecessarily importing SnapshotInfo?\n# NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods\n\n{quote} When you are overriding, no need to add javadoc to the method if you plan on inheriting changes from the super class. So javadoc for DistributedFileSystem methods you have added can be deleted.{quote} \nMethods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override. Added javadoc for them.\n   {quote}  Why are you adding @VisibleForTesting for snapshot related methods in FSNamesystem. Also please proper javadoc to the methods.{quote} \nRemoved, realized these two methods need to be public anyway.\n    {quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?{quote} \nYes. removed.\n    {quote} NamenodeRpcServer.java remove comment \"Client Protocol\" for overridden protocol methods{quote} \nDone. Thanks.\n\nI committed the patch to HDFS-2802 branch. Thank you Brandon.\n", "issueSearchSentences": ["Methods allowSnapshot and diskallowSnapshot are not inerited from FileSystem class, and thus not override.", "To provide CLI support to allow snapshot, disallow snapshot on a directory.", "provide CLI support for allow and disallow snapshot on a directory", "# FSNamesystem.java is unnecessarily importing SnapshotInfo?", "{quote} FSNamesystem.java is unnecessarily importing SnapshotInfo?"], "issueSearchScores": [0.7033001184463501, 0.596219539642334, 0.5661600828170776, 0.51451176404953, 0.4942361116409302]}
{"aId": 103, "code": "static boolean isTokenExpired(AccessToken token) throws IOException {\n    ByteArrayInputStream buf = new ByteArrayInputStream(token.getTokenID()\n        .getBytes());\n    DataInputStream in = new DataInputStream(buf);\n    long expiryDate = WritableUtils.readVLong(in);\n    return isExpired(expiryDate);\n  }", "comment": " check if a token is expired.", "issueId": "HADOOP-6176", "issueStringList": ["Adding a couple private methods to AccessTokenHandler for testing purposes", "To support some test cases being added as part of HDFS-409, a couple private methods need to be added to AccessTokenHandler.", "One is for setting token lifetime and another is checking if a token has expired.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12415521/core-03.patch", "against trunk revision 800919.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/console", "This message is automatically generated.", "+1 with a readability nit: it may be nice to explicitly state in a comment that the tests are needed to expose otherwise private methods for testing.", "May be a reasonable assumption to make on the reader's part, but clarity is always a good thing.", "There was an idea to express the essence of this modification via aspects.", "However, in order to deliver this functionality in a most timely manner this effort will be carried on separately.", "I might be filing a separate JIRA for it if a preliminary investigation will show that such approach is viable.", "I have committed this.", "Thanks, Kan!"], "SplitGT": [" check if a token is expired."], "issueString": "Adding a couple private methods to AccessTokenHandler for testing purposes\nTo support some test cases being added as part of HDFS-409, a couple private methods need to be added to AccessTokenHandler. One is for setting token lifetime and another is checking if a token has expired.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12415521/core-03.patch\n  against trunk revision 800919.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/592/console\n\nThis message is automatically generated.\n+1 with a readability nit: it may be nice to explicitly state in a comment that the tests are needed to expose otherwise private methods for testing.  May be a reasonable assumption to make on the reader's part, but clarity is always a good thing.\nThere was an idea to express the essence of this modification via aspects. However, in order to deliver this functionality in a most timely manner this effort will be carried on separately. I might be filing a separate JIRA for it if a preliminary investigation will show that such approach is viable.\n \nI have committed this.  Thanks, Kan!\n", "issueSearchSentences": ["One is for setting token lifetime and another is checking if a token has expired.", "To support some test cases being added as part of HDFS-409, a couple private methods need to be added to AccessTokenHandler.", "Adding a couple private methods to AccessTokenHandler for testing purposes", "+1 release audit.", "However, in order to deliver this functionality in a most timely manner this effort will be carried on separately."], "issueSearchScores": [0.6575900912284851, 0.531570553779602, 0.4680580794811249, 0.3349599242210388, 0.3196834921836853]}
{"aId": 104, "code": "private void updateBlocksMap(\n      Map.Entry<Block, BlocksMapINodeUpdateEntry> entry) {\n    Block block = entry.getKey();\n    BlocksMapINodeUpdateEntry value = entry.getValue();\n    if (value == null) {\n      blockManager.removeBlock(block);\n    } else {\n      BlockCollection toDelete = value.getToDelete();\n      BlockInfo originalBlockInfo = blockManager.getStoredBlock(block);\n      // The FSDirectory tree and the blocksMap share the same INode reference.\n      // Thus we use \"==\" to check if the INode for the block belongs to the\n      // current file (instead of the INode from a snapshot file).\n      if (originalBlockInfo != null\n          && toDelete == originalBlockInfo.getBlockCollection()) {\n        blockManager.addBlockCollection(originalBlockInfo,\n            value.getToReplace());\n      }\n    }\n  }", "comment": " Update the blocksMap for a given block.", "issueId": "HDFS-4150", "issueStringList": ["Update inode in blocksMap when deleting original/snapshot file", "When deleting a file/directory, instead of directly removing all the corresponding blocks, we should update inodes in blocksMap if there are snapshots for them.", "Initial patch without tests.", "Will add/run testcases for it.", "Patch with testcase.", "Updated based on changes in HDFS-4151.", "We should change the parameter of collectSubtreeBlocksAndClear(..) to a new class instead of Map<Block, BlockDeletionInfo> so the block replacement info can be iterated before the block deletion info.", "Let me create a JIRA to do it in trunk first.", "The INode replacement update may be delayed.", "Then, the old inode may be used in replication.", "We need to handle the replication computation.", "The new BlockDeletionInfo object below should be created outside the loop since it is the same for all the blocks.", "{code}", "INodeFileWithLink.collectBlocksBeyondMaxAndClear(..)", "+      // Replace the INode for all the remaining blocks in blocksMap", "+      if (m != null) {", "+        for (int i = 0; i < n; i++) {", "+          BlockDeletionInfo info = new BlockDeletionInfo(this, next);", "+          m.put(oldBlocks[i], info);", "+        }", "+      }", "{code}", "BlockDeletionInfo is not a good name since the blocks are not going to be deleted, how about renaming it to INodeReplacementInfo?", "There are some code duplicated in FSNamesystem.", "Please put them it a method.", "Hi Jing, in general the patch looks pretty good to me.", "I just have one little concern.", "There's two places where this patch contains code like this:", "{code}", "if (originalBlockInfo != null", "&& toDelete == originalBlockInfo.getBlockCollection()) {", "{code}", "I'm concerned by the use of \"==\" here, instead of \".equals\" or the like.", "Are we in fact guaranteed that the same actual object reference will be used in both places?", "(I think this is probably fine as-is, I just want to make sure.)", "Also, you might want to add a comment above this code saying why replacing the BlockCollection in the blocks map is appropriate in this case, i.e.", "in the case of a snapshot existing which still references this block.", "Nicholas and Aaron, thanks for the comments!", "Have uploaded the initial patch for refactoring INode#collectSubtreeBlocksAndClear in HDFS-4152.", "Will upload a new patch addressing Nicholas's comments after HDFS-4152 going through.", "To address Aaron's comments, I will also add more comments explaining why replacing the BlockCollection in the blocksMap.", "I have also rechecked the code and verified that the same BlockCollection/INode reference is used in both blocksMap and FSDirectory tree.", "New patch uploaded.", "+1 patch looks good.", "I have committed this.", "Thanks, Jing!", "Integrated in Hadoop-Hdfs-Snapshots-Branch-build #4 (See [https://builds.apache.org/job/Hadoop-Hdfs-Snapshots-Branch-build/4/])", "HDFS-4150.", "Update the inode in the block map when a snapshotted file or a snapshot file is deleted.", "Contributed by Jing Zhao (Revision 1406763)", "Result = FAILURE", "szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406763", "Files :", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java", "hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java"], "SplitGT": [" Update the blocksMap for a given block."], "issueString": "Update inode in blocksMap when deleting original/snapshot file\nWhen deleting a file/directory, instead of directly removing all the corresponding blocks, we should update inodes in blocksMap if there are snapshots for them.\nInitial patch without tests. Will add/run testcases for it.\nPatch with testcase.\nUpdated based on changes in HDFS-4151.\n- We should change the parameter of collectSubtreeBlocksAndClear(..) to a new class instead of Map<Block, BlockDeletionInfo> so the block replacement info can be iterated before the block deletion info.  Let me create a JIRA to do it in trunk first.\n\n- The INode replacement update may be delayed.  Then, the old inode may be used in replication.  We need to handle the replication computation.\n\n- The new BlockDeletionInfo object below should be created outside the loop since it is the same for all the blocks.\n{code}\n//INodeFileWithLink.collectBlocksBeyondMaxAndClear(..)\n+      // Replace the INode for all the remaining blocks in blocksMap\n+      if (m != null) {\n+        for (int i = 0; i < n; i++) {\n+          BlockDeletionInfo info = new BlockDeletionInfo(this, next);\n+          m.put(oldBlocks[i], info);\n+        }\n+      }\n{code}\n\n- BlockDeletionInfo is not a good name since the blocks are not going to be deleted, how about renaming it to INodeReplacementInfo?\n\n- There are some code duplicated in FSNamesystem.  Please put them it a method.\n\nHi Jing, in general the patch looks pretty good to me. I just have one little concern. There's two places where this patch contains code like this:\n\n{code}\nif (originalBlockInfo != null\n    && toDelete == originalBlockInfo.getBlockCollection()) {\n{code}\n\nI'm concerned by the use of \"==\" here, instead of \".equals\" or the like. Are we in fact guaranteed that the same actual object reference will be used in both places? (I think this is probably fine as-is, I just want to make sure.) Also, you might want to add a comment above this code saying why replacing the BlockCollection in the blocks map is appropriate in this case, i.e. in the case of a snapshot existing which still references this block.\nNicholas and Aaron, thanks for the comments! Have uploaded the initial patch for refactoring INode#collectSubtreeBlocksAndClear in HDFS-4152. Will upload a new patch addressing Nicholas's comments after HDFS-4152 going through. \n\nTo address Aaron's comments, I will also add more comments explaining why replacing the BlockCollection in the blocksMap. I have also rechecked the code and verified that the same BlockCollection/INode reference is used in both blocksMap and FSDirectory tree.\nNew patch uploaded.\n+1 patch looks good.\nI have committed this.  Thanks, Jing!\nIntegrated in Hadoop-Hdfs-Snapshots-Branch-build #4 (See [https://builds.apache.org/job/Hadoop-Hdfs-Snapshots-Branch-build/4/])\n    HDFS-4150.  Update the inode in the block map when a snapshotted file or a snapshot file is deleted. Contributed by Jing Zhao (Revision 1406763)\n\n     Result = FAILURE\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406763\nFiles : \n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt\n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java\n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java\n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeFileWithLink.java\n* /hadoop/common/branches/HDFS-2802/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java\n\n", "issueSearchSentences": ["Update inode in blocksMap when deleting original/snapshot file", "We should change the parameter of collectSubtreeBlocksAndClear(..) to a new class instead of Map<Block, BlockDeletionInfo> so the block replacement info can be iterated before the block deletion info.", "To address Aaron's comments, I will also add more comments explaining why replacing the BlockCollection in the blocksMap.", "+          m.put(oldBlocks[i], info);", "Update the inode in the block map when a snapshotted file or a snapshot file is deleted."], "issueSearchScores": [0.6099830269813538, 0.5998556613922119, 0.5871822834014893, 0.5835802555084229, 0.5777894258499146]}
{"aId": 105, "code": "void remove(Block block) {\n    synchronized (pendingReplications) {\n      pendingReplications.remove(block);\n    }\n  }", "comment": " Remove the record about the given block from pendingReplications.", "issueId": "HDFS-4072", "issueStringList": ["On file deletion remove corresponding blocks pending replication", "Currently when deleting a file, blockManager does not remove records that are corresponding to the file's blocks from pendingRelications.", "These records can only be removed after timeout (5~10 min).", "The attached test may generate the scenario where a pendingReplication record is left in BlockManager#pendingReplications until timeout.", "And a simple patch uploaded.", "Patch looks good.", "Can you combine the test and the fix?", "In the test please remove unnecessary cast to DistributedFileSystem in setup() method.", "Unrelated to this patch, since you are in that part of the code:", "# Since you are in that part of the code can you remove unnecessary exception declared as thrown from BlockManager#createBlockTokenSecretManager.", "# BlockManager#chooseTarget javadoc has incorrect @see to BlockPlacementPolicy method", "# BlockManager#queueReportedBlock has unnecessary { } around @see block", "# BlockManager#addStorageBlockImmediately() @link should have # preceding addStorageBlock().", "Similar change for method countLiveNodes().", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12549572/HDFS-4072.trunk.001.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.TestHFlush", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3357//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3357//console", "This message is automatically generated.", "Thanks for the comments Suresh.", "The modified patch uploaded.", "Adding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right?", "Seems like we should add this call to removeStoredBlock instead so we decrement each time a DN reports that the block has been deleted, and we eventually drop the replica count to 0 and removed the entry from PendingReplications.", "I think updating the test to corrupt two replicas would catch this.", "Thanks for the comment Eli!", "I think you're right: the PendingReplicationBlocks#remove only decrements the pending replication number by 1, it's not removing the whole record.", "So I guess we only need to remove the whole record about the block from PendingReplicationBlocks here, and we can still do this operation in BlockManager#removeBlock().", "bq.", "Adding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right?", "Good catch.", "bq.", "Seems like we should add this call to removeStoredBlock instead", "I do not think it works that way.", "Number of replicas reported as deleted and the number of pending replications are different.", "I think we should introduce a method to remove the pending replication given a block, instead of decrementing a count.", "I think the existing names are poorly named as well.", "Methods should be renamed:", "# add() to increment()", "# remove() to decrement()", "# New method remove that removes the block from the map", "Updated patch.", "{quote}", "I think the existing names are poorly named as well.", "Methods should be renamed:", "1.add() to increment()", "2.remove() to decrement()", "3.New method remove that removes the block from the map", "{quote}", "We have discussed the same in HDFS-4022.", "That JIRA also planning to add the same APIs, as per the comments.", "So, will just comment there to wait till this JIRA go in then, as patch already updated here.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12549639/HDFS-4072.trunk.003.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:", "org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3361//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3361//console", "This message is automatically generated.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12549639/HDFS-4072.trunk.003.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3362//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3362//console", "This message is automatically generated.", "Cool, I am surprised to see similar discussion in another jira and similar conclusions there too.", "Thanks Uma for pointing out.", "If there is no further comment on this, I will commit this by evening to unblock HDFS-4022.", "Looks good to me, perhaps in another change modify the test so it would have failed on the previous version of the patch.", "Eli, thanks for the advice.", "To address your comments, I made two replicas corrupt and checked if the pending replica size is 2 in the new testcase.", "+1 pending jenkins.", "Nice change Jing.", "Nit: \"mark a block as corrupt\" should be \"mark a couple blocks as corrupt\", we can fix this on commit.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12549721/HDFS-4072.trunk.004.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 1 new or modified test files.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 eclipse:eclipse{color}.", "The patch built with eclipse:eclipse.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.", "{color:green}+1 contrib tests{color}.", "The patch passed contrib unit tests.", "Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3364//testReport/", "Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3364//console", "This message is automatically generated.", "Canceling the patch to upload minor nit taken care of before committing the code.", "Uploading new patch with a small nit change.", "Committed the patch to trunk and branch-2.", "Thank you Jing.", "Integrated in Hadoop-trunk-Commit #2892 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2892/])", "HDFS-4072.", "On file deletion remove corresponding blocks pending replications.", "Contributed by Jing Zhao.", "(Revision 1399965)", "Result = FAILURE", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1399965", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java", "Jing, would this change be needed for branch-1 as well?", "Integrated in Hadoop-Yarn-trunk #8 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/8/])", "HDFS-4072.", "On file deletion remove corresponding blocks pending replications.", "Contributed by Jing Zhao.", "(Revision 1399965)", "Result = FAILURE", "suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1399965", "Files :", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java", "hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java"], "SplitGT": [" Remove the record about the given block from pendingReplications."], "issueString": "On file deletion remove corresponding blocks pending replication\nCurrently when deleting a file, blockManager does not remove records that are corresponding to the file's blocks from pendingRelications. These records can only be removed after timeout (5~10 min).\nThe attached test may generate the scenario where a pendingReplication record is left in BlockManager#pendingReplications until timeout.\nAnd a simple patch uploaded.\nPatch looks good. Can you combine the test and the fix?\n\nIn the test please remove unnecessary cast to DistributedFileSystem in setup() method.\n\nUnrelated to this patch, since you are in that part of the code:\n# Since you are in that part of the code can you remove unnecessary exception declared as thrown from BlockManager#createBlockTokenSecretManager.\n# BlockManager#chooseTarget javadoc has incorrect @see to BlockPlacementPolicy method\n# BlockManager#queueReportedBlock has unnecessary { } around @see block\n# BlockManager#addStorageBlockImmediately() @link should have # preceding addStorageBlock(). Similar change for method countLiveNodes().\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12549572/HDFS-4072.trunk.001.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.TestHFlush\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3357//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3357//console\n\nThis message is automatically generated.\nThanks for the comments Suresh. The modified patch uploaded.\nAdding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right? Seems like we should add this call to removeStoredBlock instead so we decrement each time a DN reports that the block has been deleted, and we eventually drop the replica count to 0 and removed the entry from PendingReplications. I think updating the test to corrupt two replicas would catch this.\nThanks for the comment Eli! I think you're right: the PendingReplicationBlocks#remove only decrements the pending replication number by 1, it's not removing the whole record. So I guess we only need to remove the whole record about the block from PendingReplicationBlocks here, and we can still do this operation in BlockManager#removeBlock().\nbq. Adding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right? \nGood catch.\n\nbq.  Seems like we should add this call to removeStoredBlock instead\nI do not think it works that way. Number of replicas reported as deleted and the number of pending replications are different. I think we should introduce a method to remove the pending replication given a block, instead of decrementing a count.\n\nI think the existing names are poorly named as well. Methods should be renamed:\n# add() to increment()\n# remove() to decrement()\n# New method remove that removes the block from the map\nUpdated patch.\n{quote}\nI think the existing names are poorly named as well. Methods should be renamed:\n\n1.add() to increment()\n2.remove() to decrement()\n3.New method remove that removes the block from the map\n{quote}\n\nWe have discussed the same in HDFS-4022. That JIRA also planning to add the same APIs, as per the comments. So, will just comment there to wait till this JIRA go in then, as patch already updated here.\n \n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12549639/HDFS-4072.trunk.003.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3361//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3361//console\n\nThis message is automatically generated.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12549639/HDFS-4072.trunk.003.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3362//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3362//console\n\nThis message is automatically generated.\nCool, I am surprised to see similar discussion in another jira and similar conclusions there too. Thanks Uma for pointing out.\n\nIf there is no further comment on this, I will commit this by evening to unblock HDFS-4022.\nLooks good to me, perhaps in another change modify the test so it would have failed on the previous version of the patch.\nEli, thanks for the advice. To address your comments, I made two replicas corrupt and checked if the pending replica size is 2 in the new testcase.\n+1 pending jenkins. Nice change Jing.\n\nNit: \"mark a block as corrupt\" should be \"mark a couple blocks as corrupt\", we can fix this on commit.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12549721/HDFS-4072.trunk.004.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/3364//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/3364//console\n\nThis message is automatically generated.\nCanceling the patch to upload minor nit taken care of before committing the code.\nUploading new patch with a small nit change.\nCommitted the patch to trunk and branch-2. Thank you Jing.\nIntegrated in Hadoop-trunk-Commit #2892 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2892/])\n    HDFS-4072. On file deletion remove corresponding blocks pending replications. Contributed by Jing Zhao. (Revision 1399965)\n\n     Result = FAILURE\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1399965\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java\n\nJing, would this change be needed for branch-1 as well?\n\nIntegrated in Hadoop-Yarn-trunk #8 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/8/])\n    HDFS-4072. On file deletion remove corresponding blocks pending replications. Contributed by Jing Zhao. (Revision 1399965)\n\n     Result = FAILURE\nsuresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1399965\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java\n\n", "issueSearchSentences": ["Adding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right?", "Adding pendingReplications.remove(block) to removeBlock means we're just decrementing the replica count by 1 right?", "So I guess we only need to remove the whole record about the block from PendingReplicationBlocks here, and we can still do this operation in BlockManager#removeBlock().", "Seems like we should add this call to removeStoredBlock instead so we decrement each time a DN reports that the block has been deleted, and we eventually drop the replica count to 0 and removed the entry from PendingReplications.", "I think we should introduce a method to remove the pending replication given a block, instead of decrementing a count."], "issueSearchScores": [0.8540975451469421, 0.8540975451469421, 0.8146554231643677, 0.7968125343322754, 0.7790096998214722]}
{"aId": 106, "code": "public static void shutdown(ExecutorService executorService, Logger logger,\n      long timeout, TimeUnit unit) {\n    try {\n      if (executorService != null) {\n        executorService.shutdown();\n        try {\n          if (!executorService.awaitTermination(timeout, unit)) {\n            executorService.shutdownNow();\n          }\n\n          if (!executorService.awaitTermination(timeout, unit)) {\n            logger.error(\"Unable to shutdown properly.\");\n          }\n        } catch (InterruptedException e) {\n          logger.error(\"Error attempting to shutdown.\", e);\n          executorService.shutdownNow();\n        }\n      }\n    } catch (Exception e) {\n      logger.error(\"Error during shutdown: \", e);\n      throw e;\n    }\n  }", "comment": " Helper routine to shutdown a executorService.", "issueId": "HADOOP-15366", "issueStringList": ["Add a helper shutdown routine in HadoopExecutor to ensure clean shutdown", "It is recommended to shut down an\u00a0{{ExecutorService}}\u00a0in two phases, first by calling\u00a0{{shutdown}}\u00a0to reject incoming tasks, and then calling\u00a0{{shutdownNow}}, if necessary, to cancel any lingering tasks.", "This Jira aims to add a helper shutdown routine in Hadoop executor\u00a0 to achieve the same.", "Patch v0 adds a helper shutdown routine for hadoop executor service.", "| (x) *{color:red}-1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 11s{color} | {color:blue} Docker mode activated.", "{color} |", "|| || || || {color:brown} Prechecks {color} ||", "| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags.", "{color} |", "| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color} |", "|| || || || {color:brown} trunk Compile Tests {color} ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 25m 29s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green} 27m 39s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 10s{color} | {color:green} branch has no errors when building and testing our client artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 31s{color} | {color:green} trunk passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |", "|| || || || {color:brown} Patch Compile Tests {color} ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green} 26m 19s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javac {color} | {color:green} 26m 19s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues.", "{color} |", "| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 11s{color} | {color:green} patch has no errors when building and testing our client artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 39s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |", "|| || || || {color:brown} Other Tests {color} ||", "| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 12s{color} | {color:green} hadoop-common in the patch passed.", "{color} |", "| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} | {color:green} The patch does not generate ASF License warnings.", "{color} |", "| {color:black}{color} | {color:black} {color} | {color:black}120m  5s{color} | {color:black} {color} |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |", "| JIRA Issue | HADOOP-15366 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12917765/HADOOP-15366.000.patch |", "| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |", "| uname | Linux fdd4a2ee4f55 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |", "| Build tool | maven |", "| Personality | /testptch/patchprocess/precommit/personality/provided.sh |", "| git revision | trunk / 6cf023f |", "| maven | version: Apache Maven 3.3.9 |", "| Default Java | 1.8.0_162 |", "| findbugs | v3.1.0-RC1 |", "|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14444/testReport/ |", "| Max.", "process+thread count | 1525 (vs. ulimit of 10000) |", "| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |", "| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14444/console |", "| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |", "This message was automatically generated.", "Thanks for working on this [~shashikant].", "+1, v0 patch looks good to me.", "I will commit this shortly.", "Thanks for the contribution [~shashikant].", "I have committed this to the trunk.", "SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13940 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13940/])", "HADOOP-15366.", "Add a helper shutdown routine in HadoopExecutor to ensure (msingh: rev 0b345b765370515d7222154ad5cae9b86f137a76)", "(edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java"], "SplitGT": [" Helper routine to shutdown a executorService."], "issueString": "Add a helper shutdown routine in HadoopExecutor to ensure clean shutdown\nIt is recommended to shut down an\u00a0{{ExecutorService}}\u00a0in two phases, first by calling\u00a0{{shutdown}}\u00a0to reject incoming tasks, and then calling\u00a0{{shutdownNow}}, if necessary, to cancel any lingering tasks. This Jira aims to add a helper shutdown routine in Hadoop executor\u00a0 to achieve the same.\nPatch v0 adds a helper shutdown routine for hadoop executor service.\n| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 11s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 25m 29s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 27m 39s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 10s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 31s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 26m 19s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 26m 19s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 11s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 39s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 12s{color} | {color:green} hadoop-common in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}120m  5s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |\r\n| JIRA Issue | HADOOP-15366 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12917765/HADOOP-15366.000.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux fdd4a2ee4f55 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 6cf023f |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_162 |\r\n| findbugs | v3.1.0-RC1 |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14444/testReport/ |\r\n| Max. process+thread count | 1525 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14444/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n\nThanks for working on this [~shashikant].\r\n\r\n+1, v0 patch looks good to me. I will commit this shortly.\nThanks for the contribution [~shashikant]. I have committed this to the trunk.\nSUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13940 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13940/])\nHADOOP-15366. Add a helper shutdown routine in HadoopExecutor to ensure (msingh: rev 0b345b765370515d7222154ad5cae9b86f137a76)\n* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java\n\n", "issueSearchSentences": ["It is recommended to shut down an\u00a0{{ExecutorService}}\u00a0in two phases, first by calling\u00a0{{shutdown}}\u00a0to reject incoming tasks, and then calling\u00a0{{shutdownNow}}, if necessary, to cancel any lingering tasks.", "Add a helper shutdown routine in HadoopExecutor to ensure clean shutdown", "This Jira aims to add a helper shutdown routine in Hadoop executor\u00a0 to achieve the same.", "Patch v0 adds a helper shutdown routine for hadoop executor service.", "Add a helper shutdown routine in HadoopExecutor to ensure (msingh: rev 0b345b765370515d7222154ad5cae9b86f137a76)"], "issueSearchScores": [0.7050577998161316, 0.5486345887184143, 0.5311284065246582, 0.526421070098877, 0.5009440183639526]}
{"aId": 107, "code": "void setSnapshottable(final String path) throws IOException {\n    writeLock();\n    try {\n      final INodeDirectory d = INodeDirectory.valueOf(dir.getINode(path), path);\n      if (d.isSnapshottable()) {\n        //The directory is already a snapshottable directory. \n        return;\n      }\n\n      final INodeDirectorySnapshottable s\n          = INodeDirectorySnapshottable.newInstance(d);\n      dir.replaceINodeDirectory(path, d, s);\n    } finally {\n      writeUnlock();\n    }\n  }", "comment": " Set the given directory as a snapshottable directory.", "issueId": "HDFS-4077", "issueStringList": ["Support snapshottable INodeDirectory", "Allow INodeDirectory to be set to snapshottable INodeDirectory so that snapshots of the directory can be created.", "h4077_20121019.patch: adds INodeDirectorySnapshottable.", "Nicholas, some minor nits:", "# \"Set the given directory path to a snapshottable directory\" - \"Set the given directory as a snapshottable directory\" would be more clearer I think.", "# I also think grouping snapshot related functionality in FSNamesystem into an inner class would be better for code organization.", "# \"Directories that can be taken snapshots.\"", "could be \"Directories where taking snapshots is allowed.\"", "h4077_20121019b.patch: updates the javadoc.", "For #2, let's move the new method to SnapshotManger in HDFS-4079.", "+1 for the patch.", "I committed the patch to HDFS-2082 branch.", "Thank you Nicholas."], "SplitGT": [" Set the given directory as a snapshottable directory."], "issueString": "Support snapshottable INodeDirectory\nAllow INodeDirectory to be set to snapshottable INodeDirectory so that snapshots of the directory can be created.\nh4077_20121019.patch: adds INodeDirectorySnapshottable.\nNicholas, some minor nits:\n# \"Set the given directory path to a snapshottable directory\" - \"Set the given directory as a snapshottable directory\" would be more clearer I think.\n# I also think grouping snapshot related functionality in FSNamesystem into an inner class would be better for code organization.\n# \"Directories that can be taken snapshots.\" could be \"Directories where taking snapshots is allowed.\"\nh4077_20121019b.patch: updates the javadoc.\n\nFor #2, let's move the new method to SnapshotManger in HDFS-4079.\n+1 for the patch. \n\nI committed the patch to HDFS-2082 branch. Thank you Nicholas.\n", "issueSearchSentences": ["# \"Set the given directory path to a snapshottable directory\" - \"Set the given directory as a snapshottable directory\" would be more clearer I think.", "Allow INodeDirectory to be set to snapshottable INodeDirectory so that snapshots of the directory can be created.", "h4077_20121019.patch: adds INodeDirectorySnapshottable.", "Support snapshottable INodeDirectory", "could be \"Directories where taking snapshots is allowed.\""], "issueSearchScores": [0.6729957461357117, 0.6238060593605042, 0.6000591516494751, 0.5353901386260986, 0.43171894550323486]}
{"aId": 108, "code": "public static void clearStatistics() {\n    AbstractFileSystem.clearStatistics();\n  }", "comment": " Clears all the statistics stored in AbstractFileSystem, for all the file systems.", "issueId": "HADOOP-6432", "issueStringList": ["Statistics support in FileContext", "FileContext should have API to get statistics from underlying file systems.", "# Can you please add a brief description of what the patch does", "# FileContext.java - remove Options.Rename import", "# All FileContext.java public methods added should have javadoc", "# FileContext.getStatistics() - add more description to the method that only scheme and authority is used for looking up the file system.", "# FileContext.getAllStatistics() - should not throw URISyntaxException.", "# AbstractFileSystem.java - IdentifyHashMap import is unused", "# AbstractFileSystem.STATISTICS_TABLE declaration exceeds 80 columns.", "# AbstractFileSystem.getStatistics() - use getUri() method get the URI with scheme and authority.", "# AbstractFileSystem and FileContext printStatistics() prints the statistics using System.out.println.", "Not sure where this is used and if we need it.", "# AbstractFileSystem.getAllStatistics - initialize statsMap size to that of STATISTICS_TABLE.", "Also it is a good idea to define copy constructor in Statistics class.", "# Is it a good idea to have FcStatisticsTest that includes common logic and have a subclass for TestLocalFcStatistics, TestHdfsFcStatistics etc?", "Could the base test use FileContextTestHelper and avoid using /tmp as directory to create tests.", "Also code could be reused by adding a method checkStatistics(int expectedBytesRead, int expectedBytesWritten, Statistics s)", "A few points regarding the patch:", "1.", "The Statistics are statically stored inside AbstractFileSystem in a map that is keyed on URI.", "Only scheme and authority part of the URI are used.", "2.", "API are added to FileContext to get the statistics.", "3.", "FileContext.getAllStatistics returns a map of URI and statistics, but the URI key contains only scheme and authority.", "HADOOP-6432.3.patch submitted for hudson tests.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12428353/HADOOP-6432.3.patch", "against trunk revision 891511.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 7 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/console", "This message is automatically generated.", "New patch fixing the javadoc warning.", "HADOOP-6432.4.patch submitted for hudson tests.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12428368/HADOOP-6432.4.patch", "against trunk revision 891511.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 7 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/testReport/", "Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/artifact/trunk/build/test/checkstyle-errors.html", "Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/console", "This message is automatically generated.", "New patch.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12469871/HADOOP-6432-trunk.5.patch", "against trunk revision 1064919.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 5 new or modified tests.", "1 javadoc.", "The javadoc tool appears to have generated 1 warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//console", "This message is automatically generated.", "# AbstractFileSystem.java - remove unused import IdentityHashMap", "# AbstractFileSystem#getStatistics() - can you make only the inner part that accesses HashMap synchronized, excluding validation of URI.", "# minor: remove empty line changes before AbstractFileSystem#getStatistics() method.", "# STATISTICS_TABLE declaration goes beyond 80 columns", "# FileContext#rename() can you change the link in javadoc from Rename#OVERWRITE to Options.Rename#OVERWRITE to fix a javadoc warning", "# FileContext.java - the newly added methods need not be synchronized as it synchronization is handled by AbstractFileSystem.", "# FileContext#getAllStatistics() in javadoc unnecessarily indicates URISyntaxException is thrown.", "# FileContext#clearStatistics() - javadoc should indicate this method clears statistics for all the file systems.", "# FileContext#getStatistics() - for @param uri, have a fullstop after \"the uri to lookup the statistics\".", "# FileContext#printStatistics() - why does this method throw IOException?", "Remove it both from javadoc and declaration.", "# Complete the @param argument for new copy constructor in Statistics.", "# FCStatisticsBaseTest - \"Base class to test File Context Statistics.\"", "instead of File Context add link to FileContext.", "# FCStatisticsBaseTest - why do you need exact URI from getSchemeAuthorityUri()?", "The API handles any URI by picking scheme and authority right?", "# TestLocalFsFCStatistics - please add comment on why you are doing blockSize + 12?", "Also please change the class javadoc to indicate that this is testing stats for LocalFs.", "Uploaded a new patch addressing the comments, submitted for Hudson tests.", "+1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12469962/HADOOP-6432-trunk.6.patch", "against trunk revision 1065959.", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 5 new or modified tests.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "+1 findbugs.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed core unit tests.", "+1 contrib tests.", "The patch passed contrib unit tests.", "+1 system test framework.", "The patch passed system test framework compile.", "Test results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//testReport/", "Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html", "Console output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//console", "This message is automatically generated.", "javadoc for rename method in FileContext was reformatted in previous patch.", "+1 for the patch.", "The only change in the last patch and the previous one was in javadoc.", "I ran javadoc manually to verify.", "I just committed this.", "Integrated in Hadoop-Common-trunk-Commit #494 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/494/])", "HADOOP-6432.", "Add Statistics support in FileContext.", "Contributed by jitendra."], "SplitGT": [" Clears all the statistics stored in AbstractFileSystem, for all the file systems."], "issueString": "Statistics support in FileContext\nFileContext should have API to get statistics from underlying file systems.\n# Can you please add a brief description of what the patch does\n# FileContext.java - remove Options.Rename import\n# All FileContext.java public methods added should have javadoc\n# FileContext.getStatistics() - add more description to the method that only scheme and authority is used for looking up the file system.\n# FileContext.getAllStatistics() - should not throw URISyntaxException.\n# AbstractFileSystem.java - IdentifyHashMap import is unused\n# AbstractFileSystem.STATISTICS_TABLE declaration exceeds 80 columns.\n# AbstractFileSystem.getStatistics() - use getUri() method get the URI with scheme and authority.\n# AbstractFileSystem and FileContext printStatistics() prints the statistics using System.out.println. Not sure where this is used and if we need it.\n# AbstractFileSystem.getAllStatistics - initialize statsMap size to that of STATISTICS_TABLE. Also it is a good idea to define copy constructor in Statistics class.\n# Is it a good idea to have FcStatisticsTest that includes common logic and have a subclass for TestLocalFcStatistics, TestHdfsFcStatistics etc? Could the base test use FileContextTestHelper and avoid using /tmp as directory to create tests. Also code could be reused by adding a method checkStatistics(int expectedBytesRead, int expectedBytesWritten, Statistics s)\n\nA few points regarding the patch: \n  1. The Statistics are statically stored inside AbstractFileSystem in a map that is keyed on URI. Only scheme and authority part of the URI are used. \n  2. API are added to FileContext to get the statistics. \n  3. FileContext.getAllStatistics returns a map of URI and statistics, but the URI key contains only scheme and authority.\nHADOOP-6432.3.patch submitted for hudson tests.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12428353/HADOOP-6432.3.patch\n  against trunk revision 891511.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 7 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/218/console\n\nThis message is automatically generated.\nNew patch fixing the javadoc warning.\nHADOOP-6432.4.patch submitted for hudson tests.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12428368/HADOOP-6432.4.patch\n  against trunk revision 891511.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 7 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/219/console\n\nThis message is automatically generated.\nNew patch.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12469871/HADOOP-6432-trunk.5.patch\n  against trunk revision 1064919.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/212//console\n\nThis message is automatically generated.\n# AbstractFileSystem.java - remove unused import IdentityHashMap\n# AbstractFileSystem#getStatistics() - can you make only the inner part that accesses HashMap synchronized, excluding validation of URI.\n# minor: remove empty line changes before AbstractFileSystem#getStatistics() method.\n# STATISTICS_TABLE declaration goes beyond 80 columns\n# FileContext#rename() can you change the link in javadoc from Rename#OVERWRITE to Options.Rename#OVERWRITE to fix a javadoc warning\n# FileContext.java - the newly added methods need not be synchronized as it synchronization is handled by AbstractFileSystem.\n# FileContext#getAllStatistics() in javadoc unnecessarily indicates URISyntaxException is thrown.\n# FileContext#clearStatistics() - javadoc should indicate this method clears statistics for all the file systems.\n# FileContext#getStatistics() - for @param uri, have a fullstop after \"the uri to lookup the statistics\".\n# FileContext#printStatistics() - why does this method throw IOException? Remove it both from javadoc and declaration.\n# Complete the @param argument for new copy constructor in Statistics.\n# FCStatisticsBaseTest - \"Base class to test File Context Statistics.\" instead of File Context add link to FileContext.\n# FCStatisticsBaseTest - why do you need exact URI from getSchemeAuthorityUri()? The API handles any URI by picking scheme and authority right?\n# TestLocalFsFCStatistics - please add comment on why you are doing blockSize + 12? Also please change the class javadoc to indicate that this is testing stats for LocalFs.\n\nUploaded a new patch addressing the comments, submitted for Hudson tests.\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12469962/HADOOP-6432-trunk.6.patch\n  against trunk revision 1065959.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//testReport/\nFindbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://hudson.apache.org/hudson/job/PreCommit-HADOOP-Build/214//console\n\nThis message is automatically generated.\njavadoc for rename method in FileContext was reformatted in previous patch.\n+1 for the patch.\nThe only change in the last patch and the previous one was in javadoc. I ran javadoc manually to verify.\nI just committed this.\nIntegrated in Hadoop-Common-trunk-Commit #494 (See [https://hudson.apache.org/hudson/job/Hadoop-Common-trunk-Commit/494/])\n    HADOOP-6432. Add Statistics support in FileContext. Contributed by jitendra.\n\n", "issueSearchSentences": ["# FileContext#clearStatistics() - javadoc should indicate this method clears statistics for all the file systems.", "# AbstractFileSystem and FileContext printStatistics() prints the statistics using System.out.println.", "# AbstractFileSystem.getAllStatistics - initialize statsMap size to that of STATISTICS_TABLE.", "The Statistics are statically stored inside AbstractFileSystem in a map that is keyed on URI.", "# minor: remove empty line changes before AbstractFileSystem#getStatistics() method."], "issueSearchScores": [0.901214063167572, 0.5093007683753967, 0.5025602579116821, 0.49778980016708374, 0.49478214979171753]}
{"aId": 110, "code": "public static int checkBufferSize(CryptoCodec codec, int bufferSize) {\n    Preconditions.checkArgument(bufferSize >= MIN_BUFFER_SIZE, \n        \"Minimum value of buffer size is \" + MIN_BUFFER_SIZE + \".\");\n    return bufferSize - bufferSize % codec.getAlgorithmBlockSize();\n  }", "comment": " Check and floor buffer size", "issueId": "HADOOP-10632", "issueStringList": ["Minor improvements to Crypto input and output streams", "Minor follow up feedback on the crypto streams", "Yi, nice work.", "following some minor comments:", "All crypto classes should be annotated as Private has Hadoop is not in the business of exposing crypto APIs as an available crypto library.", "JCEAESCTREncryptor/JCEAESCTRDecryptor can be merged into a single class taking the cipher-mode as constructor param.", "and the encrypt/decrypt would delegate to a single process() method.", "AESCTRCryptoCodec#calculateIV() the IV calculation can be done much more efficiently doing some byte shifting:", "{code}", "private static final int CTR_OFFSET = 8;", "...", "System.arraycopy(initIV, 0, IV, 0, 8);", "long l = (initIV[CTR_OFFSET + 0] << 56)", "+ ((initIV[CTR_OFFSET + 1] & 0xFF) << 48)", "+ ((initIV[CTR_OFFSET + 2] & 0xFF) << 40)", "+ ((initIV[CTR_OFFSET + 3] & 0xFF) << 32)", "+ ((initIV[CTR_OFFSET + 4] & 0xFF) << 24)", "+ ((initIV[CTR_OFFSET + 5] & 0xFF) << 16)", "+ ((initIV[CTR_OFFSET + 6] & 0xFF) << 8)", "+ (initIV[CTR_OFFSET + 7] & 0xFF);", "l += counter;", "IV[CTR_OFFSET + 0] = (byte) (l >>> 56);", "IV[CTR_OFFSET + 1] = (byte) (l >>> 48);", "IV[CTR_OFFSET + 2] = (byte) (l >>> 40);", "IV[CTR_OFFSET + 3] = (byte) (l >>> 32);", "IV[CTR_OFFSET + 4] = (byte) (l >>> 24);", "IV[CTR_OFFSET + 5] = (byte) (l >>> 16);", "IV[CTR_OFFSET + 6] = (byte) (l >>> 8);", "IV[CTR_OFFSET + 7] = (byte) (l);", "{code}", "CryptoInputStream/CryptoOutputStream, besides the MIN_BUFFER_SIZE check we could floor the specified buffer size to a multiple of the CryptoCodec#getAlgorithmBlockSize()", "CryptoInputStream/CryptoOutputStream, we should clone the key & initIV as well.", "CryptoInputStream#read(), no need for doing {{if (usingByteBufferRead.booleanValue())}}, just do {{if (usingByteBufferRead)}}, 2 places.", "CryptoInputStream#readFromUnderlyingStream(), it would be more intuitive to read if the inBuffer is passed as  parameter.", "CryptoInputStream, comment \"\\{@link #org.apache.hadoop.fs.ByteBufferReadable\\}\" should not have the '#'", "CryptoInputStream#decrypt(long position, ...) method, given that this method does not change the current position of the stream, wouldn\u2019t be simpler to create a new decryptor and use a different set of input/output buffers without touching the stream ones?", "We could also use instance vars for them and init them the first time this method is called (if it is).", "Thanks [~tucu00], I will improve them and respond to you later  :-)", "[~tucu00], thanks for your nice comments.", "The new patch includes update for all your comments except the last item which I want to discuss with you.", "{quote}", "CryptoInputStream#decrypt(long position, ...) method, given that this method does not change the current position of the stream, wouldn\u2019t be simpler to create a new decryptor and use a different set of input/output buffers without touching the stream ones?", "We could also use instance vars for them and init them the first time this method is called (if it is).", "{quote}", "I think they each have their advantages.", "The approach here, doesn\u2019t touch the stream ones, needs to create a new decryptor, and different set of input/output buffers, key, iv and some other instance vars.", "The code logic of original one is not complicated too, it needs to restore the {{outBuffer}} and {{decryptor}}, but much less code.", "Alejandro, I\u2019d personally like to keep the original one.", "Do you find some potential issue?", "Hi Alejandro, in the new patch, for {{CryptoInputStream#decrypt(long position, ...)}} method, I use a different output buffer to avoid restoring {{outBuffer}} which may cause few performance issue of bytes copy.", "For decryptor, still uses the existing one.", "Thanks Yi, looks good, a few follow ups for your consideration:", "CryptoInputStream#freeBuffers(), not sure we should tap into SUN internal APIs here.", "do we gain something by cleaning up the buffers?", "And if we do, we should first check that the DB  is a sun.nio.ch.DB instance.", "(Sorry, I\u2019ve missed this one in my first passed)", "CryptoInputStream#decrypt(), you changed the signature to take the outBuffer as param, wouldn\u2019t make sense to take both input and output buffers as parameters then, then the usage form the read-pos would be simpler.", "What I\u2019m thinking is that the read-pos/readFully should leave alone all instance vars related to normal stream reading and use a complete different set of vars that are instantiated on their first use and reset before every use.", "CryptoInputStream, read-pos() and readFully(), wouldn\u2019t we have to consider the padding based on the requested pos there?", "If so, the decrypt(), following previous comment, would have to receive the padding as param as well, no?", "On failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple.", "Also, the {{CryptoCodec}} has some javadoc links that are incorrect, when referring to classes, don't prefix the class name with #.", "Thanks Alejandro for nice review, the new patch includes update for your comments.", "{quote}", "CryptoInputStream#freeBuffers(), not sure we should tap into SUN internal APIs here.", "do we gain something by cleaning up the buffers?", "And if we do, we should first check that the DB is a sun.nio.ch.DB instance.", "(Sorry, I\u2019ve missed this one in my first passed)", "{quote}", "Let's add ByteBuffer instance check.", "{quote}", "CryptoInputStream#decrypt(), you changed the signature to take the outBuffer as param, wouldn\u2019t make sense to take both input and output buffers as parameters then, then the usage form the read-pos would be simpler.", "What I\u2019m thinking is that the read-pos/readFully should leave alone all instance vars related to normal stream reading and use a complete different set of vars that are instantiated on their first use and reset before every use.", "CryptoInputStream, read-pos() and readFully(), wouldn\u2019t we have to consider the padding based on the requested pos there?", "If so, the decrypt(), following previous comment, would have to receive the padding as param as well, no?", "{quote}", "Now we use a complete different set of vars for read-pos() and readFully(), and they are thread-safe.", "{quote}", "On failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple.", "{quote}", "OK, let's floor the buffer size.", "{quote}", "Also, the CryptoCodec has some javadoc links that are incorrect, when referring to classes, don't prefix the class name with #.", "{quote}", "Right, let's update it.", "Yi, looks very good, also how the read-pos/readFully is being handled to be thread-safe.", "2 final comments and after your opinion on those the patch is ready for commit:", "the {{getBufferSize/freeDB/freeBuffers/checkBufferSize}} methods are duplicated in the both crypto input/output stream, wouldn't make sense to move them to a {{CryptoStreamUtils}} class not to dup code?", "For read-pos/readFully logic, similarly to the {{bufferPool}} for BBs, shouldn't we have a {{decryptorPool}}?", "As the decryptor creates a cipher and this triggers a JCE selection and all that stuff in, that may be expensive/synchronized.", "Hi Alejandro, thanks for your comments.", "The new patch includes update for your comments.", "{quote}", "how the read-pos/readFully is being handled to be thread-safe.", "{quote}", "We use method local vars for read-pos/readFully.", "{quote}", "the {{getBufferSize/freeDB/freeBuffers/checkBufferSize}} methods are duplicated in the both crypto input/output stream, wouldn't make sense to move them to a CryptoStreamUtils class not to dup code?", "{quote}", "Right, let's reuse them in {{CryptoStreamUtils}}", "{quote}", "For read-pos/readFully logic, similarly to the bufferPool for BBs, shouldn't we have a decryptorPool?", "As the decryptor creates a cipher and this triggers a JCE selection and all that stuff in, that may be expensive/synchronized.", "{quote}", "Right, let's add a {{decryptorPool}}", "LGTM, +1 after the following minor corrections, the {{CryptoStreamUtils}} and {{JCEAESCTRCryptoCodec}} classes should be annotated as Private.", "Thanks Alejandro, update the patch.", "+1", "Committed to branch."], "SplitGT": [" Check and floor buffer size"], "issueString": "Minor improvements to Crypto input and output streams\nMinor follow up feedback on the crypto streams\nYi, nice work. following some minor comments:\n\nAll crypto classes should be annotated as Private has Hadoop is not in the business of exposing crypto APIs as an available crypto library.\n\nJCEAESCTREncryptor/JCEAESCTRDecryptor can be merged into a single class taking the cipher-mode as constructor param. and the encrypt/decrypt would delegate to a single process() method.\n\nAESCTRCryptoCodec#calculateIV() the IV calculation can be done much more efficiently doing some byte shifting:\n\n{code}\n  private static final int CTR_OFFSET = 8;\n...\n    System.arraycopy(initIV, 0, IV, 0, 8);\n    long l = (initIV[CTR_OFFSET + 0] << 56)\n        + ((initIV[CTR_OFFSET + 1] & 0xFF) << 48)\n        + ((initIV[CTR_OFFSET + 2] & 0xFF) << 40)\n        + ((initIV[CTR_OFFSET + 3] & 0xFF) << 32)\n        + ((initIV[CTR_OFFSET + 4] & 0xFF) << 24)\n        + ((initIV[CTR_OFFSET + 5] & 0xFF) << 16)\n        + ((initIV[CTR_OFFSET + 6] & 0xFF) << 8)\n        + (initIV[CTR_OFFSET + 7] & 0xFF);\n    l += counter;\n    IV[CTR_OFFSET + 0] = (byte) (l >>> 56);\n    IV[CTR_OFFSET + 1] = (byte) (l >>> 48);\n    IV[CTR_OFFSET + 2] = (byte) (l >>> 40);\n    IV[CTR_OFFSET + 3] = (byte) (l >>> 32);\n    IV[CTR_OFFSET + 4] = (byte) (l >>> 24);\n    IV[CTR_OFFSET + 5] = (byte) (l >>> 16);\n    IV[CTR_OFFSET + 6] = (byte) (l >>> 8);\n    IV[CTR_OFFSET + 7] = (byte) (l);\n{code}\n\nCryptoInputStream/CryptoOutputStream, besides the MIN_BUFFER_SIZE check we could floor the specified buffer size to a multiple of the CryptoCodec#getAlgorithmBlockSize()\n\nCryptoInputStream/CryptoOutputStream, we should clone the key & initIV as well.\n\nCryptoInputStream#read(), no need for doing {{if (usingByteBufferRead.booleanValue())}}, just do {{if (usingByteBufferRead)}}, 2 places.\n\nCryptoInputStream#readFromUnderlyingStream(), it would be more intuitive to read if the inBuffer is passed as  parameter.\n\nCryptoInputStream, comment \"\\{@link #org.apache.hadoop.fs.ByteBufferReadable\\}\" should not have the '#'\n\nCryptoInputStream#decrypt(long position, ...) method, given that this method does not change the current position of the stream, wouldn\u2019t be simpler to create a new decryptor and use a different set of input/output buffers without touching the stream ones? We could also use instance vars for them and init them the first time this method is called (if it is).\n\nThanks [~tucu00], I will improve them and respond to you later  :-) \n[~tucu00], thanks for your nice comments.  The new patch includes update for all your comments except the last item which I want to discuss with you.\n\n{quote}\nCryptoInputStream#decrypt(long position, ...) method, given that this method does not change the current position of the stream, wouldn\u2019t be simpler to create a new decryptor and use a different set of input/output buffers without touching the stream ones? We could also use instance vars for them and init them the first time this method is called (if it is).\n{quote}\n\nI think they each have their advantages. The approach here, doesn\u2019t touch the stream ones, needs to create a new decryptor, and different set of input/output buffers, key, iv and some other instance vars.  \nThe code logic of original one is not complicated too, it needs to restore the {{outBuffer}} and {{decryptor}}, but much less code.\nAlejandro, I\u2019d personally like to keep the original one.  Do you find some potential issue? \n\nHi Alejandro, in the new patch, for {{CryptoInputStream#decrypt(long position, ...)}} method, I use a different output buffer to avoid restoring {{outBuffer}} which may cause few performance issue of bytes copy. For decryptor, still uses the existing one.\nThanks Yi, looks good, a few follow ups for your consideration:\n\nCryptoInputStream#freeBuffers(), not sure we should tap into SUN internal APIs here. do we gain something by cleaning up the buffers? And if we do, we should first check that the DB  is a sun.nio.ch.DB instance. (Sorry, I\u2019ve missed this one in my first passed)\n\nCryptoInputStream#decrypt(), you changed the signature to take the outBuffer as param, wouldn\u2019t make sense to take both input and output buffers as parameters then, then the usage form the read-pos would be simpler. What I\u2019m thinking is that the read-pos/readFully should leave alone all instance vars related to normal stream reading and use a complete different set of vars that are instantiated on their first use and reset before every use.\n\nCryptoInputStream, read-pos() and readFully(), wouldn\u2019t we have to consider the padding based on the requested pos there?  If so, the decrypt(), following previous comment, would have to receive the padding as param as well, no?\n\nOn failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple. \n\nAlso, the {{CryptoCodec}} has some javadoc links that are incorrect, when referring to classes, don't prefix the class name with #.\n\nThanks Alejandro for nice review, the new patch includes update for your comments.\n\n{quote}\nCryptoInputStream#freeBuffers(), not sure we should tap into SUN internal APIs here. do we gain something by cleaning up the buffers? And if we do, we should first check that the DB is a sun.nio.ch.DB instance. (Sorry, I\u2019ve missed this one in my first passed)\n{quote}\n\nLet's add ByteBuffer instance check.\n\n{quote}\nCryptoInputStream#decrypt(), you changed the signature to take the outBuffer as param, wouldn\u2019t make sense to take both input and output buffers as parameters then, then the usage form the read-pos would be simpler. What I\u2019m thinking is that the read-pos/readFully should leave alone all instance vars related to normal stream reading and use a complete different set of vars that are instantiated on their first use and reset before every use.\n\nCryptoInputStream, read-pos() and readFully(), wouldn\u2019t we have to consider the padding based on the requested pos there? If so, the decrypt(), following previous comment, would have to receive the padding as param as well, no?\n{quote}\n\nNow we use a complete different set of vars for read-pos() and readFully(), and they are thread-safe.\n\n{quote}\nOn failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple. \n{quote}\nOK, let's floor the buffer size.\n\n{quote}\nAlso, the CryptoCodec has some javadoc links that are incorrect, when referring to classes, don't prefix the class name with #.\n{quote}\nRight, let's update it.\nYi, looks very good, also how the read-pos/readFully is being handled to be thread-safe.\n\n2 final comments and after your opinion on those the patch is ready for commit:\n\n* the {{getBufferSize/freeDB/freeBuffers/checkBufferSize}} methods are duplicated in the both crypto input/output stream, wouldn't make sense to move them to a {{CryptoStreamUtils}} class not to dup code?\n\n* For read-pos/readFully logic, similarly to the {{bufferPool}} for BBs, shouldn't we have a {{decryptorPool}}? As the decryptor creates a cipher and this triggers a JCE selection and all that stuff in, that may be expensive/synchronized.\n \nHi Alejandro, thanks for your comments. The new patch includes update for your comments.\n\n{quote}\nhow the read-pos/readFully is being handled to be thread-safe.\n{quote}\nWe use method local vars for read-pos/readFully.\n\n{quote}\nthe {{getBufferSize/freeDB/freeBuffers/checkBufferSize}} methods are duplicated in the both crypto input/output stream, wouldn't make sense to move them to a CryptoStreamUtils class not to dup code?\n{quote}\n\nRight, let's reuse them in {{CryptoStreamUtils}}\n\n\n{quote}\nFor read-pos/readFully logic, similarly to the bufferPool for BBs, shouldn't we have a decryptorPool? As the decryptor creates a cipher and this triggers a JCE selection and all that stuff in, that may be expensive/synchronized.\n{quote}\n\nRight, let's add a {{decryptorPool}}\nLGTM, +1 after the following minor corrections, the {{CryptoStreamUtils}} and {{JCEAESCTRCryptoCodec}} classes should be annotated as Private.\nThanks Alejandro, update the patch.\n+1\nCommitted to branch.\n", "issueSearchSentences": ["CryptoInputStream/CryptoOutputStream, besides the MIN_BUFFER_SIZE check we could floor the specified buffer size to a multiple of the CryptoCodec#getAlgorithmBlockSize()", "Let's add ByteBuffer instance check.", "OK, let's floor the buffer size.", "On failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple.", "On failing the Crypto streams constructors if the buffer size is not a multiple of the block size, I\u2019ve thought about that initially, but that seemed a too strong requirement to me, that is why I suggested flooring the request buffer to the closest previous block size multiple."], "issueSearchScores": [0.7344162464141846, 0.5911673307418823, 0.5874337553977966, 0.5796791911125183, 0.5796791911125183]}
{"aId": 112, "code": "@VisibleForTesting\n  ReplicaInfo moveReplicaToVolumeOnSameMount(ExtendedBlock block,\n      ReplicaInfo replicaInfo,\n      FsVolumeReference volumeRef) throws IOException {\n    FsVolumeImpl targetVolume = (FsVolumeImpl) volumeRef.getVolume();\n    // Move files to temp dir first\n    ReplicaInfo newReplicaInfo = targetVolume.hardLinkBlockToTmpLocation(block,\n        replicaInfo);\n    return newReplicaInfo;\n  }", "comment": " Shortcut to use hardlink to move blocks on same mount.", "issueId": "HDFS-15549", "issueStringList": ["Use Hardlink to move replica between DISK and ARCHIVE storage if on same filesystem mount", "When moving blocks between DISK/ARCHIVE, we should prefer the volume on the same underlying filesystem and use \"rename\" instead of \"copy\" to save IO.", "[~LeonG] , have you encountered some difficult problems?", "[~jianghuazhu]\u00a0I think this will depend on the implementation of\u00a0HDFS-15548", "I have put a draft patch with some tests.", "I tried to use hardlink to ensure durability when renaming the block files.", "Directly using filesystem \"rename\" can cause data loss when only block/metadata file is moved during datanode restart.", "[~hexiaoqiao] [~jingzhao] [~jianghuazhu]\u00a0Could you please take a look and let me know your thoughts.", "Thank you for working on this, [~LeonG]!", "I went through the current patch and it looks good to me in general.", "Please feel free to upload the complete version when it's ready.", "+1.", "Thank you for the contribution, [~LeonG]!", "thanks for the review [~jingzhao]\u00a0!"], "SplitGT": [" Shortcut to use hardlink to move blocks on same mount."], "issueString": "Use Hardlink to move replica between DISK and ARCHIVE storage if on same filesystem mount\nWhen moving blocks between DISK/ARCHIVE, we should prefer the volume on the same underlying filesystem and use \"rename\" instead of \"copy\" to save IO.\n[~LeonG] , have you encountered some difficult problems?\r\n\r\n\u00a0\n[~jianghuazhu]\u00a0I think this will depend on the implementation of\u00a0HDFS-15548\nI have put a draft patch with some tests.\r\n\r\nI tried to use hardlink to ensure durability when renaming the block files. Directly using filesystem \"rename\" can cause data loss when only block/metadata file is moved during datanode restart.\r\n\r\n\u00a0\r\n\r\n[~hexiaoqiao] [~jingzhao] [~jianghuazhu]\u00a0Could you please take a look and let me know your thoughts.\nThank you for working on this, [~LeonG]! I went through the current patch and it looks good to me in general. Please feel free to upload the complete version when it's ready.\n+1. Thank you for the contribution, [~LeonG]!\nthanks for the review [~jingzhao]\u00a0!\n", "issueSearchSentences": ["When moving blocks between DISK/ARCHIVE, we should prefer the volume on the same underlying filesystem and use \"rename\" instead of \"copy\" to save IO.", "Use Hardlink to move replica between DISK and ARCHIVE storage if on same filesystem mount", "I tried to use hardlink to ensure durability when renaming the block files.", "Directly using filesystem \"rename\" can cause data loss when only block/metadata file is moved during datanode restart.", "Thank you for working on this, [~LeonG]!"], "issueSearchScores": [0.6555226445198059, 0.5952630043029785, 0.4882081151008606, 0.47524064779281616, 0.33393871784210205]}
{"aId": 113, "code": "public int getEcChecksumSocketTimeout() {\n    return ecChecksumSocketTimeout;\n  }", "comment": " Returns socket timeout for computing the checksum of EC blocks", "issueId": "HDFS-15650", "issueStringList": ["Make the socket timeout for computing checksum of striped blocks configurable", "Regarding the DataNode tries to get the checksum of EC internal blocks from another DataNode for computing the checksum of striped blocks, the timeout is hard-coded now, but it should be configurable.", "PR: [https://github.com/apache/hadoop/pull/2414]", "Note that the timeout ms will be changed from 3000 to 60000 (=dfs.client.socket-timeout) in default.", "Oh, I noticed we needed to change here too: [https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java#L626].", "However there is no way to get the configuration of timeout value in DN from client (I think) now... Maybe that's why it is hard-coded?", "In that case, you can use DFSClientConf.getSocketTimeout() (dfs.client.socket-timeout) which has the default of 60 seconds too.", "[~weichiu]\u00a0Yes, but I think we need to use the same value between client and DN because there is a possibility that the scenario below is happened:", "If the client is set `dfs.client.socket-timeout` to 3000 and the DN uses the default value (60000), the client disconnects the connection despite the DN tries to fetch checksums and it continues the process even if no one want.", "I think it may\u00a0exhaust the handler thread if there is a lot of request like above... WDYT?", "Sounds like a possible scenario... thanks for sharing that.", "I think in the current DataNode design there's no good way to prevent that from happening in general.", "A workaround is to increase DN thread pool size.", "But I am aware of other cases where DN exhausts thread pool even to the point where it exceeds maximum number of open file descriptors.", "A quick remedy for this is to update the descriptionion of hdfs-default.xml", "{noformat}", "<property>", "<name>dfs.client.socket-timeout</name>", "<value>60000</value>", "<description>", "Default timeout value in milliseconds for all sockets.", "</description>", "</property>", "{noformat}", "and add reminders that in an erasure coded cluster it is recommended to make dfs.client.socket-timeout consistent on both client and DNs.", "[~weichiu]\u00a0Thanks!", "I see... :sad:", "Then, Just idea, how about using the new configuration, which is named `dfs.checksum.ec.socket-timeout` (or `dfs.ec.checksum.socket-timeout`?).", "It will be added in hdfs-default.xml like below:", "{noformat}", "<property>", "<name>dfs.checksum.ec.socket-timeout</name>", "<value>3000</value>", "<description>", "Default timeout value in milliseconds for computing the checksum of striped blocks.", "Recommended to set the same value between client and DNs in an erasure coded cluster because mismatching may cause exhausting handler threads.", "</description>", "</property>{noformat}", "I think it is easier for administrators to notice the risk because they needs to see here for understanding the configuration, and the confusion which is caused by changing behavior may be depressed since the default value is the same with the current value.", "[~yhaya] [~weichiu]\u00a0Hi!", "In our practice, when there are a large number of ec checksum scenarios (such as distcp with checksum), there will be many socket timeout, and generally retrying is normal.", "(Note:\u00a0 -HDFS-15709-\u00a0has been merged).", "I think it makes sense to fix the hard-code.", "New config `dfs.checksum.ec.socket-timeout` looks good.", "Do you have any plan to fix this issue?", "Thanks!", "Ok \ud83d\udc4d", "I honestly don't remember what held me back but I am okay to use a ec specific socket timeout given the current EC architecture.", "[~wanghongbing] [~weichiu]", "Thanks for your comment, and sorry for the late response.", "Let me update this.", "I just updated the PR now.", "Introduced the new configuration (`dfs.checksum.ec.socket-timeout`) instead of using `dfs.client.socket-timeout`.", "Please take a look when you have time.", "Thanks!", "Thanks [~yhaya]\u00a0the PR is merged and cherrypicked to trunk, branch-3.3 and branch-3.2."], "SplitGT": [" Returns socket timeout for computing the checksum of EC blocks"], "issueString": "Make the socket timeout for computing checksum of striped blocks configurable\nRegarding the DataNode tries to get the checksum of EC internal blocks from another DataNode for computing the checksum of striped blocks, the timeout is hard-coded now, but it should be configurable.\nPR: [https://github.com/apache/hadoop/pull/2414]\nNote that the timeout ms will be changed from 3000 to 60000 (=dfs.client.socket-timeout) in default.\nOh, I noticed we needed to change here too: [https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java#L626].\r\n However there is no way to get the configuration of timeout value in DN from client (I think) now... Maybe that's why it is hard-coded?\nIn that case, you can use DFSClientConf.getSocketTimeout() (dfs.client.socket-timeout) which has the default of 60 seconds too.\n[~weichiu]\u00a0Yes, but I think we need to use the same value between client and DN because there is a possibility that the scenario below is happened:\r\n * If the client is set `dfs.client.socket-timeout` to 3000 and the DN uses the default value (60000), the client disconnects the connection despite the DN tries to fetch checksums and it continues the process even if no one want.\r\n\r\nI think it may\u00a0exhaust the handler thread if there is a lot of request like above... WDYT?\nSounds like a possible scenario... thanks for sharing that. I think in the current DataNode design there's no good way to prevent that from happening in general. A workaround is to increase DN thread pool size. But I am aware of other cases where DN exhausts thread pool even to the point where it exceeds maximum number of open file descriptors. \r\n\r\nA quick remedy for this is to update the descriptionion of hdfs-default.xml\r\n{noformat}\r\n<property>\r\n  <name>dfs.client.socket-timeout</name>\r\n  <value>60000</value>\r\n  <description>\r\n    Default timeout value in milliseconds for all sockets.\r\n  </description>\r\n</property>\r\n{noformat}\r\n\r\nand add reminders that in an erasure coded cluster it is recommended to make dfs.client.socket-timeout consistent on both client and DNs.\r\n\n[~weichiu]\u00a0Thanks! I see... :sad:\r\n\r\nThen, Just idea, how about using the new configuration, which is named `dfs.checksum.ec.socket-timeout` (or `dfs.ec.checksum.socket-timeout`?). It will be added in hdfs-default.xml like below:\r\n{noformat}\r\n <property>\r\n  <name>dfs.checksum.ec.socket-timeout</name>\r\n  <value>3000</value>\r\n  <description>\r\n    Default timeout value in milliseconds for computing the checksum of striped blocks.\r\n    Recommended to set the same value between client and DNs in an erasure coded cluster because mismatching may cause exhausting handler threads.\r\n  </description>\r\n</property>{noformat}\r\nI think it is easier for administrators to notice the risk because they needs to see here for understanding the configuration, and the confusion which is caused by changing behavior may be depressed since the default value is the same with the current value.\n[~yhaya] [~weichiu]\u00a0Hi! In our practice, when there are a large number of ec checksum scenarios (such as distcp with checksum), there will be many socket timeout, and generally retrying is normal. (Note:\u00a0 -HDFS-15709-\u00a0has been merged).\u00a0I think it makes sense to fix the hard-code.\u00a0\r\n\r\nNew config `dfs.checksum.ec.socket-timeout` looks good.\u00a0Do you have any plan to fix this issue?\u00a0\r\n\r\nThanks!\nOk \ud83d\udc4d \r\nI honestly don't remember what held me back but I am okay to use a ec specific socket timeout given the current EC architecture.\n[~wanghongbing] [~weichiu]\r\nThanks for your comment, and sorry for the late response. Let me update this.\nI just updated the PR now. Introduced the new configuration (`dfs.checksum.ec.socket-timeout`) instead of using `dfs.client.socket-timeout`.\r\nPlease take a look when you have time. Thanks!\nThanks [~yhaya]\u00a0the PR is merged and cherrypicked to trunk, branch-3.3 and branch-3.2.\n", "issueSearchSentences": ["In that case, you can use DFSClientConf.getSocketTimeout() (dfs.client.socket-timeout) which has the default of 60 seconds too.", "I honestly don't remember what held me back but I am okay to use a ec specific socket timeout given the current EC architecture.", "Default timeout value in milliseconds for computing the checksum of striped blocks.", "Default timeout value in milliseconds for all sockets.", "Make the socket timeout for computing checksum of striped blocks configurable"], "issueSearchScores": [0.6328260898590088, 0.6163784265518188, 0.6075822710990906, 0.5959673523902893, 0.5763044357299805]}
