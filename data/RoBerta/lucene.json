{"aId": 2, "code": "public SnowballAnalyzer(Version matchVersion, String name, Set<?> stopWords) {\n    this(matchVersion, name);\n    stopSet = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion,\n        stopWords));\n  }", "comment": " Builds the named analyzer with the given stop words.", "issueId": "LUCENE-2165", "issueStringList": ["SnowballAnalyzer lacks a constructor that takes a Set of Stop Words", "As discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words.", "However, there is no constructor which accepts a Set, there's only the original String[] one", "This is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET).", "So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer", "I would suggest that a constructor is added to SnowballAnalyzer which accepts a Set.", "Not sure if the old String[] one should be deprecated or not.", "A sample patch against 2.9.1 to add the constructor is:", "SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000", "+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000", "@@ -67,6 +67,12 @@", "stopSet = StopFilter.makeStopSet(stopWords);", "}", "+  /** Builds the named analyzer with the given stop words.", "*/", "+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {", "+    this(matchVersion, name);", "+    stopSet = stopWordsSet;", "+  }", "+", "And in 3.0 its simply Set<?>.", "if no one objects, will commit tomorrow", "+1 looks good.", "In my opinion backport is not needed (to 2.9) but maybe for 3.0?", "Because in 3.0 there is no longer a STOP_WORD-Array in StopFilter.", "Robert, I wonder if you want to make stopSet final and assign the empty set to it in the ctor without stopwords.", "I always prefer an empty collection over null so you can simply replace the null checks with stopSet.isEmpty().", "-- kind of unrelated and we can do in sep. issue if you want.", "Committed revision 891209."], "SplitGT": [" Builds the named analyzer with the given stop words."], "issueString": "SnowballAnalyzer lacks a constructor that takes a Set of Stop Words\nAs discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words. However, there is no constructor which accepts a Set, there's only the original String[] one\n\nThis is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET). So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer\n\nI would suggest that a constructor is added to SnowballAnalyzer which accepts a Set. Not sure if the old String[] one should be deprecated or not.\n\nA sample patch against 2.9.1 to add the constructor is:\n\n\n--- SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000\n+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000\n@@ -67,6 +67,12 @@\n     stopSet = StopFilter.makeStopSet(stopWords);\n   }\n \n+  /** Builds the named analyzer with the given stop words. */\n+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {\n+    this(matchVersion, name);\n+    stopSet = stopWordsSet;\n+  }\n+\n\nAnd in 3.0 its simply Set<?>.\nif no one objects, will commit tomorrow\n+1 looks good. In my opinion backport is not needed (to 2.9) but maybe for 3.0? Because in 3.0 there is no longer a STOP_WORD-Array in StopFilter.\nRobert, I wonder if you want to make stopSet final and assign the empty set to it in the ctor without stopwords. I always prefer an empty collection over null so you can simply replace the null checks with stopSet.isEmpty(). -- kind of unrelated and we can do in sep. issue if you want.\n\nCommitted revision 891209.\n", "issueSearchSentences": ["+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {", "+    this(matchVersion, name);", "SnowballAnalyzer lacks a constructor that takes a Set of Stop Words", "As discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words.", "This is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET)."], "issueSearchScores": [0.916084349155426, 0.6615708470344543, 0.5940443277359009, 0.5826423764228821, 0.4850054979324341]}
{"aId": 3, "code": "public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n\n    try {\n      // Best effort up front validations\n      for (CodecReader leaf : readers) {\n        validateMergeReader(leaf);\n        for (FieldInfo fi : leaf.getFieldInfos()) {\n          globalFieldNumberMap.verifyFieldInfo(fi);\n        }\n        numDocs += leaf.numDocs();\n      }\n      testReserveDocs(numDocs);\n\n      synchronized (this) {\n        ensureOpen();\n        if (merges.areEnabled() == false) {\n          throw new AlreadyClosedException(\n              \"this IndexWriter is closed. Cannot execute addIndexes(CodecReaders...) API\");\n        }\n      }\n\n      MergePolicy mergePolicy = config.getMergePolicy();\n      MergePolicy.MergeSpecification spec = mergePolicy.findMerges(readers);\n      boolean mergeSuccess = false;\n      if (spec != null && spec.merges.size() > 0) {\n        try {\n          spec.merges.forEach(addIndexesMergeSource::registerMerge);\n          mergeScheduler.merge(addIndexesMergeSource, MergeTrigger.ADD_INDEXES);\n          spec.await();\n          mergeSuccess =\n              spec.merges.stream().allMatch(m -> m.hasCompletedSuccessfully().orElse(false));\n        } finally {\n          if (mergeSuccess == false) {\n            for (MergePolicy.OneMerge merge : spec.merges) {\n              if (merge.getMergeInfo() != null) {\n                deleteNewFiles(merge.getMergeInfo().files());\n              }\n            }\n          }\n        }\n      } else {\n        if (infoStream.isEnabled(\"IW\")) {\n          if (spec == null) {\n            infoStream.message(\n                \"addIndexes(CodecReaders...)\",\n                \"received null mergeSpecification from MergePolicy. No indexes to add, returning..\");\n          } else {\n            infoStream.message(\n                \"addIndexes(CodecReaders...)\",\n                \"received empty mergeSpecification from MergePolicy. No indexes to add, returning..\");\n          }\n        }\n        return docWriter.getNextSequenceNumber();\n      }\n\n      if (mergeSuccess) {\n        List<SegmentCommitInfo> infos = new ArrayList<>();\n        long totalDocs = 0;\n        for (MergePolicy.OneMerge merge : spec.merges) {\n          totalDocs += merge.totalMaxDoc;\n          if (merge.getMergeInfo() != null) {\n            infos.add(merge.getMergeInfo());\n          }\n        }\n\n        synchronized (this) {\n          if (infos.isEmpty() == false) {\n            boolean registerSegmentSuccess = false;\n            try {\n              ensureOpen();\n              // Reserve the docs, just before we update SIS:\n              reserveDocs(totalDocs);\n              registerSegmentSuccess = true;\n            } finally {\n              if (registerSegmentSuccess == false) {\n                for (SegmentCommitInfo sipc : infos) {\n                  // Safe: these files must exist\n                  deleteNewFiles(sipc.files());\n                }\n              }\n            }\n            segmentInfos.addAll(infos);\n            checkpoint();\n          }\n          seqNo = docWriter.getNextSequenceNumber();\n        }\n      } else {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\n              \"addIndexes(CodecReaders...)\", \"failed to successfully merge all provided readers.\");\n        }\n        for (MergePolicy.OneMerge merge : spec.merges) {\n          if (merge.isAborted()) {\n            throw new MergePolicy.MergeAbortedException(\"merge was aborted.\");\n          }\n          Throwable t = merge.getException();\n          if (t != null) {\n            IOUtils.rethrowAlways(t);\n          }\n        }\n        // If no merge hit an exception, and merge was not aborted, but we still failed to add\n        // indexes, fail the API\n        throw new RuntimeException(\n            \"failed to successfully merge all provided readers in addIndexes(CodecReader...)\");\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n\n    maybeMerge();\n    return seqNo;\n  }", "comment": " Merges the provided indexes into this index.", "issueId": "LUCENE-10216", "issueStringList": ["Add concurrency to addIndexes(CodecReader\u2026) API", "I work at Amazon Product Search, and we use Lucene to power search for the e-commerce platform.", "I\u2019m working on a project that involves applying metadata+ETL transforms and indexing documents on n different _indexing_ boxes, combining them into a single index on a separate _reducer_ box, and making it available for queries on m different _search_ boxes (replicas).", "Segments are asynchronously copied from indexers to reducers to searchers as they become available for the next layer to consume.", "I am using the addIndexes API to combine multiple indexes into one on the reducer boxes.", "Since we also have taxonomy data, we need to remap facet field ordinals, which means I need to use the {{addIndexes(CodecReader\u2026)}} version of this API.", "The API leverages {{SegmentMerger.merge()}} to create segments with new ordinal values while also merging all provided segments in the process.", "_This is however a blocking call that runs in a single thread._ Until we have written segments with new ordinal values, we cannot copy them to searcher boxes, which increases the time to make documents available for search.", "I was playing around with the API by creating multiple concurrent merges, each with only a single reader, creating a concurrently running 1:1 conversion from old segments to new ones (with new ordinal values).", "We follow this up with non-blocking background merges.", "This lets us copy the segments to searchers and replicas as soon as they are available, and later replace them with merged segments as background jobs complete.", "On the Amazon dataset I profiled, this gave us around 2.5 to 3x improvement in addIndexes() time.", "Each call was given about 5 readers to add on average.", "This might be useful add to Lucene.", "We could create another {{addIndexes()}} API with a {{boolean}} flag for concurrency, that internally submits multiple merge jobs (each with a single reader) to the {{ConcurrentMergeScheduler}}, and waits for them to complete before returning.", "While this is doable from outside Lucene by using your thread pool, starting multiple addIndexes() calls and waiting for them to complete, I felt it needs some understanding of what addIndexes does, why you need to wait on the merge and why it makes sense to pass a single reader in the addIndexes API.", "Out of box support in Lucene could simplify this for folks a similar use case.", "One bit from this proposal I'm not fully comfortable with is the fact that Lucene would merge each reader independently when the flag is set.", "While I understand why this makes sense for Amazon product search since you want to have many segments to make search as concurrent as possible, this is also a bit wasteful since you'd be paying the cost of doing a merge but without getting the benefits of reducing the segment count.", "What about a slightly different proposal where merges would always be executed via the configured merge scheduler, and addIndexes(CodecReader[]) would get an additional {{boolean doWait}} parameter like {{IndexWriter#forceMerge}}?", "Then you could still do what you want by using {{ConcurrentMergeScheduler}}, passing codec readers one by one to {{CodecReader#addIndexes}} with {{doWait=false}} and get resulting remapped segments as quickly as possible.", "Yet Lucene would retain the current behaviour where {{addIndexes}} also tries to avoid doing wasteful singleton merges?", "Thanks for going through the proposal, [~jpountz].", "{quote}One bit from this proposal I'm not fully comfortable with is the fact that Lucene would merge each reader independently when the flag is set.", "{quote}", "I understand your concern, I am also a bit iffy about losing the opportunity to merge segments..", "Ideally, I would like to land on a sweet middle ground - not run single segment merges, but also not add all provided segments into a single merge.", "Leverage some concurrency within addIndexes, and let background merges bring the segment count down further.", "How do you feel about making this configurable in the API with a param that defines the number of segments merged together ({{MergeFactor?}}).", "We could make it flag to keep things simple for consumers, with values like {{ONE, THREAD_WIDE, and ALL,\u00a0}}where {{ONE}} = single segment merges, {{ALL}} = all segments in one merge, and {{THREAD_WIDE}} = each merge gets ({{readers.length/numThreads)}} number of segments.", "The default here could be {{ALL}} to retain current behavior.", "{quote}Then you could still do what you want by using\u00a0{{ConcurrentMergeScheduler}}, passing codec readers one by one to\u00a0{{CodecReader#addIndexes}}\u00a0with\u00a0{{doWait=false}}\u00a0and get resulting remapped segments as quickly as possible.", "{quote}", "The doWait flag would make the API non-blocking.", "But it would add additional steps for users to track when the merges triggered by addIndexes have completed.", "For segment replication, addIndexes is not useable until its merges complete.", "Merge within addIndexes(), is what creates segments with recomputed ordinal values.", "Until that is done, there are no segments available to copy to searcher (and replica) hosts.", "We also want to bring the segment count down to as low as possible at Amazon product search.", "The tradeoff we are making here, is to first go with a higher segment count to make documents quickly available for search, then run background merges and bring down the segment count.", "This is similar to the other variant of add indexes - the {{addIndexes(Directory[])}} API, which copies all segments from provided directories into current index dir and then triggers a background merge.", "I feel making segments available quickly would be useful for anyone who has multiple replica search hosts and uses segment replication.", "bq.", "How do you feel about making this configurable in the API with a param that defines the number of segments merged together", "I worry that it would make the API too complicated.", "Thinking about it more: Ideally addIndexes would do the right thing based on the configured MergePolicy (to know how to group CodecReaders together) and MergeScheduler (to know how many threads it may use) so that we could hopefully keep the same API as today and experts users like Amazon product search could use a MergePolicy that favors many segments for search concurrency, plus possibly a ConcurrentMergeScheduler that is allowed to use many threads, and you would get the desired behavior?", "This might be a high-hanging fruit though.", "I like the idea of handling everything via MergePolicy and MergeScheduler, without changing the API.", "One wrinkle is that the existing merge framework (policy and scheduler) is written to work with segments, while the API only has readers.", "I have a high level plan for handling this based on early code reading.", "Let me know your thoughts and if you have any ideas to simplify/generalize this.", "If it makes sense, I can follow it up with a draft PR...", "\u2013", "We could create a new API in MergePolicy that does findMerges using readers - {{{}findMerges(MergeTrigger mergeTrigger, CodecReaders[] readers, MergeContext mergeContext){}}}; and returns a {{MergeSpecification}} object, in which OneMerge objects are based on how the policy choses to merge these readers.", "Users can override this API based on the concurrency they'd like in addIndexes.", "Default impl.", "can just have a single {{OneMerge}} with all readers grouped together.", "While {{OneMerge}} accepts ({{{}List<SegmentCommitInfo> segments{}}}) in its constructor, the {{SegmentMerger}} actually works on readers.", "OneMerge converts segments to {{MergedReaders}} via {{{}initMergeReaders(){}}}.", "Perhaps we could add another constructor to OneMerge that directly initializes its mergedReaders?", "Once we have a mergeSpec from policy, we can either use the existing {{MergeSource}} in IW and just add each OneMerge to {{pendingMerges,}}\u00a0or create a new mergeSource.", "I think we should at least override the {{MergeSource.merge()}} called from addIndexes, as addIndexes seems to require less work post the actual merging than IndexWriter (for e.g.", "no segments are rendered obsolete after an addIndexes merge, so we don't need to update the segments in {{SegmentInfos.applyMergeChanges()}} after merge completes).", "We want to wait for all merges triggered by addIndexes to complete before we return, which we could do by calling {{await()}} on the MergeSpec returned by policy.", "I was also thinking that we could add a new {{MergeTrigger}} to identify merges triggered by addIndexes, and handle them separately in the merge policy (and possible scheduler?).", "We may not need it if the {{findMerges(..., readers)}} API is only used for addIndexes but it may be a good idea regardless.", "I like this plan (extending {{MergePolicy}} so it also has purview over how merging is done in {{{}addIndexes(CodecReader[]){}}}).", "Reference counting might get tricky, if {{OneMerge}} or {{IndexWriter}} holding completed {{OneMerge}} instances try to {{decRef}} readers.", "To improve testing we could create a new {{LuceneTestCase}} method to {{addIndexes}} from {{Directory[]}} that randomly does so with both impls and fix tests to sometimes use that for adding indices.", "I took a shot at the MergePolicy + MergeScheduler idea.", "Have raised a draft, work-in-progress PR - https://github.com/apache/lucene/pull/633, to get some high level feedback on the approach.", "It has existing tests passing, and is pending new tests specific to this change.", "Had some thoughts and questions about the transaction model for this API...", "Currently, it either succeeds and adds all provided readers, or fails and adds none of them.", "With a merge policy splitting provided readers into groups of smaller {{OneMerge}} objects, this is slightly harder to implement.", "A OneMerge on a subset of readers may complete in the background and add itself to writer segment infos, while some others running in parallel could fail.", "One approach could be to expose this failure information to user - the exception can contain the list of readers merged and pending.", "This could simplify the overall implementation.", "My current thoughts, however, are that the present transaction logic is important.", "It is hard for users to parse the exception message, figure out which readers are pending and retry them.", "As opposed to retrying an entire API call (with all the readers), which their upstream system probably understands as a single unit.", "However, I wanted to check if loosening the transaction model for this API is a palatable approach.", "To retain the all or none, single transaction model, I am thinking that we can join on all merges at the end of the {{addIndexes()}} API, and then write their segment info files in a common lock.", "Would like to hear more thoughts or suggestions on this.", "Updated the PR to retain transactionality, while using the MergePolicy and configured MergeScheduler to trigger merges for this API", "While making this change, I found an existing [TODO|https://github.com/apache/lucene/blob/main/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java#L3158-L3159], which I think we're able to address now..", "{code:java}", "TODO: somehow we should fix this merge so it's", "abortable so that IW.close(false) is able to stop it {code}", "Earlier,\u00a0{{addIndexes(CodecReader...)}}\u00a0triggered merges directly via SegmentMerger.", "As a result, the running merges could be tracked in\u00a0{{{}Set<SegmentMerger> runningAddIndexesMerges{}}}, but any pending merge operations could not be aborted preemptively.", "Now, merges are defined via {{MergePolicy.OneMerge}}\u00a0objects, and scheduled by the\u00a0{{{}MergeScheduler{}}}.", "We are able to leverage the OneMerge abstractions to abort any pending merges.", "They are aborted when IndexWriter is rolled back or closed.", "I think the PR is ready for review, with existing tests passing and added tests for new changes.", "{{OneMerge}} distribution is now provided by a new {{findMerges(CodecReaders[])}} API in {{{}MergePolicy{}}}, and executed by {{MergeScheduler}} threads.", "I've also modified the {{MockRandomMergePolicy}} to randomly pick a highly concurrent, (one segment per reader), {{findMerges(...)}} implementation 50% of the time.", "And confirmed manually that tests pass in both scenarios (this new impl., as well as the default impl.", "being picked) (thanks Michael McCandless for the suggestion).", "{quote}", "I've also modified the MockRandomMergePolicy to randomly pick a highly concurrent, (one segment per reader), findMerges(...) implementation 50% of the time.", "{quote}", "Is this going to make 50% of our test runs non-reproducible?", "Please, let's consider the need to reproduce failing runs and not pick such highly concurrent stuff for the unit tests.", "We should be able to test it another way."], "SplitGT": [" Merges the provided indexes into this index."], "issueString": "Add concurrency to addIndexes(CodecReader\u2026) API\nI work at Amazon Product Search, and we use Lucene to power search for the e-commerce platform. I\u2019m working on a project that involves applying metadata+ETL transforms and indexing documents on n different _indexing_ boxes, combining them into a single index on a separate _reducer_ box, and making it available for queries on m different _search_ boxes (replicas). Segments are asynchronously copied from indexers to reducers to searchers as they become available for the next layer to consume.\r\n\r\nI am using the addIndexes API to combine multiple indexes into one on the reducer boxes. Since we also have taxonomy data, we need to remap facet field ordinals, which means I need to use the {{addIndexes(CodecReader\u2026)}} version of this API. The API leverages {{SegmentMerger.merge()}} to create segments with new ordinal values while also merging all provided segments in the process.\r\n\r\n_This is however a blocking call that runs in a single thread._ Until we have written segments with new ordinal values, we cannot copy them to searcher boxes, which increases the time to make documents available for search.\r\n\r\nI was playing around with the API by creating multiple concurrent merges, each with only a single reader, creating a concurrently running 1:1 conversion from old segments to new ones (with new ordinal values). We follow this up with non-blocking background merges. This lets us copy the segments to searchers and replicas as soon as they are available, and later replace them with merged segments as background jobs complete. On the Amazon dataset I profiled, this gave us around 2.5 to 3x improvement in addIndexes() time. Each call was given about 5 readers to add on average.\r\n\r\nThis might be useful add to Lucene. We could create another {{addIndexes()}} API with a {{boolean}} flag for concurrency, that internally submits multiple merge jobs (each with a single reader) to the {{ConcurrentMergeScheduler}}, and waits for them to complete before returning.\r\n\r\nWhile this is doable from outside Lucene by using your thread pool, starting multiple addIndexes() calls and waiting for them to complete, I felt it needs some understanding of what addIndexes does, why you need to wait on the merge and why it makes sense to pass a single reader in the addIndexes API.\r\n\r\nOut of box support in Lucene could simplify this for folks a similar use case.\nOne bit from this proposal I'm not fully comfortable with is the fact that Lucene would merge each reader independently when the flag is set. While I understand why this makes sense for Amazon product search since you want to have many segments to make search as concurrent as possible, this is also a bit wasteful since you'd be paying the cost of doing a merge but without getting the benefits of reducing the segment count.\r\n\r\nWhat about a slightly different proposal where merges would always be executed via the configured merge scheduler, and addIndexes(CodecReader[]) would get an additional {{boolean doWait}} parameter like {{IndexWriter#forceMerge}}?\r\n\r\nThen you could still do what you want by using {{ConcurrentMergeScheduler}}, passing codec readers one by one to {{CodecReader#addIndexes}} with {{doWait=false}} and get resulting remapped segments as quickly as possible. Yet Lucene would retain the current behaviour where {{addIndexes}} also tries to avoid doing wasteful singleton merges?\nThanks for going through the proposal, [~jpountz].\r\n{quote}One bit from this proposal I'm not fully comfortable with is the fact that Lucene would merge each reader independently when the flag is set.\u00a0\r\n{quote}\r\nI understand your concern, I am also a bit iffy about losing the opportunity to merge segments..\r\n\r\nIdeally, I would like to land on a sweet middle ground - not run single segment merges, but also not add all provided segments into a single merge. Leverage some concurrency within addIndexes, and let background merges bring the segment count down further.\r\n\r\nHow do you feel about making this configurable in the API with a param that defines the number of segments merged together ({{MergeFactor?}}). We could make it flag to keep things simple for consumers, with values like {{ONE, THREAD_WIDE, and ALL,\u00a0}}where {{ONE}} = single segment merges, {{ALL}} = all segments in one merge, and {{THREAD_WIDE}} = each merge gets ({{readers.length/numThreads)}} number of segments. The default here could be {{ALL}} to retain current behavior.\r\n\r\n\u00a0\r\n{quote}Then you could still do what you want by using\u00a0{{ConcurrentMergeScheduler}}, passing codec readers one by one to\u00a0{{CodecReader#addIndexes}}\u00a0with\u00a0{{doWait=false}}\u00a0and get resulting remapped segments as quickly as possible.\r\n{quote}\r\nThe doWait flag would make the API non-blocking. But it would add additional steps for users to track when the merges triggered by addIndexes have completed. For segment replication, addIndexes is not useable until its merges complete.\u00a0Merge within addIndexes(), is what creates segments with recomputed ordinal values. Until that is done, there are no segments available to copy to searcher (and replica) hosts.\r\n\r\nWe also want to bring the segment count down to as low as possible at Amazon product search. The tradeoff we are making here, is to first go with a higher segment count to make documents quickly available for search, then run background merges and bring down the segment count.\u00a0This is similar to the other variant of add indexes - the {{addIndexes(Directory[])}} API, which copies all segments from provided directories into current index dir and then triggers a background merge.\r\n\r\nI feel making segments available quickly would be useful for anyone who has multiple replica search hosts and uses segment replication.\r\n\r\n\u00a0\nbq. How do you feel about making this configurable in the API with a param that defines the number of segments merged together\r\n\r\nI worry that it would make the API too complicated.\r\n\r\nThinking about it more: Ideally addIndexes would do the right thing based on the configured MergePolicy (to know how to group CodecReaders together) and MergeScheduler (to know how many threads it may use) so that we could hopefully keep the same API as today and experts users like Amazon product search could use a MergePolicy that favors many segments for search concurrency, plus possibly a ConcurrentMergeScheduler that is allowed to use many threads, and you would get the desired behavior? This might be a high-hanging fruit though.\nI like the idea of handling everything via MergePolicy and MergeScheduler, without changing the API. One wrinkle is that the existing merge framework (policy and scheduler) is written to work with segments, while the API only has readers.\r\n\r\nI have a high level plan for handling this based on early code reading. Let me know your thoughts and if you have any ideas to simplify/generalize this. If it makes sense, I can follow it up with a draft PR...\r\n\r\n\u2013\r\n\r\nWe could create a new API in MergePolicy that does findMerges using readers - {{{}findMerges(MergeTrigger mergeTrigger, CodecReaders[] readers, MergeContext mergeContext){}}}; and returns a {{MergeSpecification}} object, in which OneMerge objects are based on how the policy choses to merge these readers. Users can override this API based on the concurrency they'd like in addIndexes. Default impl. can just have a single {{OneMerge}} with all readers grouped together.\r\n\r\nWhile {{OneMerge}} accepts ({{{}List<SegmentCommitInfo> segments{}}}) in its constructor, the {{SegmentMerger}} actually works on readers. OneMerge converts segments to {{MergedReaders}} via {{{}initMergeReaders(){}}}. Perhaps we could add another constructor to OneMerge that directly initializes its mergedReaders?\r\n\r\nOnce we have a mergeSpec from policy, we can either use the existing {{MergeSource}} in IW and just add each OneMerge to {{pendingMerges,}}\u00a0or create a new mergeSource. I think we should at least override the {{MergeSource.merge()}} called from addIndexes, as addIndexes seems to require less work post the actual merging than IndexWriter (for e.g. no segments are rendered obsolete after an addIndexes merge, so we don't need to update the segments in {{SegmentInfos.applyMergeChanges()}} after merge completes).\r\n\r\nWe want to wait for all merges triggered by addIndexes to complete before we return, which we could do by calling {{await()}} on the MergeSpec returned by policy.\r\n\r\nI was also thinking that we could add a new {{MergeTrigger}} to identify merges triggered by addIndexes, and handle them separately in the merge policy (and possible scheduler?). We may not need it if the {{findMerges(..., readers)}} API is only used for addIndexes but it may be a good idea regardless.\nI like this plan (extending {{MergePolicy}} so it also has purview over how merging is done in {{{}addIndexes(CodecReader[]){}}}).\r\n\r\nReference counting might get tricky, if {{OneMerge}} or {{IndexWriter}} holding completed {{OneMerge}} instances try to {{decRef}} readers.\r\n\r\nTo improve testing we could create a new {{LuceneTestCase}} method to {{addIndexes}} from {{Directory[]}} that randomly does so with both impls and fix tests to sometimes use that for adding indices.\nI took a shot at the MergePolicy + MergeScheduler idea. Have raised a draft, work-in-progress PR - https://github.com/apache/lucene/pull/633, to get some high level feedback on the approach. It has existing tests passing, and is pending new tests specific to this change.\nHad some thoughts and questions about the transaction model for this API... Currently, it either succeeds and adds all provided readers, or fails and adds none of them. \r\n\r\nWith a merge policy splitting provided readers into groups of smaller {{OneMerge}} objects, this is slightly harder to implement. A OneMerge on a subset of readers may complete in the background and add itself to writer segment infos, while some others running in parallel could fail.\r\n\r\nOne approach could be to expose this failure information to user - the exception can contain the list of readers merged and pending. This could simplify the overall implementation.\r\n\r\nMy current thoughts, however, are that the present transaction logic is important. It is hard for users to parse the exception message, figure out which readers are pending and retry them. As opposed to retrying an entire API call (with all the readers), which their upstream system probably understands as a single unit. However, I wanted to check if loosening the transaction model for this API is a palatable approach.\r\n\r\nTo retain the all or none, single transaction model, I am thinking that we can join on all merges at the end of the {{addIndexes()}} API, and then write their segment info files in a common lock. \r\n\r\nWould like to hear more thoughts or suggestions on this.\nUpdated the PR to retain transactionality, while using the MergePolicy and configured MergeScheduler to trigger merges for this API\nWhile making this change, I found an existing [TODO|https://github.com/apache/lucene/blob/main/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java#L3158-L3159], which I think we're able to address now..\r\n{code:java}\r\n// TODO: somehow we should fix this merge so it's      \r\n// abortable so that IW.close(false) is able to stop it {code}\r\nEarlier,\u00a0{{addIndexes(CodecReader...)}}\u00a0triggered merges directly via SegmentMerger. As a result, the running merges could be tracked in\u00a0{{{}Set<SegmentMerger> runningAddIndexesMerges{}}}, but any pending merge operations could not be aborted preemptively.\r\n\r\nNow, merges are defined via {{MergePolicy.OneMerge}}\u00a0objects, and scheduled by the\u00a0{{{}MergeScheduler{}}}. We are able to leverage the OneMerge abstractions to abort any pending merges. They are aborted when IndexWriter is rolled back or closed.\nI think the PR is ready for review, with existing tests passing and added tests for new changes.\r\n\r\n{{OneMerge}} distribution is now provided by a new {{findMerges(CodecReaders[])}} API in {{{}MergePolicy{}}}, and executed by {{MergeScheduler}} threads. I've also modified the {{MockRandomMergePolicy}} to randomly pick a highly concurrent, (one segment per reader), {{findMerges(...)}} implementation 50% of the time. And confirmed manually that tests pass in both scenarios (this new impl., as well as the default impl. being picked) (thanks Michael McCandless for the suggestion).\n{quote}\r\nI've also modified the MockRandomMergePolicy to randomly pick a highly concurrent, (one segment per reader), findMerges(...) implementation 50% of the time.\r\n{quote}\r\n\r\nIs this going to make 50% of our test runs non-reproducible? Please, let's consider the need to reproduce failing runs and not pick such highly concurrent stuff for the unit tests. We should be able to test it another way.\n", "issueSearchSentences": ["Add concurrency to addIndexes(CodecReader\u2026) API", "While this is doable from outside Lucene by using your thread pool, starting multiple addIndexes() calls and waiting for them to complete, I felt it needs some understanding of what addIndexes does, why you need to wait on the merge and why it makes sense to pass a single reader in the addIndexes API.", "Earlier,\u00a0{{addIndexes(CodecReader...)}}\u00a0triggered merges directly via SegmentMerger.", "Currently, it either succeeds and adds all provided readers, or fails and adds none of them.", "We could create another {{addIndexes()}} API with a {{boolean}} flag for concurrency, that internally submits multiple merge jobs (each with a single reader) to the {{ConcurrentMergeScheduler}}, and waits for them to complete before returning."], "issueSearchScores": [0.7510662078857422, 0.728858470916748, 0.6546816825866699, 0.6440273523330688, 0.5847851037979126]}
{"aId": 4, "code": "public static Class<? extends Directory> loadDirectoryClass(String clazzName) \n      throws ClassNotFoundException {\n    return Class.forName(adjustDirectoryClassName(clazzName)).asSubclass(Directory.class);\n  }", "comment": " Loads a specific Directory implementation", "issueId": "LUCENE-3586", "issueStringList": ["Choose a specific Directory implementation running the CheckIndex main", "It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.", "What about an additional main parameter?", "In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.", "Should we also consider to use a FileSwitchDirectory?", "I'm willing to contribute, could you please let me know your thoughts about it?", "I think this makes sense though.", "sometimes like in your usecase MMap just fails with OOM and since we create the directory instance based on constants like 64bit platform, OS etc.", "we could make this optional.", "another option would be to simply use NIOFSDirectory no matter where we are.", "I don't think it is necessary to use FileSwitchDirectory for CheckIndex.", "Maybe you can add a commandline option and get a specific Dir impl if specified?", "NIO, MMAP & SimpleFS should be enough and use FSDirectory.open by default.", "Luca, I made you a contributor for Lucene & Solr (JIRA Role) so you can assign issues to you.", "Cool!", "Thanks, tomorrow I'll add a patch for this.", "Hi Simon, hi Luca,", "Maybe we get some introduction and background from Luca, because in general issues are assigned to committers/contributors that have already done work for Lucene/Solr?", "Uwe", "About the issue at all:", "we should maybe have this parameter to all command line tools (e.g.", "IndexUpgrader,...).", "It would be good to have some general command line parser for those comand line tools that work on indexes, so we don't have the same/similar code several times.", "For me much more interesting is the problem you have with MMAP?", "Whats you Java VM / Lucene version and what's your complete command line to CheckIndex?", "How big is the index?", "What architecture are you using and which OS?", "Changing the chunk size is not needed in most cases, if it needs to we need complete configuration details to optimize auto detection.", "Hi Uwe, hi all,", "you're right, I know (virtually) almost every people working at Solr/Lucene, but you of course don't know me!", "I'm Luca, I'm a software developer actually working at Dutchworks in Amsterdam within the enterprise search team.", "You certainly know my colleague Martijn, Solr/Lucene committer.", "I really like the open source world and the solr/lucene project.", "These are my contributions so far: SOLR-1499 and SOLR-2591.", "Just a few, but I would like to contribute as much as I can.", "Please,don't hesitate to ask if you have any questions.", "I assigned the issue to myself because I thought I was gonna work on it, but let me know if I made some mistakes.", "Tomorrow I'll add more details about the MMAP and the OOM.", "Thanks for your help and for your point of view about the issue, it would be great to do something more generic than I initially thought, I'll have a look at the code.", "Hi Uwe,", "I'm working with lucene 3.4 and jvm 1.6.0_23 64 bit.", "I have ulimit unlimited and /proc/sys/vm/max_map_count 65530.", "The index size is 76GB, I have problems with just one segment, the first and definitely the biggest one, on which the fdt is 22GB, tvf is 13GB, prx is 4.2GB, frq is 2.8GB, and so on (others are under 1GB).", "I tried to run the checkindex with -Xmx512m and -Xms512m.", "Here is my output:", "Segments file=segments_kj numSegments=31 version=3.4 format=FORMAT_3_1 [Lucene 3.1+]", "1 of 31: name=_1iz docCount=3186709", "compound=false", "hasProx=true", "numFiles=12", "size (MB)=41,902.362", "diagnostics = {optimize=true, mergeFactor=23, os.version=2.6.27.59-1POi-x86_64, os=Linux, lucene.version=3.4.0 1167142 - mike - 2011-09-09 09:02:09, source=merge, os.arch=amd64, java.version=1.6.0_23, java.vendor=Sun Microsystems Inc.}", "has deletions [delFileName=_1iz_r.del]", "test: open reader.........FAILED", "Thanks, and please let me know if you have any questions.", "Here is the first version of my patch, please let me know your comments.", "I tried to do something more generic for command line tools and less \"static\" but I found myself reinventing the wheel, writing stuff similar to commons cli, so I gave up.", "I basically added a new option called -dir-impl to CheckIndex, IndexReader and IndexUpgrader (please let me know if I forgot some other tools).", "The new option can have the following values (case insensitive): mmap, nio or simple.", "Any other value is ignored and considered as default (FSDirectory.open).", "I noticed a finally block used to close the directory inside IndexReader, but it's missing on the other two classes, wouldn't it be better to add it to CheckIndex and IndexUpgrader?", "Forgot to mention that the patch applies to the 3x branch.", "After your comments, and maybe some changes, I'll attach a patch for the trunk as well.", "hey luca, you patch looks good.", "I wonder if we should maybe add an enum to FSDirectory like", "{code}", "enum Type {", "NIOFS,", "MMAP,", "SIMPLE;", "}", "{code}", "and add FSDirectory#open(File, Type).", "Just an idea, this would make the CommandlineUtils obsolete and I think it could be useful... not sure what others think... uwe?", "bq.", "Forgot to mention that the patch applies to the 3x branch", "usually we (I) only provide a patch against trunk since we do a merge when we backport so a patch doesn't help very often.", "So it'd be better if you'd use trunk to create patches.", "simon", "Hey Simon,", "looks better!", "I thought I wasn't allowed to edit FSDirectory just for a new command line option :)", "I suppose we don't need also FSDirectory#open(File, LockFactory, Type) because we can't specify the LockFactory from command line, right?", "Hmm, I don't think we should add an enum to FSDir here?", "Can we simply accept the class name and then just load that class (maybe prefixing oal.store so user doesn't have to type that all the time)?", "Also, can we make it a hard error if the specified name isn't recognized?", "(Instead of silently falling back to FSDir.open).", "{quote}", "Hmm, I don't think we should add an enum to FSDir here?", "Can we simply accept the class name and then just load that class (maybe prefixing oal.store so user doesn't have to type that all the time)?", "Also, can we make it a hard error if the specified name isn't recognized?", "(Instead of silently falling back to FSDir.open).", "{quote}", "That's fine as well.", "Just a little bit longer than writing NIOFS, MMAP or SIMPLE, but I guess it doesn't matter.", "Mike, do you mean to load the class using reflection or compare the input string to those three class names?", "Any other opinion?", "I think just load the classes by name via reflection?", "This way if I have my own external Dir impl somewhere I can also have CheckIndex use that...", "New patch against trunk according to Michael's hints.", "It's now possible to use external FSDirectory implementations.", "The package oal.store is used if no package is specified.", "This isn't good if someone has the FSDirectory implementation in the default package, but I'm not sure if this case is worth a fall back.", "Please, let me know what you think.", "Thanks Luca; I think this patch is close!", "One thing I noticed is that LuceneTestCase.java (under", "src/test-framework/java/org/apache/lucene/util), has a method", "newDirectoryImpl doing almost the same thing as the new static method", "here.", "Maybe merge these two (have LTC call the new one, but leave that", "random logic in LTC)?", "Separately, can we put the static method in oal.util?", "Maybe in a new", "file, CommandLineUtil.java or something?", "Good eye!", "Thanks Michael for your feedback.", "I attached a new patch version.", "The merge hasn't been so easy since LTC needs sometimes a Directory, while other times a FSDirectory.", "Furthermore commmand line utilities don't need a fallback while  tests do..before the tests we even create the directory on file system etc.", "Let me know what you think!", "Hi Luca,", "Actually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).", "So....", "I think we should go back, and leave LTC with its current (private) code for finding the FS/Directory class/instance, and keep CommandLineUtil simple?", "Sorry!", "Otherwise the patch looks great!", "Can you add the lost \" // for javadocs\" comment on one of CheckIndex's imports?", "Thanks.", "Hi Michael,", "{quote}", "Actually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).", "{quote}", "I see.", "But I think we were on the right track.", "Have a look at the last version, I kept the merged code removing that method with generics.", "If you don't like even this version I'll go back, no problem at all!", "{quote}", "Can you add the lost \" // for javadocs\" comment on one of CheckIndex's imports?", "{quote}", "Ops, I restored it.", "What is the problem with the generics code?", "I like it :-)", "New patch looks great Luca!", "I'll commit & backport shortly...", "Thanks Luca!", "Cool, thanks to you Michael!"], "SplitGT": [" Loads a specific Directory implementation"], "issueString": "Choose a specific Directory implementation running the CheckIndex main\nIt should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.\nWhat about an additional main parameter?\nIn fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.\nShould we also consider to use a FileSwitchDirectory?\nI'm willing to contribute, could you please let me know your thoughts about it?\nI think this makes sense though. sometimes like in your usecase MMap just fails with OOM and since we create the directory instance based on constants like 64bit platform, OS etc. we could make this optional. another option would be to simply use NIOFSDirectory no matter where we are. I don't think it is necessary to use FileSwitchDirectory for CheckIndex. Maybe you can add a commandline option and get a specific Dir impl if specified? NIO, MMAP & SimpleFS should be enough and use FSDirectory.open by default.\nLuca, I made you a contributor for Lucene & Solr (JIRA Role) so you can assign issues to you. \nCool! Thanks, tomorrow I'll add a patch for this.\nHi Simon, hi Luca,\n\nMaybe we get some introduction and background from Luca, because in general issues are assigned to committers/contributors that have already done work for Lucene/Solr?\n\nUwe\nAbout the issue at all:\nwe should maybe have this parameter to all command line tools (e.g. IndexUpgrader,...). It would be good to have some general command line parser for those comand line tools that work on indexes, so we don't have the same/similar code several times.\n\nFor me much more interesting is the problem you have with MMAP? Whats you Java VM / Lucene version and what's your complete command line to CheckIndex? How big is the index? What architecture are you using and which OS? Changing the chunk size is not needed in most cases, if it needs to we need complete configuration details to optimize auto detection.\nHi Uwe, hi all,\nyou're right, I know (virtually) almost every people working at Solr/Lucene, but you of course don't know me!\nI'm Luca, I'm a software developer actually working at Dutchworks in Amsterdam within the enterprise search team. You certainly know my colleague Martijn, Solr/Lucene committer. I really like the open source world and the solr/lucene project. These are my contributions so far: SOLR-1499 and SOLR-2591. Just a few, but I would like to contribute as much as I can. Please,don't hesitate to ask if you have any questions.\nI assigned the issue to myself because I thought I was gonna work on it, but let me know if I made some mistakes.\nTomorrow I'll add more details about the MMAP and the OOM. \nThanks for your help and for your point of view about the issue, it would be great to do something more generic than I initially thought, I'll have a look at the code.\n\n\nHi Uwe,\nI'm working with lucene 3.4 and jvm 1.6.0_23 64 bit. I have ulimit unlimited and /proc/sys/vm/max_map_count 65530. The index size is 76GB, I have problems with just one segment, the first and definitely the biggest one, on which the fdt is 22GB, tvf is 13GB, prx is 4.2GB, frq is 2.8GB, and so on (others are under 1GB). I tried to run the checkindex with -Xmx512m and -Xms512m. Here is my output:\n\nSegments file=segments_kj numSegments=31 version=3.4 format=FORMAT_3_1 [Lucene 3.1+]\n  1 of 31: name=_1iz docCount=3186709\n    compound=false\n    hasProx=true\n    numFiles=12\n    size (MB)=41,902.362\n    diagnostics = {optimize=true, mergeFactor=23, os.version=2.6.27.59-1POi-x86_64, os=Linux, lucene.version=3.4.0 1167142 - mike - 2011-09-09 09:02:09, source=merge, os.arch=amd64, java.version=1.6.0_23, java.vendor=Sun Microsystems Inc.}\n    has deletions [delFileName=_1iz_r.del]\n    test: open reader.........FAILED\n\nThanks, and please let me know if you have any questions.\nHere is the first version of my patch, please let me know your comments.\n\nI tried to do something more generic for command line tools and less \"static\" but I found myself reinventing the wheel, writing stuff similar to commons cli, so I gave up.\nI basically added a new option called -dir-impl to CheckIndex, IndexReader and IndexUpgrader (please let me know if I forgot some other tools). The new option can have the following values (case insensitive): mmap, nio or simple. Any other value is ignored and considered as default (FSDirectory.open).\n\nI noticed a finally block used to close the directory inside IndexReader, but it's missing on the other two classes, wouldn't it be better to add it to CheckIndex and IndexUpgrader?\nForgot to mention that the patch applies to the 3x branch. After your comments, and maybe some changes, I'll attach a patch for the trunk as well.\nhey luca, you patch looks good. I wonder if we should maybe add an enum to FSDirectory like \n{code}\nenum Type {\n  NIOFS,\n  MMAP,\n  SIMPLE;\n\n}\n\n{code}\n\n\nand add FSDirectory#open(File, Type). Just an idea, this would make the CommandlineUtils obsolete and I think it could be useful... not sure what others think... uwe? \n\nbq. Forgot to mention that the patch applies to the 3x branch\n\nusually we (I) only provide a patch against trunk since we do a merge when we backport so a patch doesn't help very often. So it'd be better if you'd use trunk to create patches.\n\nsimon\nHey Simon, \nlooks better! I thought I wasn't allowed to edit FSDirectory just for a new command line option :)\nI suppose we don't need also FSDirectory#open(File, LockFactory, Type) because we can't specify the LockFactory from command line, right?\nHmm, I don't think we should add an enum to FSDir here?  Can we simply accept the class name and then just load that class (maybe prefixing oal.store so user doesn't have to type that all the time)?\n\nAlso, can we make it a hard error if the specified name isn't recognized?  (Instead of silently falling back to FSDir.open).\n{quote}\nHmm, I don't think we should add an enum to FSDir here? Can we simply accept the class name and then just load that class (maybe prefixing oal.store so user doesn't have to type that all the time)?\n\nAlso, can we make it a hard error if the specified name isn't recognized? (Instead of silently falling back to FSDir.open).\n{quote}\n\nThat's fine as well. Just a little bit longer than writing NIOFS, MMAP or SIMPLE, but I guess it doesn't matter. Mike, do you mean to load the class using reflection or compare the input string to those three class names?\n\nAny other opinion?\nI think just load the classes by name via reflection?  This way if I have my own external Dir impl somewhere I can also have CheckIndex use that...\nNew patch against trunk according to Michael's hints.\nIt's now possible to use external FSDirectory implementations. The package oal.store is used if no package is specified. This isn't good if someone has the FSDirectory implementation in the default package, but I'm not sure if this case is worth a fall back. Please, let me know what you think.\nThanks Luca; I think this patch is close!\n\nOne thing I noticed is that LuceneTestCase.java (under\nsrc/test-framework/java/org/apache/lucene/util), has a method\nnewDirectoryImpl doing almost the same thing as the new static method\nhere.  Maybe merge these two (have LTC call the new one, but leave that\nrandom logic in LTC)?\n\nSeparately, can we put the static method in oal.util?  Maybe in a new\nfile, CommandLineUtil.java or something?\n\nGood eye! Thanks Michael for your feedback.\nI attached a new patch version. The merge hasn't been so easy since LTC needs sometimes a Directory, while other times a FSDirectory. Furthermore commmand line utilities don't need a fallback while  tests do..before the tests we even create the directory on file system etc.\nLet me know what you think!\nHi Luca,\n\nActually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).\n\nSo.... I think we should go back, and leave LTC with its current (private) code for finding the FS/Directory class/instance, and keep CommandLineUtil simple?  Sorry!\n\nOtherwise the patch looks great!  Can you add the lost \" // for javadocs\" comment on one of CheckIndex's imports?  Thanks.\nHi Michael,\n{quote}\nActually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).\n{quote}\nI see. But I think we were on the right track. Have a look at the last version, I kept the merged code removing that method with generics. If you don't like even this version I'll go back, no problem at all!\n\n{quote}\nCan you add the lost \" // for javadocs\" comment on one of CheckIndex's imports?\n{quote}\nOps, I restored it.\nWhat is the problem with the generics code? I like it :-)\nNew patch looks great Luca!  I'll commit & backport shortly...\nThanks Luca!\nCool, thanks to you Michael!\n", "issueSearchSentences": ["Actually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).", "Actually I don't like all the generics required in CommandLineUtil solely to support LTC's case; if not for LTC the util would be much simpler (just one method taking a String class name/path and returning a Directory instance).", "newDirectoryImpl doing almost the same thing as the new static method", "I think just load the classes by name via reflection?", "Can we simply accept the class name and then just load that class (maybe prefixing oal.store so user doesn't have to type that all the time)?"], "issueSearchScores": [0.5560684204101562, 0.5560684204101562, 0.4962754249572754, 0.493758887052536, 0.47835710644721985]}
{"aId": 6, "code": "private static ZkCoreNodeProps getZombieLeader(ZkController zkController, String collection, String shardId) {\n    try {\n      ZkCoreNodeProps leaderProps = zkController.getLeaderProps(collection, shardId, 1000);\n      DocCollection docCollection = zkController.getClusterState().getCollection(collection);\n      Replica replica = docCollection.getReplica(leaderProps.getNodeProps().getStr(ZkStateReader.CORE_NODE_NAME_PROP));\n      if (replica == null) return leaderProps;\n      if (!replica.getNodeName().equals(leaderProps.getNodeName())) {\n        return leaderProps;\n      }\n      return null;\n    } catch (Exception e) {\n      return null;\n    }\n  }", "comment": " Zombie leader is a replica won the election but does not exist in clusterstate", "issueId": "SOLR-12176", "issueStringList": ["Improve FORCELEADER to handle the case when a replica win the election but does not present in clusterstate", "There can be the case when a replica wins the election but it does not present in clusterstate.", "Maybe when the Overseer sent the UNLOAD request to the LEADER (in DeleteReplicaCmd), it met some exception (therefore the request never reach the LEADER), the Overseer it that case will forcefully remove the LEADER from clusterstate.", "If a shard reaches that case, users will only see a leaderless shard and call FORCELEADER won't be able to solve their problem.", "Therefore FORCELEADER should be more robust, to handle such cases.", "Zombie leader is a leader won the election (created \"/collections/leaders/shard/leader\") but does not exist in clusterstate.", "The idea of the patch is finding such zombie leader and unload it.", "I will commit soon if no one has any objection."], "SplitGT": [" Zombie leader is a replica won the election but does not exist in clusterstate"], "issueString": "Improve FORCELEADER to handle the case when a replica win the election but does not present in clusterstate\nThere can be the case when a replica wins the election but it does not present in clusterstate. Maybe when the Overseer sent the UNLOAD request to the LEADER (in DeleteReplicaCmd), it met some exception (therefore the request never reach the LEADER), the Overseer it that case will forcefully remove the LEADER from clusterstate. \r\n\r\nIf a shard reaches that case, users will only see a leaderless shard and call FORCELEADER won't be able to solve their problem. Therefore FORCELEADER should be more robust, to handle such cases.\nZombie leader is a leader won the election (created \"/collections/leaders/shard/leader\") but does not exist in clusterstate. The idea of the patch is finding such zombie leader and unload it.\nI will commit soon if no one has any objection.\n", "issueSearchSentences": ["Zombie leader is a leader won the election (created \"/collections/leaders/shard/leader\") but does not exist in clusterstate.", "The idea of the patch is finding such zombie leader and unload it.", "If a shard reaches that case, users will only see a leaderless shard and call FORCELEADER won't be able to solve their problem.", "There can be the case when a replica wins the election but it does not present in clusterstate.", "Improve FORCELEADER to handle the case when a replica win the election but does not present in clusterstate"], "issueSearchScores": [0.6963405609130859, 0.5895780324935913, 0.5421053171157837, 0.3985145092010498, 0.3951829671859741]}
{"aId": 7, "code": "public void setMaxEdits(int maxEdits) {\n    if (maxEdits < 1 || maxEdits > LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE)\n      throw new UnsupportedOperationException(\"Invalid maxEdits\");\n    this.maxEdits = maxEdits;\n  }", "comment": " The default is 2.", "issueId": "LUCENE-2507", "issueStringList": ["automaton spellchecker", "The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.", "The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.", "Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need", "a separate index to rebuild.", "prototype patch that adds 'DirectSpellChecker', with some tests showing use with IndexWriter.getReader()", "For now I only implemented Levenshtein, since its for free :) But it would be good to support re-ranking against the existing StringDistance metrics, etc.", "The larger problem is that the existing APIs are really built around this idea of a separate index, so I'm open to suggestions as to how this can be better integrated.", "there was a bug in conversion between fuzzy term enum's scaling.", "i ran some simple perf tests, this is essentially just as fast as the existing code", "with setMaxEdits(1).", "but with setMaxEdits(2) is much slower.", "i'll try to think of ways to speed it up... one idea would be to add lev automata with transposition", "support instead of using higher distances, etc.", "we have sped up this seeking a lot recently, and i improved this patch some:", "avoid calling docfreq on the suggestions, by using the TermsEnum's docfreq", "Mike had the idea that we should actually try lower edit distances first.", "The", "general use case here is a small number of suggestions (e.g.", "1), so", "we actually try edit distance=1 first... only if this doesn't give enough suggestions", "do we then try higher distances.", "I think this is a good approach here, because we are getting levenshtein directly,", "so we don't have the problem the n-gram based spellchecker has... (for reference below)", "{noformat}", "<p>As the Lucene similarity that is used to fetch the most relevant n-grammed terms", "is not the same as the edit distance strategy used to calculate the best", "matching spell-checked word from the hits that Lucene found, one usually has", "to retrieve a couple of numSug's in order to get the true best match.", "<p>I.e.", "if numSug == 1, don't count on that suggestion being the best one.", "Thus, you should set this value to <b>at least</b> 5 for a good suggestion.", "{noformat}", "Since we are actually doing levenshtein, you can safely use lower values for numSug,", "such as numSug=1", "I improved the quality and performance of this spellchecker, integrated it with the other spellchecker APIs,", "and did the Solr side.", "I think minus some more tests and docs (especially on the various options) its good to go.", "Hey Robert,", "Do you have any benchmarks for this spellchecker?", "I notice you mention a few times that you improved the performance.", "Do you know how it compares against the separate index approach?", "Equally, is this spellchecker a conceptual drop in replacement?", "By that I mean, are the suggestions it generates radically different to the separate index spellcheckers or are they along the same lines?", "bq.", "Do you have any benchmarks for this spellchecker?", "I notice you mention a few times that you improved the performance.", "Do you know how it compares against the separate index approach?", "In general I think the performance is fine.", "I did a lot of testing against the geonames database (> 2 million unique terms).", "But, it completely depends upon the parameters you set.", "Here are some that can affect performance and quality:", "avoid doing work if the query term is already spelled correctly:", "minQueryLength (example: 4), query words of 3 characters or less are not checked.", "In general, with any metric, the candidates here will mostly be nonsense anyway.", "maxQueryFrequency (example: 1% or 1):  if the query word is high frequency (e.g.", "appears in more", "than 1% of the documents its assumed to be correct, and no suggestions are given.", "You can also set this to something like 1, if say you have a small product database", "and you feel all your products are spelled completely correct in your index, and you", "don't want to *ever* suggest anything if the query term is in your products database.", "avoid doing work examining potentially bad suggestions:", "maxEdits (example: 1), the majority of misspellings are only 1 distance away.", "So if you lower this from the default \"2\" to 1, its faster and more \"lightweight\" in the sense you get less a chance of getting a bad suggestion.", "minPrefix (example: 1), most misspellings don't occur in the first character.", "For the solr example, i set this to zero (the wiki has an example correcting \"hell\" with \"dell\"), but in practice I think 1 is a good value.", "Additionally this has a practical use for solr users: you need a rather \"raw\" (e.g.", "not stemmed) analyzed field for spellchecking,", "if you set this to 1 you can re-use your reverse-wildcard field for spellchecking too, and it will never suggest reversed terms.", "thresholdFrequency (example: 1% or 1): this plays the role of Solr's \"HighFrequencyDictionary\".", "In other words, you could set this to 1 to never suggest words that only appear in a single document... in many cases these are misspellings.", "maxInspections (example: 5), the existing spellchecker uses a hardcoded 10 here.", "A lower value can work well here, since the algorithm used to draw candidates is actually levenshtein.", "However, I set the default to 5 (instead of 1), because its good to gather a few candidates for docFreq-sorting....", "but if you increase thresholdFrequency you can probably lower this.", "bq.", "Equally, is this spellchecker a conceptual drop in replacement?", "By that I mean, are the suggestions it generates radically different to the separate index spellcheckers or are they along the same lines?", "I think they are better, e.g.", "if you are ranking by an edit-distance like function such as Levenshtein or Jaro-Winkler, it makes more sense to get your *candidates* via the same or similar algorithm!", "The existing spellchecker gets candidates with n-grams...", "I think this causes a mismatch... (Of course the inverse is true, if you use NGramDistance, use the existing spellchecker!)", "Again I did a lot of testing with various corpora, and I'm not a spellchecking expert but i didn't get particularly good results from the existing spellchecker.", "And for some corpora such as geonames, it didnt seem to have the configurability I needed to tune the spellchecker to that domain.", "For example, i queried on \"zeeland\" and the existing spellchecker returned freeland, leland, ireland, deland, and cleland as suggestions.", "Whats worse, is that it created a 240MB auxiliary index when my original index was only 300MB, and it took it 141 seconds to do this.", "The idea here isn't to solve the world's spellchecking problems, its mainly to get rid of the extra index.", "I think its trivial to", "set this one up to beat SpellChecker's suggestions, because I don't think SpellChecker's suggestions are very good.", "Hi,", "Thanks for that.", "Covers my questions nicely.", "bq.", "The idea here isn't to solve the world's spellchecking problems, its mainly to get rid of the extra index.", "Yes definitely.", "I was just checking that we weren't doing that at a cost of reasonable suggestions.", "But your argument makes clear sense.", "This really is a great feature.", "bq.", "Yes definitely.", "I was just checking that we weren't doing that at a cost of reasonable suggestions.", "But your argument makes clear sense.", "Well, aspell has some test data here: http://aspell.net/test/cur/batch0.tab", "I could index some wikipedia, and run both spellcheckers?", "Additionally I suppose it would be fair to run the correct answers from this set, and see the results across both spellcheckers as far as spell-correcting already correct words (and what they suggest if they do!)", "That is a very good idea yes, but I don't think its necessary to do that before this is committed.", "We can do that afterwards, get an idea of where the spellcheckers are, and improve them through other issues if needs be.", "bq.", "That is a very good idea yes, but I don't think its necessary to do that before this is committed.", "Here's some *very rough* numbers from that batch0.tab, against the FIRE english corpus (sorry i'm still downloading wikipedia, its quite large!)", "Note, this is only relative, e.g.", "i dont even know if these terms all exist in that corpus.", "additionally, some contain punctuation etc, i only lowercased them for consistency.", "for reference, there are 547 incorrect/correct term pairs in this aspell spelling correction test.", "My corpus has ~150,000 docs, with 304,000 unique terms in the body field.", "for both spellcheckers I used all defaults, e.g.", "spellchecker.suggestSimilar(words[1].toLowerCase(), 1, reader, \"body\", true);", "||impl||Number correct[1] (out of 547)||Number correct, inverted[2] (out of 547)||Avg time in ms[3]||", "|SpellChecker|214|218|1.47ms", "|DirectSpellChecker|242|303|4.53ms", "1. using the misspelling as a query term, does the spellchecker return the correct spelling?", "2. using the correct spelling as a query term, does the spellchecker return nothing at all?", "3. this is the average time to perform an actual correction, both spellcheckers have some way to do no work at all for the common (correctly spelled) case.", "So although the benchmark itself isnt for search engine benchmarking (e.g.", "contains stopwords/punctuation), this basically shows what I've been seeing, that I think this spellchecker outperforms the existing one, and the perf cost is reasonable.", "By the way, out of curiousity i tested an alternative configuration, DirectSpellChecker with .setMaxEdits(1)", "With this \"lighter\" configuration:", "||impl||Number correct (out of 547)||Number correct, inverted (out of 547)||Avg time in ms||", "|DirectSpellChecker(n=1)|165|432|1.83ms|", "So here, you have the flexibility to have essentially the same performance as the existing spellchecker,", "and the false positive rate is hugely reduced (in this contrived test).", "You trade off only being able to", "catch 77% of the suggestions relative to the old spellchecker... but this might be good for setups", "that feel the n=2 default is too aggressive.", "And again, like the original configuration, you have no index to rebuild at all.", "They're both very fast and you get the flexibility of not having an additional index.", "+1 to committing.", "This is an awesome step forward!", "It requires no parallel index, and, it gets better accuracy (if your metric is edit distance like) at a negligible perf hit.", "It's great that it leverages the absurd speedups we've made to FuzzyQuery in 4.0.", "I'll work on cleaning up tests and doc, i think we can then commit this with the functionality it has.", "bq.", "It's great that it leverages the absurd speedups we've made to FuzzyQuery in 4.0.", "Yes, if you read that scary fuzzy paper it seems thats its original use-case all along (we just did FuzzyQuery first, and re-used it here):", "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652", "Along the same lines, I think we can then later improve both spellcheckers in easy ways.", "For example,", "it would be good to add a concept of \"SpellCheckFilter\" that can return true/false if a word is correctly spelled.", "Docfreq-based stuff helps, but if you know the language, something like hunspell could go a long way here", "to both preventing either spellchecker from trying to correct an already-correctly-spelled word or preventing", "it from suggesting misspellings.", "here's the improved docs and tests.", "I'd like to commit this one and we can iterate as discussed, hopefully improve both spellcheckers.", "Committed revision 1003642."], "SplitGT": [" The default is 2."], "issueString": "automaton spellchecker\nThe current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.\nThe terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.\n\nAlternatively, we could just do a levenshtein query directly against the index, then we wouldn't need\na separate index to rebuild.\n\nprototype patch that adds 'DirectSpellChecker', with some tests showing use with IndexWriter.getReader()\n\nFor now I only implemented Levenshtein, since its for free :) But it would be good to support re-ranking against the existing StringDistance metrics, etc.\n\nThe larger problem is that the existing APIs are really built around this idea of a separate index, so I'm open to suggestions as to how this can be better integrated.\n\n\nthere was a bug in conversion between fuzzy term enum's scaling.\n\ni ran some simple perf tests, this is essentially just as fast as the existing code\nwith setMaxEdits(1). but with setMaxEdits(2) is much slower.\n\ni'll try to think of ways to speed it up... one idea would be to add lev automata with transposition\nsupport instead of using higher distances, etc.\n\nwe have sped up this seeking a lot recently, and i improved this patch some:\n* avoid calling docfreq on the suggestions, by using the TermsEnum's docfreq\n* Mike had the idea that we should actually try lower edit distances first. The \n  general use case here is a small number of suggestions (e.g. 1), so \n  we actually try edit distance=1 first... only if this doesn't give enough suggestions \n  do we then try higher distances. \n\nI think this is a good approach here, because we are getting levenshtein directly, \nso we don't have the problem the n-gram based spellchecker has... (for reference below)\n\n{noformat}\n   * <p>As the Lucene similarity that is used to fetch the most relevant n-grammed terms\n   * is not the same as the edit distance strategy used to calculate the best\n   * matching spell-checked word from the hits that Lucene found, one usually has\n   * to retrieve a couple of numSug's in order to get the true best match.\n   *\n   * <p>I.e. if numSug == 1, don't count on that suggestion being the best one.\n   * Thus, you should set this value to <b>at least</b> 5 for a good suggestion.\n{noformat}\n\nSince we are actually doing levenshtein, you can safely use lower values for numSug,\nsuch as numSug=1\n\nI improved the quality and performance of this spellchecker, integrated it with the other spellchecker APIs,\nand did the Solr side. I think minus some more tests and docs (especially on the various options) its good to go.\n\nHey Robert,\n\nDo you have any benchmarks for this spellchecker? I notice you mention a few times that you improved the performance.  Do you know how it compares against the separate index approach? \n\nEqually, is this spellchecker a conceptual drop in replacement? By that I mean, are the suggestions it generates radically different to the separate index spellcheckers or are they along the same lines?\nbq. Do you have any benchmarks for this spellchecker? I notice you mention a few times that you improved the performance. Do you know how it compares against the separate index approach?\n\nIn general I think the performance is fine. I did a lot of testing against the geonames database (> 2 million unique terms).\nBut, it completely depends upon the parameters you set. Here are some that can affect performance and quality:\n* avoid doing work if the query term is already spelled correctly:\n** minQueryLength (example: 4), query words of 3 characters or less are not checked. \nIn general, with any metric, the candidates here will mostly be nonsense anyway.\n** maxQueryFrequency (example: 1% or 1):  if the query word is high frequency (e.g. appears in more \nthan 1% of the documents its assumed to be correct, and no suggestions are given.\nYou can also set this to something like 1, if say you have a small product database \nand you feel all your products are spelled completely correct in your index, and you \ndon't want to *ever* suggest anything if the query term is in your products database.\n* avoid doing work examining potentially bad suggestions:\n** maxEdits (example: 1), the majority of misspellings are only 1 distance away. \nSo if you lower this from the default \"2\" to 1, its faster and more \"lightweight\" in the sense you get less a chance of getting a bad suggestion.\n** minPrefix (example: 1), most misspellings don't occur in the first character. \nFor the solr example, i set this to zero (the wiki has an example correcting \"hell\" with \"dell\"), but in practice I think 1 is a good value. \nAdditionally this has a practical use for solr users: you need a rather \"raw\" (e.g. not stemmed) analyzed field for spellchecking,\nif you set this to 1 you can re-use your reverse-wildcard field for spellchecking too, and it will never suggest reversed terms.\n** thresholdFrequency (example: 1% or 1): this plays the role of Solr's \"HighFrequencyDictionary\". \nIn other words, you could set this to 1 to never suggest words that only appear in a single document... in many cases these are misspellings.\n** maxInspections (example: 5), the existing spellchecker uses a hardcoded 10 here. \nA lower value can work well here, since the algorithm used to draw candidates is actually levenshtein. \nHowever, I set the default to 5 (instead of 1), because its good to gather a few candidates for docFreq-sorting.... \nbut if you increase thresholdFrequency you can probably lower this.\n\nbq. Equally, is this spellchecker a conceptual drop in replacement? By that I mean, are the suggestions it generates radically different to the separate index spellcheckers or are they along the same lines?\n\nI think they are better, e.g. if you are ranking by an edit-distance like function such as Levenshtein or Jaro-Winkler, it makes more sense to get your *candidates* via the same or similar algorithm! The existing spellchecker gets candidates with n-grams... I think this causes a mismatch... (Of course the inverse is true, if you use NGramDistance, use the existing spellchecker!)\n\nAgain I did a lot of testing with various corpora, and I'm not a spellchecking expert but i didn't get particularly good results from the existing spellchecker.\nAnd for some corpora such as geonames, it didnt seem to have the configurability I needed to tune the spellchecker to that domain.\n\nFor example, i queried on \"zeeland\" and the existing spellchecker returned freeland, leland, ireland, deland, and cleland as suggestions.\nWhats worse, is that it created a 240MB auxiliary index when my original index was only 300MB, and it took it 141 seconds to do this.\n\nThe idea here isn't to solve the world's spellchecking problems, its mainly to get rid of the extra index. I think its trivial to\nset this one up to beat SpellChecker's suggestions, because I don't think SpellChecker's suggestions are very good.\n\nHi,\n\nThanks for that.  Covers my questions nicely.\n\nbq. The idea here isn't to solve the world's spellchecking problems, its mainly to get rid of the extra index.\n\nYes definitely.   I was just checking that we weren't doing that at a cost of reasonable suggestions.  But your argument makes clear sense.  \n\nThis really is a great feature.\nbq. Yes definitely. I was just checking that we weren't doing that at a cost of reasonable suggestions. But your argument makes clear sense.\n\nWell, aspell has some test data here: http://aspell.net/test/cur/batch0.tab\nI could index some wikipedia, and run both spellcheckers?\n\nAdditionally I suppose it would be fair to run the correct answers from this set, and see the results across both spellcheckers as far as spell-correcting already correct words (and what they suggest if they do!)\n\nThat is a very good idea yes, but I don't think its necessary to do that before this is committed.   We can do that afterwards, get an idea of where the spellcheckers are, and improve them through other issues if needs be.\nbq. That is a very good idea yes, but I don't think its necessary to do that before this is committed.\n\nHere's some *very rough* numbers from that batch0.tab, against the FIRE english corpus (sorry i'm still downloading wikipedia, its quite large!)\nNote, this is only relative, e.g. i dont even know if these terms all exist in that corpus.\nadditionally, some contain punctuation etc, i only lowercased them for consistency.\n\nfor reference, there are 547 incorrect/correct term pairs in this aspell spelling correction test.\nMy corpus has ~150,000 docs, with 304,000 unique terms in the body field.\nfor both spellcheckers I used all defaults, e.g. spellchecker.suggestSimilar(words[1].toLowerCase(), 1, reader, \"body\", true);\n\n||impl||Number correct[1] (out of 547)||Number correct, inverted[2] (out of 547)||Avg time in ms[3]||\n|SpellChecker|214|218|1.47ms\n|DirectSpellChecker|242|303|4.53ms\n\n1. using the misspelling as a query term, does the spellchecker return the correct spelling?\n2. using the correct spelling as a query term, does the spellchecker return nothing at all?\n3. this is the average time to perform an actual correction, both spellcheckers have some way to do no work at all for the common (correctly spelled) case.\n\nSo although the benchmark itself isnt for search engine benchmarking (e.g. contains stopwords/punctuation), this basically shows what I've been seeing, that I think this spellchecker outperforms the existing one, and the perf cost is reasonable.\n\nBy the way, out of curiousity i tested an alternative configuration, DirectSpellChecker with .setMaxEdits(1)\n\nWith this \"lighter\" configuration:\n||impl||Number correct (out of 547)||Number correct, inverted (out of 547)||Avg time in ms||\n|DirectSpellChecker(n=1)|165|432|1.83ms|\n\nSo here, you have the flexibility to have essentially the same performance as the existing spellchecker,\nand the false positive rate is hugely reduced (in this contrived test). You trade off only being able to\ncatch 77% of the suggestions relative to the old spellchecker... but this might be good for setups\nthat feel the n=2 default is too aggressive.\n\nAnd again, like the original configuration, you have no index to rebuild at all.\n\nThey're both very fast and you get the flexibility of not having an additional index.  +1 to committing.  \nThis is an awesome step forward!\n\nIt requires no parallel index, and, it gets better accuracy (if your metric is edit distance like) at a negligible perf hit.\n\nIt's great that it leverages the absurd speedups we've made to FuzzyQuery in 4.0.\nI'll work on cleaning up tests and doc, i think we can then commit this with the functionality it has.\n\nbq. It's great that it leverages the absurd speedups we've made to FuzzyQuery in 4.0.\n\nYes, if you read that scary fuzzy paper it seems thats its original use-case all along (we just did FuzzyQuery first, and re-used it here):\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652\n\nAlong the same lines, I think we can then later improve both spellcheckers in easy ways. For example,\nit would be good to add a concept of \"SpellCheckFilter\" that can return true/false if a word is correctly spelled.\n\nDocfreq-based stuff helps, but if you know the language, something like hunspell could go a long way here\nto both preventing either spellchecker from trying to correct an already-correctly-spelled word or preventing\nit from suggesting misspellings.\n\nhere's the improved docs and tests.\n\nI'd like to commit this one and we can iterate as discussed, hopefully improve both spellcheckers.\nCommitted revision 1003642.\n", "issueSearchSentences": ["with setMaxEdits(1).", "but with setMaxEdits(2) is much slower.", "maxEdits (example: 1), the majority of misspellings are only 1 distance away.", "By the way, out of curiousity i tested an alternative configuration, DirectSpellChecker with .setMaxEdits(1)", "maxInspections (example: 5), the existing spellchecker uses a hardcoded 10 here."], "issueSearchScores": [0.7927179336547852, 0.7470079660415649, 0.5893391370773315, 0.5368624925613403, 0.36403733491897583]}
{"aId": 8, "code": "public static double haversine(double y1, double x1, double y2, double x2, double radius) {\n    double result = 0;\n    //make sure they aren't all the same, as then we can just return 0\n    if ((x1 != x2) || (y1 != y2)) {\n      double diffX = x1 - x2;\n      double diffY = y1 - y2;\n      double hsinX = Math.sin(diffX * 0.5);\n      double hsinY = Math.sin(diffY * 0.5);\n      double h = hsinY * hsinY +\n          (Math.cos(y1) * Math.cos(y2) * hsinX * hsinX);\n      result = (radius * 2 * Math.atan2(Math.sqrt(h), Math.sqrt(1 - h)));\n    }\n    return result;\n  }", "comment": " The arguments are in radians and provided in lat,lon order.", "issueId": "LUCENE-3599", "issueStringList": ["haversine() is broken / misdocumented", "DistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.", "The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e.", "lon,lat order.", "The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.", "It turns out that all callers of this method do this!", "FYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.", "Apparently I shouldn't do that ;-)", "The attached patch changes the documentation and parameter names of haversine() to reflect how callers are actually calling it -- in lat,lon order.", "I fixed the algorithm for this as well.", "Furthermore, in Solr's HaversineFunction I changed some local variable names to reflect the true order.", "Oy ...", "I think my eyes are bleeding trying to make sense of the x/y swaps in this patch.", "David: thanks for catching this", "Committed revision 1208118.", "- trunk", "Committed revision 1208120.", "- 3x"], "SplitGT": [" The arguments are in radians and provided in lat,lon order."], "issueString": "haversine() is broken / misdocumented\nDistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.  The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e. lon,lat order.  The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.  It turns out that all callers of this method do this!\n\nFYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.  Apparently I shouldn't do that ;-)\nThe attached patch changes the documentation and parameter names of haversine() to reflect how callers are actually calling it -- in lat,lon order. I fixed the algorithm for this as well.  Furthermore, in Solr's HaversineFunction I changed some local variable names to reflect the true order.\nOy ... I think my eyes are bleeding trying to make sense of the x/y swaps in this patch.\n\nDavid: thanks for catching this\nCommitted revision 1208118. - trunk\nCommitted revision 1208120. - 3x\n\n", "issueSearchSentences": ["DistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.", "The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e.", "haversine() is broken / misdocumented", "The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.", "I think my eyes are bleeding trying to make sense of the x/y swaps in this patch."], "issueSearchScores": [0.6171892285346985, 0.486741840839386, 0.4632275700569153, 0.4351434111595154, 0.3608502149581909]}
{"aId": 9, "code": "protected final boolean incrementGraphToken() throws IOException {\n    if (graphPos < graphDepth) {\n      graphPos++;\n      currentGraph.get(graphPos).attSource.copyTo(this);\n      return true;\n    }\n    Token token = nextTokenInGraph(currentGraph.get(graphDepth));\n    if (token == null) {\n      return false;\n    }\n    graphDepth++;\n    graphPos++;\n    currentGraph.add(graphDepth, token);\n    token.attSource.copyTo(this);\n    return true;\n  }", "comment": " Move to the next token in the current route through the graph", "issueId": "LUCENE-8564", "issueStringList": ["Make it easier to iterate over graphs in tokenstreams", "We have a number of TokenFilters that read ahead in the token stream (eg synonyms, shingles) and ideally these would understand token graphs as well as linear streams.", "FixedShingleFilter already has some mechanisms to deal with graphs; this issue is to extract this logic into a GraphTokenStream class that can then be reused by other token filters", "Here is a patch adding a GraphTokenStream class.", "The class wraps an underlying token stream, and then exposes tokens via the following methods:", "incrementBaseToken() : moves the starting point of the graph forwards", "incrementGraphToken() : moves along the currently selected path through the token graph", "incrementGraph() : resets back to the base token, and selects the next path to move along.", "Returns false if all paths have been exhausted", "The patch also reimplements FixedShingleFilter using GraphTokenStream, to illustrate how much easier it is to reason about how things work.", "To protect against misuse, there are hard limits on how far ahead in the stream tokens will be read and cached, and the number of paths through the graph that can be followed from a single base token", "This sounds great \u2013 we need to make it easier to work with graph token streams!", "How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?", "bq.", "How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?", "We'd end up with extra routes through the graph available via incrementGraph()", "Let's imagine a TokenStream that looks like this: z a/b:4 c d/e:2 f g h", "Starting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h", "Then we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e", "Call incrementGraph() again; we get z b g h", "Now that all routes have been exhausted, calling incrementGraph() will return false.", "How many routes are available depends on how far down the graph you move; if in the example above you only advance as far as 'z a c' on the first branch, then incrementGraph() will move directly to the 'a b g' branch.", "I reworked this slightly; we now have GraphTokenFilter as an abstract extension of TokenFilter, rather than using a separate standalone object.", "This makes reset()/end() easier to handle.", "Updated patch with improved end() processing, passes all tests so I think it's good to go?"], "SplitGT": [" Move to the next token in the current route through the graph"], "issueString": "Make it easier to iterate over graphs in tokenstreams\nWe have a number of TokenFilters that read ahead in the token stream (eg synonyms, shingles) and ideally these would understand token graphs as well as linear streams.  FixedShingleFilter already has some mechanisms to deal with graphs; this issue is to extract this logic into a GraphTokenStream class that can then be reused by other token filters\nHere is a patch adding a GraphTokenStream class.  The class wraps an underlying token stream, and then exposes tokens via the following methods:\r\n- incrementBaseToken() : moves the starting point of the graph forwards\r\n- incrementGraphToken() : moves along the currently selected path through the token graph\r\n- incrementGraph() : resets back to the base token, and selects the next path to move along.  Returns false if all paths have been exhausted\r\n\r\nThe patch also reimplements FixedShingleFilter using GraphTokenStream, to illustrate how much easier it is to reason about how things work.\r\n\r\nTo protect against misuse, there are hard limits on how far ahead in the stream tokens will be read and cached, and the number of paths through the graph that can be followed from a single base token\nThis sounds great \u2013 we need to make it easier to work with graph token streams!\r\n\r\nHow does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?\nbq. How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?\r\n\r\nWe'd end up with extra routes through the graph available via incrementGraph()\r\n\r\nLet's imagine a TokenStream that looks like this: z a/b:4 c d/e:2 f g h\r\n\r\nStarting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h\r\nThen we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e\r\nCall incrementGraph() again; we get z b g h\r\nNow that all routes have been exhausted, calling incrementGraph() will return false.\r\n\r\nHow many routes are available depends on how far down the graph you move; if in the example above you only advance as far as 'z a c' on the first branch, then incrementGraph() will move directly to the 'a b g' branch.\nI reworked this slightly; we now have GraphTokenFilter as an abstract extension of TokenFilter, rather than using a separate standalone object.  This makes reset()/end() easier to handle.\nUpdated patch with improved end() processing, passes all tests so I think it's good to go?\n", "issueSearchSentences": ["Then we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e", "incrementGraphToken() : moves along the currently selected path through the token graph", "incrementGraph() : resets back to the base token, and selects the next path to move along.", "Starting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h", "incrementBaseToken() : moves the starting point of the graph forwards"], "issueSearchScores": [0.7182431221008301, 0.7163915634155273, 0.703116774559021, 0.6547864079475403, 0.6191370487213135]}
{"aId": 11, "code": "public long getFlushingBytes() {\n    return flushControl.getFlushingBytes();\n  }", "comment": " Returns the number of bytes currently being flushed", "issueId": "LUCENE-8471", "issueStringList": ["Expose the number of bytes currently being flushed in IndexWriter", "This is already available via the DocumentWriter and flush control.", "Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer.", "Patch.", "I changed tests that were reaching through DocumentWriter and FlushControl to get this value to use the IndexWriter method instead.", "Can we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?", "Sure.", "Should I do the same for activeBytes() and netBytes()?", "I think we should change only `flushBytes` because we only expose it for now.", "+1 LGTM", "+1 Let's maybe mention in javadocs that this is a subset of what #ramBytesUsed returns?", "+1, but could we name it {{getFlushingBytes}} instead?", "{{flushBytes}} sounds like it's going to write bytes to disk or something.", "Updated patch taking into account feedback.", "I'll commit shortly.", "Thanks all!"], "SplitGT": [" Returns the number of bytes currently being flushed"], "issueString": "Expose the number of bytes currently being flushed in IndexWriter\nThis is already available via the DocumentWriter and flush control.\u00a0 Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer.\nPatch.\u00a0 I changed tests that were reaching through DocumentWriter and FlushControl to get this value to use the IndexWriter method instead.\nCan we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?\nSure.\u00a0 Should I do the same for activeBytes() and netBytes()?\nI think we should change only `flushBytes` because we only expose it for now.\n+1 LGTM\n+1 Let's maybe mention in javadocs that this is a subset of what #ramBytesUsed returns?\n+1, but could we name it {{getFlushingBytes}} instead?\u00a0 {{flushBytes}} sounds like it's going to write bytes to disk or something.\nUpdated patch taking into account feedback.\u00a0 I'll commit shortly.\nThanks all!\n", "issueSearchSentences": ["+1, but could we name it {{getFlushingBytes}} instead?", "I think we should change only `flushBytes` because we only expose it for now.", "{{flushBytes}} sounds like it's going to write bytes to disk or something.", "Can we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?", "Expose the number of bytes currently being flushed in IndexWriter"], "issueSearchScores": [0.6900402307510376, 0.6307712197303772, 0.6009103655815125, 0.5827866792678833, 0.5057005882263184]}
{"aId": 13, "code": "@Override\n  public void setAutoGeneratePhraseQueries(boolean value) {\n    if (splitOnWhitespace == false && value == true) {\n      throw new IllegalArgumentException\n          (\"setAutoGeneratePhraseQueries(true) is disallowed when getSplitOnWhitespace() == false\");\n    }\n    this.autoGeneratePhraseQueries = value;\n  }", "comment": " Set to true if phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text. The combination splitOnWhitespace=false and autoGeneratePhraseQueries=true is disallowed.", "issueId": "LUCENE-7533", "issueStringList": ["Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false", "LUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis.", "From the javadocs for QueryParser.setAutoGeneratePhraseQueries():", "bq.phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text.", "When splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.", "E.g.", "simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.", "Patch that addresses some of this issue, with some failing tests and nocommits.", "The existing autoGeneratePhraseQueries=true approach generates queries exactly as if the query had contained quotation marks, but as I mentioned above, this is inappropriate when splitOnWhitespace=false and the query text contains spaces.", "The approach in the patch is to add a new QueryBuilder method to handle the autoGeneratePhraseQueries=true case.", "The query text is split on whitespace and these tokens' offsets are compared to those produced by the configured analyzer.", "When multiple non-overlapping tokens have offsets within the bounds of a single whitespace-separated token, a phrase query is created.", "If the original token is present as a token overlapping with the first split token, then a disjunction query is created with the original token and the phrase query of the split tokens.", "I've added a couple of tests that show posincr/poslength/offset output from SynonymFilter and WordDelimiterFilter (likely the two most frequently used analysis components that can create split tokens), and both create corrupt token graphs of various kinds (e.g.", "LUCENE-6582, LUCENE-5051), so solving this problem in a complete way just isn't possible right now.", "So I'm not happy with the approach in the patch.", "It only covers a subset of possible token graphs (e.g.", "more than one overlapping multi-term synonym doesn't work).", "And it's a lot of new code solving a problem that AFAIK no user has reported (does anybody even use autoGeneratePhraseQueries=true with classic QP?", "),", "I'd be much happier if we could somehow get TermAutomatonQuery hooked into the query parsers, and then rewrite to simpler queries if possible: LUCENE-6824.", "First thing though is unbreaking SynonymFilter and friends to produce non-broken token graphs though.", "Attempts to do this for SynonymFilter have stalled though: LUCENE-6664.", "(I have a germ of an idea that might break the logjam - I'll post over there.)", "For this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "Thoughts?", "FYI autoGeneratePhraseQueries was never added to the flexible query parser.", "+1 to move towards having proper graphs come out of analysis, and letting query parsers produce TAQ.", "I agree there is a lot of work there though :)", "Thank you for pointing to LUCENE-6824!", "I think that issue can be committed ... it had fallen past the event horizon of my TODO list.", "I'll revive it ...", "Patch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "This is ready to go.", "I'm going to commit shortly.", "I committed the patch to disallow this combination of options.", "Hopefully once we unbreak graph token streams, this can be revisited."], "SplitGT": [" Set to true if phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text.", "The combination splitOnWhitespace=false and autoGeneratePhraseQueries=true is disallowed."], "issueString": "Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false\nLUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis.\n\nFrom the javadocs for QueryParser.setAutoGeneratePhraseQueries(): \nbq.phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text.\n\nWhen splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.  E.g. simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.\nPatch that addresses some of this issue, with some failing tests and nocommits.\n\nThe existing autoGeneratePhraseQueries=true approach generates queries exactly as if the query had contained quotation marks, but as I mentioned above, this is inappropriate when splitOnWhitespace=false and the query text contains spaces.\n\nThe approach in the patch is to add a new QueryBuilder method to handle the autoGeneratePhraseQueries=true case.  The query text is split on whitespace and these tokens' offsets are compared to those produced by the configured analyzer.  When multiple non-overlapping tokens have offsets within the bounds of a single whitespace-separated token, a phrase query is created.  If the original token is present as a token overlapping with the first split token, then a disjunction query is created with the original token and the phrase query of the split tokens.\n\nI've added a couple of tests that show posincr/poslength/offset output from SynonymFilter and WordDelimiterFilter (likely the two most frequently used analysis components that can create split tokens), and both create corrupt token graphs of various kinds (e.g. LUCENE-6582, LUCENE-5051), so solving this problem in a complete way just isn't possible right now.\n\nSo I'm not happy with the approach in the patch.  It only covers a subset of possible token graphs (e.g. more than one overlapping multi-term synonym doesn't work).  And it's a lot of new code solving a problem that AFAIK no user has reported (does anybody even use autoGeneratePhraseQueries=true with classic QP?),\n\nI'd be much happier if we could somehow get TermAutomatonQuery hooked into the query parsers, and then rewrite to simpler queries if possible: LUCENE-6824.  First thing though is unbreaking SynonymFilter and friends to produce non-broken token graphs though.  Attempts to do this for SynonymFilter have stalled though: LUCENE-6664.  (I have a germ of an idea that might break the logjam - I'll post over there.)\n\nFor this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.\n\nThoughts?\nFYI autoGeneratePhraseQueries was never added to the flexible query parser.\n+1 to move towards having proper graphs come out of analysis, and letting query parsers produce TAQ.  I agree there is a lot of work there though :)\n\nThank you for pointing to LUCENE-6824!  I think that issue can be committed ... it had fallen past the event horizon of my TODO list.  I'll revive it ...\nPatch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.\n\nThis is ready to go.  I'm going to commit shortly.\nI committed the patch to disallow this combination of options.  Hopefully once we unbreak graph token streams, this can be revisited.\n", "issueSearchSentences": ["For this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "Patch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "From the javadocs for QueryParser.setAutoGeneratePhraseQueries():", "FYI autoGeneratePhraseQueries was never added to the flexible query parser.", "The existing autoGeneratePhraseQueries=true approach generates queries exactly as if the query had contained quotation marks, but as I mentioned above, this is inappropriate when splitOnWhitespace=false and the query text contains spaces."], "issueSearchScores": [0.7172647714614868, 0.6951202154159546, 0.6909976005554199, 0.6679180860519409, 0.6316945552825928]}
{"aId": 14, "code": "public int[] articulationPoints() {\n    if (det.getNumStates() == 0) {\n      return new int[0];\n    }\n    //\n    Automaton.Builder undirect = new Automaton.Builder();\n    undirect.copy(det);\n    for (int i = 0; i < det.getNumStates(); i++) {\n      int numT = det.initTransition(i, transition);\n      for (int j = 0; j < numT; j++) {\n        det.getNextTransition(transition);\n        undirect.addTransition(transition.dest, i, transition.min);\n      }\n    }\n    int numStates = det.getNumStates();\n    BitSet visited = new BitSet(numStates);\n    int[] depth = new int[det.getNumStates()];\n    int[] low = new int[det.getNumStates()];\n    int[] parent = new int[det.getNumStates()];\n    Arrays.fill(parent, -1);\n    List<Integer> points = new ArrayList<>();\n    articulationPointsRecurse(undirect.finish(), 0, 0, depth, low, parent, visited, points);\n    Collections.reverse(points);\n    return points.stream().mapToInt(p -> p).toArray();\n  }", "comment": " Returns the articulation points (or cut vertices) of the graph: https://en.wikipedia.org/wiki/Biconnected_component", "issueId": "LUCENE-7638", "issueStringList": ["Optimize graph query produced by QueryBuilder", "The QueryBuilder creates a graph query when the underlying TokenStream contains token with PositionLengthAttribute greater than 1.", "These TokenStreams are in fact graphs (lattice to be more precise) where synonyms can span on multiple terms.", "Currently the graph query is built by visiting all the path of the graph TokenStream.", "For instance if you have a synonym like \"ny, new york\" and you search for \"new york city\", the query builder would produce two pathes:", "\"new york city\", \"ny city\"", "This can quickly explode when the number of multi terms synonyms increase.", "The query \"ny ny\" for instance would produce 4 pathes and so on.", "For boolean queries with should or must clauses it should be more efficient to build a boolean query that merges all the intersections in the graph.", "So instead of \"new york city\", \"ny city\" we could produce:", "\"+((+new +york) ny) +city\"", "The attached patch is a proposal to do that instead of the all path solution.", "The patch transforms multi terms synonyms in graph query for each intersection in the graph.", "This is not done in this patch but we could also create a specialized query that gives equivalent scores to multi terms synonyms like the SynonymQuery does for single term synonyms.", "For phrase query this patch does not change the current behavior but we could also use the new method to create optimized graph SpanQuery.", "[~mattweber] I think this patch could optimize a lot of cases where multiple muli-terms synonyms are present in a single request.", "Could you take a look ?", "Maybe {{TermAutomatonQuery}} would be a good fit for that problem?", "I think the problem here is that we lose minimum should match support as that is applied AFTER query generation by building a new boolean query.", "Same thing for phrase slop even though that would not be affected by this patch.", "If we can move this logic into rewrite method of GraphQuery then we could take all that information into consideration to build a more efficient query.", "{quote}", "Maybe TermAutomatonQuery would be a good fit for that problem?", "{quote}", "For pure phrase query it's a good fit because it's a proximity query but for boolean queries the problem is different.", "We cannot build the TermAutomatonQuery directly, first we need to find the start and end state of each multi-term synonyms in the graph.", "That's what the attached patch is doing lazily, for each intersection point it creates a multi-term synonym query.", "Currently the multi-term synonym query is a boolean query but we could change the logic and use the TermAutomatonQuery instead or even create a PhaseQuery for each path in the multi-term synonym.", "This patch also handles nested multi-term synonyms which makes the detection of intersection points harder.", "Bottom point is that if we are able to extract the multi-term synonyms of the graph then we can choose more easily how we want to search and score these inner graph.", "Does this makes sense ?", "[~mattweber] I don't think we lose minimum should match support.", "It will be different but interestingly it would also solve some problems.", "For instance with the all path solution, a synonym like \"ny, new york\" with a minimum should match of 1, searching for \"ny\" would not return documents matching \"new york\".", "With the proposed solution each multi-term synonyms is considered as a single clause so \"ny\" and \"new york\" count for 1.", "I like the finite strings solution because expressing the minimum should match in percentage gives you correct hits.", "This is great though it requires to duplicate a lot of terms so I wonder if this is something that we should really target.", "By considering each multi-term synonyms as 1 clause we could simplify the problem and produce more optimized query ?", "[~jim.ferenczi] I have mixed feelings about that as I can see plus and minus of both.", "When I was originally working on this I essentially decided that everything should be passed to each path as if it was the original query.", "What do you think [~mikemccand]?", "Also, there are additional use cases that we handle in elasticsearch that have not made their way into Lucene yet that might be affected by this.", "Boolean with cutoff frequency, prefix queries, etc.", "This is an impressive patch!", "Thanks [~jim.ferenczi].", "I agree the combinatoric explosion is a serious problem/vulnerability/adversary and I love how this patch solves it.", "Can this handle \"side paths on side paths\" graph structure (I think you called this \"nested multi-term synonyms\")?", "While no analysis chain can naturally produce this today (that I know of!", "), the {{TokenStream}} attributes can easily express it.", "And you could imagine it happening in the future, e.g.", "if you use Kuromoji tokenizer or {{WordDelimiterGraphFilter}} followed by a {{SynonymGraphFilter}} (except we'd need to e.g.", "use the synonym graph filter from LUCENE-5012, which can correctly consume a graph).", "If this is expected to work maybe we should add a test case showing that?", "It seems like you don't need to be using {{Term}} here, except at the end to pass to the {{newXXXQuery}}, since everything is in a single field here, and we are hoping to move away from {{Term}} entirely (LUCENE-7632)?", "Holes are challenging for graph token streams ... can you add a test case that encounters holes, e.g.", "simulated {{StopFilter}}?", "There are at least two \"fun\" cases: a hole that cuts the graph entirely into two partitions, and a synonym spanning over a hole ... {{CannedTokenStream}} is useful for feeding such \"interesting\" cases.", "The {{Path.id}} seems to be used only for tie-breaking on compare, not for lookup in the {{TreeSet}} as the comment suggests?", "On {{minShouldMatch}}: I feel it's actually more correct to define its semantics as operating on whole synonyms, not the individual tokens in each synonym, as this patch does.", "Really, had you done the same synonym reduction at indexing time instead, so that \"new york\" in a document caused \"ny\" to be indexed as well, and then searched on \"ny\", this is the behavior you would see (\"new york\" counts as the 1 {{minShouldMatch}}).", "Of course, in general I think we are in new (to Lucene) territory here, on how graph structured queries should be matched/scored \"properly\", and I don't really know what the answer should be.", "{{TermAutomatonQuery}} [faces similar challenges|https://issues.apache.org/jira/browse/LUCENE-5815?focusedCommentId=14060665&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14060665].", "Maybe there are some IR papers out there that have studied this?", "I even think it's odd that we don't use a {{PhraseQuery}} to match that syn-expanded \"new york\" even when the user didn't put double quotes around the query: had you done that syn at index time, it would \"act like\" a {{PhraseQuery}} ... but this is an unrelated issue!", "Thanks for taking a look [~mattweber] and [~mikemccand]", "{quote}", "Can this handle \"side paths on side paths\" graph structure (I think you called this \"nested multi-term synonyms\")?", "While no analysis chain can naturally produce this today (that I know of!", "), the TokenStream attributes can easily express it.", "And you could imagine it happening in the future, e.g.", "if you use Kuromoji tokenizer or WordDelimiterGraphFilter followed by a SynonymGraphFilter (except we'd need to e.g.", "use the synonym graph filter from LUCENE-5012, which can correctly consume a graph).", "If this is expected to work maybe we should add a test case showing that?", "{quote}", "I'll add a test case because it's expected to work ;) .", "This is also the reason why this patch does not produce {code}PhraseQuery{code} for synonyms.", "For simple \"side paths\" this is easy to do but we would need to switch to Span queries for \"side paths on side paths\" so I though that it could be done in another issue.", "{quote}", "It seems like you don't need to be using Term here, except at the end to pass to the newXXXQuery, since everything is in a single field here, and we are hoping to move away from Term entirely (LUCENE-7632)?", "{quote}", "Thanks, I'll simplify the patch.", "{quote}", "Holes are challenging for graph token streams ... can you add a test case that encounters holes, e.g.", "simulated StopFilter?", "There are at least two \"fun\" cases: a hole that cuts the graph entirely into two partitions, and a synonym spanning over a hole ... CannedTokenStream is useful for feeding such \"interesting\" cases.", "{quote}", "I though that holes would not be a problem for boolean queries but now I am not sure.", "I'll test that.", "{quote}", "The Path.id seems to be used only for tie-breaking on compare, not for lookup in the TreeSet as the comment suggests?", "{quote}", "The comment is misleading.", "It is needed because I use {code}TreeSet#remove{code} which uses compare to check object equality.", "So the {code}Path.id{code} is the unique identifier for the path.", "Ok this is great.", "So going forward we should assume that synonyms are to treated together (single token or multi-token) and ideally multi-token synonyms as a phrase.", "Would it be best to move this logic into {{GraphQuery}} itself?", "This would make it so we can still detect when we are working with graph related queries and be easier to make the various optimizations talked about here.", "Maybe make {{GraphQuery}} store the graph token stream instead of the processed queries and then do the graph processing / query generation when rewrite it called?", "I pushed a new patch that changes how we build boolean graph query with multi-term synonyms.", "It first finds the articulation points of the graph and builds a boolean query for each point.", "The articulation points (or cut vertices) are computed using the algorithm described in:", "https://en.wikipedia.org/wiki/Biconnected_component", "This means that each time we find a state where side paths of different lengths start, we generate all path that start at this state and end at the next articulation points.", "If {quote}QueryBuilder#autoGenerateMultiTermSynonymsPhraseQuery{quote} is set to true, a phrase query is generated for each path, otherwise a boolean query.", "[~mattweber] [~mikemccand] can you take a look ?", "+1, the new patch looks great!", "Thanks [~jim.ferenczi].", "Thanks [~mikemccand].", "Just to be clear this patch creates a BooleanQuery with MUST clauses (or a PhraseQuery if autoGenerateMultiTermSynonymsPhraseQuery is set to true) for each synonym path.", "I'll commit shortly if there are no objections.", "[~jim.ferenczi] Sorry so late been swamped.", "Anyways, this is great!", "I really like this approach, awesome job man!"], "SplitGT": [" Returns the articulation points (or cut vertices) of the graph: https://en.wikipedia.org/wiki/Biconnected_component"], "issueString": "Optimize graph query produced by QueryBuilder\nThe QueryBuilder creates a graph query when the underlying TokenStream contains token with PositionLengthAttribute greater than 1.\nThese TokenStreams are in fact graphs (lattice to be more precise) where synonyms can span on multiple terms. \nCurrently the graph query is built by visiting all the path of the graph TokenStream. For instance if you have a synonym like \"ny, new york\" and you search for \"new york city\", the query builder would produce two pathes:\n\"new york city\", \"ny city\"\nThis can quickly explode when the number of multi terms synonyms increase. \nThe query \"ny ny\" for instance would produce 4 pathes and so on.\nFor boolean queries with should or must clauses it should be more efficient to build a boolean query that merges all the intersections in the graph. So instead of \"new york city\", \"ny city\" we could produce:\n\"+((+new +york) ny) +city\"\n\nThe attached patch is a proposal to do that instead of the all path solution.\nThe patch transforms multi terms synonyms in graph query for each intersection in the graph. This is not done in this patch but we could also create a specialized query that gives equivalent scores to multi terms synonyms like the SynonymQuery does for single term synonyms.\nFor phrase query this patch does not change the current behavior but we could also use the new method to create optimized graph SpanQuery.\n\n[~mattweber] I think this patch could optimize a lot of cases where multiple muli-terms synonyms are present in a single request. Could you take a look ?\nMaybe {{TermAutomatonQuery}} would be a good fit for that problem?\nI think the problem here is that we lose minimum should match support as that is applied AFTER query generation by building a new boolean query.  Same thing for phrase slop even though that would not be affected by this patch.  If we can move this logic into rewrite method of GraphQuery then we could take all that information into consideration to build a more efficient query.\n{quote}\nMaybe TermAutomatonQuery would be a good fit for that problem?\n{quote}\n\n\nFor pure phrase query it's a good fit because it's a proximity query but for boolean queries the problem is different. We cannot build the TermAutomatonQuery directly, first we need to find the start and end state of each multi-term synonyms in the graph. That's what the attached patch is doing lazily, for each intersection point it creates a multi-term synonym query. Currently the multi-term synonym query is a boolean query but we could change the logic and use the TermAutomatonQuery instead or even create a PhaseQuery for each path in the multi-term synonym. This patch also handles nested multi-term synonyms which makes the detection of intersection points harder. \nBottom point is that if we are able to extract the multi-term synonyms of the graph then we can choose more easily how we want to search and score these inner graph. Does this makes sense ?\n[~mattweber] I don't think we lose minimum should match support. It will be different but interestingly it would also solve some problems. For instance with the all path solution, a synonym like \"ny, new york\" with a minimum should match of 1, searching for \"ny\" would not return documents matching \"new york\". With the proposed solution each multi-term synonyms is considered as a single clause so \"ny\" and \"new york\" count for 1.\nI like the finite strings solution because expressing the minimum should match in percentage gives you correct hits. This is great though it requires to duplicate a lot of terms so I wonder if this is something that we should really target. By considering each multi-term synonyms as 1 clause we could simplify the problem and produce more optimized query ?\n[~jim.ferenczi] I have mixed feelings about that as I can see plus and minus of both.  When I was originally working on this I essentially decided that everything should be passed to each path as if it was the original query.  What do you think [~mikemccand]?  Also, there are additional use cases that we handle in elasticsearch that have not made their way into Lucene yet that might be affected by this.  Boolean with cutoff frequency, prefix queries, etc.  \nThis is an impressive patch!  Thanks [~jim.ferenczi].\n\nI agree the combinatoric explosion is a serious problem/vulnerability/adversary and I love how this patch solves it.\n\nCan this handle \"side paths on side paths\" graph structure (I think you called this \"nested multi-term synonyms\")?  While no analysis chain can naturally produce this today (that I know of!), the {{TokenStream}} attributes can easily express it.  And you could imagine it happening in the future, e.g. if you use Kuromoji tokenizer or {{WordDelimiterGraphFilter}} followed by a {{SynonymGraphFilter}} (except we'd need to e.g. use the synonym graph filter from LUCENE-5012, which can correctly consume a graph).  If this is expected to work maybe we should add a test case showing that?\n\nIt seems like you don't need to be using {{Term}} here, except at the end to pass to the {{newXXXQuery}}, since everything is in a single field here, and we are hoping to move away from {{Term}} entirely (LUCENE-7632)?\n\nHoles are challenging for graph token streams ... can you add a test case that encounters holes, e.g. simulated {{StopFilter}}?  There are at least two \"fun\" cases: a hole that cuts the graph entirely into two partitions, and a synonym spanning over a hole ... {{CannedTokenStream}} is useful for feeding such \"interesting\" cases.\n\nThe {{Path.id}} seems to be used only for tie-breaking on compare, not for lookup in the {{TreeSet}} as the comment suggests?\n\nOn {{minShouldMatch}}: I feel it's actually more correct to define its semantics as operating on whole synonyms, not the individual tokens in each synonym, as this patch does.  Really, had you done the same synonym reduction at indexing time instead, so that \"new york\" in a document caused \"ny\" to be indexed as well, and then searched on \"ny\", this is the behavior you would see (\"new york\" counts as the 1 {{minShouldMatch}}).\n\nOf course, in general I think we are in new (to Lucene) territory here, on how graph structured queries should be matched/scored \"properly\", and I don't really know what the answer should be. {{TermAutomatonQuery}} [faces similar challenges|https://issues.apache.org/jira/browse/LUCENE-5815?focusedCommentId=14060665&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14060665].  Maybe there are some IR papers out there that have studied this?\n\nI even think it's odd that we don't use a {{PhraseQuery}} to match that syn-expanded \"new york\" even when the user didn't put double quotes around the query: had you done that syn at index time, it would \"act like\" a {{PhraseQuery}} ... but this is an unrelated issue!\n\nThanks for taking a look [~mattweber] and [~mikemccand]\n\n{quote}\nCan this handle \"side paths on side paths\" graph structure (I think you called this \"nested multi-term synonyms\")? While no analysis chain can naturally produce this today (that I know of!), the TokenStream attributes can easily express it. And you could imagine it happening in the future, e.g. if you use Kuromoji tokenizer or WordDelimiterGraphFilter followed by a SynonymGraphFilter (except we'd need to e.g. use the synonym graph filter from LUCENE-5012, which can correctly consume a graph). If this is expected to work maybe we should add a test case showing that?\n{quote}\n\nI'll add a test case because it's expected to work ;) . This is also the reason why this patch does not produce {code}PhraseQuery{code} for synonyms.   For simple \"side paths\" this is easy to do but we would need to switch to Span queries for \"side paths on side paths\" so I though that it could be done in another issue. \n\n{quote}\nIt seems like you don't need to be using Term here, except at the end to pass to the newXXXQuery, since everything is in a single field here, and we are hoping to move away from Term entirely (LUCENE-7632)?\n{quote}\n\nThanks, I'll simplify the patch.\n\n{quote}\nHoles are challenging for graph token streams ... can you add a test case that encounters holes, e.g. simulated StopFilter? There are at least two \"fun\" cases: a hole that cuts the graph entirely into two partitions, and a synonym spanning over a hole ... CannedTokenStream is useful for feeding such \"interesting\" cases.\n{quote}\n\nI though that holes would not be a problem for boolean queries but now I am not sure. I'll test that.\n\n{quote}\nThe Path.id seems to be used only for tie-breaking on compare, not for lookup in the TreeSet as the comment suggests?\n{quote}\n\nThe comment is misleading. It is needed because I use {code}TreeSet#remove{code} which uses compare to check object equality. So the {code}Path.id{code} is the unique identifier for the path.\n \n\n\n\nOk this is great.  So going forward we should assume that synonyms are to treated together (single token or multi-token) and ideally multi-token synonyms as a phrase.  Would it be best to move this logic into {{GraphQuery}} itself?  This would make it so we can still detect when we are working with graph related queries and be easier to make the various optimizations talked about here.  Maybe make {{GraphQuery}} store the graph token stream instead of the processed queries and then do the graph processing / query generation when rewrite it called?\nI pushed a new patch that changes how we build boolean graph query with multi-term synonyms. It first finds the articulation points of the graph and builds a boolean query for each point. The articulation points (or cut vertices) are computed using the algorithm described in:\nhttps://en.wikipedia.org/wiki/Biconnected_component\nThis means that each time we find a state where side paths of different lengths start, we generate all path that start at this state and end at the next articulation points. If {quote}QueryBuilder#autoGenerateMultiTermSynonymsPhraseQuery{quote} is set to true, a phrase query is generated for each path, otherwise a boolean query. \n\n[~mattweber] [~mikemccand] can you take a look ?\n\n\n+1, the new patch looks great!  Thanks [~jim.ferenczi].\nThanks [~mikemccand].\nJust to be clear this patch creates a BooleanQuery with MUST clauses (or a PhraseQuery if autoGenerateMultiTermSynonymsPhraseQuery is set to true) for each synonym path. \nI'll commit shortly if there are no objections.\n[~jim.ferenczi] Sorry so late been swamped.  Anyways, this is great!  I really like this approach, awesome job man!\n", "issueSearchSentences": ["The articulation points (or cut vertices) are computed using the algorithm described in:", "It first finds the articulation points of the graph and builds a boolean query for each point.", "This means that each time we find a state where side paths of different lengths start, we generate all path that start at this state and end at the next articulation points.", "It will be different but interestingly it would also solve some problems.", "This patch also handles nested multi-term synonyms which makes the detection of intersection points harder."], "issueSearchScores": [0.61942458152771, 0.5400184392929077, 0.497947096824646, 0.410732626914978, 0.3974919319152832]}
{"aId": 16, "code": "public LongIterator iterator() {\n    return new LongIterator() {\n      private boolean hasNext = zeroCount > 0;\n      private int i = -1;\n      private long value = 0;\n\n      @Override\n      public boolean hasNext() {\n        if (hasNext) {\n          // this is only executed the first time for the special case 0 value\n          return true;\n        }\n        while (++i < vals.length) {\n          value = vals[i];\n          if (value != 0) {\n            return hasNext = true;\n          }\n        }\n        return false;\n      }\n\n      @Override\n      public long next() {\n        hasNext = false;\n        return value;\n      }\n\n    };\n  }", "comment": " Returns an iterator over the values in the set.", "issueId": "SOLR-11093", "issueStringList": ["implement Points/Numeric support for graph query", "It looks like GraphQueryTest only has tests for strings.", "We should add tests for numeric fields and then enable points randomization.", "can this be resolved?", "bq.", "can this be resolved?", "Numerics still need to be fixed for graph query - I was going to do it under this issue.", "We already have a specific issue for adding numeric supportto GraphQParser -- which you assigned to your self a few days ago: SOLR-10845", "Yeah, I was confused too at first... these are actually *different* parsers and tests.", "This one is {!graph}, the other one is {!graphTerms}.", "I hadn't even realized the latter existed and I assumed it was the former when I assigned it to myself.", "Oh .. weird, sorry for the noise: carry on.", "OK, after reviewing this more it looks like this was written for strings (and trie* numeric fields worked because they existed in the full-text index, and could be uninverted to docTermOrds).", "To make this work for numeric point fields, we'll need to:", "require docValues", "write new code to collect values from numeric docValues (i.e.", "not docTermOrds and ByteRefHash)", "write new code to create an appropriate frontier query from the collected values", "Any objections to requiring docValues?", "The only real alternative is to implement FieldCache support for Point fields to fake docValues.", "NOTE: I've \"un-subtasked\" this jira since it no longer makes sense as a subtask of SOLR-10807.", "(FWIW: I have no strong opinions about the questions you asked)", "Here's a draft patch with refactorings, creating a base class GraphEdgeCollector and pulling out term-specific code into a subclass GraphTermsCollector.", "Next TODO is to implement GraphNumericCollector that will work with points & numeric docvalues", "status: I've made more progress on this, but I'm currently taking sick day(s).", "Should be able to finish up quickly once I get back to it.", "Here's an updated draft patch that moves LongSet and LongIterator to the util package, uses them in conjunction with numeric docvalues to collect numeric values, and then uses points fields to create a set query.", "Doesn't yet work for some reason...", "I'm digging into why.", "Requiring DocValues for numerics on a GraphQuery is fine with me.", "Is there a back-compat concern?", "Maybe LongSet could use more javadocs if it's going to be shared.", "The constructor should actually assert (or throw) that the argument is a power of 2.", "And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?", "Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?", "Updated patch... tests now pass!", "bq.", "Is there a back-compat concern?", "Everything should be back compat with existing schemas.", "An interesting titbit I discovered while doing this issue: the graph query reversed the meaning of \"from\" and \"to\" wrt the join query.", "Not a bug I guess... just a different way of thinking about what \"from\" and \"to\" mean.", "bq.", "Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?", "I wasn't able to figure out how to make it simpler with an iterator approach (it seems like there always needs to be an additional check for the empty value).", "In a place where the performance difference might matter, one always has the option of grabbing the backing array though.", "Why is zeroCount an integer instead of simply a hasZero boolean?", "Here's my first iteration (subsequently changed futher below).", "All I did was rename {{hasNext}} to {{positioned}} (which I feel is more clear), added some inline comments for what the fields mean, and I added a leading check inside next().", "So this was simpler than I thought.", "{code:java}", "public LongIterator iterator() {", "return new LongIterator() {", "if this set contains zero, this iterator's initial state is already positioned", "private boolean positioned = zeroCount > 0;", "private int i = -1; // current index into vals[]", "private long value = 0; // if positioned, this is our current value", "@Override", "public boolean hasNext() {", "if (positioned) {", "return true;", "}", "while (++i < vals.length) {", "value = vals[i];", "if (value != 0) {", "return positioned = true;", "}", "}", "return false;", "}", "@Override", "public long next() {", "if (!positioned) {", "if (!hasNext()) {", "throw new NoSuchElementException();", "}", "}", "positioned = false;", "return value;", "}", "};", "}", "{code}", "And here's what I think is the most clear approach using a countdown integer:", "{code:java}", "public LongIterator iterator() {", "return new LongIterator() {", "private int remainingValues = cardinality();", "private int valsIdx = 0;", "@Override", "public boolean hasNext() {", "return remainingValues > 0;", "}", "@Override", "public long next() {", "if (!hasNext()) {", "throw new NoSuchElementException();", "}", "remainingValues--;", "if (remainingValues == 0 && zeroCount > 0) {", "return 0;", "}", "while (true) { // guaranteed to find another value if we get here", "long value = vals[valsIdx++];", "if (value != 0) {", "return value;", "}", "}", "}", "};", "{code}", "This has the benefit that we don't loop past the last value of the array.", "It also has fewer state variables.", "Also, you don't \"pay\" any advancement cost in hasNext(); it's only next() where the value is going to actually be consumed."], "SplitGT": [" Returns an iterator over the values in the set."], "issueString": "implement Points/Numeric support for graph query\nIt looks like GraphQueryTest only has tests for strings.  We should add tests for numeric fields and then enable points randomization.\ncan this be resolved?\nbq. can this be resolved?\nNumerics still need to be fixed for graph query - I was going to do it under this issue.\nWe already have a specific issue for adding numeric supportto GraphQParser -- which you assigned to your self a few days ago: SOLR-10845\nYeah, I was confused too at first... these are actually *different* parsers and tests.  This one is {!graph}, the other one is {!graphTerms}.  I hadn't even realized the latter existed and I assumed it was the former when I assigned it to myself.\nOh .. weird, sorry for the noise: carry on.\nOK, after reviewing this more it looks like this was written for strings (and trie* numeric fields worked because they existed in the full-text index, and could be uninverted to docTermOrds).\nTo make this work for numeric point fields, we'll need to:\n- require docValues\n- write new code to collect values from numeric docValues (i.e. not docTermOrds and ByteRefHash)\n- write new code to create an appropriate frontier query from the collected values\n\nAny objections to requiring docValues?  The only real alternative is to implement FieldCache support for Point fields to fake docValues.\nNOTE: I've \"un-subtasked\" this jira since it no longer makes sense as a subtask of SOLR-10807.\n\n(FWIW: I have no strong opinions about the questions you asked)\nHere's a draft patch with refactorings, creating a base class GraphEdgeCollector and pulling out term-specific code into a subclass GraphTermsCollector.\nNext TODO is to implement GraphNumericCollector that will work with points & numeric docvalues\nstatus: I've made more progress on this, but I'm currently taking sick day(s).  Should be able to finish up quickly once I get back to it.\nHere's an updated draft patch that moves LongSet and LongIterator to the util package, uses them in conjunction with numeric docvalues to collect numeric values, and then uses points fields to create a set query.\nDoesn't yet work for some reason... I'm digging into why.\nRequiring DocValues for numerics on a GraphQuery is fine with me.  Is there a back-compat concern?\n\nMaybe LongSet could use more javadocs if it's going to be shared.  The constructor should actually assert (or throw) that the argument is a power of 2.  And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?  Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?\nUpdated patch... tests now pass!\nbq. Is there a back-compat concern?\n\nEverything should be back compat with existing schemas.\nAn interesting titbit I discovered while doing this issue: the graph query reversed the meaning of \"from\" and \"to\" wrt the join query.\nNot a bug I guess... just a different way of thinking about what \"from\" and \"to\" mean.\nbq. Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?\n\nI wasn't able to figure out how to make it simpler with an iterator approach (it seems like there always needs to be an additional check for the empty value).\nIn a place where the performance difference might matter, one always has the option of grabbing the backing array though.\nWhy is zeroCount an integer instead of simply a hasZero boolean?\n\nHere's my first iteration (subsequently changed futher below).  All I did was rename {{hasNext}} to {{positioned}} (which I feel is more clear), added some inline comments for what the fields mean, and I added a leading check inside next(). So this was simpler than I thought.\n{code:java}\n/** Returns an iterator over the values in the set. */\n  public LongIterator iterator() {\n    return new LongIterator() {\n      // if this set contains zero, this iterator's initial state is already positioned\n      private boolean positioned = zeroCount > 0;\n      private int i = -1; // current index into vals[]\n      private long value = 0; // if positioned, this is our current value\n\n      @Override\n      public boolean hasNext() {\n        if (positioned) {\n          return true;\n        }\n        while (++i < vals.length) {\n          value = vals[i];\n          if (value != 0) {\n            return positioned = true;\n          }\n        }\n        return false;\n      }\n\n      @Override\n      public long next() {\n        if (!positioned) {\n          if (!hasNext()) {\n            throw new NoSuchElementException();\n          }\n        }\n        positioned = false;\n        return value;\n      }\n\n    };\n  }\n{code}\n\nAnd here's what I think is the most clear approach using a countdown integer:\n{code:java}\n\n  /** Returns an iterator over the values in the set. */\n  public LongIterator iterator() {\n    return new LongIterator() {\n      private int remainingValues = cardinality();\n      private int valsIdx = 0;\n\n      @Override\n      public boolean hasNext() {\n        return remainingValues > 0;\n      }\n\n      @Override\n      public long next() {\n        if (!hasNext()) {\n          throw new NoSuchElementException();\n        }\n        remainingValues--;\n\n        if (remainingValues == 0 && zeroCount > 0) {\n          return 0;\n        }\n        \n        while (true) { // guaranteed to find another value if we get here\n          long value = vals[valsIdx++];\n          if (value != 0) {\n            return value;\n          }\n        }\n      }\n\n    };\n{code}\nThis has the benefit that we don't loop past the last value of the array.  It also has fewer state variables. Also, you don't \"pay\" any advancement cost in hasNext(); it's only next() where the value is going to actually be consumed.\n", "issueSearchSentences": ["public LongIterator iterator() {", "public LongIterator iterator() {", "return new LongIterator() {", "return new LongIterator() {", "And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?"], "issueSearchScores": [0.8813374042510986, 0.8813374042510986, 0.8290739059448242, 0.8290739059448242, 0.7799067497253418]}
{"aId": 17, "code": "@Override\n  public boolean acceptsDocsOutOfOrder() {\n    return false;\n  }", "comment": " This collector requires that docs be collected in order, otherwise the computed number of scanned docs in the resulting EarlyTerminatingCollectorException will be meaningless.", "issueId": "SOLR-5122", "issueStringList": ["spellcheck.collateMaxCollectDocs estimates seem to be meaninless -- can lead to \"ArithmeticException: / by zero\"", "As part of SOLR-4952 SpellCheckCollatorTest started using RandomMergePolicy, and this (aparently) led to a failure in testEstimatedHitCounts.", "As far as i can tell: the test assumes that specific values would be returned as the _estimated_ \"hits\" for a colleation, and it appears that the change in MergePolicy however resulted in different segments with different term stats, causing the estimation code to produce different values then what is expected.", "I made a quick attempt to improve the test to:", "expect explicit exact values only when spellcheck.collateMaxCollectDocs is set such that the \"estimate' should actually be exact (ie: collateMaxCollectDocs  == 0 or collateMaxCollectDocs greater then the num docs in the index", "randomize the values used for collateMaxCollectDocs and confirm that the estimates are never more then the num docs in the index", "This lead to an odd \"ArithmeticException: / by zero\" error in the test, which seems to suggest that there is a genuine bug in the code for estimating the hits that only gets tickled in certain mergepolicy/segment/collateMaxCollectDocs combinations.", "Update:* This appears to be a general problem with collecting docs out of order and the estimation of hits -- i believe even if there is no divide by zero error, the estimates are largely meaningless since the docs are collected out of order.", "Attached patch applied to trunk r1511141 produces the following error...", "{noformat}", "[junit4]   2> 6500 T10 C1 oasc.SolrCore.execute [collection1] webapp=null path=null params={spellcheck=true&spellcheck.dictionary=direct&spellcheck.count=1&spellcheck.collate=true&spellcheck.maxCollationTries=1&spellcheck.maxCollations=1&spellcheck.collateExtendedResults=true&qt=spellCheckCompRH&q=teststop%3Ametnoia&spellcheck.collateMaxCollectDocs=5} hits=0 status=500 QTime=25", "[junit4]   2> 6501 T10 oasc.SolrException.log ERROR REQUEST FAILED: spellcheck=true&spellcheck.dictionary=direct&spellcheck.count=1&spellcheck.collate=true&spellcheck.maxCollationTries=1&spellcheck.maxCollations=1&spellcheck.collateExtendedResults=true&qt=spellCheckCompRH&q=teststop%3Ametnoia&spellcheck.collateMaxCollectDocs=5:java.lang.ArithmeticException: / by zero", "[junit4]   2> \t\tat org.apache.solr.spelling.SpellCheckCollator.collate(SpellCheckCollator.java:153)", "[junit4]   2> \t\tat org.apache.solr.handler.component.SpellCheckComponent.addCollationsToResponse(SpellCheckComponent.java:229)", "[junit4]   2> \t\tat org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:196)", "[junit4]   2> \t\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:208)", "[junit4]   2> \t\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)", "[junit4]   2> \t\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1845)", "[junit4]   2> \t\tat org.apache.solr.util.TestHarness.query(TestHarness.java:292)", "[junit4]   2> \t\tat org.apache.solr.util.TestHarness.query(TestHarness.java:274)", "[junit4]   2> \t\tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:609)", "[junit4]   2> \t\tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:602)", "[junit4]   2> \t\tat org.apache.solr.spelling.SpellCheckCollatorTest.testEstimatedHitCounts(SpellCheckCollatorTest.java:475)", "...", "[junit4]   2> 6501 T10 oas.SolrTestCaseJ4.tearDown ###Ending testEstimatedHitCounts", "[junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII", "[junit4] ERROR   0.35s | SpellCheckCollatorTest.testEstimatedHitCounts <<<", "[junit4]    > Throwable #1: java.lang.RuntimeException: Exception during query", "[junit4]    > \tat __randomizedtesting.SeedInfo.seed([16B4D8F74E59EE10:270F66C2EB66FEC0]:0)", "[junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:635)", "[junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:602)", "[junit4]    > \tat org.apache.solr.spelling.SpellCheckCollatorTest.testEstimatedHitCounts(SpellCheckCollatorTest.java:475)", "[junit4]    > \tat java.lang.Thread.run(Thread.java:724)", "[junit4]    > Caused by: java.lang.ArithmeticException: / by zero", "[junit4]    > \tat org.apache.solr.spelling.SpellCheckCollator.collate(SpellCheckCollator.java:153)", "[junit4]    > \tat org.apache.solr.handler.component.SpellCheckComponent.addCollationsToResponse(SpellCheckComponent.java:229)", "[junit4]    > \tat org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:196)", "[junit4]    > \tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:208)", "[junit4]    > \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)", "[junit4]    > \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1845)", "[junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:292)", "[junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:274)", "[junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:609)", "[junit4]    > \t... 42 more", "{noformat}", "The problematic line is when catching the EarlyTerminatingCollectorException exception and computing the estimate based on the last doc id collected...", "{noformat}", "hits = maxDocId / ((etce.getLastDocId() + 1) / docCollectionLimit);", "{noformat}", "Unless i'm mising something, the problem comes up when {{(etce.getLastDocId() + 1) < docCollectionLimit}} because then the integer division results in 0, which then becomes the demoninator under {{maxDocId}}", "It would be trivial to toss another \"1+\" in there to eliminate the divide by zero, but i'm confused about the basic assumption taking place here -- it smells fishy -- making any estimation based on getLastDocId() seems to only be useful if we know docs are being collected in order, and when the collateMaxCollectDocs option was added in r1479638, it did force in order collection when using hte early termination...", "https://svn.apache.org/viewvc/lucene/dev/trunk/solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java?r1=1479638&r2=1479637&pathrev=1479638", "...but in r1479645 that use of FORCE_INORDER_COLLECTION was eliminate with the msg \"removing dead code\" ...", "https://svn.apache.org/viewvc/lucene/dev/trunk/solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java?r1=1479645&r2=1479644&pathrev=1479645", "But w/o FORCE_INORDER_COLLECTION I don't see how any estimation based on the lastDocId can ever be meaningful?", "[~jdyer] can you take a look at this?", "Reviewing the comments in SOLR-3240 i think i just figured out hte \"remove dead code\" comment...", "bq.", "I'm also thinking I can safely get rid of the \"forceInorderCollection\" flag because requesting docs sorted by doc-id would enforce the same thing, right?", "...i don't think this assumption is valid.", "I don't think using the {{_docid_}} sort option affects the order that collectors recieve docs, it's just used to register a {{SortField}} using {{SortField.Type.DOC}}, which isn't used until *after* the collector collects \"all\" of the docs.", "So i think we need to add back in the FORCE_INORDER_COLLECTION", "FYI: I attempted ot do a simple revert of r1479645 and the test still fails -- but reviewing hte diff i think that's because there doesn't seem to be anything paying attention to the FORCE_INORDER_COLLECTION flag at collection time, so it's effectively useless.", "I'm at a loss to really understand what the correct fix should be at this point", "Hoss,", "I appreciate your reporting this & taking care of this as much as possible.", "Do you know offhand a failing seed for this test?", "(I've been away for awhile and might not have the jenkins log easily available.)", "I will look at this.", "Likely, I need to require docs to be collected in order and mistakenly thought this was unnecessary.", "The initial jenkins failure i saw was \"At revision 1511278\"...", "https://builds.apache.org/job/Lucene-Solr-NightlyTests-trunk/343/", "https://mail-archives.apache.org/mod_mbox/lucene-dev/201308.mbox/%3Calpine.DEB.2.02.1308070919170.13959@frisbee%3E", "{quote}", "I can reproduce this -- it's probably related to the MP randomization i", "put in ... looks like it's doing exact numeric comparisons based on term", "stats.", "I'll take a look later today...", "ant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true   -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII", "{quote}", "...regardless of he initial failure though, if you try out the patch i attached to try and improve the test coverage, then the \"reproduce\" line from the failure i posted along iwth that patch still reproduces on trunk (but you do have to manually uncomment the {{@Ignore}}...", "{code}", "ant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII", "{code}", "updated patch to trunk and included the commenting out of the {{@Ignore}} so all ou need to do is apply this patch to reproduce with the previously mentioned seed.", "The scenarios tested in testEstimatedHitCounts() seem to always pick a collector that does not accept docs out-of-order (\"TopFieldCollector$OneComparatorNonScoringCollector\").", "The problem looks like when a new segment/scorer is set, we get a new set of doc id's.", "So prior to random merges, the test naively assummed everything was on 1 segment.", "Now with multiple, all bets are off and I don't think we can be estimating hits.", "I think the best fix is to dial back the functionality here and not offer hit estimates at all.", "The functionality still would be beneficial in cases the user did not require hit-counts to be returned at all (for instance, ~rmuir mentioned using this feature with suggesters).", "Another option is to add together the doc ids for the various scorers that are looked at and pretend this is your max doc id.", "I'm torn here because I'd hate to remove functionality that has been released but on the other hand if it is always going to give lousy estimates then why fool people?", "Thoughts?", "bq.", "So prior to random merges, the test naively assummed everything was on 1 segment.", "Now with multiple, all bets are off and I don't think we can be estimating hits.", "I'm not following you here -- why don't you think the basic approach to estimation can still work?", "the only missing pieces seem to be that when an estimation is requested:", "docs *must* be collected in order -- a property that forces this behavior from EarlyTerminatingCollector.acceptsDocsOutOfOrder regardless of what the delegate cares about should do the trick.", "lastDocId needs to be absolute, not per-segment -- which could be done by tracking the reader offsets in EarlyTerminatingCollector.setNextReader and using that offset when assigning lastDocId in EarlyTerminatingCollector.collect", "...and that should make it work as you previously designed it ... right?", "here's a patch that improves EarlyTerminatingCollector to keep track of the size of each reader it collects against so that it can derive some meaning from the docIds it collects.", "As part of this patch i eliminated the use of the \"lastDocId\" to try and discourage people from trying to find specific -- instead the EarlyTerminatingCollectorException now just reports the number of docs \"collected\" out of the total number of docs \"scanned\" ... the result is that the collector doesn't really care which order it gets the AtomicReaderContexts in, however it still has to force documents to be collected in order, so that they will be in-order within a single reader so that the stats for that reader can be meaningful.", "patch includes the previous tests, plus a new test loop that we get a reasonably accurate estimate from a term that is in every other doc in the index.", "[~jdyer] - does this look right to you?", "does it address your concerns about keeping hte estimation code in place?", "Hoss,", "This looks reasonable to me.", "The test is more forgiving to variations caused by random merges, no more integer division, etc.", "I appreciate your working on this as I wouldn't have much more time until next week.", "I think your method of estimating the hits would be at least as good of what I attempted to do before.", "Committed revision 1514402.", "Committed revision 1514408."], "SplitGT": [" This collector requires that docs be collected in order, otherwise the computed number of scanned docs in the resulting EarlyTerminatingCollectorException will be meaningless."], "issueString": "spellcheck.collateMaxCollectDocs estimates seem to be meaninless -- can lead to \"ArithmeticException: / by zero\"\nAs part of SOLR-4952 SpellCheckCollatorTest started using RandomMergePolicy, and this (aparently) led to a failure in testEstimatedHitCounts.\n\nAs far as i can tell: the test assumes that specific values would be returned as the _estimated_ \"hits\" for a colleation, and it appears that the change in MergePolicy however resulted in different segments with different term stats, causing the estimation code to produce different values then what is expected.\n\nI made a quick attempt to improve the test to:\n * expect explicit exact values only when spellcheck.collateMaxCollectDocs is set such that the \"estimate' should actually be exact (ie: collateMaxCollectDocs  == 0 or collateMaxCollectDocs greater then the num docs in the index\n * randomize the values used for collateMaxCollectDocs and confirm that the estimates are never more then the num docs in the index\n\nThis lead to an odd \"ArithmeticException: / by zero\" error in the test, which seems to suggest that there is a genuine bug in the code for estimating the hits that only gets tickled in certain mergepolicy/segment/collateMaxCollectDocs combinations.\n\n*Update:* This appears to be a general problem with collecting docs out of order and the estimation of hits -- i believe even if there is no divide by zero error, the estimates are largely meaningless since the docs are collected out of order.\nAttached patch applied to trunk r1511141 produces the following error...\n\n{noformat}\n   [junit4]   2> 6500 T10 C1 oasc.SolrCore.execute [collection1] webapp=null path=null params={spellcheck=true&spellcheck.dictionary=direct&spellcheck.count=1&spellcheck.collate=true&spellcheck.maxCollationTries=1&spellcheck.maxCollations=1&spellcheck.collateExtendedResults=true&qt=spellCheckCompRH&q=teststop%3Ametnoia&spellcheck.collateMaxCollectDocs=5} hits=0 status=500 QTime=25 \n   [junit4]   2> 6501 T10 oasc.SolrException.log ERROR REQUEST FAILED: spellcheck=true&spellcheck.dictionary=direct&spellcheck.count=1&spellcheck.collate=true&spellcheck.maxCollationTries=1&spellcheck.maxCollations=1&spellcheck.collateExtendedResults=true&qt=spellCheckCompRH&q=teststop%3Ametnoia&spellcheck.collateMaxCollectDocs=5:java.lang.ArithmeticException: / by zero\n   [junit4]   2> \t\tat org.apache.solr.spelling.SpellCheckCollator.collate(SpellCheckCollator.java:153)\n   [junit4]   2> \t\tat org.apache.solr.handler.component.SpellCheckComponent.addCollationsToResponse(SpellCheckComponent.java:229)\n   [junit4]   2> \t\tat org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:196)\n   [junit4]   2> \t\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:208)\n   [junit4]   2> \t\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n   [junit4]   2> \t\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1845)\n   [junit4]   2> \t\tat org.apache.solr.util.TestHarness.query(TestHarness.java:292)\n   [junit4]   2> \t\tat org.apache.solr.util.TestHarness.query(TestHarness.java:274)\n   [junit4]   2> \t\tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:609)\n   [junit4]   2> \t\tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:602)\n   [junit4]   2> \t\tat org.apache.solr.spelling.SpellCheckCollatorTest.testEstimatedHitCounts(SpellCheckCollatorTest.java:475)\n...\n   [junit4]   2> 6501 T10 oas.SolrTestCaseJ4.tearDown ###Ending testEstimatedHitCounts\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.35s | SpellCheckCollatorTest.testEstimatedHitCounts <<<\n   [junit4]    > Throwable #1: java.lang.RuntimeException: Exception during query\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([16B4D8F74E59EE10:270F66C2EB66FEC0]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:635)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:602)\n   [junit4]    > \tat org.apache.solr.spelling.SpellCheckCollatorTest.testEstimatedHitCounts(SpellCheckCollatorTest.java:475)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:724)\n   [junit4]    > Caused by: java.lang.ArithmeticException: / by zero\n   [junit4]    > \tat org.apache.solr.spelling.SpellCheckCollator.collate(SpellCheckCollator.java:153)\n   [junit4]    > \tat org.apache.solr.handler.component.SpellCheckComponent.addCollationsToResponse(SpellCheckComponent.java:229)\n   [junit4]    > \tat org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:196)\n   [junit4]    > \tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:208)\n   [junit4]    > \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n   [junit4]    > \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1845)\n   [junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:292)\n   [junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:274)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:609)\n   [junit4]    > \t... 42 more\n{noformat}\nThe problematic line is when catching the EarlyTerminatingCollectorException exception and computing the estimate based on the last doc id collected...\n\n{noformat}\nhits = maxDocId / ((etce.getLastDocId() + 1) / docCollectionLimit);\n{noformat}\n\nUnless i'm mising something, the problem comes up when {{(etce.getLastDocId() + 1) < docCollectionLimit}} because then the integer division results in 0, which then becomes the demoninator under {{maxDocId}}\n\nIt would be trivial to toss another \"1+\" in there to eliminate the divide by zero, but i'm confused about the basic assumption taking place here -- it smells fishy -- making any estimation based on getLastDocId() seems to only be useful if we know docs are being collected in order, and when the collateMaxCollectDocs option was added in r1479638, it did force in order collection when using hte early termination...\n\nhttps://svn.apache.org/viewvc/lucene/dev/trunk/solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java?r1=1479638&r2=1479637&pathrev=1479638\n\n...but in r1479645 that use of FORCE_INORDER_COLLECTION was eliminate with the msg \"removing dead code\" ...\n\nhttps://svn.apache.org/viewvc/lucene/dev/trunk/solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java?r1=1479645&r2=1479644&pathrev=1479645\n\nBut w/o FORCE_INORDER_COLLECTION I don't see how any estimation based on the lastDocId can ever be meaningful?\n\n\n[~jdyer] can you take a look at this?\n\nReviewing the comments in SOLR-3240 i think i just figured out hte \"remove dead code\" comment...\n\nbq. I'm also thinking I can safely get rid of the \"forceInorderCollection\" flag because requesting docs sorted by doc-id would enforce the same thing, right?\n\n...i don't think this assumption is valid.  I don't think using the {{_docid_}} sort option affects the order that collectors recieve docs, it's just used to register a {{SortField}} using {{SortField.Type.DOC}}, which isn't used until *after* the collector collects \"all\" of the docs.\n\nSo i think we need to add back in the FORCE_INORDER_COLLECTION\nFYI: I attempted ot do a simple revert of r1479645 and the test still fails -- but reviewing hte diff i think that's because there doesn't seem to be anything paying attention to the FORCE_INORDER_COLLECTION flag at collection time, so it's effectively useless.\n\nI'm at a loss to really understand what the correct fix should be at this point\nHoss,\n\nI appreciate your reporting this & taking care of this as much as possible.  Do you know offhand a failing seed for this test?  (I've been away for awhile and might not have the jenkins log easily available.)  I will look at this.  Likely, I need to require docs to be collected in order and mistakenly thought this was unnecessary.\nThe initial jenkins failure i saw was \"At revision 1511278\"...\n\nhttps://builds.apache.org/job/Lucene-Solr-NightlyTests-trunk/343/\nhttps://mail-archives.apache.org/mod_mbox/lucene-dev/201308.mbox/%3Calpine.DEB.2.02.1308070919170.13959@frisbee%3E\n\n{quote}\nI can reproduce this -- it's probably related to the MP randomization i \nput in ... looks like it's doing exact numeric comparisons based on term \nstats.  I'll take a look later today...\n\nant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true   -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII\n{quote}\n\n...regardless of he initial failure though, if you try out the patch i attached to try and improve the test coverage, then the \"reproduce\" line from the failure i posted along iwth that patch still reproduces on trunk (but you do have to manually uncomment the {{@Ignore}}...\n\n{code}\nant test  -Dtestcase=SpellCheckCollatorTest -Dtests.method=testEstimatedHitCounts -Dtests.seed=16B4D8F74E59EE10 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=nl -Dtests.timezone=America/Dawson -Dtests.file.encoding=US-ASCII\n{code}\nupdated patch to trunk and included the commenting out of the {{@Ignore}} so all ou need to do is apply this patch to reproduce with the previously mentioned seed.\nThe scenarios tested in testEstimatedHitCounts() seem to always pick a collector that does not accept docs out-of-order (\"TopFieldCollector$OneComparatorNonScoringCollector\").  The problem looks like when a new segment/scorer is set, we get a new set of doc id's.  So prior to random merges, the test naively assummed everything was on 1 segment.  Now with multiple, all bets are off and I don't think we can be estimating hits.\n\nI think the best fix is to dial back the functionality here and not offer hit estimates at all.  The functionality still would be beneficial in cases the user did not require hit-counts to be returned at all (for instance, ~rmuir mentioned using this feature with suggesters).  \n\nAnother option is to add together the doc ids for the various scorers that are looked at and pretend this is your max doc id.  I'm torn here because I'd hate to remove functionality that has been released but on the other hand if it is always going to give lousy estimates then why fool people?\n\nThoughts?\nbq. So prior to random merges, the test naively assummed everything was on 1 segment. Now with multiple, all bets are off and I don't think we can be estimating hits.\n\nI'm not following you here -- why don't you think the basic approach to estimation can still work?\n\nthe only missing pieces seem to be that when an estimation is requested:\n* docs *must* be collected in order -- a property that forces this behavior from EarlyTerminatingCollector.acceptsDocsOutOfOrder regardless of what the delegate cares about should do the trick.\n* lastDocId needs to be absolute, not per-segment -- which could be done by tracking the reader offsets in EarlyTerminatingCollector.setNextReader and using that offset when assigning lastDocId in EarlyTerminatingCollector.collect\n\n...and that should make it work as you previously designed it ... right?\n\nhere's a patch that improves EarlyTerminatingCollector to keep track of the size of each reader it collects against so that it can derive some meaning from the docIds it collects.  As part of this patch i eliminated the use of the \"lastDocId\" to try and discourage people from trying to find specific -- instead the EarlyTerminatingCollectorException now just reports the number of docs \"collected\" out of the total number of docs \"scanned\" ... the result is that the collector doesn't really care which order it gets the AtomicReaderContexts in, however it still has to force documents to be collected in order, so that they will be in-order within a single reader so that the stats for that reader can be meaningful.\n\npatch includes the previous tests, plus a new test loop that we get a reasonably accurate estimate from a term that is in every other doc in the index.\n\n[~jdyer] - does this look right to you? does it address your concerns about keeping hte estimation code in place?\nHoss,\n\nThis looks reasonable to me.  The test is more forgiving to variations caused by random merges, no more integer division, etc.  I appreciate your working on this as I wouldn't have much more time until next week.  I think your method of estimating the hits would be at least as good of what I attempted to do before.\nCommitted revision 1514402.\nCommitted revision 1514408.\n\n", "issueSearchSentences": ["docs *must* be collected in order -- a property that forces this behavior from EarlyTerminatingCollector.acceptsDocsOutOfOrder regardless of what the delegate cares about should do the trick.", "I'm also thinking I can safely get rid of the \"forceInorderCollection\" flag because requesting docs sorted by doc-id would enforce the same thing, right?", "It would be trivial to toss another \"1+\" in there to eliminate the divide by zero, but i'm confused about the basic assumption taking place here -- it smells fishy -- making any estimation based on getLastDocId() seems to only be useful if we know docs are being collected in order, and when the collateMaxCollectDocs option was added in r1479638, it did force in order collection when using hte early termination...", "But w/o FORCE_INORDER_COLLECTION I don't see how any estimation based on the lastDocId can ever be meaningful?", "Likely, I need to require docs to be collected in order and mistakenly thought this was unnecessary."], "issueSearchScores": [0.53961181640625, 0.5316527485847473, 0.4735030233860016, 0.47317400574684143, 0.4164596498012543]}
{"aId": 18, "code": "public static void intToPrefixCodedBytes(final int val, final int shift, final BytesRef bytes) {\n    if ((shift & ~0x1f) != 0)  // ensure shift is 0..31\n      throw new IllegalArgumentException(\"Illegal shift value, must be 0..31\");\n    int nChars = (((31-shift)*37)>>8) + 1;    // i/7 is the same as (i*37)>>8 for i in 0..63\n    bytes.offset = 0;\n    bytes.length = nChars+1;   // one extra for the byte that contains the shift info\n    if (bytes.bytes.length < bytes.length) {\n      bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max\n    }\n    bytes.bytes[0] = (byte)(SHIFT_START_INT + shift);\n    int sortableBits = val ^ 0x80000000;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n  }", "comment": " This is method is used by NumericTokenStream.", "issueId": "LUCENE-4690", "issueStringList": ["Optimize NumericUtils.", "*ToPrefixCoded(), add versions that don't hash", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).", "edit: Uwe pointed out they were used in one place.", "Other places still don't need it.", "Current code:", "{code}", "public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "if (shift>63 || shift<0)", "throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");", "int hash, nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;", "if (bytes.bytes.length < bytes.length) {", "bytes.grow(NumericUtils.BUF_SIZE_LONG);", "}", "bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "calculate hash", "for (int i = 1; i < bytes.length; i++) {", "hash = 31*hash + bytes.bytes[i];", "}", "return hash;", "}", "{code}", "Proposed code template (i.e.", "for all of the *ToPrefixCoded methods):", "{code}", "public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "assert (shift & ~0x3f) == 0;  // ensure shift is 0..63", "int nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;   // one extra for the byte that contains the shift info", "if (bytes.bytes.length < bytes.length) {", "bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max", "}", "bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "}", "{code}", "Some of the changes:", "Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.", "Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.", "replace grow with simple allocation.", "1. grow over-allocates all the time.", "Most of the time (like here) it's wasted space.", "2. grow copies the previous buffer when allocating a bigger buffer.", "This is wasted/unneeded here.", "removes hash code calculation", "An additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.", "One equivalent of i/7 is (i*37)>>8 for i in 0..63.", "This only takes 4 cycles.", "Curious, how do you know that and/or measure that?", "bq.", "Curious, how do you know that and/or measure that?", "The number of cycles?", "It's all documented in various places.", "Of course one needs a good sense of what assembly a compiler/hotspot will emit.", "Integer multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).", "http://gmplib.org/~tege/x86-timing.pdf", "http://www.agner.org/optimize/instruction_tables.pdf", "bq.", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "{code:java}", "@Override", "public int fillBytesRef() {", "try {", "assert valueSize == 64 || valueSize == 32;", "return (valueSize == 64) ?", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "} catch (IllegalArgumentException iae) {", "return empty token before first or after last", "bytes.length = 0;", "return 0;", "}", "}", "{code}", "Other comments:", "The masking away of invalid shifts is a no-go to me.", "This leads to unexpected behaviour.", "A agree grow() does not need to be used for this stuff.", "We can simply reallocate, as we know size exactly.", "By the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.", "bq.", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "Ahh, I didn't see it because the use of the value is on a separate line from the method call.", "Makes it hard to find.", "Here's a patch that doubles the performance of NumericUtils.", "*ToPrefixCoded", "Seems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)", "Thanks, Yonik!"], "SplitGT": [" This is method is used by NumericTokenStream."], "issueString": "Optimize NumericUtils.*ToPrefixCoded(), add versions that don't hash\nAs far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).  If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).\n\nedit: Uwe pointed out they were used in one place.  Other places still don't need it.\nCurrent code:\n{code}\n  public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {\n    if (shift>63 || shift<0)\n      throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");\n    int hash, nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;\n    if (bytes.bytes.length < bytes.length) {\n      bytes.grow(NumericUtils.BUF_SIZE_LONG);\n    }\n    bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n    // calculate hash\n    for (int i = 1; i < bytes.length; i++) {\n      hash = 31*hash + bytes.bytes[i];\n    }\n    return hash;\n  }\n{code}\n\nProposed code template (i.e. for all of the *ToPrefixCoded methods):\n{code}\n  public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {\n    assert (shift & ~0x3f) == 0;  // ensure shift is 0..63\n    int nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;   // one extra for the byte that contains the shift info\n    if (bytes.bytes.length < bytes.length) {\n      bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max\n    }\n    bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n  }\n{code}\n\nSome of the changes:\n - Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.  Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.\n - replace grow with simple allocation.\n   1. grow over-allocates all the time.  Most of the time (like here) it's wasted space.\n   2. grow copies the previous buffer when allocating a bigger buffer.  This is wasted/unneeded here.\n - removes hash code calculation\nAn additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.  One equivalent of i/7 is (i*37)>>8 for i in 0..63.  This only takes 4 cycles.\nCurious, how do you know that and/or measure that?\nbq. Curious, how do you know that and/or measure that?\n\nThe number of cycles?  It's all documented in various places.  Of course one needs a good sense of what assembly a compiler/hotspot will emit.\nInteger multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).\n\nhttp://gmplib.org/~tege/x86-timing.pdf\nhttp://www.agner.org/optimize/instruction_tables.pdf\nbq. As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).\n\nThe return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\n{code:java}\n    @Override\n    public int fillBytesRef() {\n      try {\n        assert valueSize == 64 || valueSize == 32;\n        return (valueSize == 64) ? \n          NumericUtils.longToPrefixCoded(value, shift, bytes) :\n          NumericUtils.intToPrefixCoded((int) value, shift, bytes);\n      } catch (IllegalArgumentException iae) {\n        // return empty token before first or after last\n        bytes.length = 0;\n        return 0;\n      }\n    }\n{code}\n\nOther comments:\n- The masking away of invalid shifts is a no-go to me. This leads to unexpected behaviour.\n- A agree grow() does not need to be used for this stuff. We can simply reallocate, as we know size exactly.\nBy the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.\nbq. The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\nAhh, I didn't see it because the use of the value is on a separate line from the method call.  Makes it hard to find.\nHere's a patch that doubles the performance of NumericUtils.*ToPrefixCoded\n\nSeems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)\nThanks, Yonik!\n", "issueSearchSentences": ["public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "*ToPrefixCoded"], "issueSearchScores": [0.8291856050491333, 0.82151198387146, 0.7902541160583496, 0.7309226989746094, 0.5858094692230225]}
{"aId": 19, "code": "protected void seekChild(int level) throws IOException {\r\n    skipStream[level].seek(lastChildPointer);\r\n    numSkipped[level] = numSkipped[level + 1] - skipInterval[level + 1];\r\n    skipDoc[level] = lastDoc;\r\n    if (level > 0) {\r\n        childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];\r\n    }\r\n  }", "comment": " Seeks the skip entry on the given level", "issueId": "LUCENE-866", "issueStringList": ["Multi-level skipping on posting lists", "To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists.", "The default skip interval is set to 16.", "If we want to skip e. g. 100 documents,", "then it is not necessary to read 100 entries from the posting list, but only", "100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list.", "This", "speeds up conjunction (AND) and phrase queries significantly.", "However, the skip interval is always a compromise.", "If you have a very big index", "with huge posting lists and you want to skip over lets say 100k documents, then", "it is still necessary to read 100k/16 = 6250 entries from the skip list.", "For big", "indexes the skip interval could be set to a higher value, but then after a big", "skip a long scan to the target doc might be necessary.", "A solution for this compromise is to have multi-level skip lists that guarantee a", "logarithmic amount of skips to any target in the posting list.", "This patch", "implements such an approach in the following way:", "Example for skipInterval = 3:", "c            (skip level 2)", "c                 c                 c            (skip level 1)", "x     x     x     x     x     x     x     x     x     x      (skip level 0)", "d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)", "3     6     9     12    15    18    21    24    27    30     (df)", "d - document", "x - skip data", "c - skip data with child pointer", "Skip level i contains every skipInterval-th entry from skip level i-1.", "Therefore the", "number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).", "Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in", "list i-1.", "This guarantees a logarithmic amount of skips to find the target document.", "Implementations details:", "I factored the skipping code out of SegmentMerger and SegmentTermDocs to", "simplify those classes.", "The two new classes AbstractSkipListReader and", "AbstractSkipListWriter implement the skipping functionality.", "While AbstractSkipListReader and Writer take care of writing and reading the", "multiple skip levels, they do not implement an actual skip data format.", "The two", "new subclasses DefaultSkipListReader and Writer implement the skip data format", "that is currently used in Lucene (with two file pointers for the freq and prox", "file and with payload length information).", "I added this extra layer to be", "prepared for flexible indexing and different posting list formats.", "File format changes:", "I added the new parameter 'maxSkipLevels' to the term dictionary and increased the", "version of this file.", "If maxSkipLevels is set to one, then the format of the freq", "file does not change at all, because we only have one skip level as before.", "For", "backwards compatibility maxSkipLevels is set to one automatically if an index", "without the new parameter is read.", "In case maxSkipLevels > 1, then the frq file changes as follows:", "FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount", "SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels,", "floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>", "SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))", "Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not", "needed, and 2) the format of this file does not change for maxSkipLevels=1 then.", "All unit tests pass with this patch.", "Attaching the first version of this patch.", "I ran a few performance tests with this patch.", "I built a rather big", "index (about 1.2GB) using documents from Wikipedia and optimized the", "index to get big posting lists.", "I compare query evaluation time with one-level skipping (old) and", "multi-level skipping (new) and measure the amount of I/O by counting", "the number of VInts read from disk.", "Each query runs several thousand", "times.", "The following queries contain very frequent and very unique terms.", "For these queries the speedup with multi-level skipping is", "significant:", "Query: +lucene +search +engine +library", "5 total matching documents", "VInt reads: old: 143268000, new: 3933000, -97.25479520897898%", "Time: old: 7234ms, new: 1157ms, -84.00608238871993%", "Query: +apache +http +server", "181 total matching documents", "VInt reads: old: 155892000, new: 27849000, -82.13570933723346%", "Time: old: 10656ms, new: 5703ms, -46.48085585585586%", "Even though I/O is reduced for the next query, it runs a bit slower.", "I believe the reason is that the same query runs several thousand", "times, so the posting lists will be loaded into the file system", "cache and the effect of less I/O is reduced, while the skipping", "algorithm itself is a bit more complex:", "Query: +multi +level +skipping", "13 total matching documents", "VInt reads: old: 42894000, new: 39096000, -8.854385228703315%", "Time: old: 3875ms, new: 3922ms, 1.2129032258064516%", "For the next query there is slightly more I/O necessary in the", "multi-skipping case.", "This is because not many big skips can be made,", "but more skipping data has to be read.", "The top 2 skip levels are", "buffered in this first version of the patch and if no big skips", "can be made than this buffering is overhead compared to the", "single-level case:", "Query: +beatles +yellow +submarine", "78 total matching documents", "VInt reads: old: 38460000, new: 38685000, 0.5850234009360374%", "Time: old: 3172ms, new: 3265ms, 2.9319041614123584%", "However, if I change the query a little bit, then the speed-up", "is significant (due to the very frequent stop word \"the\"):", "Query: +\"the beatles\" +yellow +submarine", "77 total matching documents", "VInt reads: old: 382307000, new: 262331000, -31.38210914265237%", "Time: old: 26703ms, new: 22828ms, -14.51147811107366%", "It would be interesting to run more sophisticated benchmarks.", "To run the same query several times is not very realistic", "because it reduces the effects of the I/O savings due to caching.", "I'm not that familiar with the new benchmark stuff that has", "been added recently, but I'll try to dig into that next week.", "Looks interesting!", "Have you done any performance testing?", "I guess best case for this patch would be \"skip to a random doc\", and worst case would be skipTo(currDoc+2) in a loop  (or a conjunction across terms that appear in almost all docs).", "I haven't dug into the code, but do you avoid storing extra levels when the docfreq is small?", "What is the size cost for terms with docfreq=1?", "> and worst case would be skipTo(currDoc+2) in a loop", "> (or a conjunction across terms that appear in almost all docs).", "True that should be the worst case.", "A query with 3 stop words", "takes about 2% longer:", "Query: +the +and +a", "682737 total matching documents", "VInt reads: old: 481897700, new: 481416800, -0.09979296435737295%", "Time: old: 27812ms, new: 28406ms, 2.1357687329210413%", "Maybe a possible optimization could be to avoid using the higher levels", "after a certain amount of small skips have been performed.", "I will try out if", "this will improve things.", "> do you avoid storing extra levels when the docfreq is small?", "> What is the size cost for terms with docfreq=1?", "I only store another level if it contains at least one SkipDatum.", "So there is no overhead for terms with df=1.", "The .frq file in my index grew by 1.3% in the multi-level case", "for an index with about 170MB.", "LUCENE-888 will introduce the new method BufferedIndexInput.setBufferSize().", "Since the length of the different skip levels is known I can set the buffer length before I clone the skip stream in this patch.", "I ran some more performance experiments to test the average speedup.", "I used the same index (about 1.2GB, optimized, docs from wikipedia)", "and created 50,000 random queries.", "Each query has 3 AND terms and", "each term has a df > 100.", "Here are the results:", "old (one level skipping):", "Time: 62141 ms.", "VInt reads: 752430441", "new (multi level):", "Time: 51969 ms.", "VInt reads: 435504734", "This is a speedup of about 16% and i/o savings of 42%.", "Then I changed the algorithm a bit to not always start on the", "highest skip level to find the next skip but to start at level 0", "and walk up until the next skip point is greater than the target.", "This speeds up things further so that the overall gain is about", "20%:", "Time: 49500 ms.", "Bytes read: 435504734", "This 20% speedup is for average AND queries.", "Certain queries", "benefit even more from multi-level skipping as the results", "show that I attached earlier.", "I will attach a new patch as soon as LUCENE-888 is committed.", "New version of the patch with the following changes:", "Applies cleanly on current trunk", "Renamed AbstractSkipListReader and AbstractSkipListWriter", "to MultiLevelSkipListReader and MultiLevelSkipListWriter", "The skipTo() algorithm now starts with the lowest level", "and walks up until it finds the highest level that has a", "skip for the target.", "Uses the new BufferedIndexInput.setBufferSize() method to", "reduce memory consumption in case skip levels occupy less", "space than BufferedIndexInput.BUFFER_SIZE (1024 bytes)", "Only preload the top level into memory.", "That saves memory", "and almost doesn't affect performance.", "All unit tests pass.", "I think this patch is ready to commit now.", "Patch applies cleanly here.", "All tests pass, except for org.apache.lucene.store.TestBufferedIndexInput:", "[junit] Testsuite: org.apache.lucene.store.TestBufferedIndexInput", "[junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 2.536 sec", "[junit]", "[junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):        Caused an ERROR", "[junit] null", "[junit] java.lang.NullPointerException", "[junit]     at org.apache.lucene.store.BufferedIndexInput.setBufferSize(BufferedIndexInput.java:52)", "[junit]     at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:210)", "[junit]     at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:164)", "[junit]", "[junit]", "[junit] Test org.apache.lucene.store.TestBufferedIndexInput FAILED", "It's possible that this is due to another diff in my working copy, but a quick look did", "not reveal anything suspicious.", "In case I'm the only one with this test failing, I'll dig deeper.", "On a clean checkout it applies cleanly and this test (and all others) pass.", "Hmm -- that NullPointerException I think is from the assert I added for LUCENE-888.", "I will fix.", "The last change to BufferedIndexInput indeed makes all core tests pass again here with this patch applied.", "Thanks for reviewing Paul and Doron, and thanks for fixing the NPE, Mike!", "!", "I'm planning to commit this in a day or so.", "Nice job!", "Starting at the lowest level will probably improve the worst-case scenario for this patch (which wasn't nearly as bad as I expected in the first place).", "+1 to commit", "This patch updates the fileformats document.", "It also adds a comment", "saying that SkipDelta is only written to the tis file when DocFreq is not", "smaller than SkipInterval (as recently mentioned by Jeff in LUCENE-349).", "Committed."], "SplitGT": [" Seeks the skip entry on the given level"], "issueString": "Multi-level skipping on posting lists\nTo accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. \nThe default skip interval is set to 16. If we want to skip e. g. 100 documents, \nthen it is not necessary to read 100 entries from the posting list, but only \n100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This \nspeeds up conjunction (AND) and phrase queries significantly.\n\nHowever, the skip interval is always a compromise. If you have a very big index \nwith huge posting lists and you want to skip over lets say 100k documents, then \nit is still necessary to read 100k/16 = 6250 entries from the skip list. For big \nindexes the skip interval could be set to a higher value, but then after a big \nskip a long scan to the target doc might be necessary.\n\nA solution for this compromise is to have multi-level skip lists that guarantee a \nlogarithmic amount of skips to any target in the posting list. This patch \nimplements such an approach in the following way:\n\n  Example for skipInterval = 3:\n                                                      c            (skip level 2)\n                  c                 c                 c            (skip level 1) \n      x     x     x     x     x     x     x     x     x     x      (skip level 0)\n  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)\n      3     6     9     12    15    18    21    24    27    30     (df)\n \n  d - document\n  x - skip data\n  c - skip data with child pointer\n \nSkip level i contains every skipInterval-th entry from skip level i-1. Therefore the \nnumber of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).\n \nEach skip entry on a level i>0 contains a pointer to the corresponding skip entry in \nlist i-1. This guarantees a logarithmic amount of skips to find the target document.\n\n\nImplementations details:\n\n   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to \n     simplify those classes. The two new classes AbstractSkipListReader and \n\t AbstractSkipListWriter implement the skipping functionality.\n   * While AbstractSkipListReader and Writer take care of writing and reading the \n     multiple skip levels, they do not implement an actual skip data format. The two \n\t new subclasses DefaultSkipListReader and Writer implement the skip data format \n\t that is currently used in Lucene (with two file pointers for the freq and prox \n\t file and with payload length information). I added this extra layer to be \n\t prepared for flexible indexing and different posting list formats. \n      \n   \nFile format changes: \n\n   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the\n     version of this file. If maxSkipLevels is set to one, then the format of the freq \n\t file does not change at all, because we only have one skip level as before. For \n\t backwards compatibility maxSkipLevels is set to one automatically if an index \n\t without the new parameter is read. \n   * In case maxSkipLevels > 1, then the frq file changes as follows:\n     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount\n\t SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, \n\t                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>\n\t SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))\n\n\t Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not \n\t needed, and 2) the format of this file does not change for maxSkipLevels=1 then.\n\t \n\t \nAll unit tests pass with this patch.\nAttaching the first version of this patch.\nI ran a few performance tests with this patch. I built a rather big \nindex (about 1.2GB) using documents from Wikipedia and optimized the\nindex to get big posting lists.\n\nI compare query evaluation time with one-level skipping (old) and\nmulti-level skipping (new) and measure the amount of I/O by counting \nthe number of VInts read from disk. Each query runs several thousand\ntimes.\n\nThe following queries contain very frequent and very unique terms. \nFor these queries the speedup with multi-level skipping is \nsignificant:\n\n   Query: +lucene +search +engine +library\n   5 total matching documents\n   VInt reads: old: 143268000, new: 3933000, -97.25479520897898%\n   Time: old: 7234ms, new: 1157ms, -84.00608238871993%\n\n   Query: +apache +http +server\n   181 total matching documents\n   VInt reads: old: 155892000, new: 27849000, -82.13570933723346%\n   Time: old: 10656ms, new: 5703ms, -46.48085585585586%\n\n\nEven though I/O is reduced for the next query, it runs a bit slower.\nI believe the reason is that the same query runs several thousand\ntimes, so the posting lists will be loaded into the file system\ncache and the effect of less I/O is reduced, while the skipping\nalgorithm itself is a bit more complex:\n\n   Query: +multi +level +skipping\n   13 total matching documents\n   VInt reads: old: 42894000, new: 39096000, -8.854385228703315%\n   Time: old: 3875ms, new: 3922ms, 1.2129032258064516%\n\n\nFor the next query there is slightly more I/O necessary in the \nmulti-skipping case. This is because not many big skips can be made,\nbut more skipping data has to be read. The top 2 skip levels are \nbuffered in this first version of the patch and if no big skips\ncan be made than this buffering is overhead compared to the \nsingle-level case:\n\n   Query: +beatles +yellow +submarine\n   78 total matching documents\n   VInt reads: old: 38460000, new: 38685000, 0.5850234009360374%\n   Time: old: 3172ms, new: 3265ms, 2.9319041614123584%\n\nHowever, if I change the query a little bit, then the speed-up\nis significant (due to the very frequent stop word \"the\"):\n\n   Query: +\"the beatles\" +yellow +submarine\n   77 total matching documents\n   VInt reads: old: 382307000, new: 262331000, -31.38210914265237%\n   Time: old: 26703ms, new: 22828ms, -14.51147811107366%\n   \n   \nIt would be interesting to run more sophisticated benchmarks.\nTo run the same query several times is not very realistic \nbecause it reduces the effects of the I/O savings due to caching.\n\nI'm not that familiar with the new benchmark stuff that has\nbeen added recently, but I'll try to dig into that next week.\nLooks interesting!\n\nHave you done any performance testing?  I guess best case for this patch would be \"skip to a random doc\", and worst case would be skipTo(currDoc+2) in a loop  (or a conjunction across terms that appear in almost all docs).\n\nI haven't dug into the code, but do you avoid storing extra levels when the docfreq is small?\nWhat is the size cost for terms with docfreq=1?\n\n\n> and worst case would be skipTo(currDoc+2) in a loop \n> (or a conjunction across terms that appear in almost all docs).\n\nTrue that should be the worst case. A query with 3 stop words\ntakes about 2% longer:\n\n   Query: +the +and +a\n   682737 total matching documents\n   VInt reads: old: 481897700, new: 481416800, -0.09979296435737295%\n   Time: old: 27812ms, new: 28406ms, 2.1357687329210413%\n\nMaybe a possible optimization could be to avoid using the higher levels\nafter a certain amount of small skips have been performed. I will try out if \nthis will improve things.\n\n> do you avoid storing extra levels when the docfreq is small?\n> What is the size cost for terms with docfreq=1? \n\nI only store another level if it contains at least one SkipDatum.\nSo there is no overhead for terms with df=1.\n\nThe .frq file in my index grew by 1.3% in the multi-level case\nfor an index with about 170MB.\n\n\nLUCENE-888 will introduce the new method BufferedIndexInput.setBufferSize(). Since the length of the different skip levels is known I can set the buffer length before I clone the skip stream in this patch.\nI ran some more performance experiments to test the average speedup.\nI used the same index (about 1.2GB, optimized, docs from wikipedia)\nand created 50,000 random queries. Each query has 3 AND terms and \neach term has a df > 100. \n\nHere are the results:\n\nold (one level skipping):\nTime: 62141 ms.\nVInt reads: 752430441\n\nnew (multi level):\nTime: 51969 ms.\nVInt reads: 435504734\n\nThis is a speedup of about 16% and i/o savings of 42%.\n\nThen I changed the algorithm a bit to not always start on the \nhighest skip level to find the next skip but to start at level 0\nand walk up until the next skip point is greater than the target.\nThis speeds up things further so that the overall gain is about\n20%:\n\nTime: 49500 ms.\nBytes read: 435504734\n\nThis 20% speedup is for average AND queries. Certain queries\nbenefit even more from multi-level skipping as the results\nshow that I attached earlier.\n\n\nI will attach a new patch as soon as LUCENE-888 is committed.\nNew version of the patch with the following changes:\n\n- Applies cleanly on current trunk\n\n- Renamed AbstractSkipListReader and AbstractSkipListWriter\n  to MultiLevelSkipListReader and MultiLevelSkipListWriter\n  \n- The skipTo() algorithm now starts with the lowest level\n  and walks up until it finds the highest level that has a\n  skip for the target. \n  \n- Uses the new BufferedIndexInput.setBufferSize() method to\n  reduce memory consumption in case skip levels occupy less\n  space than BufferedIndexInput.BUFFER_SIZE (1024 bytes)\n  \n- Only preload the top level into memory. That saves memory\n  and almost doesn't affect performance.\n  \nAll unit tests pass.\n\nI think this patch is ready to commit now.\nPatch applies cleanly here.\nAll tests pass, except for org.apache.lucene.store.TestBufferedIndexInput:\n\n    [junit] Testsuite: org.apache.lucene.store.TestBufferedIndexInput\n    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 2.536 sec\n    [junit]\n    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):        Caused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.setBufferSize(BufferedIndexInput.java:52)\n    [junit]     at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:210)\n    [junit]     at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:164)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.store.TestBufferedIndexInput FAILED\n\nIt's possible that this is due to another diff in my working copy, but a quick look did\nnot reveal anything suspicious. In case I'm the only one with this test failing, I'll dig deeper.\n\n\nOn a clean checkout it applies cleanly and this test (and all others) pass.\nHmm -- that NullPointerException I think is from the assert I added for LUCENE-888.  I will fix.\nThe last change to BufferedIndexInput indeed makes all core tests pass again here with this patch applied.\nThanks for reviewing Paul and Doron, and thanks for fixing the NPE, Mike!!\n\nI'm planning to commit this in a day or so.\nNice job!  \nStarting at the lowest level will probably improve the worst-case scenario for this patch (which wasn't nearly as bad as I expected in the first place).\n\n+1 to commit\nThis patch updates the fileformats document. It also adds a comment \nsaying that SkipDelta is only written to the tis file when DocFreq is not \nsmaller than SkipInterval (as recently mentioned by Jeff in LUCENE-349).\nCommitted.\n", "issueSearchSentences": ["Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not", "Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in", "highest skip level to find the next skip but to start at level 0", "Skip level i contains every skipInterval-th entry from skip level i-1.", "old (one level skipping):"], "issueSearchScores": [0.6247475743293762, 0.6064212322235107, 0.6057982444763184, 0.5854217410087585, 0.5757538676261902]}
{"aId": 22, "code": "public void setDocumentNumber(int documentNumber) {\n  }", "comment": " Passes down the index of the document whose term vector is currently being mapped, once for each top level call to a term vector reader.", "issueId": "LUCENE-1038", "issueStringList": ["TermVectorMapper.setDocumentNumber()", "Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.", "See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341", "Committed.", "Thanks, Karl."], "SplitGT": [" Passes down the index of the document whose term vector is currently being mapped, once for each top level call to a term vector reader."], "issueString": "TermVectorMapper.setDocumentNumber()\nPasses down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  \n\nSee http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341\nCommitted.  Thanks, Karl.\n", "issueSearchSentences": ["TermVectorMapper.setDocumentNumber()", "Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.", "See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341", "Thanks, Karl.", "Committed."], "issueSearchScores": [0.7829414010047913, 0.3174057602882385, 0.21043840050697327, 0.14917179942131042, 0.0910998284816742]}
{"aId": 23, "code": "protected synchronized void acquireWriteLock() throws IOException {\n    /* NOOP */\n  }", "comment": " Does nothing by default.", "issueId": "LUCENE-986", "issueStringList": ["Refactor segmentInfos from IndexReader into its subclasses", "References to segmentInfos in IndexReader cause different kinds of problems", "for subclasses of IndexReader, like e. g. MultiReader.", "Only subclasses of IndexReader that own the index directory, namely", "SegmentReader and MultiSegmentReader, should have a SegmentInfos object", "and be able to access it.", "Further information:", "http://www.gossamer-threads.com/lists/lucene/java-dev/51808", "http://www.gossamer-threads.com/lists/lucene/java-user/52460", "A part of the refactoring work was already done in LUCENE-781", "one aspect of this that should be considered: It may not make sense for MultiReader to extend MultiSegmentReader ... as Michael says, only subclasses that own the index directory should have segmentInfos, and a MultiReader (as defined on the trunk now) can never own it's own directory.", "I haven't worked through all the implications, but perhaps the most logical refactoring would be...", "IndexReader", "...as abstract as possible given that we can't actually make methods abstract", "DirectoryIndexReader extends IndexReader", "...new class, encapsulated all the segmentInfos and locking logic currently in", "IndexReader (can definitely be made abstract if helpful)", "SegmentReader extends DirectoryIndexReader", "MultiSegmentReader extends DirectoryIndexReader", "ParallelIndexReader extends IndexReader", "FilterIndexReader extends IndexReader", "MultiReader extends IndexReader", "...(side note that i *really* haven't thought through completley: should", "MultiReader extend FilterIndexReader?)", "there would likely be some utlity functionality that could be reused between MultiReader and MultiSegmentReader ... possible as static methods in IndexReader (or a new util class)", "This is good stuff, Hoss.", "I like the DirectoryIndexReader idea.", "Sounds like a very clean and logical separation.", "I'll work through the code to understand all consequences.", "What do you think about this alternative approach:", "SegmentReader, MultiSegmentReader and MultiReader all extend IndexReader", "IndexReader.open() always returns a MultiSegmentReader instance, even", "if the index has only one segment.", "We can make some optimizations, so that", "e. g. MultiSegmentReader.termDocs() returns a SegmentTermDocs in case", "there's only one underlying SegmentReader.", "All write lock and transaction logic goes into MultiSegmentReader, as", "it is the only reader that has to obtain a write lock.", "The advantage here is that only one class (MultiSegmentReader) is", "responsible for acquiring the write lock.", "It would also make", "IndexReader.open() simpler,  because it can then return a", "MultiSegmentReader instance in all cases.", "The logic of opening the", "different segments could move from IndexReader.open() to the constructor", "of MultiSegmentReader.", "Also LUCENE-743 (IndexReader.reopen()) would", "become simpler.", "The disadvantage of this approach is apparently that always using", "a MultiSegmentReader would add some overhead when reading optimized", "indexes, but actually I don't think that this overhead would really be", "significant.", "If we go the DirectoryIndexReader way (where SegmentReader and", "MultiSegmentReader both extend DirectoryIndexReader), then we still need", "an ownDirectory variable that tells SegmentReader if it has to acquire a", "write lock or not, depending on whether it is a subreader of", "MultiSegmentReader or not.", "And of course we add another layer to the", "IndexReader hierachy.", "So I'm not sure which approach is the better one.", "I'm hoping to get some", "opionions here!", "Hoss?", "Others?", "i rarely remember a week later what i was thinking when i wrote something, but i suspect that when i suggested the DirectoryIndexReader i was assuming it would have everything directory/lock related thta currently exists in the IndexReader base class (including the directoryOwner boolean) ... in cases where there is a single Segment in a directory, there will be SegmentReader with directoryOwner==true ... in the multi segment cases, the MultiSegmentReader will have directoryOwner==true, and it's sub SegmentReaders will all have directoryOwner==false.", "...", "...the key point of DirectoryIndexReader being that any subclass *can* own a directory (and automaticly inherits all the code for dealing with locks properly when it needs/wants to) but doesn't always *have* to own the directory.", "meanwhile MultiReader (and ParallelIndexReader and FilteredIndexReader) make no attempt at owning a directory, and inherit no code for doing so (or for dealing with the locking of such non existent directories)", "I don't really know enough about the performance characteristics of SegmentReader vs a MultiSegmentReader of one segment to have a sense of how possible/practical it would be to eliminate the need for SegmentReader and replace it completely with MultiSegmentReader ...", "one hitch might be that SegmentReader.get is public, and in order to keep supporting it, SegmentReader still needs to have/inherit the same segment info and directory owning/locking code that we want to move out of IndexReader (so just putting it MultiSegmentReader won't fly unless we kill that public method)", "Here is the patch with the DirectoryIndexReader approach.", "It moves SegmentInfos and Directory from IndexReader into", "DirectoryIndexReader, as well as the commit and rollback logic.", "MultiSegmentReader and SegmentReader extend DirectoryIndexReader.", "MultiReader extends IndexReader now and uses some methods from", "MultiSegmentReader that I made static.", "I added the method acquireWriteLock() to IndexReader that does", "nothing by default.", "Subclasses must implement it if they require", "a write lock (so does DirectoryIndexReader now).", "IndexReader is very abstract now and almost all logic moved into", "the subclasses.", "The methods isOptimized(), isCurrent(), and", "getVersion() all throw an UnsupportedOperationException (UOE).", "I", "further deprecated the IndexReader constructor that takes a", "Directory as argument.", "As soon as we remove this ctr the method", "IndexReader.getDirectory() should throw an UOE as well.", "I added", "a TODO comment as a reminder.", "All unit tests pass with this patch.", "> one hitch might be that SegmentReader.get is public, and in order to keep", "> supporting it, SegmentReader still needs to have/inherit the same segment info", "> and directory owning/locking code that we want to move out of IndexReader (so", "> just putting it MultiSegmentReader won't fly unless we kill that public method).", "OK, I implemented the DirectoryIndexReader approach.", "Also because I'm not sure", "about the performance characteristics of a MultiSegmentReader acting as a", "SegmentReader.", "I'd like to commit this rather soon.", "A review of the patch would be highly", "appreciated.", "Michael: I've been meaning to look at this, but haven't had the time ... your recent update has goaded me :)", "just to clarify: the patch you added on September 12th is your latest patch right?", "... it's not clear from you comment on the 17th if you intended to attach an update and something went wrong.", "I ask because i'm haivng trouble applying the patch from the 12th ... i must be tired because i can't understand why, it doesn't look like the files have changed since you posted the patch, so i'm not sure what it's complaining about ...  visually everything seems to match up...", "hossman@coaster:~/lucene/lucene$ svn update", "At revision 576718.", "hossman@coaster:~/lucene/lucene$ svn status", "?", "lucene-986.patch", "hossman@coaster:~/lucene/lucene$ patch --dry-run -p0 -i lucene-986.patch patching file src/java/org/apache/lucene/index/DirectoryIndexReader.java", "patching file src/java/org/apache/lucene/index/FilterIndexReader.java", "patching file src/java/org/apache/lucene/index/IndexReader.java", "patching file src/java/org/apache/lucene/index/MultiReader.java", "Hunk #1 FAILED at 19.", "1 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/lucene/index/MultiReader.java.rej", "patching file src/java/org/apache/lucene/index/MultiSegmentReader.java", "Hunk #1 FAILED at 30.", "Hunk #2 FAILED at 39.", "Hunk #3 FAILED at 156.", "Hunk #4 FAILED at 171.", "Hunk #5 FAILED at 256.", "Hunk #6 FAILED at 278.", "6 out of 6 hunks FAILED -- saving rejects to file src/java/org/apache/lucene/index/MultiSegmentReader.java.rej", "patching file src/java/org/apache/lucene/index/ParallelReader.java", "patching file src/java/org/apache/lucene/index/SegmentReader.java", "Hunk #1 succeeded at 32 with fuzz 2.", "patching file src/test/org/apache/lucene/index/TestMultiReader.java", "hossman@coaster:~/lucene/lucene$", "I got the patch to apply cleanly (see mailing list for details)  On the whole it looks really good, i'm attaching an updated version with some minor improvements (mainly javadocs), but first a few questions...", "just to clarify: IndexReader(Directory) is only around for", "backwards compatibility in subclasses right?", "And the only reason", "for the \"private Directory\" is to keep supporting the directory()", "method for any subclasses relying on it?", "IndexReader() says it can be removed once the other constructor is", "removed ... why?", "is that just pointing out that we can rely on the", "default constructor?", "since one of the goals is that IndexReaders which don't own their", "directory shouldn't utilize segmentInfos, would it make sense to", "eliminate directoryOwner from DirectoryIndexReader and instead test", "whether (null == segmentInfos) ?", "the way TestMultiReader is setup with the \"mode\" is a bit confusing", "... perhaps instead we should make a subclass where only openReader", "is overwritten (it will inherit the individual test methods) ?", "here's the list of tweaks I made...", "added a note in the IndexReader class javadocs about", "methods that throw UnSupOp but aren't abstract.", "added javadocs to the IndexReader(Directory) constructor based on my", "understanding", "added back javadocs to IndexReader methods that now throw UnSupOp to", "make it clear what they do to callers looking at the API, but made", "it clear tthe default impls throw UnSupOp", "made IndexReader.directory() throw UnSupOp if directory is null,", "enhanced it's javadocs.", "* NOOP */ in IndexReader.acquireWriteLock so it's clear to code analysis tools that it's not a mistake.", "small additions to javadocs for DirectoryIndexReader class, but", "these should probably be elaborated on (depending on what you think", "of my segmentInfos==null idea above)", "made DirectoryIndexReader(...) call init(...) to eliminate a small", "about of duplication.", "> I got the patch to apply cleanly (see mailing list for details)", "Thanks, Hoss!", "I'm using TortoiseSVN, I have to check how to set those", "default properties for new files correctly.", ">    * just to clarify: IndexReader(Directory) is only around for", ">      backwards compatibility in subclasses right?", "And the only reason", ">      for the \"private Directory\" is to keep supporting the directory()", ">      method for any subclasses relying on it?", "Yes, correct.", ">    * IndexReader() says it can be removed once the other constructor is", ">      removed ... why?", "is that just pointing out that we can rely on the", ">      default constructor?", "Yes, just a suggested simplification.", "Keeping the constructor wouldn't hurt", "though.", ">    * since one of the goals is that IndexReaders which don't own their", ">      directory shouldn't utilize segmentInfos, would it make sense to", ">      eliminate directoryOwner from DirectoryIndexReader and instead test", ">      whether (null == segmentInfos) ?", "Sounds good, will do...", ">    * the way TestMultiReader is setup with the \"mode\" is a bit confusing", ">      ... perhaps instead we should make a subclass where only openReader", ">      is overwritten (it will inherit the individual test methods) ?", "Yes, that's cleaner, will make that change as well.", "> here's the list of tweaks I made...", "The improvements look good to me.", "Thanks for the review, Hoss!", "I'll attach a new patch shortly.", "In addition to Hoss' changes this patch:", "Removes the boolean directoryOwner variable from DirectoryIndexReader", "and checks for segmentInfos != null instead.", "I also added a comment", "to DirectoryIndexReader about this.", "TestMultiReader now extends the new unit test TestMultiSegmentReader", "and overwrites the method openReader().", "I'm planning to commit this in a day or so if nobody objects.", "Committed Rev.", "577596"], "SplitGT": [" Does nothing by default."], "issueString": "Refactor segmentInfos from IndexReader into its subclasses\nReferences to segmentInfos in IndexReader cause different kinds of problems\nfor subclasses of IndexReader, like e. g. MultiReader.\n\nOnly subclasses of IndexReader that own the index directory, namely \nSegmentReader and MultiSegmentReader, should have a SegmentInfos object\nand be able to access it.\n\nFurther information:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/51808\nhttp://www.gossamer-threads.com/lists/lucene/java-user/52460\n\nA part of the refactoring work was already done in LUCENE-781\none aspect of this that should be considered: It may not make sense for MultiReader to extend MultiSegmentReader ... as Michael says, only subclasses that own the index directory should have segmentInfos, and a MultiReader (as defined on the trunk now) can never own it's own directory.\n\nI haven't worked through all the implications, but perhaps the most logical refactoring would be...\n\n * IndexReader \n    ...as abstract as possible given that we can't actually make methods abstract\n    * DirectoryIndexReader extends IndexReader\n       ...new class, encapsulated all the segmentInfos and locking logic currently in \n          IndexReader (can definitely be made abstract if helpful)\n       * SegmentReader extends DirectoryIndexReader\n       * MultiSegmentReader extends DirectoryIndexReader\n    * ParallelIndexReader extends IndexReader\n    * FilterIndexReader extends IndexReader\n    * MultiReader extends IndexReader\n       ...(side note that i *really* haven't thought through completley: should \n          MultiReader extend FilterIndexReader?)\n\nthere would likely be some utlity functionality that could be reused between MultiReader and MultiSegmentReader ... possible as static methods in IndexReader (or a new util class)\n\n\nThis is good stuff, Hoss. I like the DirectoryIndexReader idea.\nSounds like a very clean and logical separation.\n\nI'll work through the code to understand all consequences.\nWhat do you think about this alternative approach:\n\n- SegmentReader, MultiSegmentReader and MultiReader all extend IndexReader\n- IndexReader.open() always returns a MultiSegmentReader instance, even\nif the index has only one segment. We can make some optimizations, so that\ne. g. MultiSegmentReader.termDocs() returns a SegmentTermDocs in case \nthere's only one underlying SegmentReader.\n- All write lock and transaction logic goes into MultiSegmentReader, as\nit is the only reader that has to obtain a write lock.\n\nThe advantage here is that only one class (MultiSegmentReader) is\nresponsible for acquiring the write lock. It would also make \nIndexReader.open() simpler,  because it can then return a \nMultiSegmentReader instance in all cases. The logic of opening the \ndifferent segments could move from IndexReader.open() to the constructor\nof MultiSegmentReader. Also LUCENE-743 (IndexReader.reopen()) would\nbecome simpler.\nThe disadvantage of this approach is apparently that always using\na MultiSegmentReader would add some overhead when reading optimized \nindexes, but actually I don't think that this overhead would really be\nsignificant.\n\nIf we go the DirectoryIndexReader way (where SegmentReader and \nMultiSegmentReader both extend DirectoryIndexReader), then we still need\nan ownDirectory variable that tells SegmentReader if it has to acquire a\nwrite lock or not, depending on whether it is a subreader of \nMultiSegmentReader or not. And of course we add another layer to the\nIndexReader hierachy. \n\nSo I'm not sure which approach is the better one. I'm hoping to get some\nopionions here! Hoss? Others?\ni rarely remember a week later what i was thinking when i wrote something, but i suspect that when i suggested the DirectoryIndexReader i was assuming it would have everything directory/lock related thta currently exists in the IndexReader base class (including the directoryOwner boolean) ... in cases where there is a single Segment in a directory, there will be SegmentReader with directoryOwner==true ... in the multi segment cases, the MultiSegmentReader will have directoryOwner==true, and it's sub SegmentReaders will all have directoryOwner==false. ...\n\n...the key point of DirectoryIndexReader being that any subclass *can* own a directory (and automaticly inherits all the code for dealing with locks properly when it needs/wants to) but doesn't always *have* to own the directory.  meanwhile MultiReader (and ParallelIndexReader and FilteredIndexReader) make no attempt at owning a directory, and inherit no code for doing so (or for dealing with the locking of such non existent directories)\n\nI don't really know enough about the performance characteristics of SegmentReader vs a MultiSegmentReader of one segment to have a sense of how possible/practical it would be to eliminate the need for SegmentReader and replace it completely with MultiSegmentReader ... \n\none hitch might be that SegmentReader.get is public, and in order to keep supporting it, SegmentReader still needs to have/inherit the same segment info and directory owning/locking code that we want to move out of IndexReader (so just putting it MultiSegmentReader won't fly unless we kill that public method)\nHere is the patch with the DirectoryIndexReader approach.\n\nIt moves SegmentInfos and Directory from IndexReader into\nDirectoryIndexReader, as well as the commit and rollback logic.\n\nMultiSegmentReader and SegmentReader extend DirectoryIndexReader. \nMultiReader extends IndexReader now and uses some methods from \nMultiSegmentReader that I made static.\n\nI added the method acquireWriteLock() to IndexReader that does \nnothing by default. Subclasses must implement it if they require \na write lock (so does DirectoryIndexReader now).\n\nIndexReader is very abstract now and almost all logic moved into\nthe subclasses. The methods isOptimized(), isCurrent(), and \ngetVersion() all throw an UnsupportedOperationException (UOE). I \nfurther deprecated the IndexReader constructor that takes a\nDirectory as argument. As soon as we remove this ctr the method\nIndexReader.getDirectory() should throw an UOE as well. I added\na TODO comment as a reminder.\n\nAll unit tests pass with this patch.\n> one hitch might be that SegmentReader.get is public, and in order to keep \n> supporting it, SegmentReader still needs to have/inherit the same segment info \n> and directory owning/locking code that we want to move out of IndexReader (so \n> just putting it MultiSegmentReader won't fly unless we kill that public method).\n\nOK, I implemented the DirectoryIndexReader approach. Also because I'm not sure \nabout the performance characteristics of a MultiSegmentReader acting as a \nSegmentReader. \n\nI'd like to commit this rather soon. A review of the patch would be highly \nappreciated.\nMichael: I've been meaning to look at this, but haven't had the time ... your recent update has goaded me :)\n\njust to clarify: the patch you added on September 12th is your latest patch right?  ... it's not clear from you comment on the 17th if you intended to attach an update and something went wrong.\n\nI ask because i'm haivng trouble applying the patch from the 12th ... i must be tired because i can't understand why, it doesn't look like the files have changed since you posted the patch, so i'm not sure what it's complaining about ...  visually everything seems to match up...\n\nhossman@coaster:~/lucene/lucene$ svn update\nAt revision 576718.\nhossman@coaster:~/lucene/lucene$ svn status\n?      lucene-986.patch\nhossman@coaster:~/lucene/lucene$ patch --dry-run -p0 -i lucene-986.patch patching file src/java/org/apache/lucene/index/DirectoryIndexReader.java\npatching file src/java/org/apache/lucene/index/FilterIndexReader.java\npatching file src/java/org/apache/lucene/index/IndexReader.java\npatching file src/java/org/apache/lucene/index/MultiReader.java\nHunk #1 FAILED at 19.\n1 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/lucene/index/MultiReader.java.rej\npatching file src/java/org/apache/lucene/index/MultiSegmentReader.java\nHunk #1 FAILED at 30.\nHunk #2 FAILED at 39.\nHunk #3 FAILED at 156.\nHunk #4 FAILED at 171.\nHunk #5 FAILED at 256.\nHunk #6 FAILED at 278.\n6 out of 6 hunks FAILED -- saving rejects to file src/java/org/apache/lucene/index/MultiSegmentReader.java.rej\npatching file src/java/org/apache/lucene/index/ParallelReader.java\npatching file src/java/org/apache/lucene/index/SegmentReader.java\nHunk #1 succeeded at 32 with fuzz 2.\npatching file src/test/org/apache/lucene/index/TestMultiReader.java\nhossman@coaster:~/lucene/lucene$ \n\nI got the patch to apply cleanly (see mailing list for details)  On the whole it looks really good, i'm attaching an updated version with some minor improvements (mainly javadocs), but first a few questions...\n\n* just to clarify: IndexReader(Directory) is only around for\n  backwards compatibility in subclasses right?  And the only reason\n  for the \"private Directory\" is to keep supporting the directory()\n  method for any subclasses relying on it?\n\n* IndexReader() says it can be removed once the other constructor is\n  removed ... why? is that just pointing out that we can rely on the\n  default constructor?\n\n* since one of the goals is that IndexReaders which don't own their\n  directory shouldn't utilize segmentInfos, would it make sense to\n  eliminate directoryOwner from DirectoryIndexReader and instead test\n  whether (null == segmentInfos) ?\n\n* the way TestMultiReader is setup with the \"mode\" is a bit confusing\n  ... perhaps instead we should make a subclass where only openReader\n  is overwritten (it will inherit the individual test methods) ?\n\n\n\nhere's the list of tweaks I made...\n\n* added a note in the IndexReader class javadocs about\n  methods that throw UnSupOp but aren't abstract.\n* added javadocs to the IndexReader(Directory) constructor based on my \n  understanding\n* added back javadocs to IndexReader methods that now throw UnSupOp to\n  make it clear what they do to callers looking at the API, but made\n  it clear tthe default impls throw UnSupOp\n* made IndexReader.directory() throw UnSupOp if directory is null,\n  enhanced it's javadocs.\n* /* NOOP */ in IndexReader.acquireWriteLock so it's clear to code analysis tools that it's not a mistake.\n* small additions to javadocs for DirectoryIndexReader class, but\n  these should probably be elaborated on (depending on what you think\n  of my segmentInfos==null idea above)\n* made DirectoryIndexReader(...) call init(...) to eliminate a small\n  about of duplication.\n\n> I got the patch to apply cleanly (see mailing list for details) \n\nThanks, Hoss! I'm using TortoiseSVN, I have to check how to set those\ndefault properties for new files correctly.\n\n>    * just to clarify: IndexReader(Directory) is only around for\n>      backwards compatibility in subclasses right? And the only reason\n>      for the \"private Directory\" is to keep supporting the directory()\n>      method for any subclasses relying on it?\n\nYes, correct.\n\n>    * IndexReader() says it can be removed once the other constructor is\n>      removed ... why? is that just pointing out that we can rely on the\n>      default constructor?\n\nYes, just a suggested simplification. Keeping the constructor wouldn't hurt\nthough.\n\n>    * since one of the goals is that IndexReaders which don't own their\n>      directory shouldn't utilize segmentInfos, would it make sense to\n>      eliminate directoryOwner from DirectoryIndexReader and instead test\n>      whether (null == segmentInfos) ?\n\nSounds good, will do...\n\n>    * the way TestMultiReader is setup with the \"mode\" is a bit confusing\n>      ... perhaps instead we should make a subclass where only openReader\n>      is overwritten (it will inherit the individual test methods) ?\n\nYes, that's cleaner, will make that change as well.\n\n> here's the list of tweaks I made...\n\nThe improvements look good to me.\nThanks for the review, Hoss! I'll attach a new patch shortly.\nIn addition to Hoss' changes this patch:\n\n   * Removes the boolean directoryOwner variable from DirectoryIndexReader\n     and checks for segmentInfos != null instead. I also added a comment\n     to DirectoryIndexReader about this.\n\n   * TestMultiReader now extends the new unit test TestMultiSegmentReader\n     and overwrites the method openReader().\n    \n\nI'm planning to commit this in a day or so if nobody objects.\nCommitted Rev. 577596\n\n", "issueSearchSentences": ["responsible for acquiring the write lock.", "I added the method acquireWriteLock() to IndexReader that does", "it is the only reader that has to obtain a write lock.", "* NOOP */ in IndexReader.acquireWriteLock so it's clear to code analysis tools that it's not a mistake.", "a write lock (so does DirectoryIndexReader now)."], "issueSearchScores": [0.8060785531997681, 0.7891080379486084, 0.7119194269180298, 0.6316759586334229, 0.6270395517349243]}
{"aId": 24, "code": "public Version getIndexCreatedVersion() {\n    return indexCreatedVersion;\n  }", "comment": " Return the version that was used to initially create the index.", "issueId": "LUCENE-7703", "issueStringList": ["Record the version that was used at index creation time", "SegmentInfos already records the version that was used to write a commit and the version that was used to write the oldest segment in the index.", "In addition to those, I think it could be useful to record the Lucene version that was used to create the index.", "I think it could help with:", "Debugging: there are things that change based on Lucene versions, for instance we will reject broken offsets in term vectors as of 7.0.", "Knowing the version that was used to create the index can be very useful to know what assumptions we can make about an index.", "Backward compatibility.", "The codec API helped simplify backward compatibility of the index files a lot.", "However for everything that is done on top of the codec API like analysis or the computation of length norm factors, backward compatibility needs to be handled on top of Lucene.", "Maybe we could simplify this?", "+1, I think this is important info for the index.", "Thanks Mike for confirming it would be useful.", "So I gave it a try, see the attached patch.", "It should be fine for regular usage of IndexWriter with calls to add/updateDocument.", "However it is not totally clear to me how we should deal with addIndexes.", "For the one that takes a list of codec readers, I don't think there is much we can do anyway since the version is not exposed (and it would not make much sense anyway?).", "For the one that takes a list of directories, we could either reject the call if versions differ (this is what the patch is doing), or be lenient but this has the major drawback that any assumptions we might make based on the created version could break.", "Any opinions?", "+1 to the patch."], "SplitGT": [" Return the version that was used to initially create the index."], "issueString": "Record the version that was used at index creation time\nSegmentInfos already records the version that was used to write a commit and the version that was used to write the oldest segment in the index. In addition to those, I think it could be useful to record the Lucene version that was used to create the index. I think it could help with:\n - Debugging: there are things that change based on Lucene versions, for instance we will reject broken offsets in term vectors as of 7.0. Knowing the version that was used to create the index can be very useful to know what assumptions we can make about an index.\n - Backward compatibility. The codec API helped simplify backward compatibility of the index files a lot. However for everything that is done on top of the codec API like analysis or the computation of length norm factors, backward compatibility needs to be handled on top of Lucene. Maybe we could simplify this?\n+1, I think this is important info for the index.\nThanks Mike for confirming it would be useful. So I gave it a try, see the attached patch. It should be fine for regular usage of IndexWriter with calls to add/updateDocument. However it is not totally clear to me how we should deal with addIndexes. For the one that takes a list of codec readers, I don't think there is much we can do anyway since the version is not exposed (and it would not make much sense anyway?). For the one that takes a list of directories, we could either reject the call if versions differ (this is what the patch is doing), or be lenient but this has the major drawback that any assumptions we might make based on the created version could break. Any opinions?\n+1 to the patch.\n", "issueSearchSentences": ["Knowing the version that was used to create the index can be very useful to know what assumptions we can make about an index.", "Record the version that was used at index creation time", "SegmentInfos already records the version that was used to write a commit and the version that was used to write the oldest segment in the index.", "In addition to those, I think it could be useful to record the Lucene version that was used to create the index.", "For the one that takes a list of directories, we could either reject the call if versions differ (this is what the patch is doing), or be lenient but this has the major drawback that any assumptions we might make based on the created version could break."], "issueSearchScores": [0.6469780206680298, 0.5883723497390747, 0.4846055507659912, 0.48159146308898926, 0.36842355132102966]}
{"aId": 25, "code": "public Object getFirstValue(String name) {\n    Object v = _fields.get( name );\n    if (v == null || !(v instanceof Collection)) return v;\n    Collection c = (Collection)v;\n    if (c.size() > 0 ) {\n      return c.iterator().next();\n    }\n    return null;\n  }", "comment": " returns the first value for a field", "issueId": "SOLR-280", "issueStringList": ["slightly more efficient SolrDocument implementation", "Following discussion in SOLR-272", "This implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.", "The API changes slightly in that:", "getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.", "getFirstValue( name ) returns a single value for the field.", "This is intended to make things easier for client applications.", "We could go further and store boosted values as:", "class BoostedValue {", "float boost;", "Object value;", "}", "but I think that makes the implementation to convoluted.", "If we went this route, we would need to check each value before passing it back to the client.", "I think one should be able to simply set a field value without a copy being made:", "public Object setField(String name, Collection value)  {", "return _fields.put(name, value);", "}", "One area I'm concerned about performance is on the output side of things.", "If we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.", "Just something to keep in mind.", "Another thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)", "This is a new implementation where SolrInputDocument *does not* extend SolrDocument.", "This way each can be optimized for how they are most frequently used.", "This adds:", "public class SolrInputField", "{", "final String name;", "float boost = 1.0f;", "Object value = null;", "}", "and SolrInputDocument keeps a Map<String,SolrInputField>", "This still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later."], "SplitGT": [" returns the first value for a field"], "issueString": "slightly more efficient SolrDocument implementation\nFollowing discussion in SOLR-272\n\nThis implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.  The API changes slightly in that:\n\n getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.\n\ngetFirstValue( name ) returns a single value for the field.  This is intended to make things easier for client applications.\nWe could go further and store boosted values as:\n\nclass BoostedValue {\n  float boost;\n  Object value;\n} \n\nbut I think that makes the implementation to convoluted.  If we went this route, we would need to check each value before passing it back to the client.\nI think one should be able to simply set a field value without a copy being made:\n \npublic Object setField(String name, Collection value)  {\n  return _fields.put(name, value);\n}\n\nOne area I'm concerned about performance is on the output side of things.\nIf we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.  Just something to keep in mind.\n\nAnother thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)\n\nThis is a new implementation where SolrInputDocument *does not* extend SolrDocument.  This way each can be optimized for how they are most frequently used.  \n\nThis adds:\n\npublic class SolrInputField \n{\n  final String name;\n  float boost = 1.0f;\n  Object value = null;\n}\n\nand SolrInputDocument keeps a Map<String,SolrInputField>\n\nThis still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later.\n\n", "issueSearchSentences": ["getFirstValue( name ) returns a single value for the field.", "getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.", "Object value = null;", "Object value;", "public Object setField(String name, Collection value)  {"], "issueSearchScores": [0.812806248664856, 0.7490425109863281, 0.5230199098587036, 0.5151416063308716, 0.5092163681983948]}
{"aId": 26, "code": "public final boolean maybeRefresh() throws IOException {\n    ensureOpen();\n\n    // Ensure only 1 thread does reopen at once; other threads just return immediately:\n    final boolean doTryRefresh = refreshLock.tryLock();\n    if (doTryRefresh) {\n      try {\n        doMaybeRefresh();\n      } finally {\n        refreshLock.unlock();\n      }\n    }\n\n    return doTryRefresh;\n  }", "comment": " If it returns false it means another thread is currently refreshing.", "issueId": "LUCENE-4025", "issueStringList": ["ReferenceManager.maybeRefresh should allow the caller to block", "ReferenceManager.maybeRefresh() returns a boolean today, specifying whether the maybeRefresh logic was executed by the caller or not.", "If it's false, it means that another thread is currently refreshing and the call returns immediately.", "I think that that's inconvenient to the caller.", "I.e., if you wanted to do something like:", "{code}", "writer.commit();", "searcherManager.maybeRefresh();", "searcherManager.acquire();", "{code}", "It'd be better if you could guarantee that when the maybeRefresh() call returned, the follow on acquire() will return a refreshed IndexSearcher.", "Even if you omit the commit instruction, it'd be good if you can guarantee that.", "I don't quite see the benefit of having the caller thread not block if there's another thread currently refreshing.", "In, I believe, most cases, you'd anyway have just one thread calling maybeRefresh().", "Even if not, the only benefit of not blocking is if you have commit() followed by maybeRefresh() logic done by some threads, while other threads acquire searchers - maybe then you wouldn't care if another thread is currently doing the refresh?", "Actually, I tend to think that not blocking is buggy?", "I mean, what if two threads do commit() + maybeRefresh().", "The first thread finishes commit, enters maybeRefresh(), acquires the lock and reopens the Reader.", "Then the second thread does its commit(), enters maybeRefresh, fails to obtain the lock and exits.", "Its changes won't be exposed by SM until the next maybeRefresh() is called.", "So it looks to me like current logic may be buggy in that sense?", "I'm torn here...", "Always blocking seems dangerous because \"simple\" apps, that call .maybeRefresh() from searching threads, will suddenly see all search threads block when a reopen is happening ...", "I think it's too trappy to do that.", "I think when it comes to concurrent code you should try hard to not have \"blocks all threads\" trappiness...", "But I also agree it's a hassle now for callers that want to ensure specific changes are now visible... though, we do have NRTManager for that use case.", "Maybe we can add an optional boolean (doWait, defaulting to false) to maybeRefresh?", "Or we a separate method for it (maybeRefreshAndWaitIfItsAlreadyHappening), or, we can make decoupled method (waitForRefreshToFinish) and change maybeRefresh to return 3 results (IS_CURRENT, DID_REFRESH, OTHER_THREAD_IS_REFRESHING)... or, something else?", "isn't this exactly why things like the ExecutorService and/or Future APIs exist?", "let the caller decide if they want the methods they call to be executed in the current thread or as part of a thread pool and/or block until the logic is executed (with a possible time limit)", "bq.", "I'm torn here...", "I'm also torn here :).", "bq.", "Always blocking seems dangerous because \"simple\" apps, that call .maybeRefresh() from searching threads, will suddenly see all search threads block when a reopen is happening", "maybeRefresh says that you should call this *periodically*.", "I consider SearcherManager kind of advanced API.", "Calling maybeRefresh() on every search is like calling IndexReader.openIfChanged before every search.", "If we need to, let's document the implications of the method.", "Not blocking might be buggy, while blocking might affect your performance.", "I think that the performance issue is really and edge case of stupidity, while the former is a simple expectation.", "maybeRefresh documents that if it returns false, it might be that the next acquire won't return the most up-to-date IndexSearcher, but it doesn't give you any way to ensure that.", "As for the extra API, can we start by unnecessarily complicating the API?", "It looks to me that the API is clear with plenty of sample code and documentation (Mike, you even wrote a couple of blog posts about it).", "It's just a matter of semantics and if we tell people to call maybeRefresh periodically, then let's help them by ensuring this call does something.", "my take on this is that if you really want this point in time semantics you are describing you should use your own application locking on top.", "You can simply use a read/write lock and acquire the write lock if you enter the \"commit\" block which I assume you don't do too frequently.", "The other option is to simply call maybeRefresh in an empty loop.", "The non-blocking fashion is a big asset here IMO since you can't build a non-blocking app with blocking components but you can do the other way around.", "bq.", "Not blocking might be buggy, while blocking might affect your performance.", "that is bogus IMO.", "Calling maybeRefresh in an empty loop is not good.", "You'd want to at least add some sleep in between calls.", "And then you need to decide on the sleep interval.", "Complicate things.", "What's not clear to me is why was this API made non-blocking in the first place?", "Did someone complain about it being blocking?", "bq.", "The non-blocking fashion is a big asset here IMO since you can't build a non-blocking app with blocking components but you can do the other way around.", "That's generally a correct statement, but who said that in the context of a ReferenceManager you want to build a non-blocking app?", "Perhaps a maybeRefreshBlocking() will be the best compromise after all.", "That method won't return anything and will just block on reopenLock.", "I'll create a patch, let's see how it looks first.", "While I'm at it, I'll rename reopenLock to refreshLock (reopenLock was from the time it was in SearcherManager).", "Patch adds maybeRefreshBlocking which blocks on refreshLock.", "It also:", "Renames reopenLock to refreshLock", "Shares the refresh logic between maybeRefresh and maybeRefreshBlocking.", "Switches to use ReentrantLock instead of Semaphore(1):", "It allows to protectively obtain the lock in doMaybeRefresh (see comment in the code)", "It is better in general because it's equivalent to Semaphore(1), yet protects against an accidental bug where someone changes Semaphore(1) to Semaphore(>1).", "I'll add a CHANGES entry after the patch is reviewed and we agree on the approach.", "Approach & patch look good!", "Maybe change javadocs to state that \"you must call maybeRefresh or maybeRefreshBlocking periodically\" (now each one states you must call that one).", "Also maybe say \"if another thread is currently refreshing, this method blocks until that thread completes\".", "Thanks Mike.", "I updated the javadoc as you suggest and added a CHANGES entry.", "I think this is ready to commit.", "+1, looks good.", "Thanks Shai!", "Committed rev 1332699.", "I also added randomly calling it from TestSearcherManager.", "Thanks Mike for the review !"], "SplitGT": [" If it returns false it means another thread is currently refreshing."], "issueString": "ReferenceManager.maybeRefresh should allow the caller to block\nReferenceManager.maybeRefresh() returns a boolean today, specifying whether the maybeRefresh logic was executed by the caller or not. If it's false, it means that another thread is currently refreshing and the call returns immediately.\n\nI think that that's inconvenient to the caller. I.e., if you wanted to do something like:\n{code}\nwriter.commit();\nsearcherManager.maybeRefresh();\nsearcherManager.acquire();\n{code}\nIt'd be better if you could guarantee that when the maybeRefresh() call returned, the follow on acquire() will return a refreshed IndexSearcher. Even if you omit the commit instruction, it'd be good if you can guarantee that.\n\nI don't quite see the benefit of having the caller thread not block if there's another thread currently refreshing. In, I believe, most cases, you'd anyway have just one thread calling maybeRefresh(). Even if not, the only benefit of not blocking is if you have commit() followed by maybeRefresh() logic done by some threads, while other threads acquire searchers - maybe then you wouldn't care if another thread is currently doing the refresh?\n\nActually, I tend to think that not blocking is buggy? I mean, what if two threads do commit() + maybeRefresh(). The first thread finishes commit, enters maybeRefresh(), acquires the lock and reopens the Reader. Then the second thread does its commit(), enters maybeRefresh, fails to obtain the lock and exits. Its changes won't be exposed by SM until the next maybeRefresh() is called.\n\nSo it looks to me like current logic may be buggy in that sense?\nI'm torn here...\n\nAlways blocking seems dangerous because \"simple\" apps, that call .maybeRefresh() from searching threads, will suddenly see all search threads block when a reopen is happening ... I think it's too trappy to do that.  I think when it comes to concurrent code you should try hard to not have \"blocks all threads\" trappiness...\n\nBut I also agree it's a hassle now for callers that want to ensure specific changes are now visible... though, we do have NRTManager for that use case.\n\nMaybe we can add an optional boolean (doWait, defaulting to false) to maybeRefresh?  Or we a separate method for it (maybeRefreshAndWaitIfItsAlreadyHappening), or, we can make decoupled method (waitForRefreshToFinish) and change maybeRefresh to return 3 results (IS_CURRENT, DID_REFRESH, OTHER_THREAD_IS_REFRESHING)... or, something else?\nisn't this exactly why things like the ExecutorService and/or Future APIs exist?  let the caller decide if they want the methods they call to be executed in the current thread or as part of a thread pool and/or block until the logic is executed (with a possible time limit)\nbq. I'm torn here...\n\nI'm also torn here :).\n\nbq. Always blocking seems dangerous because \"simple\" apps, that call .maybeRefresh() from searching threads, will suddenly see all search threads block when a reopen is happening\n\nmaybeRefresh says that you should call this *periodically*. I consider SearcherManager kind of advanced API. Calling maybeRefresh() on every search is like calling IndexReader.openIfChanged before every search. If we need to, let's document the implications of the method.\n\nNot blocking might be buggy, while blocking might affect your performance. I think that the performance issue is really and edge case of stupidity, while the former is a simple expectation. maybeRefresh documents that if it returns false, it might be that the next acquire won't return the most up-to-date IndexSearcher, but it doesn't give you any way to ensure that.\n\nAs for the extra API, can we start by unnecessarily complicating the API? It looks to me that the API is clear with plenty of sample code and documentation (Mike, you even wrote a couple of blog posts about it). It's just a matter of semantics and if we tell people to call maybeRefresh periodically, then let's help them by ensuring this call does something.\nmy take on this is that if you really want this point in time semantics you are describing you should use your own application locking on top. You can simply use a read/write lock and acquire the write lock if you enter the \"commit\" block which I assume you don't do too frequently. The other option is to simply call maybeRefresh in an empty loop. The non-blocking fashion is a big asset here IMO since you can't build a non-blocking app with blocking components but you can do the other way around.\n\nbq. Not blocking might be buggy, while blocking might affect your performance.\n\nthat is bogus IMO.  \nCalling maybeRefresh in an empty loop is not good. You'd want to at least add some sleep in between calls. And then you need to decide on the sleep interval. Complicate things.\n\nWhat's not clear to me is why was this API made non-blocking in the first place? Did someone complain about it being blocking?\n\nbq. The non-blocking fashion is a big asset here IMO since you can't build a non-blocking app with blocking components but you can do the other way around.\n\nThat's generally a correct statement, but who said that in the context of a ReferenceManager you want to build a non-blocking app?\n\nPerhaps a maybeRefreshBlocking() will be the best compromise after all. That method won't return anything and will just block on reopenLock. I'll create a patch, let's see how it looks first. While I'm at it, I'll rename reopenLock to refreshLock (reopenLock was from the time it was in SearcherManager).\nPatch adds maybeRefreshBlocking which blocks on refreshLock. It also:\n\n* Renames reopenLock to refreshLock\n* Shares the refresh logic between maybeRefresh and maybeRefreshBlocking.\n* Switches to use ReentrantLock instead of Semaphore(1):\n** It allows to protectively obtain the lock in doMaybeRefresh (see comment in the code)\n** It is better in general because it's equivalent to Semaphore(1), yet protects against an accidental bug where someone changes Semaphore(1) to Semaphore(>1).\n\nI'll add a CHANGES entry after the patch is reviewed and we agree on the approach.\nApproach & patch look good!\n\nMaybe change javadocs to state that \"you must call maybeRefresh or maybeRefreshBlocking periodically\" (now each one states you must call that one).  Also maybe say \"if another thread is currently refreshing, this method blocks until that thread completes\".\nThanks Mike. I updated the javadoc as you suggest and added a CHANGES entry.\n\nI think this is ready to commit.\n+1, looks good.  Thanks Shai!\nCommitted rev 1332699.\n\nI also added randomly calling it from TestSearcherManager.\n\nThanks Mike for the review !\n", "issueSearchSentences": ["In, I believe, most cases, you'd anyway have just one thread calling maybeRefresh().", "ReferenceManager.maybeRefresh() returns a boolean today, specifying whether the maybeRefresh logic was executed by the caller or not.", "The first thread finishes commit, enters maybeRefresh(), acquires the lock and reopens the Reader.", "If it's false, it means that another thread is currently refreshing and the call returns immediately.", "Calling maybeRefresh() on every search is like calling IndexReader.openIfChanged before every search."], "issueSearchScores": [0.7448663711547852, 0.6778887510299683, 0.6667060256004333, 0.6642621755599976, 0.6348398923873901]}
{"aId": 32, "code": "public JavaUtilRegexCapabilities()  {\n    this.flags = 0;\n  }", "comment": " Default constructor that uses java.util.regex.Pattern with its default flags.", "issueId": "LUCENE-1745", "issueStringList": ["Add ability to specify compilation/matching flags to RegexCapabiltiies implementations", "The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression.", "While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.", "Therefore, this improvement request is to add the capability to provide those modification flags to either implementation.", "I've developed a working implementation that makes minor additions to the existing code.", "The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided.", "This provides complete backwards compatibility.", "For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields.", "These are pass through to the underlying implementation.", "They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.", "Proposed changes:", "For the JavaUtilRegexCapabilities.java, the following is the changes made.", "private int flags = 0;", "Define the optional flags from Pattern that can be used.", "Do this here to keep Pattern contained within this class.", "public final int FLAG_CANON_EQ = Pattern.CANON_EQ;", "public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;", "public final int FLAG_COMMENTS = Pattern.COMMENTS;", "public final int FLAG_DOTALL = Pattern.DOTALL;", "public final int FLAG_LITERAL = Pattern.LITERAL;", "public final int FLAG_MULTILINE = Pattern.MULTILINE;", "public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;", "public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;", "Default constructor that uses java.util.regex.Pattern", "with its default flags.", "public JavaUtilRegexCapabilities()  {", "this.flags = 0;", "}", "Constructor that allows for the modification of the flags that", "the java.util.regex.Pattern will use to compile the regular expression.", "This gives the user the ability to fine-tune how the regular expression", "to match the functionlity that they need.", "The {@link java.util.regex.Pattern Pattern} class supports specifying", "these fields via the regular expression text itself, but this gives the caller", "another option to modify the behavior.", "Useful in cases where the regular expression text", "cannot be modified, or if doing so is undesired.", "@flags The flags that are ORed together.", "public JavaUtilRegexCapabilities(int flags) {", "this.flags = flags;", "}", "public void compile(String pattern) {", "this.pattern = Pattern.compile(pattern, this.flags);", "}", "For the JakartaRegexpCapabilties.java, the following is changed:", "private int flags = RE.MATCH_NORMAL;", "Flag to specify normal, case-sensitive matching behaviour.", "This is the default.", "public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;", "Flag to specify that matching should be case-independent (folded)", "public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;", "Contructs a RegexCapabilities with the default MATCH_NORMAL match style.", "public JakartaRegexpCapabilities() {}", "Constructs a RegexCapabilities with the provided match flags.", "Multiple flags should be ORed together.", "@param flags The matching style", "public JakartaRegexpCapabilities(int flags)", "{", "this.flags = flags;", "}", "public void compile(String pattern) {", "regexp = new RE(pattern, this.flags);", "}", "Adding patch file with complete changes.", "All tests passed and included additional test cases to test new functionality.", "What is the status of having this patch reviewed and commited to the next 2.x release?", "Incorrectly added the \"Fixed Version\" in the last update.", "Patch looks good Marc; I plan to commit shortly.", "I think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).", "Thanks Marc!", "Thanks Michael.", "Look forward to getting my hands on 2.9 when its released."], "SplitGT": [" Default constructor that uses java.util.regex.Pattern with its default flags."], "issueString": "Add ability to specify compilation/matching flags to RegexCapabiltiies implementations\nThe Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. \n\nI've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.\n\nProposed changes:\n\nFor the JavaUtilRegexCapabilities.java, the following is the changes made.\n\n  private int flags = 0;\n  \n  // Define the optional flags from Pattern that can be used.\n  // Do this here to keep Pattern contained within this class.\n  \n  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;\n  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;\n  public final int FLAG_COMMENTS = Pattern.COMMENTS;\n  public final int FLAG_DOTALL = Pattern.DOTALL;\n  public final int FLAG_LITERAL = Pattern.LITERAL;\n  public final int FLAG_MULTILINE = Pattern.MULTILINE;\n  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;\n  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;\n  \n  /**\n   * Default constructor that uses java.util.regex.Pattern \n   * with its default flags.\n   */\n  public JavaUtilRegexCapabilities()  {\n    this.flags = 0;\n  }\n  \n  /**\n   * Constructor that allows for the modification of the flags that\n   * the java.util.regex.Pattern will use to compile the regular expression.\n   * This gives the user the ability to fine-tune how the regular expression \n   * to match the functionlity that they need. \n   * The {@link java.util.regex.Pattern Pattern} class supports specifying \n   * these fields via the regular expression text itself, but this gives the caller\n   * another option to modify the behavior. Useful in cases where the regular expression text\n   * cannot be modified, or if doing so is undesired.\n   * \n   * @flags The flags that are ORed together.\n   */\n  public JavaUtilRegexCapabilities(int flags) {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    this.pattern = Pattern.compile(pattern, this.flags);\n  }\n\n\nFor the JakartaRegexpCapabilties.java, the following is changed:\n\n  private int flags = RE.MATCH_NORMAL;\n\n  /**\n   * Flag to specify normal, case-sensitive matching behaviour. This is the default.\n   */\n  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;\n  \n  /**\n   * Flag to specify that matching should be case-independent (folded)\n   */\n  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;\n \n  /**\n   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.\n   */\n  public JakartaRegexpCapabilities() {}\n  \n  /**\n   * Constructs a RegexCapabilities with the provided match flags.\n   * Multiple flags should be ORed together.\n   * \n   * @param flags The matching style\n   */\n  public JakartaRegexpCapabilities(int flags)\n  {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    regexp = new RE(pattern, this.flags);\n  }\n\nAdding patch file with complete changes. All tests passed and included additional test cases to test new functionality.\nWhat is the status of having this patch reviewed and commited to the next 2.x release?\nIncorrectly added the \"Fixed Version\" in the last update.\nPatch looks good Marc; I plan to commit shortly.\n\nI think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).\nThanks Marc!\nThanks Michael. Look forward to getting my hands on 2.9 when its released.\n", "issueSearchSentences": ["public JavaUtilRegexCapabilities()  {", "public JakartaRegexpCapabilities() {}", "public JavaUtilRegexCapabilities(int flags) {", "For the JavaUtilRegexCapabilities.java, the following is the changes made.", "public JakartaRegexpCapabilities(int flags)"], "issueSearchScores": [0.9272987842559814, 0.8298814296722412, 0.7313797473907471, 0.6819899082183838, 0.6803196668624878]}
{"aId": 34, "code": "public String normalizeNumber(String number) {\n    try {\n      BigDecimal normalizedNumber = parseNumber(new NumberBuffer(number));\n      if (normalizedNumber == null) {\n        return number;\n      }\n      return normalizedNumber.stripTrailingZeros().toPlainString();\n    } catch (NumberFormatException | ArithmeticException e) {\n      // Return the source number in case of error, i.e. malformed input\n      return number;\n    }\n  }", "comment": " Normalizes a Japanese number", "issueId": "LUCENE-3922", "issueStringList": ["Add Japanese Kanji number normalization to Kuromoji", "Japanese people use Kanji numerals instead of Arabic numerals for writing price, address and so on.", "i.e 12\u4e074800\u5186(124,800JPY), \u4e8c\u756a\u753a\u4e09\u30ce\u4e8c(3-2 Nibancho) and \u5341\u4e8c\u6708(December).", "So, we would like to normalize those Kanji numerals to Arabic numerals (I don't think we need to have a capability to normalize to Kanji numerals).", "Thanks a lot, Kazu.", "This is a good idea to add.", "Patches are of course also very welcome!", ":)", "We, RONDHUIT, have done this kind of normalization (and more!).", "You may be interested in:", "http://www.rondhuit-demo.com/RCSS/api/overview-summary.html#featured-japanese", "||Summary||normalization sample||", "|\u6f22\u6570\u5b57=>\u7b97\u7528\u6570\u5b57\u6b63\u898f\u5316|\u56db\u4e03=>47, \u56db\u5341\u4e03=>47, \u56db\u62fe\u4e03=>47, \u56db\u3007\u4e03=>407|", "|\u548c\u66a6=>\u897f\u66a6\u6b63\u898f\u5316|\u662d\u548c\u56db\u4e03\u5e74\u3001\u662d\u548c\u56db\u5341\u4e03\u5e74\u3001\u662d\u548c\u56db\u62fe\u4e03\u5e74=>1972\u5e74, \u662d\u548c\u516d\u5341\u56db\u5e74\u3001\u5e73\u6210\u5143\u5e74=>1989\u5e74|", "Koji, this is very nice.", "Does the kanji number normalizer ({{KanjiNumberCharFilter}}) also deal with combinations of kanji and arabic numbers like Kazu's price example?", "Is the above code you refer to something that can go into Lucene or is it non-free software?", "Koji, Thank you for your comment.", "I am very interested in the normalizer you have mentioned.", "Is it possible to choose to concatenate suffix/prefix(\u5e74/\u6708/\u5186, etc.)", "to the Arabic numbers?", "I've attached a work-in-progress patch for {{trunk}} that implements a {{CharFilter}} that normalizes Japanese numbers.", "These are some TODOs and implementation considerations I have that I'd be thankful to get feedback on:", "Buffering the entire input on the first read should be avoided.", "The primary reason this is done is because I was thinking to add some regexps before and after kanji numeric strings to qualify their normalization, i.e.", "to only normalize strings that starts with \uffe5, JPY or ends with \u5186, to only normalize monetary amounts in Japanese yen.", "However, this probably isn't necessary as we can probably can use {{Matcher.requireEnd()}} and {{Matcher.hitEnd()}} to decide if we need to read more input.", "(Thanks, Robert!)", "Is qualifying the numbers to be normalized with prefix and suffix regexps useful, i.e.", "to only normalize monetary amounts?", "How do we deal with leading zeros?", "Currently, \"007\" and \"\u25ef\u25ef\u4e03\" becomes \"7\" today.", "Do we want an option to preserve leading zeros?", "How large numbers do we care about supporting?", "Some of the larger numbers are surrogates, which complicates implementation, but they're certainly possible.", "If we don't care about really large numbers, we can probably be fine working with {{long}} instead of {{BigInteger}}.", "Polite numbers and some other variants aren't supported, i.e.", "\u58f1, \u5f10, \u53c2, etc., but they can easily be added.", "We can also add the obsolete variants if that's useful somehow.", "Are these useful?", "Do we want them available via an option?", "Number formats such as \"\uff11\u5104\uff12\uff0c\uff13\uff14\uff15\u4e07\uff16\uff0c\uff17\uff18\uff19\" isn't supported - we don't deal with the comma today, but this can be added.", "The same applies to \"\uff11\uff12\u3000\uff13\uff14\uff15\" where there's a space that separates thousands like in French.", "Numbers like \"2\u30fb2\u5146\" aren't supported, but can be added.", "Only integers are supported today, so we can't parse \"\u3007\u30fb\u4e00\u4e8c\u4e09\u56db\", which becomes \"0\" and \"1234\" as separate tokens instead of \"0.1234\"", "There are probably other considerations, too, that I doesn't immediately come to mind.", "Numbers are fairly complicated and feedback on direction for further implementation is most appreciated.", "Thanks.", "Hi Christian,", "Great!", "I will test your patch and get back to you!", "!", "Thanks,", "Kazu", "Kazuaki, do have any comment on this fix?", "Sorry for this late reply.", "Although I have some request to improve capability, this is very helpful and nice charfilter for me.", "Thank you!", "Christian!", "!", "My requests are the following:", "Is it difficult to support numbers with period as the following?", "\uff13\uff0e\uff12\u5146\u5186", "\uff15\uff0e\uff12\u5104\u5186", "On the other hand, I agree with Christian to not preserving leading zeros.", "So, \"\u25ef\u25ef\u4e03\" doesn't need to become \"007\".", "I think It would be helpful that this charfilter supports old Kanji numeric characters (\"KYU-KANJI\" or \"DAIJI\") such as \u58f1, \u58f9 (One), \u5f0c, \u5f10, \u8cb3 (Two), \u5f0d, \u53c2,\u53c3 (Three), or configureable.", "bq.", "On the other hand, I agree with Christian to not preserving leading zeros.", "So, \"\u25ef\u25ef\u4e03\" doesn't need to become \"007\".", "This example shows why leading zeros should be preserved :)", "There are different kinds of text search.", "Searching for media titles like James Bond movies is a very different thing from searching newspaper articles.", "You might want to find \"\u25ef\u25ef\u4e03\" as the Japanese-language release and \"007\" as the English-language release.", "These numbers are brands, not numbers.", "Lance, you may be right.", "Although I have never seen that Japanese people use Kanji numbers for James Bond movies :-), I can't say that we never use Kanji for that kind of expression.", "Christian, Is it possible to choose preserve leading zeros or not?", "{quote}", "Is it difficult to support numbers with period as the following?", "\uff13\uff0e\uff12\u5146\u5186", "\uff15\uff0e\uff12\u5104\u5186", "{quote}", "Supporting this is no problem and a good idea.", "{quote}", "I think It would be helpful that this charfilter supports old Kanji numeric characters (\"KYU-KANJI\" or \"DAIJI\") such as \u58f1, \u58f9 (One), \u5f0c, \u5f10, \u8cb3 (Two), \u5f0d, \u53c2,\u53c3 (Three), or configureable.", "{quote}", "This is also easy to support.", "As for making preserving zeros configurable, that's also possible, of course.", "It's great to get more feedback on what sort of functionality we need and what should be configurable options.", "Hopefully, we can find a good balance without adding too much complexity.", "Thanks for the feedback.", "The following examples are false positive case:", "\"\u59ff\u4e09\u56db\u90ce\" became \"\u59ff\", \"34\", \"\u90ce\"", "\"\u5c0f\u6797\u4e00\u8336\" became \"\u5c0f\u6797\", \"1\", \"\u8336\"", "\"\u9234\u6728\u4e00\u90ce\" became \"\u9234\u6728\", \"1\", \"\u90ce\"", "Can we prevent this behavior?", "Thanks, Kazu.", "I'm aware of the issue and the thinking is to rework this as a {{TokenFilter}} and use anchoring options with surrounding tokens to decide if normalisation should take place, i.e.", "if the preceding token is \uffe5 or the following token is \u5186 in the case of normalising prices.", "It might also be helpful to look into using POS-info for this to benefit from what we actually know about the token, i.e.", "to not apply normalisation if the POS tag is a person name.", "Other suggestions and ideas are of course most welcome.", "Hi Christian,", "That's what I am thinking.", "I think TokenFilter would be a good choice to implement that feature.", "We can use POS tag to recognize what a token is.", "We can apply normalization if a token is a numeral prefix/suffix with numerals.", "Hi Christian, Kazuaki", "+1, TokenFilter implementation.", "And I think that it is helpful, this TokenFilter expand token arabic number and kanji number, like a synonym filter feature.", "Ohtani-san,", "I saw your tweet about this earlier and it sounds like a very good idea.", "Thanks.", "I will try to set aside some time to work on this.", "It would be nice if we can choose expand them or normalize them.", "I have a concern that Solr's query-side synonym expansion doesn't work well if number of tokens are different between original tokens and synonym tokens, especially if we want to do phrase matching with query-side synonym expansion will be a disaster (Of course, reduction or index-side would be better.", "But, we sometimes need to use TokenFilter that provides such capability in query-side.)", "So, I would like to choose the configuration that Kanji numerals normalize to Arabic numerals or Arabic numerals store along with Kanji numerals.", "Gaute and myself have been doing some work on this and we have rewritten this as a {{TokenFilter}}.", "A few comments:", "We have added support for numbers such as \uff13\uff0e\uff12\u5146\u5186 as you requested, Kazu.", "We could potentially use a POS-tag attribute from Kuromoji to identify number that we are composing, but perhaps not relying on POS-tags makes this filter also useful in the case of n-gramming.", "We haven't implemented any of the anchoring logic discussed above, i.e.", "if we to restrict normalization to prices, etc.", "Is this useful to have?", "Input such as {{1,5}} becomes {{15}} after normalization, which could be undesired.", "Is this bad input or do we want anchoring to retain these numbers?", "One thing though, in order to support some of this number parsing, i.e.", "cases such as \uff13\uff0e\uff12\u5146\u5186, we need to use Kuromoji in a mode that retains punctuation characters.", "There's also an unresolved issue found by {{checkRandomData}} that we haven't tracked down and fixed, yet.", "This is a work in progress and feedback is welcome.", "I've attached a new patch.", "The {{checkRandomData}} issues were caused by improper handling of token composition for graphs (bug found by [~gaute]).", "Tokens preceded by position increment zero token are left untouched and so are stacked/synonym tokens.", "We'll do some more testing and add some documentation before we move forward to commit this.", "Gaute and myself have done testing on real-world data and we've uncovered and fixed a couple of corner-case issues.", "Our todo items are as follows:", "# Do additional testing and possible add additional number formats", "# Document some unsupported cases in unit-tests", "# Add class-level javadoc", "# Add a Solr factory", "Added factory and wrote javadoc.", "[~cm] , sounds great!", "Can I test this feature?", "If yes, what version should I use?", "Please feel free to test it.", "Feedback is very welcome.", "The patch is against {{trunk}} and this should make it into 5.1.", "New patch with CHANGES.txt and services entry.", "Will do some end-to-end testing next.", "Updated patch with decimal number support, additional javadoc and the test code now makes precommit happy.", "Token-attributes such as part-of-speech, readings, etc.", "for the normalized token is currently inherited from the last token used when composing the normalized number.", "Since these values are likely to be wrong, I'm inclined to set this attributes to null or a reasonable default.", "I'm very happy to hear your thoughts on this.", "Minor updates to javadoc.", "I'll leave reading attributes, etc.", "unchanged for now and get back to resolving this once we have better mechanisms in place for updating some of the Japanese token attributes downstream."], "SplitGT": [" Normalizes a Japanese number"], "issueString": "Add Japanese Kanji number normalization to Kuromoji\nJapanese people use Kanji numerals instead of Arabic numerals for writing price, address and so on. i.e 12\u4e074800\u5186(124,800JPY), \u4e8c\u756a\u753a\u4e09\u30ce\u4e8c(3-2 Nibancho) and \u5341\u4e8c\u6708(December).  So, we would like to normalize those Kanji numerals to Arabic numerals (I don't think we need to have a capability to normalize to Kanji numerals).\n\n \nThanks a lot, Kazu.\n\nThis is a good idea to add.  Patches are of course also very welcome! :)\nWe, RONDHUIT, have done this kind of normalization (and more!). You may be interested in:\n\nhttp://www.rondhuit-demo.com/RCSS/api/overview-summary.html#featured-japanese\n\n||Summary||normalization sample||\n|\u6f22\u6570\u5b57=>\u7b97\u7528\u6570\u5b57\u6b63\u898f\u5316|\u56db\u4e03=>47, \u56db\u5341\u4e03=>47, \u56db\u62fe\u4e03=>47, \u56db\u3007\u4e03=>407|\n|\u548c\u66a6=>\u897f\u66a6\u6b63\u898f\u5316|\u662d\u548c\u56db\u4e03\u5e74\u3001\u662d\u548c\u56db\u5341\u4e03\u5e74\u3001\u662d\u548c\u56db\u62fe\u4e03\u5e74=>1972\u5e74, \u662d\u548c\u516d\u5341\u56db\u5e74\u3001\u5e73\u6210\u5143\u5e74=>1989\u5e74|\n\nKoji, this is very nice.\n\nDoes the kanji number normalizer ({{KanjiNumberCharFilter}}) also deal with combinations of kanji and arabic numbers like Kazu's price example?\n\nIs the above code you refer to something that can go into Lucene or is it non-free software?\nKoji, Thank you for your comment. I am very interested in the normalizer you have mentioned. Is it possible to choose to concatenate suffix/prefix(\u5e74/\u6708/\u5186, etc.) to the Arabic numbers?\nI've attached a work-in-progress patch for {{trunk}} that implements a {{CharFilter}} that normalizes Japanese numbers.\n\nThese are some TODOs and implementation considerations I have that I'd be thankful to get feedback on:\n\n* Buffering the entire input on the first read should be avoided.  The primary reason this is done is because I was thinking to add some regexps before and after kanji numeric strings to qualify their normalization, i.e. to only normalize strings that starts with \uffe5, JPY or ends with \u5186, to only normalize monetary amounts in Japanese yen.  However, this probably isn't necessary as we can probably can use {{Matcher.requireEnd()}} and {{Matcher.hitEnd()}} to decide if we need to read more input. (Thanks, Robert!)\n\n* Is qualifying the numbers to be normalized with prefix and suffix regexps useful, i.e. to only normalize monetary amounts?\n\n* How do we deal with leading zeros?  Currently, \"007\" and \"\u25ef\u25ef\u4e03\" becomes \"7\" today.  Do we want an option to preserve leading zeros?\n\n* How large numbers do we care about supporting?  Some of the larger numbers are surrogates, which complicates implementation, but they're certainly possible.  If we don't care about really large numbers, we can probably be fine working with {{long}} instead of {{BigInteger}}.\n\n* Polite numbers and some other variants aren't supported, i.e. \u58f1, \u5f10, \u53c2, etc., but they can easily be added.  We can also add the obsolete variants if that's useful somehow.  Are these useful?  Do we want them available via an option?\n\n* Number formats such as \"\uff11\u5104\uff12\uff0c\uff13\uff14\uff15\u4e07\uff16\uff0c\uff17\uff18\uff19\" isn't supported - we don't deal with the comma today, but this can be added.  The same applies to \"\uff11\uff12\u3000\uff13\uff14\uff15\" where there's a space that separates thousands like in French.  Numbers like \"2\u30fb2\u5146\" aren't supported, but can be added.\n\n* Only integers are supported today, so we can't parse \"\u3007\u30fb\u4e00\u4e8c\u4e09\u56db\", which becomes \"0\" and \"1234\" as separate tokens instead of \"0.1234\"\n\nThere are probably other considerations, too, that I doesn't immediately come to mind.\n\nNumbers are fairly complicated and feedback on direction for further implementation is most appreciated.  Thanks.\nHi Christian,\n\nGreat! I will test your patch and get back to you!!\n\nThanks,\nKazu\nKazuaki, do have any comment on this fix?\nSorry for this late reply.\n\nAlthough I have some request to improve capability, this is very helpful and nice charfilter for me.\nThank you! Christian!!\n\nMy requests are the following:\n\nIs it difficult to support numbers with period as the following?\n\uff13\uff0e\uff12\u5146\u5186\n\uff15\uff0e\uff12\u5104\u5186\n\nOn the other hand, I agree with Christian to not preserving leading zeros. So, \"\u25ef\u25ef\u4e03\" doesn't need to become \"007\".\n\nI think It would be helpful that this charfilter supports old Kanji numeric characters (\"KYU-KANJI\" or \"DAIJI\") such as \u58f1, \u58f9 (One), \u5f0c, \u5f10, \u8cb3 (Two), \u5f0d, \u53c2,\u53c3 (Three), or configureable.\nbq. On the other hand, I agree with Christian to not preserving leading zeros. So, \"\u25ef\u25ef\u4e03\" doesn't need to become \"007\".\nThis example shows why leading zeros should be preserved :)\n\nThere are different kinds of text search. Searching for media titles like James Bond movies is a very different thing from searching newspaper articles. You might want to find \"\u25ef\u25ef\u4e03\" as the Japanese-language release and \"007\" as the English-language release. These numbers are brands, not numbers. \nLance, you may be right.  Although I have never seen that Japanese people use Kanji numbers for James Bond movies :-), I can't say that we never use Kanji for that kind of expression.\n\nChristian, Is it possible to choose preserve leading zeros or not?\n{quote}\nIs it difficult to support numbers with period as the following?\n\uff13\uff0e\uff12\u5146\u5186\n\uff15\uff0e\uff12\u5104\u5186\n{quote}\n\nSupporting this is no problem and a good idea.\n\n{quote}\nI think It would be helpful that this charfilter supports old Kanji numeric characters (\"KYU-KANJI\" or \"DAIJI\") such as \u58f1, \u58f9 (One), \u5f0c, \u5f10, \u8cb3 (Two), \u5f0d, \u53c2,\u53c3 (Three), or configureable.\n{quote}\n\nThis is also easy to support.\n\nAs for making preserving zeros configurable, that's also possible, of course.\n\nIt's great to get more feedback on what sort of functionality we need and what should be configurable options. Hopefully, we can find a good balance without adding too much complexity.\n\nThanks for the feedback.\nThe following examples are false positive case:\n\"\u59ff\u4e09\u56db\u90ce\" became \"\u59ff\", \"34\", \"\u90ce\"\n\"\u5c0f\u6797\u4e00\u8336\" became \"\u5c0f\u6797\", \"1\", \"\u8336\"\n\"\u9234\u6728\u4e00\u90ce\" became \"\u9234\u6728\", \"1\", \"\u90ce\"\n\nCan we prevent this behavior?\nThanks, Kazu.\n\nI'm aware of the issue and the thinking is to rework this as a {{TokenFilter}} and use anchoring options with surrounding tokens to decide if normalisation should take place, i.e. if the preceding token is \uffe5 or the following token is \u5186 in the case of normalising prices.\n\nIt might also be helpful to look into using POS-info for this to benefit from what we actually know about the token, i.e. to not apply normalisation if the POS tag is a person name.\n\nOther suggestions and ideas are of course most welcome.\n\nHi Christian,\n\nThat's what I am thinking. I think TokenFilter would be a good choice to implement that feature. We can use POS tag to recognize what a token is. We can apply normalization if a token is a numeral prefix/suffix with numerals. \n\nHi Christian, Kazuaki\n\n+1, TokenFilter implementation.\nAnd I think that it is helpful, this TokenFilter expand token arabic number and kanji number, like a synonym filter feature.\nOhtani-san,\n\nI saw your tweet about this earlier and it sounds like a very good idea.  Thanks.\n\nI will try to set aside some time to work on this.\nIt would be nice if we can choose expand them or normalize them.\n\nI have a concern that Solr's query-side synonym expansion doesn't work well if number of tokens are different between original tokens and synonym tokens, especially if we want to do phrase matching with query-side synonym expansion will be a disaster (Of course, reduction or index-side would be better. But, we sometimes need to use TokenFilter that provides such capability in query-side.) So, I would like to choose the configuration that Kanji numerals normalize to Arabic numerals or Arabic numerals store along with Kanji numerals. \nGaute and myself have been doing some work on this and we have rewritten this as a {{TokenFilter}}.\n\nA few comments:\n\n* We have added support for numbers such as \uff13\uff0e\uff12\u5146\u5186 as you requested, Kazu.\n* We could potentially use a POS-tag attribute from Kuromoji to identify number that we are composing, but perhaps not relying on POS-tags makes this filter also useful in the case of n-gramming.\n* We haven't implemented any of the anchoring logic discussed above, i.e. if we to restrict normalization to prices, etc. Is this useful to have?\n* Input such as {{1,5}} becomes {{15}} after normalization, which could be undesired. Is this bad input or do we want anchoring to retain these numbers?\n\nOne thing though, in order to support some of this number parsing, i.e. cases such as \uff13\uff0e\uff12\u5146\u5186, we need to use Kuromoji in a mode that retains punctuation characters.\n\nThere's also an unresolved issue found by {{checkRandomData}} that we haven't tracked down and fixed, yet.\n\nThis is a work in progress and feedback is welcome.\nI've attached a new patch.\n\nThe {{checkRandomData}} issues were caused by improper handling of token composition for graphs (bug found by [~gaute]). Tokens preceded by position increment zero token are left untouched and so are stacked/synonym tokens.\n\nWe'll do some more testing and add some documentation before we move forward to commit this.\nGaute and myself have done testing on real-world data and we've uncovered and fixed a couple of corner-case issues.\n\nOur todo items are as follows:\n\n# Do additional testing and possible add additional number formats\n# Document some unsupported cases in unit-tests\n# Add class-level javadoc\n# Add a Solr factory\n\n\nAdded factory and wrote javadoc.\n[~cm] , sounds great! Can I test this feature? If yes, what version should I use?\nPlease feel free to test it.  Feedback is very welcome.\n\nThe patch is against {{trunk}} and this should make it into 5.1.\nNew patch with CHANGES.txt and services entry.\n\nWill do some end-to-end testing next.\nUpdated patch with decimal number support, additional javadoc and the test code now makes precommit happy.\n\nToken-attributes such as part-of-speech, readings, etc. for the normalized token is currently inherited from the last token used when composing the normalized number. Since these values are likely to be wrong, I'm inclined to set this attributes to null or a reasonable default.\n\nI'm very happy to hear your thoughts on this.\n\n\nMinor updates to javadoc.\n\nI'll leave reading attributes, etc. unchanged for now and get back to resolving this once we have better mechanisms in place for updating some of the Japanese token attributes downstream.\n", "issueSearchSentences": ["One thing though, in order to support some of this number parsing, i.e.", "for the normalized token is currently inherited from the last token used when composing the normalized number.", "We, RONDHUIT, have done this kind of normalization (and more!).", "Number formats such as \"\uff11\u5104\uff12\uff0c\uff13\uff14\uff15\u4e07\uff16\uff0c\uff17\uff18\uff19\" isn't supported - we don't deal with the comma today, but this can be added.", "So, we would like to normalize those Kanji numerals to Arabic numerals (I don't think we need to have a capability to normalize to Kanji numerals)."], "issueSearchScores": [0.5824574828147888, 0.5400293469429016, 0.5235203504562378, 0.5170548558235168, 0.5101762413978577]}
{"aId": 36, "code": "public Field(String name, TokenStream tokenStream, TermVector termVector) {\n    if (name == null)\n      throw new NullPointerException(\"name cannot be null\");\n    if (tokenStream == null)\n      throw new NullPointerException(\"tokenStream cannot be null\");\n    \n    this.name = name.intern();        // field names are interned\n    this.fieldsData = tokenStream;\n    \n    this.isStored = false;\n    this.isCompressed = false;\n    \n    this.isIndexed = true;\n    this.isTokenized = true;\n    \n    this.isBinary = false;\n    \n    setStoreTermVector(termVector);\n  }", "comment": " Create a tokenized and indexed field that is not stored, optionally with storing term vectors.", "issueId": "LUCENE-580", "issueStringList": ["Pre-analyzed fields", "Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.", "There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.", "Field.java.diff", "DocumentWriter.java.diff", "The description should be \"This patch\"... and not \"This page\"...", "This patch will be useful for users LUCENE-755, the payloads patch.", "That patch adds \"payloads\" to tokens, but using it to add a few tokens with payloads in some field can be ugly because you need to split the code into two places: at one place you add the field, only text, and at another place you need to write a special analyzer which will work only on that field, recognize the specific tokens and add the payloads to them.", "This patch makes this easier, because when you add a field, you can add it pre-analyzed, already as a list of tokens, and these tokens will already have their special payloads in them.", "I have just a few comments on this patch:", "1.", "The description above suggests that it might not work if the same field name is used for two Field's, one stored and the other preanalyzed.", "I think it is important that this combination (as well as all other combinations) are supported.", "I actually use all these combinations in my code, and I don't see why it should cause problems.", "2.", "The patch has some strange changes in the comments, changing the word \"Index\" to \"NotificationService\".", "I bet this wasn't intentional :-)", "3.", "The new Field constructor still has a \"Index\" paramter, taking TOKENIZED, UN_TOKENIZED or NO_NORMS (only NO is forbidden).", "I wonder, what's the difference between TOKENIZED and UN_TOKENIZED in this case?", "The NO_NORMS is a very useful case, because it allows you to do something not previously possible in Lucene (a tokenized field, but without norms).", "Perhaps this parameter should be better documented in the javadoc comment.", "4.", "In the new Field constructor's comment, the phrase \"if name or reader\" should be \"if name or tokenStream\".", "Thanks!", "Nadav Har'El [18/Jan/07 08:21 AM]", "> The description above suggests that it might not work if the same", "> field name is used for two Field's, one stored and the other preanalyzed.", "> I think it is important that this combination (as well as all other", "> combinations) are supported.", "I actually use all these combinations in my", "> code, and I don't see why it should cause problems.", "Actually, I can't remember why I thought there could be a problem, nor can I think of one now.", "This code is from my pre-tests era, and it should could need some.", "If you like this  strategy, I think it would be more elegant passing a factory rather than the actual token stream.", "> The patch has some strange changes in the comments, changing the word", ">  \"Index\" to \"NotificationService\".", "I bet this wasn't intentional :-)", "Hehe, that must be an old refactoring search and replace incident.", "Michael, let me know if you start working on this.", "I would not mind a discussion on how the token stream supplier should look like.", "For one it should be an interface/class that supplies the token stream and not a TokenStream it self as it could only be read once.", "Or?", "Perhaps it would be nice if standard term vector, positions et c. was available for easy extraction?", "Pass a TokenStream to the constructor of the pre analyzed field, buffer it up in a LinkedHashMap, or something in that direction.", "An implementation suggestion.", "The project will not compile as there are two Fieldable implementations that does not implement the tokenStreamFactoryValue() method, LazyField and FieldForMerge.", "I suspect they could just return null values, but I leave it open as I'm not sure.", "Patch contains:", "public interface TokenStreamFactory {", "public abstract TokenStream factory() throws IOException;", "}", "public class SimplePreAnalyzedField implements TokenStreamFactory {", "public class CachedPreAnalyzedField implements TokenStreamFactory {", "Hi Karl,", "thank you for your comments and your new patch!", "I actually updated you initial patch a couple of days ago to apply cleanly on the current trunk as I found it quite simple and straightforward.", "I have a question regarding the new patch: What kind of use cases do you have in mind with the cached field?", "I would think that a TokenStream will only be read once by the DocumentWriter?", "Currently you can add a field with a Reader, and as far as I know is it not guaranteed that a Reader supports reset(), which means it can only be read once, too.", "Thoughts?", "Michael", "25 apr 2007 kl.", "10.23 skrev Michael Busch (JIRA):", "> What kind of use cases do you have in mind with the cached field?", "I made the inital implementation for text mining purposes -- I needed the term vector prior to inserting the document to the index.", "Back then I analyzed, cached it up, did my secondary analysis of the vector, and finally reconstructed the token stream and passed it to the field.", "I think it would be easier to just pass a token stream to an extention of CachedPreAn.. that also features a termFreqVector(), termPosVector(), et c.", "Karl", "Karl,", "thanks again for your suggestions.", "I created a patch with a slightly different approach compared to your latest patch.", "Similar to java.io.Reader I added the public method reset() to TokenStream, which does nothing per default.", "Subclasses may or may not overwrite this method.", "I also added the new class CachingTokenFilter to the analysis package which does the same as your CachedPreAnalyzedField.", "Before the DocumentWriter consumes the TokenStream it calls reset() reposition the stream at the beginning.", "With this approach it is not neccessary anymore to introduce a TokenStreamFactory and the PreAnalyzedField classes, which is simpler and more consistent with the Analyzer API in my opinion.", "Yet it also allows to consume the Tokens of a stream more than once, which should satisfy your needs?", "Please let me know what you thing about this new patch.", "Maybe other committers could take a look as well, since this is an API change (well, extension) to two very common classes: TokenStream and Field.", "28 apr 2007 kl.", "20.42 skrev Michael Busch (JIRA):", "> Please let me know what you thing about this new patch.", "+1.", "Very clean.", "A patch that supports omitting norms.", "Create a tokenized and indexed field that is not stored, optionally with", "storing term vectors.", "@param name The name of the field", "@param tokenStream The reader with the content", "+   * @param index Must be tokenized or no norms.", "@param termVector Whether term vector should be stored", "@throws NullPointerException if name or reader is <code>null</code>", "public Field(String name, TokenStream tokenStream, TermVector termVector) {", "+ public Field(String name, TokenStream tokenStream, Index index, TermVector termVector) {", "if (name == null)", "throw new NullPointerException(\"name cannot be null\");", "if (tokenStream == null)", "throw new NullPointerException(\"tokenStream cannot be null\");", "+    if (index != Index.TOKENIZED && index != Index.NO_NORMS) {", "+      throw new IllegalArgumentException(\"index must be either TOKENIZED or NO_NORMS\");", "+    }", "Also fixed some copy/paste related javadoc errors.", "I'd like to keep the new Constructor consistent with the Constructor that takes a Reader argument.", "You can use Fieldable.setOmitNorms() to disable norms.", "How about two constructors?", "I hesitate to add even more constructors to Field, since it has already a bunch of them.", "And I don't see the value in the constructor you proposed, because the same can be achieved by using setOmitNorms().", "I think it is a bit confusing to have the Index parameter with only two out of four allowed values in the constructor.", "I just committed this.", "Thanks a lot for your patches and the productive discussions, Karl!"], "SplitGT": [" Create a tokenized and indexed field that is not stored, optionally with storing term vectors."], "issueString": "Pre-analyzed fields\nAdds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.\n\nThere might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.\nField.java.diff\nDocumentWriter.java.diff\nThe description should be \"This patch\"... and not \"This page\"...\nThis patch will be useful for users LUCENE-755, the payloads patch. That patch adds \"payloads\" to tokens, but using it to add a few tokens with payloads in some field can be ugly because you need to split the code into two places: at one place you add the field, only text, and at another place you need to write a special analyzer which will work only on that field, recognize the specific tokens and add the payloads to them. This patch makes this easier, because when you add a field, you can add it pre-analyzed, already as a list of tokens, and these tokens will already have their special payloads in them.\n\nI have just a few comments on this patch:\n\n1. The description above suggests that it might not work if the same field name is used for two Field's, one stored and the other preanalyzed. I think it is important that this combination (as well as all other combinations) are supported. I actually use all these combinations in my code, and I don't see why it should cause problems.\n\n2. The patch has some strange changes in the comments, changing the word \"Index\" to \"NotificationService\". I bet this wasn't intentional :-)\n\n3. The new Field constructor still has a \"Index\" paramter, taking TOKENIZED, UN_TOKENIZED or NO_NORMS (only NO is forbidden). I wonder, what's the difference between TOKENIZED and UN_TOKENIZED in this case? The NO_NORMS is a very useful case, because it allows you to do something not previously possible in Lucene (a tokenized field, but without norms). Perhaps this parameter should be better documented in the javadoc comment.\n\n4. In the new Field constructor's comment, the phrase \"if name or reader\" should be \"if name or tokenStream\".\n\nThanks!\nNadav Har'El [18/Jan/07 08:21 AM]\n\n> The description above suggests that it might not work if the same\n> field name is used for two Field's, one stored and the other preanalyzed. \n> I think it is important that this combination (as well as all other \n> combinations) are supported. I actually use all these combinations in my\n> code, and I don't see why it should cause problems.\n\nActually, I can't remember why I thought there could be a problem, nor can I think of one now.  This code is from my pre-tests era, and it should could need some.\n\nIf you like this  strategy, I think it would be more elegant passing a factory rather than the actual token stream.\n\n> The patch has some strange changes in the comments, changing the word\n>  \"Index\" to \"NotificationService\". I bet this wasn't intentional :-)\n\nHehe, that must be an old refactoring search and replace incident.\n\n\nMichael, let me know if you start working on this. I would not mind a discussion on how the token stream supplier should look like. For one it should be an interface/class that supplies the token stream and not a TokenStream it self as it could only be read once. Or?\nPerhaps it would be nice if standard term vector, positions et c. was available for easy extraction? Pass a TokenStream to the constructor of the pre analyzed field, buffer it up in a LinkedHashMap, or something in that direction.\nAn implementation suggestion. The project will not compile as there are two Fieldable implementations that does not implement the tokenStreamFactoryValue() method, LazyField and FieldForMerge. I suspect they could just return null values, but I leave it open as I'm not sure.\n\nPatch contains:\n\npublic interface TokenStreamFactory {\n  public abstract TokenStream factory() throws IOException;\n}\n\n/** Encapuslates a single instance of TokenStream that can be passed on ONCE. */\npublic class SimplePreAnalyzedField implements TokenStreamFactory {\n\n/** Caches an instance of TokenStream in a List, reassembled to a TokenStream for each call to factory() */\npublic class CachedPreAnalyzedField implements TokenStreamFactory {\n\nHi Karl,\n\nthank you for your comments and your new patch! I actually updated you initial patch a couple of days ago to apply cleanly on the current trunk as I found it quite simple and straightforward. \n\nI have a question regarding the new patch: What kind of use cases do you have in mind with the cached field? I would think that a TokenStream will only be read once by the DocumentWriter? Currently you can add a field with a Reader, and as far as I know is it not guaranteed that a Reader supports reset(), which means it can only be read once, too. Thoughts?\n\nMichael\n25 apr 2007 kl. 10.23 skrev Michael Busch (JIRA):\n> What kind of use cases do you have in mind with the cached field?\n\nI made the inital implementation for text mining purposes -- I needed the term vector prior to inserting the document to the index. Back then I analyzed, cached it up, did my secondary analysis of the vector, and finally reconstructed the token stream and passed it to the field. I think it would be easier to just pass a token stream to an extention of CachedPreAn.. that also features a termFreqVector(), termPosVector(), et c.\n\nKarl\nKarl,\n\nthanks again for your suggestions. I created a patch with a slightly different approach compared to your latest patch. Similar to java.io.Reader I added the public method reset() to TokenStream, which does nothing per default. Subclasses may or may not overwrite this method. I also added the new class CachingTokenFilter to the analysis package which does the same as your CachedPreAnalyzedField. Before the DocumentWriter consumes the TokenStream it calls reset() reposition the stream at the beginning.\n\nWith this approach it is not neccessary anymore to introduce a TokenStreamFactory and the PreAnalyzedField classes, which is simpler and more consistent with the Analyzer API in my opinion. Yet it also allows to consume the Tokens of a stream more than once, which should satisfy your needs?\n\nPlease let me know what you thing about this new patch. Maybe other committers could take a look as well, since this is an API change (well, extension) to two very common classes: TokenStream and Field.\n28 apr 2007 kl. 20.42 skrev Michael Busch (JIRA):\n> Please let me know what you thing about this new patch.\n\n+1. Very clean.\n\nA patch that supports omitting norms.\n\n /**\n   * Create a tokenized and indexed field that is not stored, optionally with \n   * storing term vectors.\n   * \n   * @param name The name of the field\n   * @param tokenStream The reader with the content\n+   * @param index Must be tokenized or no norms.\n   * @param termVector Whether term vector should be stored\n   * @throws NullPointerException if name or reader is <code>null</code>\n   */ \n - public Field(String name, TokenStream tokenStream, TermVector termVector) {\n+ public Field(String name, TokenStream tokenStream, Index index, TermVector termVector) {\n\n    if (name == null)\n      throw new NullPointerException(\"name cannot be null\");\n    if (tokenStream == null)\n      throw new NullPointerException(\"tokenStream cannot be null\");\n+    if (index != Index.TOKENIZED && index != Index.NO_NORMS) {\n+      throw new IllegalArgumentException(\"index must be either TOKENIZED or NO_NORMS\");\n+    }\n\n\nAlso fixed some copy/paste related javadoc errors.\nI'd like to keep the new Constructor consistent with the Constructor that takes a Reader argument. You can use Fieldable.setOmitNorms() to disable norms.\nHow about two constructors?\nI hesitate to add even more constructors to Field, since it has already a bunch of them. And I don't see the value in the constructor you proposed, because the same can be achieved by using setOmitNorms(). I think it is a bit confusing to have the Index parameter with only two out of four allowed values in the constructor.\nI just committed this. Thanks a lot for your patches and the productive discussions, Karl!\n", "issueSearchSentences": ["public Field(String name, TokenStream tokenStream, TermVector termVector) {", "+ public Field(String name, TokenStream tokenStream, Index index, TermVector termVector) {", "> field name is used for two Field's, one stored and the other preanalyzed.", "There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.", "In the new Field constructor's comment, the phrase \"if name or reader\" should be \"if name or tokenStream\"."], "issueSearchScores": [0.9123595952987671, 0.8516240119934082, 0.6587268114089966, 0.6556286811828613, 0.6440141201019287]}
{"aId": 37, "code": "public long softUpdateDocument(Term term, Iterable<? extends IndexableField> doc, Field... softDeletes) throws IOException {\n    if (term == null) {\n      throw new IllegalArgumentException(\"term must not be null\");\n    }\n    if (softDeletes == null || softDeletes.length == 0) {\n      throw new IllegalArgumentException(\"at least one soft delete must be present\");\n    }\n    return updateDocument(DocumentsWriterDeleteQueue.newNode(buildDocValuesUpdate(term, softDeletes, false)), doc);\n  }", "comment": " One use of this API is to retain older versions of documents instead of replacing them. The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.", "issueId": "LUCENE-8200", "issueStringList": ["Allow doc-values to be updated atomically together with a document", "Today we can only update a document by deleting all previously indexed documents for the given term.", "In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)", "This change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted.", "This is a spinnoff from LUCENE-8198", "here is a github commit for reference https://github.com/s1monw/lucene-solr/commit/ff6bd484590a02d169b9259a465ce70069f83e82", "the public IW api looks good and simple.", "There's a missing space in the javadocs \"flush may happen only afterthe add\"", "I didn't review any of the impl details or tests.", "+1, patch looks great; I left a minor comment on the github commit.", "Amazing how little code the change requires, and it's a nice approach for soft deletes.", "thanks [~mikemccand] I attached a new patch.", "I am aiming to push this tomorrow unless anybody objects.", "One think that I realized is that we can't utilize this for index sorting since fields that participate in the sort can't be updated.", "But I think that is not a major issue.", "//cc [~rcmuir]", "Cool feature!", "I like the name \"softUpdateDocuments\" for this.", "Maybe the CHANGES.txt entry should make reference to the particular method so user's can easily get to further explanatory javadocs.", "I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.", "For example:", "bq.", "One use of this API is to retain older versions of documents instead of replacing them.", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.", "The patch did not apply for me {{git apply ../patches/LUCENE-8200.patch}} ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "ok sure.", "> I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.", "For example:", "I can do that.", "> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "my repo is aligned with whatever is on  _https://git-wip-us.apache.org/repos/asf/lucene-solr.git_ it has not been forked from a specific mirror on github which doesn't matter at all here.", "I don't really understand why you bring this up like a second time here.", "it's unrelated.", "{quote}my repo is aligned with whatever is on _[https://git-wip-us.apache.org/repos/asf/lucene-solr.git_] it has not been forked from a specific mirror on github which doesn't matter at all here.", "I don't really understand why you bring this up like a second time here.", "it's unrelated.", "{quote}", "1st time was just an unrelated question; sorry if asking it annoyed you.", "2nd time is perhaps in retrospect misattributed.", "I thought that your repo shared no lineage with ASF's but now I think I'm mistaken.", "If it didn't share lineage (which was my thinking at the time I commented), it would explain why \"git apply ...\" didn't work.", "But it's not that.", "It didn't work when I tried to apply it in IntelliJ either.", "A mystery; ah well.", "Does \"git apply ...\" work for anyone else here for this patch?", "My usual method is to apply patches directly in IntelliJ but when that failed I went to the CLI and found it failed there too, so I'm stumped.", "Raw patches are harder to review as thoroughly as one can in an IDE (of course).", "[~dsmiley] I updated the patch and uploaded it.", "I think the previous patch had 2 commits in it.", "Sorry for the confusion.", "I applied it with _patch -p1 < ../LUCENE-8200.patch_ just fine", "That was it; thanks.", "IntelliJ likes this one, and so does {{git apply ...}}", "thanks everybody"], "SplitGT": [" One use of this API is to retain older versions of documents instead of replacing them.", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time."], "issueString": " Allow doc-values to be updated atomically together with a document\nToday we can only update a document by deleting all previously indexed documents for the given term. In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)\r\n    \r\nThis change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted. This is a spinnoff from LUCENE-8198\r\n\nhere is a github commit for reference https://github.com/s1monw/lucene-solr/commit/ff6bd484590a02d169b9259a465ce70069f83e82\nthe public IW api looks good and simple.\r\n\r\nThere's a missing space in the javadocs \"flush may happen only afterthe add\"\r\n\r\nI didn't review any of the impl details or tests.\n+1, patch looks great; I left a minor comment on the github commit.\u00a0 Amazing how little code the change requires, and it's a nice approach for soft deletes.\nthanks [~mikemccand] I attached a new patch. I am aiming to push this tomorrow unless anybody objects. One think that I realized is that we can't utilize this for index sorting since fields that participate in the sort can't be updated. But I think that is not a major issue. //cc [~rcmuir]\nCool feature!  I like the name \"softUpdateDocuments\" for this.  Maybe the CHANGES.txt entry should make reference to the particular method so user's can easily get to further explanatory javadocs.\r\n\r\nI suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.  For example:\r\nbq. One use of this API is to retain older versions of documents instead of replacing them. The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.\r\n\r\nThe patch did not apply for me {{git apply ../patches/LUCENE-8200.patch}} ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\n> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\r\nok sure.\r\n\r\n> I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is. For example:\r\nI can do that.\r\n\r\n> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\r\n\r\nmy repo is aligned with whatever is on  _https://git-wip-us.apache.org/repos/asf/lucene-solr.git_ it has not been forked from a specific mirror on github which doesn't matter at all here. I don't really understand why you bring this up like a second time here. it's unrelated. \r\n\n{quote}my repo is aligned with whatever is on _[https://git-wip-us.apache.org/repos/asf/lucene-solr.git_] it has not been forked from a specific mirror on github which doesn't matter at all here. I don't really understand why you bring this up like a second time here. it's unrelated.\r\n{quote}\r\n1st time was just an unrelated question; sorry if asking it annoyed you.\r\n2nd time is perhaps in retrospect misattributed.  I thought that your repo shared no lineage with ASF's but now I think I'm mistaken.  If it didn't share lineage (which was my thinking at the time I commented), it would explain why \"git apply ...\" didn't work.  But it's not that.  It didn't work when I tried to apply it in IntelliJ either.  A mystery; ah well.  Does \"git apply ...\" work for anyone else here for this patch?  My usual method is to apply patches directly in IntelliJ but when that failed I went to the CLI and found it failed there too, so I'm stumped.  Raw patches are harder to review as thoroughly as one can in an IDE (of course).\n[~dsmiley] I updated the patch and uploaded it. I think the previous patch had 2 commits in it. Sorry for the confusion. I applied it with _patch -p1 < ../LUCENE-8200.patch_ just fine\nThat was it; thanks. IntelliJ likes this one, and so does {{git apply ...}}\nthanks everybody\n", "issueSearchSentences": ["Today we can only update a document by deleting all previously indexed documents for the given term.", "This change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted.", "In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)", "I like the name \"softUpdateDocuments\" for this.", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time."], "issueSearchScores": [0.7839134931564331, 0.716931939125061, 0.6598406434059143, 0.5953419208526611, 0.5657602548599243]}
{"aId": 38, "code": "public LongIterator iterator() {\n    return new LongIterator() {\n      private int remainingValues = cardinality();\n      private int valsIdx = 0;\n\n      @Override\n      public boolean hasNext() {\n        return remainingValues > 0;\n      }\n\n      @Override\n      public long next() {\n        if (!hasNext()) {\n          throw new NoSuchElementException();\n        }\n        remainingValues--;\n\n        if (remainingValues == 0 && zeroCount > 0) {\n          return 0;\n        }\n\n        while (true) { // guaranteed to find another value if we get here\n          long value = vals[valsIdx++];\n          if (value != 0) {\n            return value;\n          }\n        }\n      }\n\n    };\n  }", "comment": " Returns an iterator over the values in the set.", "issueId": "SOLR-11093", "issueStringList": ["implement Points/Numeric support for graph query", "It looks like GraphQueryTest only has tests for strings.", "We should add tests for numeric fields and then enable points randomization.", "can this be resolved?", "bq.", "can this be resolved?", "Numerics still need to be fixed for graph query - I was going to do it under this issue.", "We already have a specific issue for adding numeric supportto GraphQParser -- which you assigned to your self a few days ago: SOLR-10845", "Yeah, I was confused too at first... these are actually *different* parsers and tests.", "This one is {!graph}, the other one is {!graphTerms}.", "I hadn't even realized the latter existed and I assumed it was the former when I assigned it to myself.", "Oh .. weird, sorry for the noise: carry on.", "OK, after reviewing this more it looks like this was written for strings (and trie* numeric fields worked because they existed in the full-text index, and could be uninverted to docTermOrds).", "To make this work for numeric point fields, we'll need to:", "require docValues", "write new code to collect values from numeric docValues (i.e.", "not docTermOrds and ByteRefHash)", "write new code to create an appropriate frontier query from the collected values", "Any objections to requiring docValues?", "The only real alternative is to implement FieldCache support for Point fields to fake docValues.", "NOTE: I've \"un-subtasked\" this jira since it no longer makes sense as a subtask of SOLR-10807.", "(FWIW: I have no strong opinions about the questions you asked)", "Here's a draft patch with refactorings, creating a base class GraphEdgeCollector and pulling out term-specific code into a subclass GraphTermsCollector.", "Next TODO is to implement GraphNumericCollector that will work with points & numeric docvalues", "status: I've made more progress on this, but I'm currently taking sick day(s).", "Should be able to finish up quickly once I get back to it.", "Here's an updated draft patch that moves LongSet and LongIterator to the util package, uses them in conjunction with numeric docvalues to collect numeric values, and then uses points fields to create a set query.", "Doesn't yet work for some reason...", "I'm digging into why.", "Requiring DocValues for numerics on a GraphQuery is fine with me.", "Is there a back-compat concern?", "Maybe LongSet could use more javadocs if it's going to be shared.", "The constructor should actually assert (or throw) that the argument is a power of 2.", "And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?", "Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?", "Updated patch... tests now pass!", "bq.", "Is there a back-compat concern?", "Everything should be back compat with existing schemas.", "An interesting titbit I discovered while doing this issue: the graph query reversed the meaning of \"from\" and \"to\" wrt the join query.", "Not a bug I guess... just a different way of thinking about what \"from\" and \"to\" mean.", "bq.", "Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?", "I wasn't able to figure out how to make it simpler with an iterator approach (it seems like there always needs to be an additional check for the empty value).", "In a place where the performance difference might matter, one always has the option of grabbing the backing array though.", "Why is zeroCount an integer instead of simply a hasZero boolean?", "Here's my first iteration (subsequently changed futher below).", "All I did was rename {{hasNext}} to {{positioned}} (which I feel is more clear), added some inline comments for what the fields mean, and I added a leading check inside next().", "So this was simpler than I thought.", "{code:java}", "public LongIterator iterator() {", "return new LongIterator() {", "if this set contains zero, this iterator's initial state is already positioned", "private boolean positioned = zeroCount > 0;", "private int i = -1; // current index into vals[]", "private long value = 0; // if positioned, this is our current value", "@Override", "public boolean hasNext() {", "if (positioned) {", "return true;", "}", "while (++i < vals.length) {", "value = vals[i];", "if (value != 0) {", "return positioned = true;", "}", "}", "return false;", "}", "@Override", "public long next() {", "if (!positioned) {", "if (!hasNext()) {", "throw new NoSuchElementException();", "}", "}", "positioned = false;", "return value;", "}", "};", "}", "{code}", "And here's what I think is the most clear approach using a countdown integer:", "{code:java}", "public LongIterator iterator() {", "return new LongIterator() {", "private int remainingValues = cardinality();", "private int valsIdx = 0;", "@Override", "public boolean hasNext() {", "return remainingValues > 0;", "}", "@Override", "public long next() {", "if (!hasNext()) {", "throw new NoSuchElementException();", "}", "remainingValues--;", "if (remainingValues == 0 && zeroCount > 0) {", "return 0;", "}", "while (true) { // guaranteed to find another value if we get here", "long value = vals[valsIdx++];", "if (value != 0) {", "return value;", "}", "}", "}", "};", "{code}", "This has the benefit that we don't loop past the last value of the array.", "It also has fewer state variables.", "Also, you don't \"pay\" any advancement cost in hasNext(); it's only next() where the value is going to actually be consumed.", "I'll commit the latter version tonight.", "bq.", "I'll commit the latter version tonight.", "Sure, I don't think I really have a preference (and I'd have to benchmark to know if it was any faster)"], "SplitGT": [" Returns an iterator over the values in the set."], "issueString": "implement Points/Numeric support for graph query\nIt looks like GraphQueryTest only has tests for strings.  We should add tests for numeric fields and then enable points randomization.\ncan this be resolved?\nbq. can this be resolved?\nNumerics still need to be fixed for graph query - I was going to do it under this issue.\nWe already have a specific issue for adding numeric supportto GraphQParser -- which you assigned to your self a few days ago: SOLR-10845\nYeah, I was confused too at first... these are actually *different* parsers and tests.  This one is {!graph}, the other one is {!graphTerms}.  I hadn't even realized the latter existed and I assumed it was the former when I assigned it to myself.\nOh .. weird, sorry for the noise: carry on.\nOK, after reviewing this more it looks like this was written for strings (and trie* numeric fields worked because they existed in the full-text index, and could be uninverted to docTermOrds).\nTo make this work for numeric point fields, we'll need to:\n- require docValues\n- write new code to collect values from numeric docValues (i.e. not docTermOrds and ByteRefHash)\n- write new code to create an appropriate frontier query from the collected values\n\nAny objections to requiring docValues?  The only real alternative is to implement FieldCache support for Point fields to fake docValues.\nNOTE: I've \"un-subtasked\" this jira since it no longer makes sense as a subtask of SOLR-10807.\n\n(FWIW: I have no strong opinions about the questions you asked)\nHere's a draft patch with refactorings, creating a base class GraphEdgeCollector and pulling out term-specific code into a subclass GraphTermsCollector.\nNext TODO is to implement GraphNumericCollector that will work with points & numeric docvalues\nstatus: I've made more progress on this, but I'm currently taking sick day(s).  Should be able to finish up quickly once I get back to it.\nHere's an updated draft patch that moves LongSet and LongIterator to the util package, uses them in conjunction with numeric docvalues to collect numeric values, and then uses points fields to create a set query.\nDoesn't yet work for some reason... I'm digging into why.\nRequiring DocValues for numerics on a GraphQuery is fine with me.  Is there a back-compat concern?\n\nMaybe LongSet could use more javadocs if it's going to be shared.  The constructor should actually assert (or throw) that the argument is a power of 2.  And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?  Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?\nUpdated patch... tests now pass!\nbq. Is there a back-compat concern?\n\nEverything should be back compat with existing schemas.\nAn interesting titbit I discovered while doing this issue: the graph query reversed the meaning of \"from\" and \"to\" wrt the join query.\nNot a bug I guess... just a different way of thinking about what \"from\" and \"to\" mean.\nbq. Since the set size is known, maybe it could be adjusted to use a countDown integer approach, yet still be very fast/simple?\n\nI wasn't able to figure out how to make it simpler with an iterator approach (it seems like there always needs to be an additional check for the empty value).\nIn a place where the performance difference might matter, one always has the option of grabbing the backing array though.\nWhy is zeroCount an integer instead of simply a hasZero boolean?\n\nHere's my first iteration (subsequently changed futher below).  All I did was rename {{hasNext}} to {{positioned}} (which I feel is more clear), added some inline comments for what the fields mean, and I added a leading check inside next(). So this was simpler than I thought.\n{code:java}\n/** Returns an iterator over the values in the set. */\n  public LongIterator iterator() {\n    return new LongIterator() {\n      // if this set contains zero, this iterator's initial state is already positioned\n      private boolean positioned = zeroCount > 0;\n      private int i = -1; // current index into vals[]\n      private long value = 0; // if positioned, this is our current value\n\n      @Override\n      public boolean hasNext() {\n        if (positioned) {\n          return true;\n        }\n        while (++i < vals.length) {\n          value = vals[i];\n          if (value != 0) {\n            return positioned = true;\n          }\n        }\n        return false;\n      }\n\n      @Override\n      public long next() {\n        if (!positioned) {\n          if (!hasNext()) {\n            throw new NoSuchElementException();\n          }\n        }\n        positioned = false;\n        return value;\n      }\n\n    };\n  }\n{code}\n\nAnd here's what I think is the most clear approach using a countdown integer:\n{code:java}\n\n  /** Returns an iterator over the values in the set. */\n  public LongIterator iterator() {\n    return new LongIterator() {\n      private int remainingValues = cardinality();\n      private int valsIdx = 0;\n\n      @Override\n      public boolean hasNext() {\n        return remainingValues > 0;\n      }\n\n      @Override\n      public long next() {\n        if (!hasNext()) {\n          throw new NoSuchElementException();\n        }\n        remainingValues--;\n\n        if (remainingValues == 0 && zeroCount > 0) {\n          return 0;\n        }\n        \n        while (true) { // guaranteed to find another value if we get here\n          long value = vals[valsIdx++];\n          if (value != 0) {\n            return value;\n          }\n        }\n      }\n\n    };\n{code}\nThis has the benefit that we don't loop past the last value of the array.  It also has fewer state variables. Also, you don't \"pay\" any advancement cost in hasNext(); it's only next() where the value is going to actually be consumed.\nI'll commit the latter version tonight.\nbq. I'll commit the latter version tonight.\n\nSure, I don't think I really have a preference (and I'd have to benchmark to know if it was any faster)\n", "issueSearchSentences": ["public LongIterator iterator() {", "public LongIterator iterator() {", "return new LongIterator() {", "return new LongIterator() {", "And it appears that the LongIterator returned *must* be used in a standard fashion in which hasNext is *always* called before next(); yes?"], "issueSearchScores": [0.8638166785240173, 0.8638166785240173, 0.8154793381690979, 0.8154793381690979, 0.7565392851829529]}
{"aId": 39, "code": "public JavaUtilRegexCapabilities(int flags) {\n    this.flags = flags;\n  }", "comment": " Constructor that allows for the modification of the flags that the java.util.regex.Pattern will use to compile the regular expression. This gives the user the ability to fine-tune how the regular expression to match the functionlity that they need. The java.util.regex.Pattern Pattern class supports specifying these fields via the regular expression text itself, but this gives the caller another option to modify the behavior. Useful in cases where the regular expression text cannot be modified, or if doing so is undesired.", "issueId": "LUCENE-1745", "issueStringList": ["Add ability to specify compilation/matching flags to RegexCapabiltiies implementations", "The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression.", "While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.", "Therefore, this improvement request is to add the capability to provide those modification flags to either implementation.", "I've developed a working implementation that makes minor additions to the existing code.", "The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided.", "This provides complete backwards compatibility.", "For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields.", "These are pass through to the underlying implementation.", "They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.", "Proposed changes:", "For the JavaUtilRegexCapabilities.java, the following is the changes made.", "private int flags = 0;", "Define the optional flags from Pattern that can be used.", "Do this here to keep Pattern contained within this class.", "public final int FLAG_CANON_EQ = Pattern.CANON_EQ;", "public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;", "public final int FLAG_COMMENTS = Pattern.COMMENTS;", "public final int FLAG_DOTALL = Pattern.DOTALL;", "public final int FLAG_LITERAL = Pattern.LITERAL;", "public final int FLAG_MULTILINE = Pattern.MULTILINE;", "public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;", "public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;", "Default constructor that uses java.util.regex.Pattern", "with its default flags.", "public JavaUtilRegexCapabilities()  {", "this.flags = 0;", "}", "Constructor that allows for the modification of the flags that", "the java.util.regex.Pattern will use to compile the regular expression.", "This gives the user the ability to fine-tune how the regular expression", "to match the functionlity that they need.", "The {@link java.util.regex.Pattern Pattern} class supports specifying", "these fields via the regular expression text itself, but this gives the caller", "another option to modify the behavior.", "Useful in cases where the regular expression text", "cannot be modified, or if doing so is undesired.", "@flags The flags that are ORed together.", "public JavaUtilRegexCapabilities(int flags) {", "this.flags = flags;", "}", "public void compile(String pattern) {", "this.pattern = Pattern.compile(pattern, this.flags);", "}", "For the JakartaRegexpCapabilties.java, the following is changed:", "private int flags = RE.MATCH_NORMAL;", "Flag to specify normal, case-sensitive matching behaviour.", "This is the default.", "public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;", "Flag to specify that matching should be case-independent (folded)", "public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;", "Contructs a RegexCapabilities with the default MATCH_NORMAL match style.", "public JakartaRegexpCapabilities() {}", "Constructs a RegexCapabilities with the provided match flags.", "Multiple flags should be ORed together.", "@param flags The matching style", "public JakartaRegexpCapabilities(int flags)", "{", "this.flags = flags;", "}", "public void compile(String pattern) {", "regexp = new RE(pattern, this.flags);", "}", "Adding patch file with complete changes.", "All tests passed and included additional test cases to test new functionality.", "What is the status of having this patch reviewed and commited to the next 2.x release?", "Incorrectly added the \"Fixed Version\" in the last update.", "Patch looks good Marc; I plan to commit shortly.", "I think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).", "Thanks Marc!", "Thanks Michael.", "Look forward to getting my hands on 2.9 when its released."], "SplitGT": [" Constructor that allows for the modification of the flags that the java.util.regex.Pattern will use to compile the regular expression.", "This gives the user the ability to fine-tune how the regular expression to match the functionlity that they need.", "The java.util.regex.Pattern Pattern class supports specifying these fields via the regular expression text itself, but this gives the caller another option to modify the behavior.", "Useful in cases where the regular expression text cannot be modified, or if doing so is undesired."], "issueString": "Add ability to specify compilation/matching flags to RegexCapabiltiies implementations\nThe Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. \n\nI've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.\n\nProposed changes:\n\nFor the JavaUtilRegexCapabilities.java, the following is the changes made.\n\n  private int flags = 0;\n  \n  // Define the optional flags from Pattern that can be used.\n  // Do this here to keep Pattern contained within this class.\n  \n  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;\n  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;\n  public final int FLAG_COMMENTS = Pattern.COMMENTS;\n  public final int FLAG_DOTALL = Pattern.DOTALL;\n  public final int FLAG_LITERAL = Pattern.LITERAL;\n  public final int FLAG_MULTILINE = Pattern.MULTILINE;\n  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;\n  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;\n  \n  /**\n   * Default constructor that uses java.util.regex.Pattern \n   * with its default flags.\n   */\n  public JavaUtilRegexCapabilities()  {\n    this.flags = 0;\n  }\n  \n  /**\n   * Constructor that allows for the modification of the flags that\n   * the java.util.regex.Pattern will use to compile the regular expression.\n   * This gives the user the ability to fine-tune how the regular expression \n   * to match the functionlity that they need. \n   * The {@link java.util.regex.Pattern Pattern} class supports specifying \n   * these fields via the regular expression text itself, but this gives the caller\n   * another option to modify the behavior. Useful in cases where the regular expression text\n   * cannot be modified, or if doing so is undesired.\n   * \n   * @flags The flags that are ORed together.\n   */\n  public JavaUtilRegexCapabilities(int flags) {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    this.pattern = Pattern.compile(pattern, this.flags);\n  }\n\n\nFor the JakartaRegexpCapabilties.java, the following is changed:\n\n  private int flags = RE.MATCH_NORMAL;\n\n  /**\n   * Flag to specify normal, case-sensitive matching behaviour. This is the default.\n   */\n  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;\n  \n  /**\n   * Flag to specify that matching should be case-independent (folded)\n   */\n  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;\n \n  /**\n   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.\n   */\n  public JakartaRegexpCapabilities() {}\n  \n  /**\n   * Constructs a RegexCapabilities with the provided match flags.\n   * Multiple flags should be ORed together.\n   * \n   * @param flags The matching style\n   */\n  public JakartaRegexpCapabilities(int flags)\n  {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    regexp = new RE(pattern, this.flags);\n  }\n\nAdding patch file with complete changes. All tests passed and included additional test cases to test new functionality.\nWhat is the status of having this patch reviewed and commited to the next 2.x release?\nIncorrectly added the \"Fixed Version\" in the last update.\nPatch looks good Marc; I plan to commit shortly.\n\nI think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).\nThanks Marc!\nThanks Michael. Look forward to getting my hands on 2.9 when its released.\n", "issueSearchSentences": ["public JavaUtilRegexCapabilities(int flags) {", "public JakartaRegexpCapabilities(int flags)", "Constructs a RegexCapabilities with the provided match flags.", "For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields.", "public JavaUtilRegexCapabilities()  {"], "issueSearchScores": [0.9741030335426331, 0.9124295115470886, 0.7311785221099854, 0.6931700706481934, 0.6814870834350586]}
{"aId": 44, "code": "public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }", "comment": " This can be used to feed the highlighter with a pre-parsed token stream.", "issueId": "LUCENE-4479", "issueStringList": ["TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets", "The javadocs for TokenSources.getTokenStream(Terms, boolean) state:", "\"Low level api.", "Returns a token stream or null if no offset info available", "in index.", "This can be used to feed the highlighter with a pre-parsed token", "stream\"", "However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null.", "This has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets.", "All highlighting markup is prepended to the beginning of the field.", "Patch with a test illustrating the problem", "{quote}", "However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null.", "{quote}", "Actually thats not true: you will get an IllegalArgumentException because OffsetAttribute won't allow offsets to be set to negative values (and the DocsAndPositionsEnum will return -1).", "So something else is going on here!", "but the behavior of throwing IAE is still bogus, as it disagrees with the javadocs...", "I just feel like there is another bug involved too.", "The javadocs are also wrong, because looking at the code logic it really needs positions AND offsets.", "Ah, but TokenStreamFromTermPositionVector checks if offsets are there, and works round it if they're not.", "Which is presumably also a bug?", "So this snippet:", "if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {", "return new TokenStreamFromTermPositionVector(tpv);", "}", "should presumably also check for offsets, and then the extra hasOffsets check in TSFTPV can be removed.", "I'm not sure whats going on here :) If you can sort it out it would be fantastic.", "In general if the method requires that the Terms object has offsets or positions the checks should just look like:", "if (tpv.hasOffsets())", "...", "if (tpv.hasPositions())", "...", "But I'm really confused what the contracts of these methods should be.", "definitely bugs :)", "It looks as though these classes are only used by the DefaultSolrHighlighter, which is broken under these circumstances.", "I'll make those changes, and see if any tests fail...", "Patch, changing the API slightly:", "getTokenStream(Terms, bool) now throws IllegalArgumentException if the Terms does not have offsets.", "renames getTokenStream(IndexReader, int, String) to getTokenStreamWithOffsets.", "Not sure I like the name...  Also added some javadocs here.", "Instead of throwing IllegalArgumentExceptions, this just returns null if it can't build a tokenstream for whatever reason (no termvector, no offsets or positions).", "The only API consumer here is DefaultSolrHighlighter, which I've edited accordingly.", "I also added a test to Solr to ensure that fields with termvectors and positions but no offsets are correctly highlighted.", "All tests pass", "Alan this patch looks good.", "Thanks for tackling this.", "Do you know off-hand if we have a test anywhere for the case of offsets but NO positions (which there is logic to handle, and looks correct, I'm just curious).", "TokenSourcesTest.testOverlapWithOffset() tests it implicitly, although it would be nice to set the FieldType explicitly to document it I suppose.", "you're right: thanks!", "New patch, with CHANGES entry.", "Could someone cast a quick eye over it, and I'll commit.", "with the current patch, 'ant precommit' fails:", "{noformat}", "...", "ecj-javadoc-lint-src:", "[ecj-lint] Compiling 45 source files", "[ecj-lint] ----------", "[ecj-lint] 1.", "ERROR in /Users/rmuir/workspace/lucene-trunk/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java (at line 275)", "[ecj-lint] \t* @throws IOException", "[ecj-lint] \t          ^^^^^^^^^^^", "[ecj-lint] Javadoc: Description expected after this reference", "[ecj-lint] ----------", "[ecj-lint] 1 problem (1 error)", "BUILD FAILED", "{noformat}", "Patch, fixing javadocs errors.", "Ant precommit runs successfully.", "trunk: 1400504", "branch_4x: 1400505"], "SplitGT": [" This can be used to feed the highlighter with a pre-parsed token stream."], "issueString": "TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets\nThe javadocs for TokenSources.getTokenStream(Terms, boolean) state:\n\n\"Low level api. Returns a token stream or null if no offset info available\nin index. This can be used to feed the highlighter with a pre-parsed token\nstream\"\n\nHowever, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null.\n\nThis has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets.  All highlighting markup is prepended to the beginning of the field.\nPatch with a test illustrating the problem\n{quote}\nHowever, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null.\n{quote}\n\nActually thats not true: you will get an IllegalArgumentException because OffsetAttribute won't allow offsets to be set to negative values (and the DocsAndPositionsEnum will return -1). \n\nSo something else is going on here!\nbut the behavior of throwing IAE is still bogus, as it disagrees with the javadocs... I just feel like there is another bug involved too.\n\nThe javadocs are also wrong, because looking at the code logic it really needs positions AND offsets.\nAh, but TokenStreamFromTermPositionVector checks if offsets are there, and works round it if they're not.  Which is presumably also a bug?\nSo this snippet:\n\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\nshould presumably also check for offsets, and then the extra hasOffsets check in TSFTPV can be removed.\nI'm not sure whats going on here :) If you can sort it out it would be fantastic.\n\nIn general if the method requires that the Terms object has offsets or positions the checks should just look like:\nif (tpv.hasOffsets())\n...\nif (tpv.hasPositions())\n...\n\nBut I'm really confused what the contracts of these methods should be. definitely bugs :)\n\nIt looks as though these classes are only used by the DefaultSolrHighlighter, which is broken under these circumstances.  I'll make those changes, and see if any tests fail...\nPatch, changing the API slightly:\n\n- getTokenStream(Terms, bool) now throws IllegalArgumentException if the Terms does not have offsets.\n\n- renames getTokenStream(IndexReader, int, String) to getTokenStreamWithOffsets.  Not sure I like the name...  Also added some javadocs here.  Instead of throwing IllegalArgumentExceptions, this just returns null if it can't build a tokenstream for whatever reason (no termvector, no offsets or positions).\n\nThe only API consumer here is DefaultSolrHighlighter, which I've edited accordingly.  I also added a test to Solr to ensure that fields with termvectors and positions but no offsets are correctly highlighted.\n\nAll tests pass\nAlan this patch looks good. Thanks for tackling this.\n\nDo you know off-hand if we have a test anywhere for the case of offsets but NO positions (which there is logic to handle, and looks correct, I'm just curious).\n\nTokenSourcesTest.testOverlapWithOffset() tests it implicitly, although it would be nice to set the FieldType explicitly to document it I suppose.\nyou're right: thanks!\nNew patch, with CHANGES entry.  Could someone cast a quick eye over it, and I'll commit.\nwith the current patch, 'ant precommit' fails:\n{noformat}\n...\n-ecj-javadoc-lint-src:\n [ecj-lint] Compiling 45 source files\n [ecj-lint] ----------\n [ecj-lint] 1. ERROR in /Users/rmuir/workspace/lucene-trunk/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java (at line 275)\n [ecj-lint] \t* @throws IOException\n [ecj-lint] \t          ^^^^^^^^^^^\n [ecj-lint] Javadoc: Description expected after this reference\n [ecj-lint] ----------\n [ecj-lint] 1 problem (1 error)\n\nBUILD FAILED\n{noformat}\nPatch, fixing javadocs errors.  Ant precommit runs successfully.\ntrunk: 1400504\nbranch_4x: 1400505\n", "issueSearchSentences": ["return new TokenStreamFromTermPositionVector(tpv);", "getTokenStream(Terms, bool) now throws IllegalArgumentException if the Terms does not have offsets.", "TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets", "However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null.", "However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null."], "issueSearchScores": [0.7545266151428223, 0.7536539435386658, 0.7219336032867432, 0.7203128337860107, 0.7203128337860107]}
{"aId": 46, "code": "public HunspellStemmer(HunspellDictionary dictionary) {\n    this(dictionary, 2);\n  }", "comment": " Uses the default recursion cap of 2 (based on Hunspell documentation).", "issueId": "LUCENE-4542", "issueStringList": ["Make RECURSION_CAP in HunspellStemmer configurable", "Currently there is", "private static final int RECURSION_CAP = 2;", "in the code of the class HunspellStemmer.", "It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex.", "it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1).", "It would be nice to be able to tune this number as needed.", "AFAIK this number (2) was chosen arbitrary.", "(it's a first issue in my life, so please forgive me any mistakes done).", "+1 I absolutely agree we need to make this change.", "There is another issue (I can't remember what just yet and I'm using a bad connection) where the recursion cap was causing analysis loops.", "Do you want to create a patch?", "We need to maintain backwards compatibility so the default experience should be using RECURSION_CAP as it is today.", "However users should be able to pass in a value as well (that includes the HunspellStemFilterFactory).", "I'd prefer not to create a patch myself, I don't feel so comfortable with lucene code.", "As Piotr doesn't want to provide the patch, I'll do it for him :) Simple patch adding a new constructor that allows to pass additional parameter - the recursion cap.", "The old constructor is there and the default value for recursion cap is 2.", "Chris I've attached a second patch which includes changes to Solr HunspellFilter and its factory.", "Please review it and say if you want any changes to be made to it.", "I'll be glad to do it.", "Rafa\u0142,", "Thanks for creating the patches, they are looking great.", "Couple of very small improvements:", "Can we mark recursionCap as final?", "Can we improve the javadoc for the recursionCap parameter so it's clear what purpose it serves?", "Maybe also drop in a comment at the field about how the recursion cap of 2 is the default value based on documentation about Hunspell (as opposed to something we arbitrarily chose).", "Attached the patch with improved javadocs and recursionCap marked as final in the HunspellStemmer.", "Chris anything else should be done here in your opinion or is it ready to be committed ?", "It looks great, thanks!", "I'll take care of it soon.", "+1 on having this merged into Lucene", "+1 from me also.", "[~cmale] do we need updates on the patches or they are OK?", "I think it would be nice to have this merged so we can try and fix https://issues.apache.org/jira/browse/LUCENE-4311 as well (this ticket seems to be related).", "Committed, thanks!", "Reopening to backport to 4.4, based on conversation with Adrien on #lucene-dev IRC."], "SplitGT": [" Uses the default recursion cap of 2 (based on Hunspell documentation)."], "issueString": "Make RECURSION_CAP in HunspellStemmer configurable\nCurrently there is \nprivate static final int RECURSION_CAP = 2;\nin the code of the class HunspellStemmer. It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex. it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1). It would be nice to be able to tune this number as needed.\nAFAIK this number (2) was chosen arbitrary.\n\n(it's a first issue in my life, so please forgive me any mistakes done).\n+1 I absolutely agree we need to make this change.  There is another issue (I can't remember what just yet and I'm using a bad connection) where the recursion cap was causing analysis loops.  \n\nDo you want to create a patch? We need to maintain backwards compatibility so the default experience should be using RECURSION_CAP as it is today.  However users should be able to pass in a value as well (that includes the HunspellStemFilterFactory).\nI'd prefer not to create a patch myself, I don't feel so comfortable with lucene code. \nAs Piotr doesn't want to provide the patch, I'll do it for him :) Simple patch adding a new constructor that allows to pass additional parameter - the recursion cap. The old constructor is there and the default value for recursion cap is 2. \nChris I've attached a second patch which includes changes to Solr HunspellFilter and its factory. Please review it and say if you want any changes to be made to it. I'll be glad to do it.\nRafa\u0142,\n\nThanks for creating the patches, they are looking great.  Couple of very small improvements:\n\n- Can we mark recursionCap as final?\n- Can we improve the javadoc for the recursionCap parameter so it's clear what purpose it serves?\n- Maybe also drop in a comment at the field about how the recursion cap of 2 is the default value based on documentation about Hunspell (as opposed to something we arbitrarily chose).\nAttached the patch with improved javadocs and recursionCap marked as final in the HunspellStemmer.\nChris anything else should be done here in your opinion or is it ready to be committed ?\nIt looks great, thanks! I'll take care of it soon.\n+1 on having this merged into Lucene\n+1 from me also. [~cmale] do we need updates on the patches or they are OK?\nI think it would be nice to have this merged so we can try and fix https://issues.apache.org/jira/browse/LUCENE-4311 as well (this ticket seems to be related).\nCommitted, thanks!\nReopening to backport to 4.4, based on conversation with Adrien on #lucene-dev IRC.\n", "issueSearchSentences": ["in the code of the class HunspellStemmer.", "It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex.", "However users should be able to pass in a value as well (that includes the HunspellStemFilterFactory).", "Maybe also drop in a comment at the field about how the recursion cap of 2 is the default value based on documentation about Hunspell (as opposed to something we arbitrarily chose).", "Chris I've attached a second patch which includes changes to Solr HunspellFilter and its factory."], "issueSearchScores": [0.6871576309204102, 0.6186583638191223, 0.3952879309654236, 0.34810012578964233, 0.3466219902038574]}
{"aId": 49, "code": "final Collection<String> createCompoundFile(String fileName, final SegmentInfo info)\n          throws IOException {\n\n    // Now merge all added files\n    Collection<String> files = info.files();\n    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, fileName, checkAbort);\n    for (String file : files) {\n      assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) \n                : \".del file is not allowed in .cfs: \" + file;\n      assert !file.substring(file.lastIndexOf('.') + 1).startsWith(IndexFileNames.SEPARATE_NORMS_EXTENSION) \n                : \"separate norms file (.s*) is not allowed in .cfs: \" + file;\n      cfsWriter.addFile(file);\n    }\n\n    // Perform the merge\n    cfsWriter.close();\n\n    return files;\n  }", "comment": " NOTE: this method creates a compound file for all files returned by info.files. While, generally, this may include separate norms and deletion files, this SegmentInfo must not reference such files when this method is called, because they are not allowed within a compound file.", "issueId": "LUCENE-3143", "issueStringList": ["SegmentMerger should assert .del and .s* files are not passed to createCompoundFile", "Spinoff from LUCENE-3126.", "SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms).", "Today, that method is called from code which ensures that, but we should:", "# Add some documentation to clarify that.", "# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.", "Will post a patch soon", "Patch against 3x, adds asserts to SM + test to ensure they are not allowed.", "All core tests pass.", "Looks good!", "Maybe sharpen the comment to something like: this method creates a compound file for all files returned by info.files().", "While, generally, this may include separate norms and deletion files, this SegmentInfo must not reference such files when this method is called, because they are not allowed within a compound file.", "Something like that...?", "bq.", "Maybe sharpen the comment to something like", "Thanks Mike.", "Changed as you suggest.", "Committed revision 1127601 (3x).", "Committed revision 1127602 (trunk)."], "SplitGT": [" NOTE: this method creates a compound file for all files returned by info.files.", "While, generally, this may include separate norms and deletion files, this SegmentInfo must not reference such files when this method is called, because they are not allowed within a compound file."], "issueString": "SegmentMerger should assert .del and .s* files are not passed to createCompoundFile\nSpinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:\n# Add some documentation to clarify that.\n# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.\n\nWill post a patch soon\nPatch against 3x, adds asserts to SM + test to ensure they are not allowed. All core tests pass.\nLooks good!\n\nMaybe sharpen the comment to something like: this method creates a compound file for all files returned by info.files().  While, generally, this may include separate norms and deletion files, this SegmentInfo must not reference such files when this method is called, because they are not allowed within a compound file.  Something like that...?\nbq. Maybe sharpen the comment to something like\n\nThanks Mike. Changed as you suggest.\n\nCommitted revision 1127601 (3x).\nCommitted revision 1127602 (trunk).\n", "issueSearchSentences": ["SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms).", "SegmentMerger should assert .del and .s* files are not passed to createCompoundFile", "While, generally, this may include separate norms and deletion files, this SegmentInfo must not reference such files when this method is called, because they are not allowed within a compound file.", "Maybe sharpen the comment to something like: this method creates a compound file for all files returned by info.files().", "Today, that method is called from code which ensures that, but we should:"], "issueSearchScores": [0.6439003944396973, 0.5923044681549072, 0.546790361404419, 0.5275615453720093, 0.27344200015068054]}
{"aId": 50, "code": "public String normalizeNumber(String number) {\n    try {\n      BigDecimal normalizedNumber = parseNumber(new NumberBuffer(number));\n      if (normalizedNumber == null) {\n        return number;\n      }\n      return normalizedNumber.stripTrailingZeros().toPlainString();\n    } catch (NumberFormatException | ArithmeticException e) {\n      // Return the source number in case of error, i.e. malformed input\n      return number;\n    }\n  }", "comment": " Normalizes a Korean number", "issueId": "LUCENE-8812", "issueStringList": ["add KoreanNumberFilter to Nori(Korean) Analyzer", "This is a follow-up issue to LUCENE-8784.", "The KoreanNumberFilter is a TokenFilter that normalizes Korean numbers to regular Arabic decimal numbers in half-width characters.", "Logic is similar to JapaneseNumberFilter.", "It should be able to cover the following test cases.", "1) Korean Word to Number", "\uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31 => 102500", "2) 1 character conversion", "\uc77c\uc601\uc601\uc601 => 1000", "3) Decimal Point Calculation", "3.2\ucc9c => 3200", "4) Comma between three digits", "4,647.0010 => 4647.001", "Hi [~jim.ferenczi] :D", "Thank you for applying the LUCENE-8784 patch!", "I checked that this patch can be applied into latest code.", "(no conflict)", "What do you think about this patch?", "The patch looks good [~danmuzi], I wonder if it would be difficult to have a base class for the Japanese and Korean number filter since they share a large amount of code.", "However I think it's ok to merge this first and we can tackle the merge in a follow up, wdyt ?", "Thank you for your reply, [~jim.ferenczi] :D", "{quote}I wonder if it would be difficult to have a base class for the Japanese and Korean number filter since they share a large amount of code.", "However I think it's ok to merge this first and we can tackle the merge in a follow up, wdyt ?", "{quote}", "I think it is an awesome refactoring.", "If the refactoring is done, we can also share this TokenFilter in SmartChineseAnalyzer.", "(Chinese and Japanese use the same numeric characters)", "The amount of code will also be reduced.", "I think the NumberFilter (new abstract class) can be in the org.apache.lucene.analysis.core(analysis-common) or org.apache.lucene.analysis(lucene-core) package, what do you think?", "In my personal opinion, analysis-common seems to be correct, but it is a little bit ambiguous.", "Sorry I didn't see your reply.", "I agree with you that it is ambiguous to put it in analysis-common so +1 to add it in the nori module for now and revisit if/when we create a separate module for the mecab tokenizer.", "Thank you for your reply.", "[~jim.ferenczi] :D", "Awesome.", "I'll submit this patch.", "This is the first time that I submit a patch manually.", "I will look for the manual and proceed, but it can take a little time.", "Hi [~jim.ferenczi] :D", "I pushed my commit to *branch_8x* and *master* branch.", "I checked and it seems to be reflected fine.", "So I'll resolve this issue.", "Let me know if there are some problems.", "I re-opened this issue because the error was occurred in branch_8x branch.", "([https://jenkins.thetaphi.de/job/Lucene-Solr-8.x-Windows/298/])", "After the error was found, the commit to fix the error was pushed.", "The cause of the error was that when I wrote the test(TestKoreanNumberFilter),", "I used Java 9 try-with-resources style, and the error was occurred in branch_8x because it is based on Java 8.", "So I disable it including the master branch, and when the 9.0 version is officially released, I will rework it at that time.", "And I made a slight mistake.", "I did not mention the issue number when committing to the branch_8x branch.", "I wanted to change the commit message, but it could not be changed because it is a protected branch.", "(force push is blocked)", "So I was forced to revert.", "Anyway, both problems(error + wrong commit message) are all my fault, and I will be careful.", "I will resolve this issue when the Jenkins build is completed."], "SplitGT": [" Normalizes a Korean number"], "issueString": "add KoreanNumberFilter to Nori(Korean) Analyzer\nThis is a follow-up issue to LUCENE-8784.\r\n\r\nThe KoreanNumberFilter is a TokenFilter that normalizes Korean numbers to regular Arabic decimal numbers in half-width characters.\r\n\r\nLogic is similar to JapaneseNumberFilter.\r\nIt should be able to cover the following test cases.\r\n\r\n1) Korean Word to Number\r\n\uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31 => 102500\r\n\r\n2) 1 character conversion\r\n\uc77c\uc601\uc601\uc601 => 1000\r\n\r\n3) Decimal Point Calculation\r\n3.2\ucc9c => 3200\r\n\r\n4) Comma between three digits\r\n4,647.0010 => 4647.001\nHi [~jim.ferenczi] :D\r\nThank you for applying the LUCENE-8784 patch!\r\nI checked that this patch can be applied into latest code. (no conflict)\r\n\r\nWhat do you think about this patch?\nThe patch looks good [~danmuzi], I wonder if it would be difficult to have a base class for the Japanese and Korean number filter since they share a large amount of code. However I think it's ok to merge this first and we can tackle the merge in a follow up, wdyt ?\nThank you for your reply, [~jim.ferenczi] :D\r\n{quote}I wonder if it would be difficult to have a base class for the Japanese and Korean number filter since they share a large amount of code. However I think it's ok to merge this first and we can tackle the merge in a follow up, wdyt ?\r\n{quote}\r\nI think it is an awesome refactoring.\r\n If the refactoring is done, we can also share this TokenFilter in SmartChineseAnalyzer. (Chinese and Japanese use the same numeric characters)\r\n The amount of code will also be reduced.\r\n\r\nI think the NumberFilter (new abstract class) can be in the org.apache.lucene.analysis.core(analysis-common) or org.apache.lucene.analysis(lucene-core) package, what do you think?\r\n In my personal opinion, analysis-common seems to be correct, but it is a little bit ambiguous.\nSorry I didn't see your reply. I agree with you that it is ambiguous to put it in analysis-common so +1 to add it in the nori module for now and revisit if/when we create a separate module for the mecab tokenizer.\u00a0\nThank you for your reply. [~jim.ferenczi] :D\r\n\r\nAwesome. I'll submit this patch.\r\n\r\nThis is the first time that I submit a patch manually.\r\nI will look for the manual and proceed, but it can take a little time.\nHi [~jim.ferenczi] :D\r\n\r\nI pushed my commit to *branch_8x* and *master* branch.\r\n I checked and it seems to be reflected fine.\r\n So I'll resolve this issue.\r\nLet me know if there are some problems.\nI re-opened this issue because the error was occurred in branch_8x branch.\r\n ([https://jenkins.thetaphi.de/job/Lucene-Solr-8.x-Windows/298/])\r\n After the error was found, the commit to fix the error was pushed.\r\n The cause of the error was that when I wrote the test(TestKoreanNumberFilter),\r\n I used Java 9 try-with-resources style, and the error was occurred in branch_8x because it is based on Java 8.\r\n So I disable it including the master branch, and when the 9.0 version is officially released, I will rework it at that time.\r\n\r\nAnd I made a slight mistake.\r\n I did not mention the issue number when committing to the branch_8x branch.\r\n I wanted to change the commit message, but it could not be changed because it is a protected branch. (force push is blocked)\r\n So I was forced to revert.\r\n\r\nAnyway, both problems(error + wrong commit message) are all my fault, and I will be careful.\r\n\r\nI will resolve this issue when the Jenkins build is completed.\n", "issueSearchSentences": ["The KoreanNumberFilter is a TokenFilter that normalizes Korean numbers to regular Arabic decimal numbers in half-width characters.", "1) Korean Word to Number", "Logic is similar to JapaneseNumberFilter.", "The cause of the error was that when I wrote the test(TestKoreanNumberFilter),", "(Chinese and Japanese use the same numeric characters)"], "issueSearchScores": [0.5732558965682983, 0.4512033760547638, 0.40815216302871704, 0.4079286456108093, 0.4039919376373291]}
{"aId": 52, "code": "public TopDocs search(Query query, int n)\n    throws IOException {\n    return search(query, null, n);\n  }", "comment": " Finds the top n hits for query.", "issueId": "LUCENE-1371", "issueStringList": ["Add Searcher.search(Query, int)", "Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.", "This way there is a simple API for users to retrieve the top N results for a Query."], "SplitGT": [" Finds the top n hits for query."], "issueString": "Add Searcher.search(Query, int)\nNow that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.\n\nThis way there is a simple API for users to retrieve the top N results for a Query.\n\n", "issueSearchSentences": ["This way there is a simple API for users to retrieve the top N results for a Query.", "Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.", "Add Searcher.search(Query, int)"], "issueSearchScores": [0.6226487755775452, 0.5722543001174927, 0.5010186433792114]}
{"aId": 54, "code": "public static boolean getAllowDocsOutOfOrder() {\n    return allowDocsOutOfOrder;\n  }", "comment": " Whether hit docs may be collected out of docid order.", "issueId": "LUCENE-730", "issueStringList": ["Restore top level disjunction performance", "This patch restores the performance of top level disjunctions.", "The introduction of BooleanScorer2 had impacted this as reported", "on java-user on 21 Nov 2006 by Stanislav Jordanov.", "This patches BooleanScorer2 to use BooleanScorer in the score(HitCollector) method.", "This also patches BooleanScorer to accept a minimum number of optional matchers.", "The patch also disables some test code: the use of checkSkipTo in QueryUtils", "caused a test failure in TestBoolean2 with the above changes.", "I think this", "could be expected because of the changed document scoring order", "for top level disjunction queries.", "At the moment I don't know how to resolve this.", "With the complete patch, all tests pass here.", "Just a quick note that I contacted Stanislav Jordanov about Paul's patch here.", "Stanislav only used BooleanScorer.setUseScorer14() and that restored performance for him, but he did not try this patch (and won't be doing that as he's not working with Lucene at the moment).", "Paul, what is special about the number 32 here (BooleanScorer2):", "+    if ((requiredScorers.size() == 0) &&", "+        prohibitedScorers.size() < 32) {", "+      // fall back to BooleanScorer, scores documents somewhat out of order", "+      BooleanScorer bs = new BooleanScorer(getSimilarity(), minNrShouldMatch);", "Why can we use BooleanScorer if there are less than 32 prohibited clauses, but not otherwise?", "Thanks.", "32 is the max number of required + prohibited clauses in the orig BooleanScorer (because it uses an int as a bitfield for each document in the current id range being considered).", "Further to Yonik's answer, I have not done any tests with prohibited scorers comparing BooleanScorer and BooleanScorer2.", "It is quite possible that using skipTo() on any prohibited scorer (via BooleanScorer2) is generally faster than using BooleanScorer.", "Prohibited clauses in queries are quite seldom, so it is going to be difficult to find out whether a smaller value than 32 would be generally optimal.", "I've committed this (changed a few minor things in the patch)...without benchmarking BS vs. BS2 with < 32 prohibited clauses.", "Hm, if I exposed that 32 as a static setter method, then one could easily benchmark and compare BS vs. BS2 with Doron's contrib/benchmark.", "As discussed on java-dev the default behavior of BooleanScorer should be to return the documents in order, because there are people who rely in their apps on that.", "Docs out of order should only be allowed if BooleanQuery.setUseScorer14(true) is set explicitly.", "With this patch the old BooleanScorer is only used if BooleanQuery.setUseScorer14(true) is set.", "It also enables the tests in QueryUtils again that check if the docs are returned in order.", "All tests pass.", "The patch applies cleanly here, all core tests pass.", "And I like the allowDocsOutOfOrder approach.", "Thanks for reviewing, Paul!", "I will commit this soon if nobody objects...", "No objection, only some remarks.", "One bigger issue:", "The latest patch defaults to docs in order above performance,", "but my personal taste is to have performance by default.", "And some smaller ones:", "One could still adapt QueryUtills to take the possibility", "of docs out of order into account.", "Some performance tests with prohibited scorers could still", "be needed to find out which of the boolean scorers does better", "on them.", "Two comments:", "With this patch the class BooleanWeight is not", "in (direct) use anymore - it is extended by", "BooleanWeight2 and then only the latter is used,", "and creates either Scorer2 or Scorer.", "We could", "get rid of BolleanWeight2, and have a single", "class BooleanWeight.", "Javadocs for useScorer14 methods:", "Indicates that 1.4 BooleanScorer should be used.", "Being static, This setting is system wide.", "Scoring in 1.4 mode may be faster.", "But note that unlike the default behavior, it does", "not guarantee that docs are collected in docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "public static void setUseScorer14(boolean use14) {", "Whether 1.4 BooleanScorer should be used.", "@see #setUseScorer14(boolean)", "public static boolean getUseScorer14() {", "> The latest patch defaults to docs in order above performance,", "> but my personal taste is to have performance by default.", "I think it makes more sense to \"default\" to the most consistent rigidly defined behavior (docs in order), since that behavior will work (by definition) for any caller regardless of whether the caller expects the docs in order or not.", "people who find performance lacking can then assess their needs and make a conscious choice to change the setting, and see if it actually improves performance in their use cases.", "(ie: \"avoid premature optimization\" and all that)", "> The latest patch defaults to docs in order above performance,", "> but my personal taste is to have performance by default.", "I agree with Hoss here.", "IMO allowing docs out of order is a big", "API change.", "I think if people switch to 2.2 they just want it", "to work as before without having to add special settings.", "If", "they need better performance for certain types of queries and", "they know that their application can deal with docs out of order", "they can enable the faster scoring.", "So my vote is +1 for docs in order by default.", "> Some performance tests with prohibited scorers could still", "> be needed to find out which of the boolean scorers does better", "> on them.", "That'd be helpful.", "However, I'm currently working on some other", "issues.", "Maybe you or others would have some time to run those", "tests?", "> With this patch the class BooleanWeight is not", "> in (direct) use anymore - it is extended by", "> BooleanWeight2 and then only the latter is used,", "> and creates either Scorer2 or Scorer.", "We could", "> get rid of BolleanWeight2, and have a single", "> class BooleanWeight.", "Agree.", "Will do.", "> Javadocs for useScorer14 methods:", "This is good!", "Thanks Doron, I will add the javadocs", "to my patch.", "New patch with the following changes:", "Removes BooleanWeight2", "Javadocs for useScorer14 methods provided by Doron", "(Is the patch reversed?", "It did not apply at the first attempt,", "probably because my working copy is not the same as the trunk.)", "After ant clean, the boolean tests still pass here:", "ant -Dtestcase='TestBool*' test-core", "A slight improvement for the javadocs of BooleanQuery.java.", "In the javadocs of the scorer() method it is indicated that a BooleanScorer2", "will always be used, so it is better to mention here that BooleanScorer2", "delegates to a 1.4 scorer in some cases:", "Indicates that BooleanScorer2 will delegate", "the scoring to a 1.4 BooleanScorer", "for most queries without required clauses.", "Being static, this setting is system wide.", "Scoring in 1.4 mode may be faster.", "But note that unlike the default behavior, it does", "not guarantee that docs are collected in docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "public static void setUseScorer14(boolean use14) {", "useScorer14 = use14;", "}", "> A slight improvement for the javadocs of BooleanQuery.java.", "> In the javadocs of the scorer() method it is indicated that a BooleanScorer2", "> will always be used, so it is better to mention here that BooleanScorer2", "> delegates to a 1.4 scorer in some cases:", "Maybe we should just deprecate the useScorer14 methods and add new methods", "allowDocsOutOfOrder.", "That should be easier to understand for the users.", "And probably most users don't know (or don't care about) the differences", "between BooleanScorer and BooleanScorer2 anyway.", "New patch that deprecates the useScorer14 methods and adds new", "methods:", "Indicates whether hit docs may be collected out of docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "Being static, this setting is system wide.", "If docs out of order are allowed scoring might be faster", "for certain queries (disjunction queries with less than", "32 prohibited terms).", "This setting has no effect for", "other queries.", "public static void setAllowDocsOutOfOrder(boolean allow);", "Whether hit docs may be collected out of docid order.", "@see #setAllowDocsOutOfOrder(boolean)", "public static boolean getAllowDocsOutOfOrder();", "I think this is easier to understand for the users because it", "tells them what they need to know (docs in or out of order)", "and hides technical details (BooleanScorer vs. BooleanScorer2).", "All tests pass.", "I just committed the latest patch.", "Thanks everyone!"], "SplitGT": [" Whether hit docs may be collected out of docid order."], "issueString": "Restore top level disjunction performance\nThis patch restores the performance of top level disjunctions. \nThe introduction of BooleanScorer2 had impacted this as reported\non java-user on 21 Nov 2006 by Stanislav Jordanov.\n\nThis patches BooleanScorer2 to use BooleanScorer in the score(HitCollector) method.\nThis also patches BooleanScorer to accept a minimum number of optional matchers.\n\nThe patch also disables some test code: the use of checkSkipTo in QueryUtils\ncaused a test failure in TestBoolean2 with the above changes. I think this\ncould be expected because of the changed document scoring order\nfor top level disjunction queries.\nAt the moment I don't know how to resolve this.\n\nWith the complete patch, all tests pass here.\n\nJust a quick note that I contacted Stanislav Jordanov about Paul's patch here.  Stanislav only used BooleanScorer.setUseScorer14() and that restored performance for him, but he did not try this patch (and won't be doing that as he's not working with Lucene at the moment).\n\nPaul, what is special about the number 32 here (BooleanScorer2):\n\n+    if ((requiredScorers.size() == 0) &&\n+        prohibitedScorers.size() < 32) {\n+      // fall back to BooleanScorer, scores documents somewhat out of order\n+      BooleanScorer bs = new BooleanScorer(getSimilarity(), minNrShouldMatch);\n\nWhy can we use BooleanScorer if there are less than 32 prohibited clauses, but not otherwise?  Thanks.\n\n32 is the max number of required + prohibited clauses in the orig BooleanScorer (because it uses an int as a bitfield for each document in the current id range being considered).\nFurther to Yonik's answer, I have not done any tests with prohibited scorers comparing BooleanScorer and BooleanScorer2.\n\nIt is quite possible that using skipTo() on any prohibited scorer (via BooleanScorer2) is generally faster than using BooleanScorer. Prohibited clauses in queries are quite seldom, so it is going to be difficult to find out whether a smaller value than 32 would be generally optimal.\n\n\n\nI've committed this (changed a few minor things in the patch)...without benchmarking BS vs. BS2 with < 32 prohibited clauses.\n\nHm, if I exposed that 32 as a static setter method, then one could easily benchmark and compare BS vs. BS2 with Doron's contrib/benchmark.\n\nAs discussed on java-dev the default behavior of BooleanScorer should be to return the documents in order, because there are people who rely in their apps on that. Docs out of order should only be allowed if BooleanQuery.setUseScorer14(true) is set explicitly.\nWith this patch the old BooleanScorer is only used if BooleanQuery.setUseScorer14(true) is set. It also enables the tests in QueryUtils again that check if the docs are returned in order.\n\nAll tests pass.\nThe patch applies cleanly here, all core tests pass.\nAnd I like the allowDocsOutOfOrder approach.\n\nThanks for reviewing, Paul!\n\nI will commit this soon if nobody objects...\nNo objection, only some remarks.\n\nOne bigger issue:\n\nThe latest patch defaults to docs in order above performance,\nbut my personal taste is to have performance by default.\n\nAnd some smaller ones:\n\nOne could still adapt QueryUtills to take the possibility\nof docs out of order into account.\n\nSome performance tests with prohibited scorers could still\nbe needed to find out which of the boolean scorers does better\non them.\n\nTwo comments: \n\nWith this patch the class BooleanWeight is not\nin (direct) use anymore - it is extended by \nBooleanWeight2 and then only the latter is used, \nand creates either Scorer2 or Scorer. We could \nget rid of BolleanWeight2, and have a single \nclass BooleanWeight.\n\nJavadocs for useScorer14 methods:\n  /**\n   * Indicates that 1.4 BooleanScorer should be used.\n   * Being static, This setting is system wide.\n   * Scoring in 1.4 mode may be faster.\n   * But note that unlike the default behavior, it does \n   * not guarantee that docs are collected in docid\n   * order. In other words, with this setting, \n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1. \n   */\n  public static void setUseScorer14(boolean use14) {\n\n  /**\n   * Whether 1.4 BooleanScorer should be used.\n   * @see #setUseScorer14(boolean)\n   */\n  public static boolean getUseScorer14() {\n\n\n> The latest patch defaults to docs in order above performance,\n> but my personal taste is to have performance by default.\n\nI think it makes more sense to \"default\" to the most consistent rigidly defined behavior (docs in order), since that behavior will work (by definition) for any caller regardless of whether the caller expects the docs in order or not.\n\npeople who find performance lacking can then assess their needs and make a conscious choice to change the setting, and see if it actually improves performance in their use cases.\n\n(ie: \"avoid premature optimization\" and all that)\n> The latest patch defaults to docs in order above performance,\n> but my personal taste is to have performance by default.\n\nI agree with Hoss here. IMO allowing docs out of order is a big\nAPI change. I think if people switch to 2.2 they just want it\nto work as before without having to add special settings. If \nthey need better performance for certain types of queries and \nthey know that their application can deal with docs out of order\nthey can enable the faster scoring. \nSo my vote is +1 for docs in order by default.\n\n> Some performance tests with prohibited scorers could still\n> be needed to find out which of the boolean scorers does better\n> on them. \n\nThat'd be helpful. However, I'm currently working on some other\nissues. Maybe you or others would have some time to run those\ntests?\n> With this patch the class BooleanWeight is not\n> in (direct) use anymore - it is extended by \n> BooleanWeight2 and then only the latter is used, \n> and creates either Scorer2 or Scorer. We could \n> get rid of BolleanWeight2, and have a single \n> class BooleanWeight.\n\nAgree. Will do.\n\n> Javadocs for useScorer14 methods:\n\nThis is good! Thanks Doron, I will add the javadocs\nto my patch.\nNew patch with the following changes:\n\n- Removes BooleanWeight2\n- Javadocs for useScorer14 methods provided by Doron\n(Is the patch reversed? It did not apply at the first attempt,\nprobably because my working copy is not the same as the trunk.)\nAfter ant clean, the boolean tests still pass here:\nant -Dtestcase='TestBool*' test-core\n\nA slight improvement for the javadocs of BooleanQuery.java.\nIn the javadocs of the scorer() method it is indicated that a BooleanScorer2\nwill always be used, so it is better to mention here that BooleanScorer2\ndelegates to a 1.4 scorer in some cases:\n\n  /**\n   * Indicates that BooleanScorer2 will delegate\n   * the scoring to a 1.4 BooleanScorer\n   * for most queries without required clauses.\n   * Being static, this setting is system wide.\n   * Scoring in 1.4 mode may be faster.\n   * But note that unlike the default behavior, it does\n   * not guarantee that docs are collected in docid\n   * order. In other words, with this setting,\n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1.\n   */\n  public static void setUseScorer14(boolean use14) {\n    useScorer14 = use14;\n  }\n\n> A slight improvement for the javadocs of BooleanQuery.java.\n> In the javadocs of the scorer() method it is indicated that a BooleanScorer2\n> will always be used, so it is better to mention here that BooleanScorer2\n> delegates to a 1.4 scorer in some cases:\n\nMaybe we should just deprecate the useScorer14 methods and add new methods\nallowDocsOutOfOrder. That should be easier to understand for the users. \nAnd probably most users don't know (or don't care about) the differences\nbetween BooleanScorer and BooleanScorer2 anyway.\nNew patch that deprecates the useScorer14 methods and adds new\nmethods:\n\n  /**\n   * Indicates whether hit docs may be collected out of docid\n   * order. In other words, with this setting, \n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1.\n   * Being static, this setting is system wide.\n   * If docs out of order are allowed scoring might be faster\n   * for certain queries (disjunction queries with less than\n   * 32 prohibited terms). This setting has no effect for \n   * other queries.\n   */\n  public static void setAllowDocsOutOfOrder(boolean allow);\n  \n  /**\n   * Whether hit docs may be collected out of docid order.\n   * @see #setAllowDocsOutOfOrder(boolean)\n   */\n  public static boolean getAllowDocsOutOfOrder();\n  \n\nI think this is easier to understand for the users because it \ntells them what they need to know (docs in or out of order) \nand hides technical details (BooleanScorer vs. BooleanScorer2).\n\nAll tests pass.\n\nI just committed the latest patch. Thanks everyone!\n", "issueSearchSentences": ["public static boolean getAllowDocsOutOfOrder();", "And I like the allowDocsOutOfOrder approach.", "allowDocsOutOfOrder.", "@see #setAllowDocsOutOfOrder(boolean)", "public static void setAllowDocsOutOfOrder(boolean allow);"], "issueSearchScores": [0.9378422498703003, 0.7440503239631653, 0.7410639524459839, 0.7369663715362549, 0.7296792268753052]}
{"aId": 57, "code": "public JakartaRegexpCapabilities(int flags)\n  {\n    this.flags = flags;\n  }", "comment": " Constructs a RegexCapabilities with the provided match flags. Multiple flags should be ORed together.", "issueId": "LUCENE-1745", "issueStringList": ["Add ability to specify compilation/matching flags to RegexCapabiltiies implementations", "The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression.", "While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.", "Therefore, this improvement request is to add the capability to provide those modification flags to either implementation.", "I've developed a working implementation that makes minor additions to the existing code.", "The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided.", "This provides complete backwards compatibility.", "For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields.", "These are pass through to the underlying implementation.", "They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.", "Proposed changes:", "For the JavaUtilRegexCapabilities.java, the following is the changes made.", "private int flags = 0;", "Define the optional flags from Pattern that can be used.", "Do this here to keep Pattern contained within this class.", "public final int FLAG_CANON_EQ = Pattern.CANON_EQ;", "public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;", "public final int FLAG_COMMENTS = Pattern.COMMENTS;", "public final int FLAG_DOTALL = Pattern.DOTALL;", "public final int FLAG_LITERAL = Pattern.LITERAL;", "public final int FLAG_MULTILINE = Pattern.MULTILINE;", "public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;", "public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;", "Default constructor that uses java.util.regex.Pattern", "with its default flags.", "public JavaUtilRegexCapabilities()  {", "this.flags = 0;", "}", "Constructor that allows for the modification of the flags that", "the java.util.regex.Pattern will use to compile the regular expression.", "This gives the user the ability to fine-tune how the regular expression", "to match the functionlity that they need.", "The {@link java.util.regex.Pattern Pattern} class supports specifying", "these fields via the regular expression text itself, but this gives the caller", "another option to modify the behavior.", "Useful in cases where the regular expression text", "cannot be modified, or if doing so is undesired.", "@flags The flags that are ORed together.", "public JavaUtilRegexCapabilities(int flags) {", "this.flags = flags;", "}", "public void compile(String pattern) {", "this.pattern = Pattern.compile(pattern, this.flags);", "}", "For the JakartaRegexpCapabilties.java, the following is changed:", "private int flags = RE.MATCH_NORMAL;", "Flag to specify normal, case-sensitive matching behaviour.", "This is the default.", "public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;", "Flag to specify that matching should be case-independent (folded)", "public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;", "Contructs a RegexCapabilities with the default MATCH_NORMAL match style.", "public JakartaRegexpCapabilities() {}", "Constructs a RegexCapabilities with the provided match flags.", "Multiple flags should be ORed together.", "@param flags The matching style", "public JakartaRegexpCapabilities(int flags)", "{", "this.flags = flags;", "}", "public void compile(String pattern) {", "regexp = new RE(pattern, this.flags);", "}", "Adding patch file with complete changes.", "All tests passed and included additional test cases to test new functionality.", "What is the status of having this patch reviewed and commited to the next 2.x release?", "Incorrectly added the \"Fixed Version\" in the last update.", "Patch looks good Marc; I plan to commit shortly.", "I think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).", "Thanks Marc!", "Thanks Michael.", "Look forward to getting my hands on 2.9 when its released."], "SplitGT": [" Constructs a RegexCapabilities with the provided match flags.", "Multiple flags should be ORed together."], "issueString": "Add ability to specify compilation/matching flags to RegexCapabiltiies implementations\nThe Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. \n\nI've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.\n\nProposed changes:\n\nFor the JavaUtilRegexCapabilities.java, the following is the changes made.\n\n  private int flags = 0;\n  \n  // Define the optional flags from Pattern that can be used.\n  // Do this here to keep Pattern contained within this class.\n  \n  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;\n  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;\n  public final int FLAG_COMMENTS = Pattern.COMMENTS;\n  public final int FLAG_DOTALL = Pattern.DOTALL;\n  public final int FLAG_LITERAL = Pattern.LITERAL;\n  public final int FLAG_MULTILINE = Pattern.MULTILINE;\n  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;\n  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;\n  \n  /**\n   * Default constructor that uses java.util.regex.Pattern \n   * with its default flags.\n   */\n  public JavaUtilRegexCapabilities()  {\n    this.flags = 0;\n  }\n  \n  /**\n   * Constructor that allows for the modification of the flags that\n   * the java.util.regex.Pattern will use to compile the regular expression.\n   * This gives the user the ability to fine-tune how the regular expression \n   * to match the functionlity that they need. \n   * The {@link java.util.regex.Pattern Pattern} class supports specifying \n   * these fields via the regular expression text itself, but this gives the caller\n   * another option to modify the behavior. Useful in cases where the regular expression text\n   * cannot be modified, or if doing so is undesired.\n   * \n   * @flags The flags that are ORed together.\n   */\n  public JavaUtilRegexCapabilities(int flags) {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    this.pattern = Pattern.compile(pattern, this.flags);\n  }\n\n\nFor the JakartaRegexpCapabilties.java, the following is changed:\n\n  private int flags = RE.MATCH_NORMAL;\n\n  /**\n   * Flag to specify normal, case-sensitive matching behaviour. This is the default.\n   */\n  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;\n  \n  /**\n   * Flag to specify that matching should be case-independent (folded)\n   */\n  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;\n \n  /**\n   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.\n   */\n  public JakartaRegexpCapabilities() {}\n  \n  /**\n   * Constructs a RegexCapabilities with the provided match flags.\n   * Multiple flags should be ORed together.\n   * \n   * @param flags The matching style\n   */\n  public JakartaRegexpCapabilities(int flags)\n  {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    regexp = new RE(pattern, this.flags);\n  }\n\nAdding patch file with complete changes. All tests passed and included additional test cases to test new functionality.\nWhat is the status of having this patch reviewed and commited to the next 2.x release?\nIncorrectly added the \"Fixed Version\" in the last update.\nPatch looks good Marc; I plan to commit shortly.\n\nI think it's unlikely we'll do a 2.4.2 release, since 2.9 is just around the corner (this will be included in 2.9).\nThanks Marc!\nThanks Michael. Look forward to getting my hands on 2.9 when its released.\n", "issueSearchSentences": ["public JakartaRegexpCapabilities(int flags)", "public JavaUtilRegexCapabilities(int flags) {", "public JakartaRegexpCapabilities() {}", "Constructs a RegexCapabilities with the provided match flags.", "public JavaUtilRegexCapabilities()  {"], "issueSearchScores": [0.9616442918777466, 0.9045867919921875, 0.6839596033096313, 0.664372444152832, 0.6288142204284668]}
{"aId": 63, "code": "public SortedDocValues getSortedDocValues() {\n    return in;\n  }", "comment": " Return the wrapped SortedDocValues", "issueId": "LUCENE-5304", "issueStringList": ["SingletonSortedSetDocValues should allow for getting back the wrapped instance", "This idea was mentioned by Robert on LUCENE-5300", "Some codecs or FieldCache impls use SingletonSortedSetDocValues when a field which is supposed to be multi-valued is actually single-valued.", "By having a getter on this class to get back the wrapped SortedDocValues instance, we could add more specialization (which often already exists, eg.", "Solr's DocValuesFacets already have a specialized impl for SortedDocValues).", "Here is a patch that exposes the SortedDocValues instance in SingletonSortedSetDocValues and makes Solr's DocValuesFacets specialize accumulation when a SortedSetDocValues instance extends SingletonSortedSetDocValues.", "+1, thats exactly what I had in mind.", "Committed.", "Thanks Robert for the review!"], "SplitGT": [" Return the wrapped SortedDocValues"], "issueString": "SingletonSortedSetDocValues should allow for getting back the wrapped instance\nThis idea was mentioned by Robert on LUCENE-5300\n\nSome codecs or FieldCache impls use SingletonSortedSetDocValues when a field which is supposed to be multi-valued is actually single-valued. By having a getter on this class to get back the wrapped SortedDocValues instance, we could add more specialization (which often already exists, eg. Solr's DocValuesFacets already have a specialized impl for SortedDocValues).\n\nHere is a patch that exposes the SortedDocValues instance in SingletonSortedSetDocValues and makes Solr's DocValuesFacets specialize accumulation when a SortedSetDocValues instance extends SingletonSortedSetDocValues.\n+1, thats exactly what I had in mind.\nCommitted. Thanks Robert for the review!\n", "issueSearchSentences": ["Solr's DocValuesFacets already have a specialized impl for SortedDocValues).", "By having a getter on this class to get back the wrapped SortedDocValues instance, we could add more specialization (which often already exists, eg.", "SingletonSortedSetDocValues should allow for getting back the wrapped instance", "Here is a patch that exposes the SortedDocValues instance in SingletonSortedSetDocValues and makes Solr's DocValuesFacets specialize accumulation when a SortedSetDocValues instance extends SingletonSortedSetDocValues.", "Some codecs or FieldCache impls use SingletonSortedSetDocValues when a field which is supposed to be multi-valued is actually single-valued."], "issueSearchScores": [0.689430296421051, 0.6574651598930359, 0.6181566715240479, 0.5919033288955688, 0.5080082416534424]}
{"aId": 64, "code": "public final void close() {\n    files = null;\n  }", "comment": " Closes the store to future operations, releasing associated memory.", "issueId": "LUCENE-623", "issueStringList": ["RAMDirectory.close() should have a comment about not releasing any resources", "I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.", "It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources.", "I propose a trivial patch, which does two very simple things:", "1.", "RAMDirectory.close(), instead of being a no-op, sets files=null.", "This allows the garbage collector to collect all the memory that was used for this RAMDirectory (this is the best we can do in Java).", "2.", "Make the documentation more explicit, from \"Closes the store to future operations.\"", "to \"Closes the store to future operations, releasing associated memory.\"", "Note that now, after a close(), any operations on the RAMDirectory will likely result in a NullPointerException.", "I don't think this matters, as the documentation clearly says that after a close() you're not allowed to operate on this object.", "Patch commited.", "Note that this patch caused an NPE in TestIndexWriterMerging.testLucene, but after reviewing the test I'm of the opinion that it made an invalid assumption about the order of events that was allowed, to the effect of...", "Directory merged = new RAMDirectory();", "...", "merged.close();", "...", "IndexReader reader = IndexReader.open(merged);", "...I modified the test slightly to only close the directory once all uses of it were finished and included it in the commit."], "SplitGT": [" Closes the store to future operations, releasing associated memory."], "issueString": "RAMDirectory.close() should have a comment about not releasing any resources\nI wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.\nIt might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources.\nI propose a trivial patch, which does two very simple things:\n\n1. RAMDirectory.close(), instead of being a no-op, sets files=null. This allows the garbage collector to collect all the memory that was used for this RAMDirectory (this is the best we can do in Java).\n\n2. Make the documentation more explicit, from \"Closes the store to future operations.\" to \"Closes the store to future operations, releasing associated memory.\"\n\nNote that now, after a close(), any operations on the RAMDirectory will likely result in a NullPointerException. I don't think this matters, as the documentation clearly says that after a close() you're not allowed to operate on this object.\n\nPatch commited.  \n\nNote that this patch caused an NPE in TestIndexWriterMerging.testLucene, but after reviewing the test I'm of the opinion that it made an invalid assumption about the order of events that was allowed, to the effect of...\n\n\n    Directory merged = new RAMDirectory();\n    ...\n    merged.close();\n    ...\n    IndexReader reader = IndexReader.open(merged);\n\n...I modified the test slightly to only close the directory once all uses of it were finished and included it in the commit.\n", "issueSearchSentences": ["RAMDirectory.close(), instead of being a no-op, sets files=null.", "RAMDirectory.close() should have a comment about not releasing any resources", "I don't think this matters, as the documentation clearly says that after a close() you're not allowed to operate on this object.", "merged.close();", "I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory."], "issueSearchScores": [0.7468821406364441, 0.6424374580383301, 0.6310443878173828, 0.5744231939315796, 0.5631924867630005]}
{"aId": 69, "code": "public void setSplitOnWhitespace(boolean splitOnWhitespace) {\n    if (splitOnWhitespace == false && getAutoGeneratePhraseQueries() == true) {\n      throw new IllegalArgumentException\n          (\"setSplitOnWhitespace(false) is disallowed when getAutoGeneratePhraseQueries() == true\");\n    }\n    this.splitOnWhitespace = splitOnWhitespace;\n  }", "comment": " The combination splitOnWhitespace=false and autoGeneratePhraseQueries=true is disallowed.", "issueId": "LUCENE-7533", "issueStringList": ["Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false", "LUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis.", "From the javadocs for QueryParser.setAutoGeneratePhraseQueries():", "bq.phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text.", "When splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.", "E.g.", "simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.", "Patch that addresses some of this issue, with some failing tests and nocommits.", "The existing autoGeneratePhraseQueries=true approach generates queries exactly as if the query had contained quotation marks, but as I mentioned above, this is inappropriate when splitOnWhitespace=false and the query text contains spaces.", "The approach in the patch is to add a new QueryBuilder method to handle the autoGeneratePhraseQueries=true case.", "The query text is split on whitespace and these tokens' offsets are compared to those produced by the configured analyzer.", "When multiple non-overlapping tokens have offsets within the bounds of a single whitespace-separated token, a phrase query is created.", "If the original token is present as a token overlapping with the first split token, then a disjunction query is created with the original token and the phrase query of the split tokens.", "I've added a couple of tests that show posincr/poslength/offset output from SynonymFilter and WordDelimiterFilter (likely the two most frequently used analysis components that can create split tokens), and both create corrupt token graphs of various kinds (e.g.", "LUCENE-6582, LUCENE-5051), so solving this problem in a complete way just isn't possible right now.", "So I'm not happy with the approach in the patch.", "It only covers a subset of possible token graphs (e.g.", "more than one overlapping multi-term synonym doesn't work).", "And it's a lot of new code solving a problem that AFAIK no user has reported (does anybody even use autoGeneratePhraseQueries=true with classic QP?", "),", "I'd be much happier if we could somehow get TermAutomatonQuery hooked into the query parsers, and then rewrite to simpler queries if possible: LUCENE-6824.", "First thing though is unbreaking SynonymFilter and friends to produce non-broken token graphs though.", "Attempts to do this for SynonymFilter have stalled though: LUCENE-6664.", "(I have a germ of an idea that might break the logjam - I'll post over there.)", "For this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "Thoughts?", "FYI autoGeneratePhraseQueries was never added to the flexible query parser.", "+1 to move towards having proper graphs come out of analysis, and letting query parsers produce TAQ.", "I agree there is a lot of work there though :)", "Thank you for pointing to LUCENE-6824!", "I think that issue can be committed ... it had fallen past the event horizon of my TODO list.", "I'll revive it ...", "Patch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "This is ready to go.", "I'm going to commit shortly.", "I committed the patch to disallow this combination of options.", "Hopefully once we unbreak graph token streams, this can be revisited."], "SplitGT": [" The combination splitOnWhitespace=false and autoGeneratePhraseQueries=true is disallowed."], "issueString": "Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false\nLUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis.\n\nFrom the javadocs for QueryParser.setAutoGeneratePhraseQueries(): \nbq.phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text.\n\nWhen splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.  E.g. simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.\nPatch that addresses some of this issue, with some failing tests and nocommits.\n\nThe existing autoGeneratePhraseQueries=true approach generates queries exactly as if the query had contained quotation marks, but as I mentioned above, this is inappropriate when splitOnWhitespace=false and the query text contains spaces.\n\nThe approach in the patch is to add a new QueryBuilder method to handle the autoGeneratePhraseQueries=true case.  The query text is split on whitespace and these tokens' offsets are compared to those produced by the configured analyzer.  When multiple non-overlapping tokens have offsets within the bounds of a single whitespace-separated token, a phrase query is created.  If the original token is present as a token overlapping with the first split token, then a disjunction query is created with the original token and the phrase query of the split tokens.\n\nI've added a couple of tests that show posincr/poslength/offset output from SynonymFilter and WordDelimiterFilter (likely the two most frequently used analysis components that can create split tokens), and both create corrupt token graphs of various kinds (e.g. LUCENE-6582, LUCENE-5051), so solving this problem in a complete way just isn't possible right now.\n\nSo I'm not happy with the approach in the patch.  It only covers a subset of possible token graphs (e.g. more than one overlapping multi-term synonym doesn't work).  And it's a lot of new code solving a problem that AFAIK no user has reported (does anybody even use autoGeneratePhraseQueries=true with classic QP?),\n\nI'd be much happier if we could somehow get TermAutomatonQuery hooked into the query parsers, and then rewrite to simpler queries if possible: LUCENE-6824.  First thing though is unbreaking SynonymFilter and friends to produce non-broken token graphs though.  Attempts to do this for SynonymFilter have stalled though: LUCENE-6664.  (I have a germ of an idea that might break the logjam - I'll post over there.)\n\nFor this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.\n\nThoughts?\nFYI autoGeneratePhraseQueries was never added to the flexible query parser.\n+1 to move towards having proper graphs come out of analysis, and letting query parsers produce TAQ.  I agree there is a lot of work there though :)\n\nThank you for pointing to LUCENE-6824!  I think that issue can be committed ... it had fallen past the event horizon of my TODO list.  I'll revive it ...\nPatch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.\n\nThis is ready to go.  I'm going to commit shortly.\nI committed the patch to disallow this combination of options.  Hopefully once we unbreak graph token streams, this can be revisited.\n", "issueSearchSentences": ["For this issue, maybe instead of my patch, for now, we just disallow autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "Patch that disallows autoGeneratePhraseQueries=true when splitOnWhitespace=false.", "When splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.", "Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false", "LUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis."], "issueSearchScores": [0.6722996234893799, 0.6457289457321167, 0.6315320730209351, 0.4969485104084015, 0.4771304130554199]}
{"aId": 70, "code": "public void forceMergeDeletes() throws CorruptIndexException, IOException {\n    forceMergeDeletes(true);\n  }", "comment": " To see how many deletions you have pending in your index, call IndexReader#numDeletedDocs.", "issueId": "LUCENE-3577", "issueStringList": ["rename expungeDeletes", "Similar to optimize(), expungeDeletes() has a misleading name.", "We already had problems with this on the user list because TieredMergePolicy", "didn't 'expunge' all their deletes.", "Also I think expunge is the wrong word, because expunge makes it seem", "like you just wrangle up the deletes and kick them out of the party and", "that it should be fast.", "Also I think this method could do with some javadocs cleanup:", "from the javadocs it is practically begging you to call it if you", "ever delete, but doesn't TieredMP already handle this well?", "+1.", "The name does not indicate how horribly costly the operation is.", "And it leads to apps deleting/updating a few docs and then calling .expungeDeletes.", "We could also remove the method entirely?", "TieredMP already \"favors\" merges that reclaim more deletes (other things being equal), and you can change how strongly it does so (TMP.setReclaimDeletesWeight).", "In practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.", "bq.", "In practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.", "If there are just a few deletes in a few small segments, using optimize instead of expungeDeletes is much more expensive?", "Although, it doesn't really seem like an important use case (ensuring there are no deletes).", "bq.", "If there are just a few deletes in a few small segments, using optimize instead of expungeDeletes is much more expensive?", "that's what i was wondering ...", "most incrementally updated indexes i've seen related to structured content (ie: products, news, blogs, patents, etc...) the \"recent\" documents are the only things likely to get updates (ie: a news story published in the past hour has a decent change of getting an update, a news story published yesterday might get a typo fixed, but a news story published a year ago isn't likely to ever get updated) so in a traditional merged segment structure the newer/smaller segments are the only ones that tend to have delets -- the bigger older segments are mostly stagnant except when involved in merging.", "An expungeDelets call that only touches the small \"recent\" segments is going to be a lot faster then a full optimize, correct?", "bq.", "Although, it doesn't really seem like an important use case (ensuring there are no deletes).", "I'm constantly surprised by the number of people who are really picky about ensuring that their tf/idf numbers are *exact* because they use them in a weird way -- it's definitely an expert level concern, but if those people are willing to spend the time expunging deletes and we already have the code, might as well leave it in right?", "i think this is really just a question of naming/documentation: the method doesn't sound as sexy as optimize, but if someone stumbles upon it and thinks \"oh wow, i guess i have to call this for my deletes to really be deleted\" that's bad.", "likewise the javadocs encourage/imply that it this method *should* be called, instead of just explaining that it *can* be called and what it does.", "I don't have a good suggestion for the name, but the doc is really the issue...", "{quote}", "...When an index has many document deletions (or updates to existing documents), it's best to either call optimize or expungeDeletes to remove all unused data in the index associated with the deleted documents.", "To see how many deletions you have pending in your index, call IndexReader.numDeletedDocs() This saves disk space and memory usage while searching.", "...", "{quote}", "...nothing in that description describes the downsides/cost of the method.", "How about forceMergeDeletes?", "{quote}", "I'm constantly surprised by the number of people who are really picky about ensuring that their tf/idf numbers are exact because they use them in a weird way", "{quote}", "Do they know how we store normalization factors?", ":)", "Patch w/ rote rename to forceMergeDeletes."], "SplitGT": [" To see how many deletions you have pending in your index, call IndexReader#numDeletedDocs."], "issueString": "rename expungeDeletes\nSimilar to optimize(), expungeDeletes() has a misleading name.\n\nWe already had problems with this on the user list because TieredMergePolicy\ndidn't 'expunge' all their deletes.\n\nAlso I think expunge is the wrong word, because expunge makes it seem\nlike you just wrangle up the deletes and kick them out of the party and\nthat it should be fast.\n\n\n\n\nAlso I think this method could do with some javadocs cleanup:\nfrom the javadocs it is practically begging you to call it if you\never delete, but doesn't TieredMP already handle this well?\n+1.  The name does not indicate how horribly costly the operation is.\n\nAnd it leads to apps deleting/updating a few docs and then calling .expungeDeletes.\n\nWe could also remove the method entirely?  TieredMP already \"favors\" merges that reclaim more deletes (other things being equal), and you can change how strongly it does so (TMP.setReclaimDeletesWeight).\n\nIn practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.\nbq. In practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.\n\nIf there are just a few deletes in a few small segments, using optimize instead of expungeDeletes is much more expensive?\nAlthough, it doesn't really seem like an important use case (ensuring there are no deletes).\nbq. If there are just a few deletes in a few small segments, using optimize instead of expungeDeletes is much more expensive?\n\nthat's what i was wondering ... \n\nmost incrementally updated indexes i've seen related to structured content (ie: products, news, blogs, patents, etc...) the \"recent\" documents are the only things likely to get updates (ie: a news story published in the past hour has a decent change of getting an update, a news story published yesterday might get a typo fixed, but a news story published a year ago isn't likely to ever get updated) so in a traditional merged segment structure the newer/smaller segments are the only ones that tend to have delets -- the bigger older segments are mostly stagnant except when involved in merging.  An expungeDelets call that only touches the small \"recent\" segments is going to be a lot faster then a full optimize, correct?\n\nbq. Although, it doesn't really seem like an important use case (ensuring there are no deletes).\n\nI'm constantly surprised by the number of people who are really picky about ensuring that their tf/idf numbers are *exact* because they use them in a weird way -- it's definitely an expert level concern, but if those people are willing to spend the time expunging deletes and we already have the code, might as well leave it in right?\n\ni think this is really just a question of naming/documentation: the method doesn't sound as sexy as optimize, but if someone stumbles upon it and thinks \"oh wow, i guess i have to call this for my deletes to really be deleted\" that's bad.  likewise the javadocs encourage/imply that it this method *should* be called, instead of just explaining that it *can* be called and what it does.\n\nI don't have a good suggestion for the name, but the doc is really the issue...\n\n{quote}\n...When an index has many document deletions (or updates to existing documents), it's best to either call optimize or expungeDeletes to remove all unused data in the index associated with the deleted documents. To see how many deletions you have pending in your index, call IndexReader.numDeletedDocs() This saves disk space and memory usage while searching. ...\n{quote}\n\n...nothing in that description describes the downsides/cost of the method.\nHow about forceMergeDeletes?\n{quote}\nI'm constantly surprised by the number of people who are really picky about ensuring that their tf/idf numbers are exact because they use them in a weird way\n{quote}\n\nDo they know how we store normalization factors? :)\nPatch w/ rote rename to forceMergeDeletes.\n", "issueSearchSentences": ["How about forceMergeDeletes?", "Patch w/ rote rename to forceMergeDeletes.", "In practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.", "In practice expungeDeletes will usually be just like forceMerge(1) so for apps that must have no deletes (eg maybe they need docFreq to be 100% accurate), they can call forceMerge(1) instead.", "Although, it doesn't really seem like an important use case (ensuring there are no deletes)."], "issueSearchScores": [0.8105268478393555, 0.725366473197937, 0.6858410835266113, 0.6858410835266113, 0.600279688835144]}
{"aId": 71, "code": "public void registerCore(String collection) {\n    AtomicBoolean reconstructState = new AtomicBoolean(false);\n    collectionWatches.compute(collection, (k, v) -> {\n      if (v == null) {\n        reconstructState.set(true);\n        v = new CollectionWatch();\n      }\n      v.coreRefCount++;\n      return v;\n    });\n    if (reconstructState.get()) {\n      new StateWatcher(collection).refreshAndWatch();\n      synchronized (getUpdateLock()) {\n        constructState();\n      }\n    }\n  }", "comment": " Not a public API.", "issueId": "SOLR-8323", "issueStringList": ["Add CollectionWatcher API to ZkStateReader", "An API to watch for changes to collection state would be a generally useful thing, both internally and for client use.", "Patch outlining the basic idea.", "This adds two new interfaces, CollectionStateWatcher and CollectionStatePredicate.", "The first can be registered for a particular collection with ZkStateReader and is called when the state of that collection changes (as determined by the internal watcher of that collection's state.json node).", "The second is used in a new ZkStateReader.waitForState() method, and is called on a state change to see if the state of a collection matches a predicate.", "There are also forwarding methods on CloudSolrClient for use by SolrJ clients, and a couple of helper methods on DocCollection and Replica to easily check for collection liveness.", "The new interfaces lend themselves nicely to use as Java 8 functional interfaces, and the TestCollectionStateWatchers test demonstrate both lambdas and method references here.", "This should make it easy to replace some of the helper methods (eg waitForThingsToLevelOut, waitForRecoveriesToFinish) in our tests with methods available to SolrJ.", "A caveat: this is only implemented for collections with their own state.json.", "I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.", "bq.", "waitForThingsToLevelOut", "That one is pretty test specific.", "bq.", "I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.", "Someone should remove stateformat=1 for Solr 6 in an ideal world.", "bq.", "Someone should remove stateformat=1 for Solr 6 in an ideal world", "Absolutely.", "Maybe this new API should just go into trunk for now?", "If Solr 6 is coming early in the new year it makes sense to start adding things that don't need to worry about back-compatibility.", "Updated patch, using the SolrCloudTestCase from SOLR-8758.", "This has required a couple of tweaks to the collection-watching code in ZkStateReader, to allow for watching of non-existent collections.", "Why is DocCollection.isFullyActive() static?", "bq.", "stateWatchers.putIfAbsent(collection, Collections.synchronizedList(new ArrayList<>()));", "You want computeIfAbsent() here to avoid the allocations.", "If waitForState() exits with the TimeoutException, the watcher never gets removed.", "There is a fundamental problem with how interestingCollections is getting managed now; there are external controls on that set, but now it's mixed up with the CollectionStateWatcher API.", "As an example, CollectionStateWatcher adds but never removes; and an external caller could call removeZkWatcher on a collection that there's a listener for.", "The way the code is structured with setCreationWatch and refreshAndWatch doesn't make sense to me.", "Why in the heck are they recursive?", "I don't think you need all this.", "I suspect what you really want is to move the call to notifyStateWatchers() and handle it more intelligently to not fire events if the state hasn't actually changed.", "Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages.", "Thanks for the review, Scott!", "Here's an update patch.", "bq.", "Why is DocCollection.isFullyActive() static?", "Because the DocCollection passed to onStateChanged() may be null if the collection doesn't exist, or has been deleted.", "bq.", "If waitForState() exits with the TimeoutException, the watcher never gets removed.", "Fixed.", "bq.", "Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages", "Done, thanks - that's considerably simpler.", "bq.", "There is a fundamental problem with how interestingCollections is getting managed now", "I've restructured this entirely.", "Watches keep track of a) how many cores they have interested in them, and b) how many state watchers there are.", "Changes to a CollectionWatch state are always done inside a ConcurrentHashMap.compute() method to keep them atomic.", "This simplifies the watch handling in ZKController as well, and removes the abstraction leak where external objects controlled when to remove watches.", "I haven't forgotten this one, going to give the new patch a look this week (today or tomorrow)", "Looking at this now.", "BTW, a Github PR might actually make this way easier....", "I like the scheme of reference counting the ZkController core references", "Could collectionWatches and interestingCollections be unified into a single thing?", "collectionWatches.keySet should always be equal to interestingCollections, so I don't a reason to have both", "nit: make the static type of collectionWatchers be ConcurrentMap?", "Conveys intent better and plays nicer in IDE.", "unregisterCore needs a better guard against under-referencing, since it can be called from the outside.", "A caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.", "Might even be advisable to throw an exception here on under reference.", "{code}", "LOG.info(\"Deleting data for [{}]\", coll);", "notifyStateWatchers(coll, newState);", "{code}", "newState is always null (IDE warning) so maybe just pass in null", "{code}", "if (watchers.size() == 0)", "return;", "{code}", "No need to early exit here, the loop will do it anyway", "In notifyStateWatchers you can avoid some copies but just re-assigning the instance variable to a new empty set, and taking ownership of the existing set to fire events on.", "In getStateWatchers() you probably still want to wrap in a compute function to avoid weird race conditions and memory-visibility problems.", "In particular there's absolutely no ordering guarantees on the reference to watch.stateWatchers", "fetchCollectionState() expectExists parameter doesn't make sense to me...", "I would have thought that if a non-null watcher is passed in, you always want to setup an exists watch if the node doesn't exist.", "And if a null watcher is passed in, calling exists() is a waste of energy.", "registerCore/ unregisterCore should probably retain the previous doc:", "getStateWatchers() could return null vs. empty set to differentiate between whether or not the collection is being watched, which would improve the precision of test assertions.", "I did get one failure on a test run:", "\"Did not see a fully active cluster after 30 seconds\"", "But second run it passed.", "Wow, thanks for the very thorough review Scott!", "Here's an updated patch.", "bq.", "Could collectionWatches and interestingCollections be unified into a single thing?", "Unfortunately not, as it's needed to detect collections which have migrated from state format 1 to state format 2.", "There's almost certainly a nicer way of doing that, though - maybe in a follow-up issue?", "bq.", "make the static type of collectionWatchers be ConcurrentMap?", "I disagree here - we don't use any of the concurrent methods, so I think just using Map is fine?", "bq.", "unregisterCore needs a better guard against under-referencing", "Added.", "I don't think throwing an exception is necessary, although maybe we should log a warning in this case?", "bq.", "newState is always null", "changed", "bq.", "No need to early exit here", "changed", "bq.", "In notifyStateWatchers you can avoid some copies...", "I think this ends up as a wash, given that you may end up creating multiple HashSets?", "And we're only copying references, after all.", "bq.", "In getStateWatchers() you probably still want to wrap in a compute function...", "Compute() doesn't help here, I don't think?", "And given that it's a test-only method, I'm not too concerned about accuracy.", "I've made it return a copy rather than return the original set, which should stop weird things happening to it once it's been returned, though.", "bq.", "fetchCollectionState() expectExists parameter doesn't make sense to me", "Again, this is due to state format 1 - a collection state might be in clusterstate.json, so the collection-specific state might not exist.", "I agree about the null watcher though, and have added a check around the exists call for that.", "bq.", "registerCore/ unregisterCore should probably retain the previous doc:", "Added back", "bq.", "getStateWatchers() could return null vs. empty set", "Nice idea, added.", "I've also added an explicit test for state format 1 collections, and updated the code so that it actually works :)", "Don't suppose I could convince you to open a github PR?", "Would make it much easier to review.", ":D", "Pull request opened, review away!", "I see that you've already committed some changes to the way legacy collections are dealt with, so we may well be able to remove the 'interestingcollections' list - will give it a go.", "[~dragonsinth] any comments on the pull request?", "I'd like to get this in soon, as it will make it easier to clean up a bunch of tests.", "Sorry!", "I'll look at it today.", "Been swamped with other stuff.", ":(", "Pardon the distraction to the fine work going on here but I'd like to possibly emulate this code review process on other issue(s).", "Is it necessary to create a branch on some other/personal repo and then issue a pull request (as was done here I see) or is it possible for someone to review commits to a branch on our repo/mirror?", "I'm thinking SOLR-5750 -- https://github.com/apache/lucene-solr/commits/solr-5750   (feel free to make a comment to test).", "Pretty sure you can create pull requests from branches within the same repository, so there's no need to have your own clone if you don't want one.", "Final patch.", "I think this is ready!", "I already LGTM'd the github PR, I don't think I need to look at the patch file?", "Thanks for all the reviewing Scott!", "Now on to SOLR-9056 :)"], "SplitGT": [" Not a public API."], "issueString": "Add CollectionWatcher API to ZkStateReader\nAn API to watch for changes to collection state would be a generally useful thing, both internally and for client use.\nPatch outlining the basic idea.\n\nThis adds two new interfaces, CollectionStateWatcher and CollectionStatePredicate.  The first can be registered for a particular collection with ZkStateReader and is called when the state of that collection changes (as determined by the internal watcher of that collection's state.json node).  The second is used in a new ZkStateReader.waitForState() method, and is called on a state change to see if the state of a collection matches a predicate.  There are also forwarding methods on CloudSolrClient for use by SolrJ clients, and a couple of helper methods on DocCollection and Replica to easily check for collection liveness.\n\nThe new interfaces lend themselves nicely to use as Java 8 functional interfaces, and the TestCollectionStateWatchers test demonstrate both lambdas and method references here.\n\nThis should make it easy to replace some of the helper methods (eg waitForThingsToLevelOut, waitForRecoveriesToFinish) in our tests with methods available to SolrJ.\n\nA caveat: this is only implemented for collections with their own state.json.  I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.\nbq. waitForThingsToLevelOut\n\nThat one is pretty test specific.\n\nbq.  I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.\n\nSomeone should remove stateformat=1 for Solr 6 in an ideal world.\nbq. Someone should remove stateformat=1 for Solr 6 in an ideal world\n\nAbsolutely.  Maybe this new API should just go into trunk for now?  If Solr 6 is coming early in the new year it makes sense to start adding things that don't need to worry about back-compatibility.\nUpdated patch, using the SolrCloudTestCase from SOLR-8758.\n\nThis has required a couple of tweaks to the collection-watching code in ZkStateReader, to allow for watching of non-existent collections.\n- Why is DocCollection.isFullyActive() static?\n\nbq. stateWatchers.putIfAbsent(collection, Collections.synchronizedList(new ArrayList<>()));\n\n - You want computeIfAbsent() here to avoid the allocations.\n\n- If waitForState() exits with the TimeoutException, the watcher never gets removed.\n\n- There is a fundamental problem with how interestingCollections is getting managed now; there are external controls on that set, but now it's mixed up with the CollectionStateWatcher API.  As an example, CollectionStateWatcher adds but never removes; and an external caller could call removeZkWatcher on a collection that there's a listener for.\n\n- The way the code is structured with setCreationWatch and refreshAndWatch doesn't make sense to me. Why in the heck are they recursive?  I don't think you need all this.  I suspect what you really want is to move the call to notifyStateWatchers() and handle it more intelligently to not fire events if the state hasn't actually changed.  Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages.\nThanks for the review, Scott!  Here's an update patch.\n\nbq. Why is DocCollection.isFullyActive() static?\n\nBecause the DocCollection passed to onStateChanged() may be null if the collection doesn't exist, or has been deleted.\n\nbq. If waitForState() exits with the TimeoutException, the watcher never gets removed.\n\nFixed.\n\nbq. Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages\n\nDone, thanks - that's considerably simpler.\n\nbq. There is a fundamental problem with how interestingCollections is getting managed now\n\nI've restructured this entirely.  Watches keep track of a) how many cores they have interested in them, and b) how many state watchers there are.  Changes to a CollectionWatch state are always done inside a ConcurrentHashMap.compute() method to keep them atomic.  This simplifies the watch handling in ZKController as well, and removes the abstraction leak where external objects controlled when to remove watches.\nI haven't forgotten this one, going to give the new patch a look this week (today or tomorrow)\nLooking at this now.  BTW, a Github PR might actually make this way easier....\nI like the scheme of reference counting the ZkController core references\nCould collectionWatches and interestingCollections be unified into a single thing?\ncollectionWatches.keySet should always be equal to interestingCollections, so I don't a reason to have both\nnit: make the static type of collectionWatchers be ConcurrentMap?  Conveys intent better and plays nicer in IDE.\nunregisterCore needs a better guard against under-referencing, since it can be called from the outside.\nA caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.\nMight even be advisable to throw an exception here on under reference.\n{code}\nLOG.info(\"Deleting data for [{}]\", coll);\nnotifyStateWatchers(coll, newState);\n{code}\n\nnewState is always null (IDE warning) so maybe just pass in null\n{code}\n    if (watchers.size() == 0)\n      return;\n{code}\n\nNo need to early exit here, the loop will do it anyway\nIn notifyStateWatchers you can avoid some copies but just re-assigning the instance variable to a new empty set, and taking ownership of the existing set to fire events on.\nIn getStateWatchers() you probably still want to wrap in a compute function to avoid weird race conditions and memory-visibility problems.  In particular there's absolutely no ordering guarantees on the reference to watch.stateWatchers\nfetchCollectionState() expectExists parameter doesn't make sense to me... I would have thought that if a non-null watcher is passed in, you always want to setup an exists watch if the node doesn't exist.  And if a null watcher is passed in, calling exists() is a waste of energy.\nregisterCore/ unregisterCore should probably retain the previous doc:\n\n/** This is not a public API. Only used by ZkController */\n\ngetStateWatchers() could return null vs. empty set to differentiate between whether or not the collection is being watched, which would improve the precision of test assertions.\nI did get one failure on a test run:\n\"Did not see a fully active cluster after 30 seconds\"\n\nBut second run it passed.\nWow, thanks for the very thorough review Scott!  Here's an updated patch.\n\nbq. Could collectionWatches and interestingCollections be unified into a single thing?\n\nUnfortunately not, as it's needed to detect collections which have migrated from state format 1 to state format 2.  There's almost certainly a nicer way of doing that, though - maybe in a follow-up issue?\n\nbq. make the static type of collectionWatchers be ConcurrentMap?\n\nI disagree here - we don't use any of the concurrent methods, so I think just using Map is fine?\n\nbq. unregisterCore needs a better guard against under-referencing\n\nAdded.  I don't think throwing an exception is necessary, although maybe we should log a warning in this case?\n\nbq. newState is always null\n\nchanged\n\nbq. No need to early exit here\n\nchanged\n\nbq. In notifyStateWatchers you can avoid some copies...\n\nI think this ends up as a wash, given that you may end up creating multiple HashSets?  And we're only copying references, after all.\n\nbq. In getStateWatchers() you probably still want to wrap in a compute function...\n\nCompute() doesn't help here, I don't think?  And given that it's a test-only method, I'm not too concerned about accuracy.  I've made it return a copy rather than return the original set, which should stop weird things happening to it once it's been returned, though.\n\nbq. fetchCollectionState() expectExists parameter doesn't make sense to me\n\nAgain, this is due to state format 1 - a collection state might be in clusterstate.json, so the collection-specific state might not exist.  I agree about the null watcher though, and have added a check around the exists call for that.\n\nbq. registerCore/ unregisterCore should probably retain the previous doc:\n\nAdded back\n\nbq. getStateWatchers() could return null vs. empty set\n\nNice idea, added.\n\nI've also added an explicit test for state format 1 collections, and updated the code so that it actually works :)\nDon't suppose I could convince you to open a github PR?  Would make it much easier to review. :D\nPull request opened, review away!  I see that you've already committed some changes to the way legacy collections are dealt with, so we may well be able to remove the 'interestingcollections' list - will give it a go.\n[~dragonsinth] any comments on the pull request?  I'd like to get this in soon, as it will make it easier to clean up a bunch of tests.\nSorry! I'll look at it today. Been swamped with other stuff. :(\nPardon the distraction to the fine work going on here but I'd like to possibly emulate this code review process on other issue(s).  Is it necessary to create a branch on some other/personal repo and then issue a pull request (as was done here I see) or is it possible for someone to review commits to a branch on our repo/mirror?  I'm thinking SOLR-5750 -- https://github.com/apache/lucene-solr/commits/solr-5750   (feel free to make a comment to test).\nPretty sure you can create pull requests from branches within the same repository, so there's no need to have your own clone if you don't want one.\nFinal patch.  I think this is ready!\nI already LGTM'd the github PR, I don't think I need to look at the patch file?\nThanks for all the reviewing Scott!  Now on to SOLR-9056 :)\n", "issueSearchSentences": ["The first can be registered for a particular collection with ZkStateReader and is called when the state of that collection changes (as determined by the internal watcher of that collection's state.json node).", "A caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.", "This has required a couple of tweaks to the collection-watching code in ZkStateReader, to allow for watching of non-existent collections.", "There is a fundamental problem with how interestingCollections is getting managed now; there are external controls on that set, but now it's mixed up with the CollectionStateWatcher API.", "An API to watch for changes to collection state would be a generally useful thing, both internally and for client use."], "issueSearchScores": [0.5936209559440613, 0.5502444505691528, 0.5356934070587158, 0.5286288857460022, 0.5226193070411682]}
{"aId": 75, "code": "public Map<String, SearchComponent> getSearchComponents() {\n    return Collections.unmodifiableMap(searchComponents);\n  }", "comment": " Accessor for all the Search Components", "issueId": "SOLR-557", "issueStringList": ["Add SolrCore.getSearchComponents()", "Return an unmodifiable Map of the core's Search Components, similar to the request handlers.", "Code is:", "Accessor for all the Search Components", "@return An unmodifiable Map of Search Components", "public Map<String, SearchComponent> getSearchComponents() {", "return Collections.unmodifiableMap(searchComponents);", "}", "I will commit once SVN is up again."], "SplitGT": [" Accessor for all the Search Components"], "issueString": "Add SolrCore.getSearchComponents()\nReturn an unmodifiable Map of the core's Search Components, similar to the request handlers.\n\nCode is:\n/**\n   * Accessor for all the Search Components \n   * @return An unmodifiable Map of Search Components\n   */\n  public Map<String, SearchComponent> getSearchComponents() {\n    return Collections.unmodifiableMap(searchComponents);\n  }\n\nI will commit once SVN is up again.\n", "issueSearchSentences": ["public Map<String, SearchComponent> getSearchComponents() {", "return Collections.unmodifiableMap(searchComponents);", "@return An unmodifiable Map of Search Components", "Return an unmodifiable Map of the core's Search Components, similar to the request handlers.", "Add SolrCore.getSearchComponents()"], "issueSearchScores": [0.9340697526931763, 0.8307406902313232, 0.81072998046875, 0.6366015672683716, 0.6017576456069946]}
{"aId": 76, "code": "public boolean isCurrent() throws CorruptIndexException, IOException {\n    for (int i = 0; i < subReaders.length; i++) {\n      if (!subReaders[i].isCurrent()) {\n        return false;\n      }\n    }\n    \n    // all subreaders are up to date\n    return true;\n  }", "comment": " Checks recursively if all subreaders are up to date.", "issueId": "LUCENE-781", "issueStringList": ["NPE in MultiReader.isCurrent() and getVersion()", "I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase.", "For getVersion(), we should throw a better exception that NPE.", "I will commit unless someone objects or has a better idea.", "I checked - the fix is working and code seems right.", "While we are looking at this, there are a few more IndexReader methods", "which are not implemented by MultiReader.", "These 3 methods seems ok:", "document(int)", "would work because IndexReader would send to document(int,FieldSelector)", "which is implemented in MultiReader.", "termDocs(Term),", "termPositions(Term)", "would both work because IndexReader implementations goes to termDocs() or", "to termPositions(), which both are implemented in MultiReader.", "These 3 methods should probably be fixed:", "isOptimized()", "would fail - similar to isCurrent()", "setNorm(int, String, float)", "would fail too, similar reason.", "directory()", "would not fail, but fall to return the directory of reader[0],", "is this a correct behavior?", "this is because MultiReader() (constructor) calls super with reader[0] -", "again, I am not sure, is this correct?", "(why allowing to create", "a multi-reader with no readers at all?)", "Thanks for your feedback.", "I have committed my patch (but moved the testcase to TestMultiReader instead of TestMultiSearcher) and will try to address the other issues you found in the next few days.", "Thus I'm not closing this issue yet.", "i haven't looked atthe patch, but i'm a little confused by the issue summary ... for the benefit of people who might encounter this NPE and find this bug when searching, can we clarify under what circumstances MultiReader has this problem.", "after all: a MultiReader is returned by Indexreader.open anytime the index has more then one segment right?", "... i can't imagine that *no one* using a multisegment index has ever tried calling isCurrent() before.", "is this specific to some special use case (or is the reason we're just now noticing the problem because it's a bug recently introduced in the trunk?)", "Hoss, you're right, this breaks MultiReader, I will revert the patch.", "Funny that the test cases didn't notice that.", "Maybe because they use such a small amount of documents that they never need a MultiReader?", "The NPE happens when one constructs the MultiReader with its only public constructor, MultiReader(IndexReader[] subReaders).", "This construtor is never used in Lucene, not even in the test cases.", "I thought it would not break MultiReader, just do unnecessary work for that method...?", "Same new test (using that (readers[]) constructor) would fail also in previous versions.", "I think main difference is that for the MultiReader created inside IndexReader, (1) all readers share the same directory, and (2) it maintains a SegmentsInfos read from that single directory.", "Now this is not the case for the other (but still valid (?))", "usage of MultiReader - because there is no single directory (well, not necessarily) and hence no SegmentInfos for the MultiReader.", "So it seems a possible fix would be:", "define a boolean e.g.", "isWholeIndex predicate in MultiReader", "would be true when constructed with a non null dir and a non null segmentInfos", "base operation upon it:", "if isWholeIndex call super.isCurrent() otherwise do the (multi) logic in current fix.", "i wasn't suggesting that the patch was flawed -- just trying to clarify what circumstances would cause an \"NPE in MultiReader.isCurrent() and getVersion()\"", "it sounds like my comment has spawned a seperate issue...", "if there are currently no tests for a multi-directory MultiReader then there certainly should be -- if the methods mentioned in this issue all currently throw an Exception on a multi-directory MultiReader we should either: a) define what the meaning of those methods is in that case, and implement them accordingly; or b) make those methods throw UnsupportedOperationException (or somethign similar) in thta case.", "...either way, i'd still like clarification as to the orriginal point of this issue ... what was the bug?", "what would trigger the NPE?", "> ...either way, i'd still like clarification as to the orriginal point of this", "> issue ... what was the bug?", "what would trigger the NPE?", "It is triggers by having two ways to construct a MultiReader:", "(1) as IndexReader does it for the regular (multi segment) index", "MultiReader(Directory directory, SegmentInfos sis, boolean closeDirectory, IndexReader[] subReaders)", "(2) as anyone can use it, for aggregating results from any indexes:", "MultiReader(IndexReader[] subReaders)", "In (1) all readers use the same directory, and there is a single SegnentInfos.", "This is the standard, tested way.", "In (2) there is no single dir and no single SegmentInfos.", "This is the \"general\", less tested way.", "In this option, dir (of the multiReader) is initialized to that of subReader[0].", "This seems spooky to me.", "Also in this option, SegmentInfos in null.", "It makes sense, since readers can be anything - but this is the cause for the NPE.", "BTW, after (being surprised by) your first comment on this, I checked in 1.9.1 - the test (of case (2)) fails there as well.", "so the fundamental issue is two radically different use cases of MultiReader -- and these methods really only have meaning when talking about a single directory.", "if getVersion, isCurrent and isOptimized, have *never* worked with a MultiReader constructed using \"new MultiReader(IndexReader[])\" then throwing UnsupportedOperationException definitely seems like the best course of action ... the semantics of those methods don't really make sense on a multi-directory index.", "for setNorm we should be able to loop over the sub readers and call setNorm on each right?", "the 50 thousand dollar question is should directory() be modified to throw UnsupportedOperationException even though it doesn't currently throw an NPE ?", "... i think it should.", "I think the MultiReader(IndexReader[]) constructor should allways call super(null) --  anyone currently relying on MultiReader.directory() it to return the directory of the \"first\" IndexReader should be able to easily change their code.", "if we want to make it really easy we could provide a MultiReader.getSubReader(int n) method.", "I agree, except for isCurrent() - why not iterating the readers only for case 2?", "After all it seems like a useful API also in this case.", "let's say we do implement isCurrent for a multi directory MultiReader as a loop over teh sub readers that returns true if all of them return true.", "If a client calls MultiReader.isCurrent() and gets back \"false\" ... what do they do with that information?", "That information only seems usefull if they know how the MultiReader was built - if they know it was built from multiple readers, then can allways do that loop themselves.", "if they don't know how the MultiReader was constructed then can't attempt to reopen it so what's the point of knowing wether it's up to date?", "(argueable the app may just want to provide monitoring info about the low level index: but if that's the case the app should probably get that info at the level where it knows it's open a Reader across multiple directories)", "In general: if it's never worked, then we aren't under any burden to make it work if there isnt' a clear meaning for the value.", "One could write an application that groups readers to multiReaders in more than 1 level, i.e.", "r1,r2,r3 grouped to rr1,   r4,r5,r6 grouped to rr2,   rr1,rr2 grouped to rrr.", "If rrr.isCurrent() throws unsupported, the application needs to question recursively.", "I am not aware of such an application, so you could argue this is only theoretic, still it demonstrates a strength of Lucene.", "Also, here too, as argued above, even if the answer is false (not current), the application would need to apply the same recursive logic to reopen the non-current reader and reconstruct the multi-reader.", "So I agree it is valid to throw unsupported.", "Just that it feels a bit uncomfortable to throw unsupported for existing API of a method with well defined meaning that is quite easy to implement (relying on that anyhow it was never implemented correctly).", "Lemme put it this way: I'd rather write a new IndexReaderUtils class, with a static isMultiReaderCurrent(MultiReader) method that uses instanceOf to recursively walk all fo the sub indexes then to make MultiReader.isCurrent() do that ... because then people using hte method are clear about what the value of that boolean means.", "> Just that it feels a bit uncomfortable to throw unsupported for existing API of a method with", "> well defined meaning that is quite easy to implement (relying on that anyhow it was never", "> implemented correctly).", "I agree, it feels dirty ... but it feels safer too.", "i certainly won't obejct if someone commits a recursive method for isCurrent -- it's just not my prefrence.", "I would object to a recursive isOptimized ... that one really doens't make sense at all for a multi-directory MultiReader ... in theory it should allways return false since by definition the index is not a single segment, but if you do that, so code could try to optimize it.", "updated patch", "updated patch", "I've attached an updated patch that now throws an exception for isCurrent() and that does the same for the other methods which Doron noted not to work with the public constructor either.", "Except doSetNorm() which seems to work okay without any patch.", "comments based on cursory read of latest patch(es)...", "1) I still think the IndexReader[] constructor should *allways* call super(null) ... the current behavior could mask future problems if/when new methods get added to IndexReader.", "2) what about MultiReader.directory() ?", "... shoulnd't that throw Unsupported if false == hasSegmentInfos ?", "#2 is a good example of why i believe in #1 ...", "I think the cleanest solution here is it to separate MultiReader into two", "classes: MultiSegmentReader (package-protected) and MultiReader", "(public) that extends MultiSegmentReader.", "This would also help to implement LUCENE-743 cleaner.", "I'll attach a patch here soon.", "This patch:", "Adds the new class MultiSegmentReader which contains almost", "all code from MultiReader, except the public constructor.", "Makes MultiTermEnum, MultiTermDocs and MultiTermPositions", "inner, static classes of MultiSegmentReader.", "Adds the method isCurrent() to MultiReader, which recursively", "checks if all subreaders are up to date.", "MultiReader now throws UnsupportedOperationException when", "isOptimized() or getVersion() is called.", "Enables the isCurrent() test in TestMultiReader that was", "disabled due to this issue.", "All tests pass.", "> MultiSegmentReader (package-protected) and MultiReader (public) that extends MultiSegmentReader", "Hmm.", "I've never much liked having a non-public class as a base class for a public class.", "But I can't think of a good reason, except that it makes the javadoc a bit odd, since the non-public class is named there, when normally everything shown in javadoc is public.", "> except that it makes the javadoc a bit odd, since the non-public class", "> is named there, when normally everything shown in javadoc is public.", "Is it??", "I looked into the javadocs built with this patch and I can't see", "the name MultiSegmentReader.", "It looks like before, as if MultiReader", "would still extend IndexReader:", "http://people.apache.org/~buschmi/lucene-781/api/org/apache/lucene/index/MultiReader.html", "If there are no objections against separating MultiReader into two classes", "I would like to commit my patch in a day or two.", "I think the javadocs look", "fine, the class MultiSegmentReader does not appear there.", "One thing I'm not sure about: Should MultiReader throw an", "UnsupportedOperationException or simply return false when isOptimized() is", "called?", "Returning false for isOptimized() seems fine.", "> I looked into the javadocs built with this patch and I can't see the name MultiSegmentReader.", "Great!", "They've improved javadoc since I last tried that.", "I remove my reservations.", "+1", "Committed.", "MultiSegmentReader.isOptimized() now always returns false."], "SplitGT": [" Checks recursively if all subreaders are up to date."], "issueString": "NPE in MultiReader.isCurrent() and getVersion()\nI'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea.\nI checked - the fix is working and code seems right.\n\nWhile we are looking at this, there are a few more IndexReader methods \nwhich are not implemented by MultiReader.\n\nThese 3 methods seems ok:\n- document(int)\n  would work because IndexReader would send to document(int,FieldSelector) \n  which is implemented in MultiReader.\n- termDocs(Term),  \n- termPositions(Term)\n  would both work because IndexReader implementations goes to termDocs() or \n  to termPositions(), which both are implemented in MultiReader.\n\nThese 3 methods should probably be fixed:\n- isOptimized() \n  would fail - similar to isCurrent()\n- setNorm(int, String, float)\n  would fail too, similar reason.\n- directory()\n  would not fail, but fall to return the directory of reader[0], \n  is this a correct behavior?\n  this is because MultiReader() (constructor) calls super with reader[0] - \n  again, I am not sure, is this correct? (why allowing to create \n  a multi-reader with no readers at all?)\n\n\nThanks for your feedback. I have committed my patch (but moved the testcase to TestMultiReader instead of TestMultiSearcher) and will try to address the other issues you found in the next few days. Thus I'm not closing this issue yet.\n\ni haven't looked atthe patch, but i'm a little confused by the issue summary ... for the benefit of people who might encounter this NPE and find this bug when searching, can we clarify under what circumstances MultiReader has this problem.\n\nafter all: a MultiReader is returned by Indexreader.open anytime the index has more then one segment right? ... i can't imagine that *no one* using a multisegment index has ever tried calling isCurrent() before.\n\nis this specific to some special use case (or is the reason we're just now noticing the problem because it's a bug recently introduced in the trunk?)\nHoss, you're right, this breaks MultiReader, I will revert the patch. Funny that the test cases didn't notice that. Maybe because they use such a small amount of documents that they never need a MultiReader? The NPE happens when one constructs the MultiReader with its only public constructor, MultiReader(IndexReader[] subReaders). This construtor is never used in Lucene, not even in the test cases.\nI thought it would not break MultiReader, just do unnecessary work for that method...?\n\nSame new test (using that (readers[]) constructor) would fail also in previous versions. \n\nI think main difference is that for the MultiReader created inside IndexReader, (1) all readers share the same directory, and (2) it maintains a SegmentsInfos read from that single directory. \n\nNow this is not the case for the other (but still valid (?)) usage of MultiReader - because there is no single directory (well, not necessarily) and hence no SegmentInfos for the MultiReader. \n\nSo it seems a possible fix would be:\n- define a boolean e.g. isWholeIndex predicate in MultiReader \n- would be true when constructed with a non null dir and a non null segmentInfos \n- base operation upon it: \n- if isWholeIndex call super.isCurrent() otherwise do the (multi) logic in current fix.\ni wasn't suggesting that the patch was flawed -- just trying to clarify what circumstances would cause an \"NPE in MultiReader.isCurrent() and getVersion()\"\n\nit sounds like my comment has spawned a seperate issue...\n\nif there are currently no tests for a multi-directory MultiReader then there certainly should be -- if the methods mentioned in this issue all currently throw an Exception on a multi-directory MultiReader we should either: a) define what the meaning of those methods is in that case, and implement them accordingly; or b) make those methods throw UnsupportedOperationException (or somethign similar) in thta case.\n\n\n...either way, i'd still like clarification as to the orriginal point of this issue ... what was the bug?  what would trigger the NPE?\n> ...either way, i'd still like clarification as to the orriginal point of this \n> issue ... what was the bug? what would trigger the NPE? \n\nIt is triggers by having two ways to construct a MultiReader:\n(1) as IndexReader does it for the regular (multi segment) index\n      MultiReader(Directory directory, SegmentInfos sis, boolean closeDirectory, IndexReader[] subReaders)\n(2) as anyone can use it, for aggregating results from any indexes:\n     MultiReader(IndexReader[] subReaders)\n\nIn (1) all readers use the same directory, and there is a single SegnentInfos.\nThis is the standard, tested way.\n\nIn (2) there is no single dir and no single SegmentInfos. \nThis is the \"general\", less tested way.\nIn this option, dir (of the multiReader) is initialized to that of subReader[0]. \nThis seems spooky to me.\nAlso in this option, SegmentInfos in null.\nIt makes sense, since readers can be anything - but this is the cause for the NPE.\n\nBTW, after (being surprised by) your first comment on this, I checked in 1.9.1 - the test (of case (2)) fails there as well.\nso the fundamental issue is two radically different use cases of MultiReader -- and these methods really only have meaning when talking about a single directory.\n\nif getVersion, isCurrent and isOptimized, have *never* worked with a MultiReader constructed using \"new MultiReader(IndexReader[])\" then throwing UnsupportedOperationException definitely seems like the best course of action ... the semantics of those methods don't really make sense on a multi-directory index.\n\nfor setNorm we should be able to loop over the sub readers and call setNorm on each right?\n\nthe 50 thousand dollar question is should directory() be modified to throw UnsupportedOperationException even though it doesn't currently throw an NPE ? ... i think it should.  I think the MultiReader(IndexReader[]) constructor should allways call super(null) --  anyone currently relying on MultiReader.directory() it to return the directory of the \"first\" IndexReader should be able to easily change their code.  if we want to make it really easy we could provide a MultiReader.getSubReader(int n) method.\nI agree, except for isCurrent() - why not iterating the readers only for case 2? After all it seems like a useful API also in this case.\nlet's say we do implement isCurrent for a multi directory MultiReader as a loop over teh sub readers that returns true if all of them return true.   If a client calls MultiReader.isCurrent() and gets back \"false\" ... what do they do with that information?\n\nThat information only seems usefull if they know how the MultiReader was built - if they know it was built from multiple readers, then can allways do that loop themselves.  if they don't know how the MultiReader was constructed then can't attempt to reopen it so what's the point of knowing wether it's up to date?\n\n(argueable the app may just want to provide monitoring info about the low level index: but if that's the case the app should probably get that info at the level where it knows it's open a Reader across multiple directories)\n\nIn general: if it's never worked, then we aren't under any burden to make it work if there isnt' a clear meaning for the value.\nOne could write an application that groups readers to multiReaders in more than 1 level, i.e. r1,r2,r3 grouped to rr1,   r4,r5,r6 grouped to rr2,   rr1,rr2 grouped to rrr.    If rrr.isCurrent() throws unsupported, the application needs to question recursively. \n\nI am not aware of such an application, so you could argue this is only theoretic, still it demonstrates a strength of Lucene.  Also, here too, as argued above, even if the answer is false (not current), the application would need to apply the same recursive logic to reopen the non-current reader and reconstruct the multi-reader. \n\nSo I agree it is valid to throw unsupported.\n\nJust that it feels a bit uncomfortable to throw unsupported for existing API of a method with well defined meaning that is quite easy to implement (relying on that anyhow it was never implemented correctly). \n\nLemme put it this way: I'd rather write a new IndexReaderUtils class, with a static isMultiReaderCurrent(MultiReader) method that uses instanceOf to recursively walk all fo the sub indexes then to make MultiReader.isCurrent() do that ... because then people using hte method are clear about what the value of that boolean means.\n\n> Just that it feels a bit uncomfortable to throw unsupported for existing API of a method with\n> well defined meaning that is quite easy to implement (relying on that anyhow it was never\n> implemented correctly).\n\nI agree, it feels dirty ... but it feels safer too.\n\ni certainly won't obejct if someone commits a recursive method for isCurrent -- it's just not my prefrence.   I would object to a recursive isOptimized ... that one really doens't make sense at all for a multi-directory MultiReader ... in theory it should allways return false since by definition the index is not a single segment, but if you do that, so code could try to optimize it.\n\n\nupdated patch\nupdated patch\nI've attached an updated patch that now throws an exception for isCurrent() and that does the same for the other methods which Doron noted not to work with the public constructor either. Except doSetNorm() which seems to work okay without any patch.\n\ncomments based on cursory read of latest patch(es)...\n\n1) I still think the IndexReader[] constructor should *allways* call super(null) ... the current behavior could mask future problems if/when new methods get added to IndexReader.\n2) what about MultiReader.directory() ? ... shoulnd't that throw Unsupported if false == hasSegmentInfos ?\n\n#2 is a good example of why i believe in #1 ... \nI think the cleanest solution here is it to separate MultiReader into two\nclasses: MultiSegmentReader (package-protected) and MultiReader\n(public) that extends MultiSegmentReader. \nThis would also help to implement LUCENE-743 cleaner.\n\nI'll attach a patch here soon.\nThis patch:\n\n   * Adds the new class MultiSegmentReader which contains almost\n     all code from MultiReader, except the public constructor.\n\t \n   * Makes MultiTermEnum, MultiTermDocs and MultiTermPositions\n     inner, static classes of MultiSegmentReader.\n\t \n   * Adds the method isCurrent() to MultiReader, which recursively\n     checks if all subreaders are up to date.\n\n   * MultiReader now throws UnsupportedOperationException when\n     isOptimized() or getVersion() is called.\n\t \n   * Enables the isCurrent() test in TestMultiReader that was\n     disabled due to this issue.\n\t \nAll tests pass.\n> MultiSegmentReader (package-protected) and MultiReader (public) that extends MultiSegmentReader\n\nHmm.  I've never much liked having a non-public class as a base class for a public class.  But I can't think of a good reason, except that it makes the javadoc a bit odd, since the non-public class is named there, when normally everything shown in javadoc is public.\n> except that it makes the javadoc a bit odd, since the non-public class \n> is named there, when normally everything shown in javadoc is public.\n\nIs it?? I looked into the javadocs built with this patch and I can't see\nthe name MultiSegmentReader. It looks like before, as if MultiReader \nwould still extend IndexReader:\nhttp://people.apache.org/~buschmi/lucene-781/api/org/apache/lucene/index/MultiReader.html\n\nIf there are no objections against separating MultiReader into two classes\nI would like to commit my patch in a day or two. I think the javadocs look \nfine, the class MultiSegmentReader does not appear there.\n\nOne thing I'm not sure about: Should MultiReader throw an \nUnsupportedOperationException or simply return false when isOptimized() is \ncalled?\n\nReturning false for isOptimized() seems fine.\n> I looked into the javadocs built with this patch and I can't see the name MultiSegmentReader.\n\nGreat!  They've improved javadoc since I last tried that.  I remove my reservations.  +1\nCommitted. \n\nMultiSegmentReader.isOptimized() now always returns false.\n", "issueSearchSentences": ["checks if all subreaders are up to date.", "let's say we do implement isCurrent for a multi directory MultiReader as a loop over teh sub readers that returns true if all of them return true.", "would fail - similar to isCurrent()", "I agree, except for isCurrent() - why not iterating the readers only for case 2?", "Adds the method isCurrent() to MultiReader, which recursively"], "issueSearchScores": [0.738955020904541, 0.6461471319198608, 0.6215769648551941, 0.6041895747184753, 0.5997270345687866]}
{"aId": 79, "code": "public void setDateResolution(DateTools.Resolution dateResolution) {\n    this.dateResolution = dateResolution;\n  }", "comment": " Sets the default date resolution used by RangeQueries for fields for which no specific date resolutions has been set. Field specific resolutions can be set with #setDateResolution(String,DateTools.Resolution).", "issueId": "LUCENE-732", "issueStringList": ["Support DateTools in QueryParser", "The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values.", "However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated.", "In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.", "This patch replaces the use of DateField in QueryParser by DateTools.", "Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:", "Sets the default date resolution used by RangeQueries for fields for which no", "specific date resolutions has been set.", "Field specific resolutions can be set", "with {@link #setDateResolution(String, DateTools.Resolution)}.", "@param dateResolution the default date resolution to set", "public void setDateResolution(DateTools.Resolution dateResolution);", "Sets the date resolution used by RangeQueries for a specific field.", "@param field field for which the date resolution is to be set", "@param dateResolution date resolution to set", "public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);", "(I also added the corresponding getter methods).", "Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.", "The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY.", "Please let me know if you think we should use a different resolution as default.", "I extended TestQueryParser to test this new feature.", "All unit tests pass.", "I'm not sure if most people use DateTools already, as it has just been added in Lucene 1.9.", "Maybe you could consider an option (yes, yet another option isn't nice, I know)?", "Otherwise we need to properly document how to continue using DateField, i.e.", "by extending QueryParser and overwriting this method I guess.", "cleanest way to be backwards compatible would be to not have an initial default Resolution, and use DateField if no Resolution can be found for the field specified.", "Existing clients would still get DateField for all dates, clients that add a call to setDateResolution(\"foo\", BAR); would get a DateTools formated query with resolution BAR for field foo, but other pre-existing fields would still use DateField, and clients that call setDateResolution(BAR); would allways get a DateTools formatted query, either at resolution BAR or at some other resolution if they also use the two arg setDateResolution", "Actually it is documented in the QueryParser how to use a different format for dates:", "...", "feature also assumes that your index uses the {@link DateTools} class to store dates.", "If you use a different format and you still want QueryParser", "to turn local dates in range queries into valid queries you need to create your own", "query parser that inherits QueryParser and overwrites", "{@link #getRangeQuery(String, String, String, boolean)}.", "...", "And the javadoc of DateField says:", "Deprecated.", "If you build a new index, use DateTools instead.", "This class is included for use with existing indices and will be removed in a future release.", "So the question is in how many future releases we want to support DateField.", "If we still want to support it in 2.1 I agree to Hoss that his suggestion would be a nice and clean way to do that and I can easily change the patch accordingly.", "It avoids having another option in QueryParser.", "This new patch contains the changes suggested by Hoss:", "by default the QueryParser uses DateField to format dates for all fields", "if a date resolution has been set using void setDateResolution(DateTools.Resolution) then DateTools is used for all fields with the given resolution", "if a field specific date resolution has been set using setDateResolution(String, DateTools.Resolution) then DateTools is used for the given field with the given date resolution; other fields are not affected", "So with this patch the behaviour of QueryParser does not change unless either setDateResolution(DateTools.Resolution) or setDateResolution(String, DateTools.Resolution) have been called.", "Changed the summary to better reflect the second version of the patch.", "+1 for queryparser_datetools2.patch", "the only nitpick i have is with the class level javadocs...", "<p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g.", "<tt>date:[6/1/2005 TO 6/4/2005]</tt>", "produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04.", "Note", "that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.", "By default a date is formatted using the deprecated {@link DateField} for compatibility reasons.", "To use the new {@link DateTools} to format dates, a {@link DateTools.Resolution} has to be set.</p>", "<p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)", "...the word \"format\" is used to mean two very differnet things in this paragraph, we should clean that up so it's clera that Locale determines the DateFormat QP uses when trying to parse input  in [a TO b] syntax, while either DateField or DateTools are used to create the \"indexed\" value queried against.", "You're right Hoss, the word \"format\" is used ambiguously in the javadoc.", "We could change it to", "<p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g.", "<tt>date:[6/1/2005 TO 6/4/2005]</tt>", "produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04.", "Note", "that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.", "By default a date is converted into a search term using the deprecated {@link DateField} for compatibility reasons.", "To use the new {@link DateTools} to convert dates, a {@link DateTools.Resolution} has to be set.</p>", "<p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)}", "or {@link #setDateResolution(String, DateTools.Resolution)}.", "The former sets the default date resolution for all fields, whereas the latter can", "be used to set field specific date resolutions.", "Field specific date resolutions take, if set, precedence over the default date resolution.", "</p>", "<p>If you use neither {@link DateField} nor {@link DateTools} in your index, you can create your own", "query parser that inherits QueryParser and overwrites {@link #getRangeQuery(String, String, String, boolean)} to", "use a different method for date conversion.", "</p>", "Sounds better?", "Do you want me to create another patch that includes this javadoc?", "I've commited queryparser_datetools2.patch with two changes...", "1) revised the class level javadocs based on Michael's last comment.", "2) reinstated the old TestQueryParser.testDateRange() as testLegacyDateRange() to attempt to future proof against breaking backwards compatibility."], "SplitGT": [" Sets the default date resolution used by RangeQueries for fields for which no specific date resolutions has been set.", "Field specific resolutions can be set with #setDateResolution(String,DateTools.Resolution)."], "issueString": "Support DateTools in QueryParser\nThe QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.\n\nThis patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:\n\n  /**\n   * Sets the default date resolution used by RangeQueries for fields for which no\n   * specific date resolutions has been set. Field specific resolutions can be set\n   * with {@link #setDateResolution(String, DateTools.Resolution)}.\n   *  \n   * @param dateResolution the default date resolution to set\n   */\n  public void setDateResolution(DateTools.Resolution dateResolution);\n  \n  /**\n   * Sets the date resolution used by RangeQueries for a specific field.\n   *  \n   * @param field field for which the date resolution is to be set \n   * @param dateResolution date resolution to set\n   */\n  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);\n\n(I also added the corresponding getter methods).\n\nNow the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.\nThe initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. \n\nPlease let me know if you think we should use a different resolution as default.\n\nI extended TestQueryParser to test this new feature.\n\nAll unit tests pass.\n\nI'm not sure if most people use DateTools already, as it has just been added in Lucene 1.9. Maybe you could consider an option (yes, yet another option isn't nice, I know)? Otherwise we need to properly document how to continue using DateField, i.e. by extending QueryParser and overwriting this method I guess.\n\ncleanest way to be backwards compatible would be to not have an initial default Resolution, and use DateField if no Resolution can be found for the field specified.  Existing clients would still get DateField for all dates, clients that add a call to setDateResolution(\"foo\", BAR); would get a DateTools formated query with resolution BAR for field foo, but other pre-existing fields would still use DateField, and clients that call setDateResolution(BAR); would allways get a DateTools formatted query, either at resolution BAR or at some other resolution if they also use the two arg setDateResolution\nActually it is documented in the QueryParser how to use a different format for dates:\n\n   ...\n    * feature also assumes that your index uses the {@link DateTools} class to store dates.\n    * If you use a different format and you still want QueryParser\n    * to turn local dates in range queries into valid queries you need to create your own\n    * query parser that inherits QueryParser and overwrites\n    * {@link #getRangeQuery(String, String, String, boolean)}.\n   ...\n\nAnd the javadoc of DateField says:\n\n   Deprecated. If you build a new index, use DateTools instead. This class is included for use with existing indices and will be removed in a future release.\n\nSo the question is in how many future releases we want to support DateField. If we still want to support it in 2.1 I agree to Hoss that his suggestion would be a nice and clean way to do that and I can easily change the patch accordingly. It avoids having another option in QueryParser.\nThis new patch contains the changes suggested by Hoss:\n- by default the QueryParser uses DateField to format dates for all fields\n- if a date resolution has been set using void setDateResolution(DateTools.Resolution) then DateTools is used for all fields with the given resolution\n- if a field specific date resolution has been set using setDateResolution(String, DateTools.Resolution) then DateTools is used for the given field with the given date resolution; other fields are not affected\n\nSo with this patch the behaviour of QueryParser does not change unless either setDateResolution(DateTools.Resolution) or setDateResolution(String, DateTools.Resolution) have been called.\nChanged the summary to better reflect the second version of the patch.\n+1 for queryparser_datetools2.patch\n\nthe only nitpick i have is with the class level javadocs...\n\n  * <p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g. <tt>date:[6/1/2005 TO 6/4/2005]</tt>\n  * produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04. Note\n  * that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.\n  * By default a date is formatted using the deprecated {@link DateField} for compatibility reasons.\n  * To use the new {@link DateTools} to format dates, a {@link DateTools.Resolution} has to be set.</p>\n  * <p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)\n\n\n...the word \"format\" is used to mean two very differnet things in this paragraph, we should clean that up so it's clera that Locale determines the DateFormat QP uses when trying to parse input  in [a TO b] syntax, while either DateField or DateTools are used to create the \"indexed\" value queried against.\nYou're right Hoss, the word \"format\" is used ambiguously in the javadoc. We could change it to\n\n * <p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g. <tt>date:[6/1/2005 TO 6/4/2005]</tt>\n * produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04. Note\n * that the format of the accepted input depends on {@link #setLocale(Locale) the locale}. \n * By default a date is converted into a search term using the deprecated {@link DateField} for compatibility reasons.\n * To use the new {@link DateTools} to convert dates, a {@link DateTools.Resolution} has to be set.</p> \n * <p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)}\n * or {@link #setDateResolution(String, DateTools.Resolution)}. The former sets the default date resolution for all fields, whereas the latter can\n * be used to set field specific date resolutions. Field specific date resolutions take, if set, precedence over the default date resolution.\n * </p>\n * <p>If you use neither {@link DateField} nor {@link DateTools} in your index, you can create your own\n * query parser that inherits QueryParser and overwrites {@link #getRangeQuery(String, String, String, boolean)} to\n * use a different method for date conversion.\n * </p> \n\nSounds better? Do you want me to create another patch that includes this javadoc?\nI've commited queryparser_datetools2.patch with two changes...\n\n1) revised the class level javadocs based on Michael's last comment.\n2) reinstated the old TestQueryParser.testDateRange() as testLegacyDateRange() to attempt to future proof against breaking backwards compatibility.\n", "issueSearchSentences": ["public void setDateResolution(DateTools.Resolution dateResolution);", "@param dateResolution date resolution to set", "with {@link #setDateResolution(String, DateTools.Resolution)}.", "or {@link #setDateResolution(String, DateTools.Resolution)}.", "@param dateResolution the default date resolution to set"], "issueSearchScores": [0.9755961298942566, 0.8981526494026184, 0.8972591161727905, 0.8883832693099976, 0.8834935426712036]}
{"aId": 80, "code": "public void release(IndexSearcher s) throws IOException {\n    s.getIndexReader().decRef();\n  }", "comment": " NOTE: it's fine to call this after close.", "issueId": "LUCENE-3486", "issueStringList": ["Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used", "The idea is similar to SOLR-2809 (adding searcher leases to Solr).", "This utility class sits above whatever your source is for \"the", "current\" searcher (eg NRTManager, SearcherManager, etc.", "), and records", "(holds a reference to) each searcher in recent history.", "The idea is to ensure that when a user does a follow-on action (clicks", "next page, drills down/up), or when two or more searcher invocations", "within a single user search need to happen against the same searcher", "(eg in distributed search), you can retrieve the same searcher you", "used \"last time\".", "I think with the new searchAfter API (LUCENE-2215), doing follow-on", "searches on the same searcher is more important, since the \"bottom\"", "(score/docID) held for that API can easily shift when a new searcher", "is opened.", "When you do a \"new\" search, you record the searcher you used with the", "manager, and it returns to you a long token (currently just the", "IR.getVersion()), which you can later use to retrieve the same", "searcher.", "Separately you must periodically call prune(), to prune the old", "searchers, ideally from the same thread / at the same time that", "you open a new searcher.", "Patch.", "What a cool object, Mike !", "And the javadocs are very good too.", "Perhaps instead of \"recordTimeSec = System.nanoTime()/1000000000.0;\" you can use TimeUnit.NANOS.toSeconds?", "Just for clarity, and get rid of this monstrous number :).", "Typo: \"such that if the use performs\": 'use' --> 'user'", "About this code:", "{code}", "+    if (tracker == null) {", "+      tracker = new SearcherTracker(searcher);", "+      if (searchers.putIfAbsent(version, tracker) != null) {", "+        // Another thread beat us -- must decRef to undo", "+        // incRef done by SearcherTracker ctor:", "+        searcher.getIndexReader().decRef();", "+      }", "{code}", "Would it be better if SearcherTracker has a close() method and we call it instead of decRef()-ing on our own?", "Seems cleaner to me, and I always like to see the code that incRef/new closer to decRef/close.", "And if tomorrow SearcherTracker needs to clear other things too, we're already covered.", "About this \"// nocommit -- maybe make it 'public' that you just decRef?\"", "--> do you mean whether we should jdoc that that's all we're doing?", "If so, why commit to just that?", "I don't think it contributes to the user ...", "I have a problem with the Pruner interface.", "It has a single method prune which takes an IndexSearcher and ageSec (BTW, why is it double and not long?).", "And there's PruneByAge impl.", "But, what other impls could there be for this interface, if not by age?", "On the other hand, I can certainly see someone, perhaps w/ NRT, not wanting to keep too many searchers around, and instead of committing to an age, he'll want to use a hard number (like, the newest 5 searchers) - that interface makes it impossible to impl.", "If you think however that pruning by age, is the only scenario that makes sense, then I suggest removing the interface and having just the impl.", "Otherwise, perhaps a different interface should be created, one that receives a list of searchers, with their age, and returns a list of searchers that should be released?", "Just an idea.", "Hmm, now that I read prune(Pruner) jdoc, I can see how someone could impl \"newest 5 searchers\" by just counting up to 5 in its doPrune() calls, because prune(Pruner) guarantees that the searchers are passed newest to oldest.", "But still I wonder if the interface is not too limited.", "Looks very good !", "bq.", "Perhaps instead of \"recordTimeSec = System.nanoTime()/1000000000.0;\" you can use TimeUnit.NANOS.toSeconds?", "Just for clarity, and get rid of this monstrous number .", "I was wanting to have the seconds be a double, but, I agree that's", "overkill; I think a \"typical\" pruning time should be maybe 10 minutes", "and so having \"int seconds\" is OK.", "I'll change it and use", "TimeUnit.NANOSECONDS.toSeconds.", "bq.", "Typo: \"such that if the use performs\": 'use' --> 'user'", "Thanks I'll fix.", "bq.", "Would it be better if SearcherTracker has a close() method and we call it instead of decRef()-ing on our own?", "I agree -- I'll fix.", "bq.", "About this \"// nocommit \u2013 maybe make it 'public' that you just decRef?\"", "--> do you mean whether we should jdoc that that's all we're doing?", "If so, why commit to just that?", "I don't think it contributes to the user ...", "Well... the release is sort of \"spooky\" in that you can freely call it after close, which is why I thought about making its impl public; but I agree, let's leave it private and just keep the NOTE that it's fine to call after close.", "{quote}", "I have a problem with the Pruner interface.", "It has a single method prune which takes an IndexSearcher and ageSec (BTW, why is it double and not long?).", "And there's PruneByAge impl.", "But, what other impls could there be for this interface, if not by age?", "On the other hand, I can certainly see someone, perhaps w/ NRT, not wanting to keep too many searchers around, and instead of committing to an age, he'll want to use a hard number (like, the newest 5 searchers) - that interface makes it impossible to impl.", "If you think however that pruning by age, is the only scenario that makes sense, then I suggest removing the interface and having just the impl.", "Otherwise, perhaps a different interface should be created, one that receives a list of searchers, with their age, and returns a list of searchers that should be released?", "Just an idea.", "Hmm, now that I read prune(Pruner) jdoc, I can see how someone could impl \"newest 5 searchers\" by just counting up to 5 in its doPrune() calls, because prune(Pruner) guarantees that the searchers are passed newest to oldest.", "But still I wonder if the interface is not too limited.", "{quote}", "Right, my idea was to make it really easy to prune-by-age, since", "that's the common case, but also make it possible to do more", "expert policies.", "I think prune-by-count is possible, but maybe more interesting would", "be prune-by-total-segments-size, ie, if a large merge commits, this", "metric would cut back on the number of searchers so that the net RAM", "tied up is lower.", "Not sure this is really needed in practice as large", "merges don't complete often and it's unlikely you'd hit more than one in", "your time window...", "Thanks Shai!", "Patch w/ Shai's suggestions, and tweaked some jdocs.", "Looks good.", "I noticed you marked close() with @Override.", "Are we on Java 6 in 3.x?", "bq.", "I noticed you marked close() with @Override.", "Are we on Java 6 in 3.x?", "Sigh, no we are not; in 3.x I'll have to comment that out.", "bq.", "Looks good.", "I noticed you marked close() with @Override.", "Are we on Java 6 in 3.x?", "@Override is all over the place in Solr!", "?", "Jason: Java 5 does not allow @Override on interfaces!", "Mike: Why do we need the duplicate of IOUtils in SearcherLifetimeManager.close()?", "You can also use IOUtils.closeSafely(Collection)?", "Thanks Uwe, you're right -- new patch, to just use IOUtils.", "I also make an effort to catch mis-use (record called while close is running)."], "SplitGT": [" NOTE: it's fine to call this after close."], "issueString": "Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used\nThe idea is similar to SOLR-2809 (adding searcher leases to Solr).\n\nThis utility class sits above whatever your source is for \"the\ncurrent\" searcher (eg NRTManager, SearcherManager, etc.), and records\n(holds a reference to) each searcher in recent history.\n\nThe idea is to ensure that when a user does a follow-on action (clicks\nnext page, drills down/up), or when two or more searcher invocations\nwithin a single user search need to happen against the same searcher\n(eg in distributed search), you can retrieve the same searcher you\nused \"last time\".\n\nI think with the new searchAfter API (LUCENE-2215), doing follow-on\nsearches on the same searcher is more important, since the \"bottom\"\n(score/docID) held for that API can easily shift when a new searcher\nis opened.\n\nWhen you do a \"new\" search, you record the searcher you used with the\nmanager, and it returns to you a long token (currently just the\nIR.getVersion()), which you can later use to retrieve the same\nsearcher.\n\nSeparately you must periodically call prune(), to prune the old\nsearchers, ideally from the same thread / at the same time that\nyou open a new searcher.\nPatch.\nWhat a cool object, Mike ! And the javadocs are very good too.\n\n* Perhaps instead of \"recordTimeSec = System.nanoTime()/1000000000.0;\" you can use TimeUnit.NANOS.toSeconds? Just for clarity, and get rid of this monstrous number :).\n\n* Typo: \"such that if the use performs\": 'use' --> 'user'\n\n* About this code:\n{code}\n+    if (tracker == null) {\n+      tracker = new SearcherTracker(searcher);\n+      if (searchers.putIfAbsent(version, tracker) != null) {\n+        // Another thread beat us -- must decRef to undo\n+        // incRef done by SearcherTracker ctor:\n+        searcher.getIndexReader().decRef();\n+      }\n{code}\nWould it be better if SearcherTracker has a close() method and we call it instead of decRef()-ing on our own? Seems cleaner to me, and I always like to see the code that incRef/new closer to decRef/close. And if tomorrow SearcherTracker needs to clear other things too, we're already covered.\n\n* About this \"// nocommit -- maybe make it 'public' that you just decRef?\" --> do you mean whether we should jdoc that that's all we're doing? If so, why commit to just that? I don't think it contributes to the user ...\n\n* I have a problem with the Pruner interface. It has a single method prune which takes an IndexSearcher and ageSec (BTW, why is it double and not long?). And there's PruneByAge impl. But, what other impls could there be for this interface, if not by age?\n** On the other hand, I can certainly see someone, perhaps w/ NRT, not wanting to keep too many searchers around, and instead of committing to an age, he'll want to use a hard number (like, the newest 5 searchers) - that interface makes it impossible to impl.\n\nIf you think however that pruning by age, is the only scenario that makes sense, then I suggest removing the interface and having just the impl. Otherwise, perhaps a different interface should be created, one that receives a list of searchers, with their age, and returns a list of searchers that should be released? Just an idea.\n\nHmm, now that I read prune(Pruner) jdoc, I can see how someone could impl \"newest 5 searchers\" by just counting up to 5 in its doPrune() calls, because prune(Pruner) guarantees that the searchers are passed newest to oldest. But still I wonder if the interface is not too limited.\n\nLooks very good !\nbq. Perhaps instead of \"recordTimeSec = System.nanoTime()/1000000000.0;\" you can use TimeUnit.NANOS.toSeconds? Just for clarity, and get rid of this monstrous number .\n\nI was wanting to have the seconds be a double, but, I agree that's\noverkill; I think a \"typical\" pruning time should be maybe 10 minutes\nand so having \"int seconds\" is OK.  I'll change it and use\nTimeUnit.NANOSECONDS.toSeconds.\n\nbq. Typo: \"such that if the use performs\": 'use' --> 'user'\n\nThanks I'll fix.\n\nbq. Would it be better if SearcherTracker has a close() method and we call it instead of decRef()-ing on our own?\n\nI agree -- I'll fix.\n\nbq. About this \"// nocommit \u2013 maybe make it 'public' that you just decRef?\" --> do you mean whether we should jdoc that that's all we're doing? If so, why commit to just that? I don't think it contributes to the user ...\n\nWell... the release is sort of \"spooky\" in that you can freely call it after close, which is why I thought about making its impl public; but I agree, let's leave it private and just keep the NOTE that it's fine to call after close.\n\n{quote}\nI have a problem with the Pruner interface. It has a single method prune which takes an IndexSearcher and ageSec (BTW, why is it double and not long?). And there's PruneByAge impl. But, what other impls could there be for this interface, if not by age?\nOn the other hand, I can certainly see someone, perhaps w/ NRT, not wanting to keep too many searchers around, and instead of committing to an age, he'll want to use a hard number (like, the newest 5 searchers) - that interface makes it impossible to impl.\nIf you think however that pruning by age, is the only scenario that makes sense, then I suggest removing the interface and having just the impl. Otherwise, perhaps a different interface should be created, one that receives a list of searchers, with their age, and returns a list of searchers that should be released? Just an idea.\n\nHmm, now that I read prune(Pruner) jdoc, I can see how someone could impl \"newest 5 searchers\" by just counting up to 5 in its doPrune() calls, because prune(Pruner) guarantees that the searchers are passed newest to oldest. But still I wonder if the interface is not too limited.\n{quote}\n\nRight, my idea was to make it really easy to prune-by-age, since\nthat's the common case, but also make it possible to do more\nexpert policies.\n\nI think prune-by-count is possible, but maybe more interesting would\nbe prune-by-total-segments-size, ie, if a large merge commits, this\nmetric would cut back on the number of searchers so that the net RAM\ntied up is lower.  Not sure this is really needed in practice as large\nmerges don't complete often and it's unlikely you'd hit more than one in\nyour time window...\n\nThanks Shai!\n\nPatch w/ Shai's suggestions, and tweaked some jdocs.\nLooks good. I noticed you marked close() with @Override. Are we on Java 6 in 3.x?\nbq. I noticed you marked close() with @Override. Are we on Java 6 in 3.x?\n\nSigh, no we are not; in 3.x I'll have to comment that out.\nbq. Looks good. I noticed you marked close() with @Override. Are we on Java 6 in 3.x?\n\n@Override is all over the place in Solr!?\nJason: Java 5 does not allow @Override on interfaces!\nMike: Why do we need the duplicate of IOUtils in SearcherLifetimeManager.close()? You can also use IOUtils.closeSafely(Collection)?\nThanks Uwe, you're right -- new patch, to just use IOUtils.  I also make an effort to catch mis-use (record called while close is running).\n", "issueSearchSentences": ["+        searcher.getIndexReader().decRef();", "you open a new searcher.", "Well... the release is sort of \"spooky\" in that you can freely call it after close, which is why I thought about making its impl public; but I agree, let's leave it private and just keep the NOTE that it's fine to call after close.", "is opened.", "I noticed you marked close() with @Override."], "issueSearchScores": [0.44243931770324707, 0.4002915024757385, 0.39540690183639526, 0.3856947422027588, 0.3526138365268707]}
{"aId": 83, "code": "public static QParser getParser(String qstr, String defaultParser, SolrQueryRequest req) throws ParseException {\n    // SolrParams localParams = QueryParsing.getLocalParams(qstr, req.getParams());\n\n    String stringIncludingLocalParams = qstr;\n    SolrParams localParams = null;\n    SolrParams globalParams = req.getParams();\n    boolean valFollowedParams = true;\n    int localParamsEnd = -1;\n\n    if (qstr != null && qstr.startsWith(QueryParsing.LOCALPARAM_START)) {\n      Map<String, String> localMap = new HashMap<String, String>();\n      localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localMap, globalParams);\n\n      String val = localMap.get(QueryParsing.V);\n      if (val != null) {\n        // val was directly specified in localParams via v=<something> or v=$arg\n        valFollowedParams = false;\n      } else {\n        // use the remainder of the string as the value\n        valFollowedParams = true;\n        val = qstr.substring(localParamsEnd);\n        localMap.put(QueryParsing.V, val);\n      }\n      localParams = new MapSolrParams(localMap);\n    }\n\n\n    String parserName;\n    \n    if (localParams == null) {\n      parserName = defaultParser;\n    } else {\n      parserName = localParams.get(QueryParsing.TYPE,defaultParser);\n      qstr = localParams.get(\"v\");\n    }\n\n    parserName = parserName==null ? QParserPlugin.DEFAULT_QTYPE : parserName;\n\n    QParserPlugin qplug = req.getCore().getQueryPlugin(parserName);\n    QParser parser =  qplug.createParser(qstr, localParams, req.getParams(), req);\n\n    parser.stringIncludingLocalParams = stringIncludingLocalParams;\n    parser.valFollowedParams = valFollowedParams;\n    parser.localParamsEnd = localParamsEnd;\n    return parser;\n  }", "comment": " Create a QParser to parse qstr, assuming that the default query parser is defaultParser. The query parser may be overridden by local parameters in the query string itself. For example if defaultParser=\"dismax\" and qstr=foo, then the dismax query parser will be used to parse and construct the query object. However if qstr={!prefix f=myfieldfoo then the prefix query parser will be used.", "issueId": "SOLR-3313", "issueStringList": ["Rename \"Query Type\" to \"Request Handler\" in SolrJ APIs", "Nobody should speak of \"query types\" any more; it's \"request handlers\".", "I understand we want to retain the \"qt\" parameter as such but I think we should change the names of it wherever we can find it.", "We can leave some older API methods in place as deprecated.", "As an example, in SolrJ I have to call solrQuery.setQueryType(\"/blah\") instead of setRequestHandler()", "Yeah, that makes sense.", "The Admin UI was already updated to reflect this in another issue, so clarifying scope of summary to be specific about SolrJ.", "I'm seeing numerous usages of \"query type\" as a reference to the query parser too :-(   Ugh.", "I'll clean those up to be \"query parser\" or similar based on context.", "An example in QParser.getParser()  (my emphasis added with bold):", "BEFORE:{quote}", "Create a <code>QParser</code> to parse <code>qstr</code>,", "assuming that the default *query type* is <code>*defaultType*</code>.", "The *query type* may be overridden by local parameters in the query", "string itself.", "For example if *defaultType*=<code>\"dismax\"</code>", "and qstr=<code>foo</code>, then the dismax query parser will be used", "to parse and construct the query object.", "However", "if qstr=<code>\\{!prefix f=myfield}foo</code>", "then the prefix query parser will be used.", "public static QParser getParser(String qstr, String *defaultType*, SolrQueryRequest req) throws ParseException", "{quote}", "AFTER: {quote}", "Create a <code>QParser</code> to parse <code>qstr</code>,", "assuming that the default *query parser* is <code>*defaultParser*</code>.", "The *query parser* may be overridden by local parameters in the query", "string itself.", "For example if *defaultParser*=<code>\"dismax\"</code>", "and qstr=<code>foo</code>, then the dismax query parser will be used", "to parse and construct the query object.", "However", "if qstr=<code>\\{!prefix f=myfield}foo</code>", "then the prefix query parser will be used.", "public static QParser getParser(String qstr, String *defaultParser*, SolrQueryRequest req) throws ParseException", "{quote}", "Attached is a patch focusing on the renames where the Request Handler was intended.", "I will use a separate patch for cases where the query parser was intended.", "I committed SOLR-3313_Rename_Query_Type_to_Request_Handler.patch to trunk r1351932 and to 4x r1351933.", "And attached are renames related to \"query type\" being a reference to the parser.", "I made some wiki changes here and there too, at least to some of the more current wiki pages.", "Committed to trunk in r1352760 and 4x in r1352761.", "Closing."], "SplitGT": [" Create a QParser to parse qstr, assuming that the default query parser is defaultParser.", "The query parser may be overridden by local parameters in the query string itself.", "For example if defaultParser=\"dismax\" and qstr=foo, then the dismax query parser will be used to parse and construct the query object.", "However if qstr={!prefix f=myfieldfoo then the prefix query parser will be used."], "issueString": "Rename \"Query Type\" to \"Request Handler\" in SolrJ APIs\nNobody should speak of \"query types\" any more; it's \"request handlers\".  I understand we want to retain the \"qt\" parameter as such but I think we should change the names of it wherever we can find it.  We can leave some older API methods in place as deprecated.\n\nAs an example, in SolrJ I have to call solrQuery.setQueryType(\"/blah\") instead of setRequestHandler()\n\n\nYeah, that makes sense.\nThe Admin UI was already updated to reflect this in another issue, so clarifying scope of summary to be specific about SolrJ.\n\nI'm seeing numerous usages of \"query type\" as a reference to the query parser too :-(   Ugh.  I'll clean those up to be \"query parser\" or similar based on context.  An example in QParser.getParser()  (my emphasis added with bold):\n\nBEFORE:{quote}\n  Create a <code>QParser</code> to parse <code>qstr</code>,\n  assuming that the default *query type* is <code>*defaultType*</code>.\n  The *query type* may be overridden by local parameters in the query\n  string itself.  For example if *defaultType*=<code>\"dismax\"</code>\n  and qstr=<code>foo</code>, then the dismax query parser will be used\n  to parse and construct the query object.  However\n  if qstr=<code>\\{!prefix f=myfield}foo</code>\n  then the prefix query parser will be used.\n\n  public static QParser getParser(String qstr, String *defaultType*, SolrQueryRequest req) throws ParseException\n{quote}\nAFTER: {quote}\n  Create a <code>QParser</code> to parse <code>qstr</code>,\n  assuming that the default *query parser* is <code>*defaultParser*</code>.\n  The *query parser* may be overridden by local parameters in the query\n  string itself.  For example if *defaultParser*=<code>\"dismax\"</code>\n  and qstr=<code>foo</code>, then the dismax query parser will be used\n  to parse and construct the query object.  However\n  if qstr=<code>\\{!prefix f=myfield}foo</code>\n  then the prefix query parser will be used.\n\n  public static QParser getParser(String qstr, String *defaultParser*, SolrQueryRequest req) throws ParseException\n{quote}\nAttached is a patch focusing on the renames where the Request Handler was intended.  I will use a separate patch for cases where the query parser was intended.\nI committed SOLR-3313_Rename_Query_Type_to_Request_Handler.patch to trunk r1351932 and to 4x r1351933. \nAnd attached are renames related to \"query type\" being a reference to the parser.\n\nI made some wiki changes here and there too, at least to some of the more current wiki pages.\nCommitted to trunk in r1352760 and 4x in r1352761.  Closing.\n", "issueSearchSentences": ["public static QParser getParser(String qstr, String *defaultParser*, SolrQueryRequest req) throws ParseException", "public static QParser getParser(String qstr, String *defaultType*, SolrQueryRequest req) throws ParseException", "assuming that the default *query parser* is <code>*defaultParser*</code>.", "The *query parser* may be overridden by local parameters in the query", "Create a <code>QParser</code> to parse <code>qstr</code>,"], "issueSearchScores": [0.8740330934524536, 0.8595069646835327, 0.7024897933006287, 0.6600519418716431, 0.6371122598648071]}
{"aId": 84, "code": "public Object getFieldValue(String name) \n  {\n    SolrInputField field = getField(name);\n    Object o = null;\n    if (field!=null) o = field.getFirstValue();\n    return o;\n  }", "comment": " Get the first value for a field.", "issueId": "SOLR-280", "issueStringList": ["slightly more efficient SolrDocument implementation", "Following discussion in SOLR-272", "This implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.", "The API changes slightly in that:", "getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.", "getFirstValue( name ) returns a single value for the field.", "This is intended to make things easier for client applications.", "We could go further and store boosted values as:", "class BoostedValue {", "float boost;", "Object value;", "}", "but I think that makes the implementation to convoluted.", "If we went this route, we would need to check each value before passing it back to the client.", "I think one should be able to simply set a field value without a copy being made:", "public Object setField(String name, Collection value)  {", "return _fields.put(name, value);", "}", "One area I'm concerned about performance is on the output side of things.", "If we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.", "Just something to keep in mind.", "Another thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)", "This is a new implementation where SolrInputDocument *does not* extend SolrDocument.", "This way each can be optimized for how they are most frequently used.", "This adds:", "public class SolrInputField", "{", "final String name;", "float boost = 1.0f;", "Object value = null;", "}", "and SolrInputDocument keeps a Map<String,SolrInputField>", "This still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later.", ">The API changes mostly affect solrj users.", "being one of those heavily affected users i created the attached patch to make us unaffected.", "(or at least i went from a few hundred compile errors to 0)", "the following methods were added back and are mostly 1-5 line wrappers to the existing methods or underlying datastructures.", "setField(String, Object)", "getFieldValue(String)", "getFieldValues(String)", "addField(String, Object)", "getFieldNames()", "will", "added in r552514", "thanks Will", "in rev552521, I changed the Float variables to float and default everything to 1.0.  if we have wrapper functions, this seems better then autoboxing/checking null values.", "I agree... things are looking good guys!"], "SplitGT": [" Get the first value for a field."], "issueString": "slightly more efficient SolrDocument implementation\nFollowing discussion in SOLR-272\n\nThis implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.  The API changes slightly in that:\n\n getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.\n\ngetFirstValue( name ) returns a single value for the field.  This is intended to make things easier for client applications.\nWe could go further and store boosted values as:\n\nclass BoostedValue {\n  float boost;\n  Object value;\n} \n\nbut I think that makes the implementation to convoluted.  If we went this route, we would need to check each value before passing it back to the client.\nI think one should be able to simply set a field value without a copy being made:\n \npublic Object setField(String name, Collection value)  {\n  return _fields.put(name, value);\n}\n\nOne area I'm concerned about performance is on the output side of things.\nIf we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.  Just something to keep in mind.\n\nAnother thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)\n\nThis is a new implementation where SolrInputDocument *does not* extend SolrDocument.  This way each can be optimized for how they are most frequently used.  \n\nThis adds:\n\npublic class SolrInputField \n{\n  final String name;\n  float boost = 1.0f;\n  Object value = null;\n}\n\nand SolrInputDocument keeps a Map<String,SolrInputField>\n\nThis still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later.\n\n>The API changes mostly affect solrj users. \n\nbeing one of those heavily affected users i created the attached patch to make us unaffected.  (or at least i went from a few hundred compile errors to 0)\n\nthe following methods were added back and are mostly 1-5 line wrappers to the existing methods or underlying datastructures.\n\nsetField(String, Object)\ngetFieldValue(String)\ngetFieldValues(String)\naddField(String, Object)\ngetFieldNames() \n\n- will\nadded in r552514\n\nthanks Will\nin rev552521, I changed the Float variables to float and default everything to 1.0.  if we have wrapper functions, this seems better then autoboxing/checking null values.\nI agree... things are looking good guys!\n", "issueSearchSentences": ["getFirstValue( name ) returns a single value for the field.", "getFieldValue(String)", "getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.", "getFieldValues(String)", "Object value = null;"], "issueSearchScores": [0.7825433611869812, 0.7341588139533997, 0.7238409519195557, 0.5650429129600525, 0.5557817220687866]}
{"aId": 86, "code": "public void close() throws IOException {\n    synchronized (searcherLock) {\n      ensureOpen();\n      closed = true;\n      if (searcher != null) {\n        searcher.close();\n      }\n      searcher = null;\n    }\n  }", "comment": " Close the IndexSearcher used by this SpellChecker", "issueId": "LUCENE-2108", "issueStringList": ["SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally", "I can't find any way to close the IndexSearcher (and IndexReader) that", "is being used by SpellChecker internally.", "I've worked around this issue by keeping a single SpellChecker open", "for each index, but I'd really like to be able to close it and", "reopen it on demand without leaking file descriptors.", "Could we add a close() method to SpellChecker that will close the", "IndexSearcher and null the reference to it?", "And perhaps add some code", "that reopens the searcher if the reference to it is null?", "Or would", "that break thread safety of SpellChecker?", "The attached patch adds a close method but leaves it to the user to", "call setSpellIndex to reopen the searcher if desired.", "Patch that adds a close method to SpellChecker.", "The method calls close on the searcher used and then nulls the reference so that a new IndexSearcher will be created by the next call to setSpellIndex", "Shouldn't the new close() method be public?", "Haha, this is why I said the patch should be \"pretty\" trivial, instead of just \"trivial\" :-)", "Yes, it should certainly be private.", "No idea how that happend.", "Must have been sleeping at the keyboad.", "Note that you said \"private\" again ;)  I'm starting to wonder if you are not human!", "Is this a turing test?", "OK, ok, I'll make it public, and port back to the 3.0 branch!", "Dude, you have be to a human to make mistakes as stupid as these!", "(pubic void close, public void close, public void close...)", "bq.", "Dude, you have be to a human to make mistakes as stupid as these!", "Good point :)", "Thanks Eirik!", "Mike / Eirik,", "If you set the searcher to null you might risk a NPE if suggestSimilar() or other methods are called afterwards.", "I would like to see something like ensureOpen() which throws an AlreadyClosedException  or something similar.", "I will upload a suggestion in a second but need to run so tread it just as a suggestion.", "Simon", "Something like that would be more appropriate IMO", "Simon,", "Yes, that sound excactly like what I was thinking when I said \"some code", "that reopens the searcher if the reference to it is null\".", "I just didn't include it in my patch because I couldn't figure out how to do it properly.", "I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?", "Reopening to get the AlreadyClosedException in there...", "Well not exactly.", "Simon's suggestion was just to throw an AlreadyClosedException instead of a NullPointerException which is probably ok and definitely easier.", "bq.", "I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?", "this class is not threadsafe anyway.", "If you look at this snippet:", "{code}", "close the old searcher, if there was one", "if (searcher != null) {", "searcher.close();", "}", "searcher = new IndexSearcher(this.spellIndex, true);", "{code}", "there could be a race if you concurrently reindex or set a new dictionary.", "IMO this should either be documented or made threadsafe.", "The close method should invalidate the spellchecker - it should not be possible to use a already closed Spellchecker.", "The searcher should be somehow ref counted so that if there is a searcher still in use you can concurrently reindex / add a new dictionary to ensure that the same searcher is used throughout suggestSimilar().", "I will take care of it once I get back tomorrow.", "Just a reminder - we need to fix the CHANGES.TXT entry once this is done.", "Mike,", "Please account for my demonstrated stupidity when considering this suggestion for thread safety policy / goals:", "1) Concurrent invocations of  suggestSimilar() should not interfere with each other.", "2) An invocation of any of the write methods (setSpellIndex, clearIndex, indexDictionary) should not interfere with aleady invoked suggestSimilar", "3) All calls to write methods should be serialized (We could probably synchronize these methods?)", "If we synchronize any writes to the searcher reference, couldn't suggestSimilar just start its work by putting searcher in a local variable and use that instead of the field?", "I guess concurrency is hard to get right..", "Eirik, could you open a new issue to address SpellChecker's non-thread-safety?", "I actually thing simply documenting clearly that it's not thread safe is fine.", "bq.", "Just a reminder - we need to fix the CHANGES.TXT entry once this is done.", "Simon how about you do this, and take this issue (to commit your improvement to throw ACE not NPE)?", "Thanks ;)", "bq.", "Eirik, could you open a new issue to address SpellChecker's non-thread-safety?", "I actually thing simply documenting clearly that it's not thread safe is fine.", "Mike, IMO thread-safety and close() are closely related.", "If you close the spellchecker you don't want other threads to access the spellchecker anymore.", "I would keep that inside this issue and rather add close + make it threadsafe in one go.", "Since spellchecker instances are shared between threads already we should rather try to make it thread-safe than documenting it.", "I see this as a bug though as you really want to share the spellchecker (essentially the searcher) instance instead of opening one for each thread.", "bq.", "I would keep that inside this issue and rather add close + make it threadsafe in one go.", "OK that sounds good then!", "This patch adds a close operation to SpellChecker and enables thread-safety.", "I added a testcase for concurrency as well as the close method - comments and review welcome!", "Some feedback on the patch:", "If you back-port this to 2.9, you can't use any of the", "java.util.concurrent.", "*", "I'm not sure you need a separate SearcherHolder class -- can't you", "re-use IndexReader's decRef/incRef?", "You don't need to do this.XXX in most places (maybe you're coming", "from eg Python?", ";) ).", "Maybe rename \"releaseNewSearcher\" -> \"swapSearcher\"?", "(Because it", "releases the old one and installs the new one).", "I think there are some thread safety issues -- eg", "getSearcherHolder isn't sync'd, so, when you incRef", "this.searcherHolder at the start, but then return", "this.searcherHolder at the end, it could be two different", "instances.", "bq.", "If you back-port this to 2.9, you can't use any of the java.util.concurrent.", "*", "Very good point!", "- didn't thought about back porting at all.", "bq.", "I'm not sure you need a separate SearcherHolder class - can't you re-use IndexReader's decRef/incRef?", "I guess I did not see the simplicity the reader offers - blind due to  java.util.concurrent.", "* :)", "bq.", "I think there are some thread safety issues..", "this is weird - on my dev machine and in the patch it is not synchronized.. on the machine I run the tests it is.", "Anyway you are right.", "I changed the code to be compatible with 2.9 using indexReaders.dec/incRef.. will attache in a minute", "updated patch - this one does not use a Holder class or any java 5 classes for backcompat with 2.9", "Looks good!", "Nice and simple.", "Only small concern... you hold the lock while opening the new searcher.", "It would be better to open the new searcher without the lock, then only acquire the lock to do the swap; this way any respell requests that come in don't block while the searcher is being opened (because obtainSearcher() needs to get the lock).", "bq.", "Only small concern... you hold the lock while opening the new searcher....", "I fixed that one - very important for performance though!", "I found another issue, write access is not synchronized at all so it is possible to concurrently reindex or at least call indexDictionary() concurrently.", "In the first place this is not a huge issue as the writer will raise an exception but if the spellIndex is reset while the indexDicitonary is still in progress we could have inconsistencies with searcher and spellindex.", "I added another lock for write operations for now,", "Patch looks good Simon!", "I will commit this tomorrow if nobody objects.", "committed in revision 887532", "Mike, thanks for review.", "We should backport this change to 2.9 - can you commit that please, I can not though.", "bq.", "We should backport this change to 2.9 - can you commit that please, I can not though.", "And, to 3.0.", "OK will do...", "Mike, I just realized that we need to change the test as it uses java5", "classes.", "I will provide you a patch compatible w. 1.4 later.", "On Dec 5, 2009 2:35 PM, \"Michael McCandless (JIRA)\" <jira@apache.org> wrote:", "[", "https://issues.apache.org/jira/browse/LUCENE-2108?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12786406#action_12786406]", "Michael McCandless commented on LUCENE-2108:", "bq.", "We should backport this change to 2.9 - can you commit that please, I", "can not though.", "And, to 3.0.", "OK will do...", "by SpellChecker internally", "LUCENE-2108.patch, LUCENE-2108.patch, LUCENE-2108.patch, LUCENE-2108.patch", "This message is automatically generated by JIRA.", "You can reply to this email to add a comment to the issue online."], "SplitGT": [" Close the IndexSearcher used by this SpellChecker"], "issueString": "SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally\nI can't find any way to close the IndexSearcher (and IndexReader) that\nis being used by SpellChecker internally.\n\nI've worked around this issue by keeping a single SpellChecker open\nfor each index, but I'd really like to be able to close it and\nreopen it on demand without leaking file descriptors.\n\nCould we add a close() method to SpellChecker that will close the\nIndexSearcher and null the reference to it? And perhaps add some code\nthat reopens the searcher if the reference to it is null? Or would\nthat break thread safety of SpellChecker?\n\nThe attached patch adds a close method but leaves it to the user to\ncall setSpellIndex to reopen the searcher if desired.\nPatch that adds a close method to SpellChecker. The method calls close on the searcher used and then nulls the reference so that a new IndexSearcher will be created by the next call to setSpellIndex\nShouldn't the new close() method be public?\nHaha, this is why I said the patch should be \"pretty\" trivial, instead of just \"trivial\" :-)\n\nYes, it should certainly be private. No idea how that happend. Must have been sleeping at the keyboad.\nNote that you said \"private\" again ;)  I'm starting to wonder if you are not human!  Is this a turing test?\n\nOK, ok, I'll make it public, and port back to the 3.0 branch!\n\nDude, you have be to a human to make mistakes as stupid as these!\n\n(pubic void close, public void close, public void close...)\nbq. Dude, you have be to a human to make mistakes as stupid as these!\n\nGood point :)\n\n\nThanks Eirik!\nMike / Eirik,\n\nIf you set the searcher to null you might risk a NPE if suggestSimilar() or other methods are called afterwards. I would like to see something like ensureOpen() which throws an AlreadyClosedException  or something similar. I will upload a suggestion in a second but need to run so tread it just as a suggestion.\n\nSimon \nSomething like that would be more appropriate IMO\nSimon,\n\nYes, that sound excactly like what I was thinking when I said \"some code\nthat reopens the searcher if the reference to it is null\".\n\nI just didn't include it in my patch because I couldn't figure out how to do it properly.\n\nI'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?\n\n\nReopening to get the AlreadyClosedException in there...\nWell not exactly. Simon's suggestion was just to throw an AlreadyClosedException instead of a NullPointerException which is probably ok and definitely easier.\nbq. I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?\n\nthis class is not threadsafe anyway. If you look at this snippet:\n{code}\n // close the old searcher, if there was one\nif (searcher != null) {\n  searcher.close();\n}\nsearcher = new IndexSearcher(this.spellIndex, true);\n{code}\nthere could be a race if you concurrently reindex or set a new dictionary. IMO this should either be documented or made threadsafe. The close method should invalidate the spellchecker - it should not be possible to use a already closed Spellchecker.\nThe searcher should be somehow ref counted so that if there is a searcher still in use you can concurrently reindex / add a new dictionary to ensure that the same searcher is used throughout suggestSimilar(). \n\nI will take care of it once I get back tomorrow.\nJust a reminder - we need to fix the CHANGES.TXT entry once this is done. \nMike,\n\nPlease account for my demonstrated stupidity when considering this suggestion for thread safety policy / goals:\n\n1) Concurrent invocations of  suggestSimilar() should not interfere with each other.\n2) An invocation of any of the write methods (setSpellIndex, clearIndex, indexDictionary) should not interfere with aleady invoked suggestSimilar\n3) All calls to write methods should be serialized (We could probably synchronize these methods?)\n\nIf we synchronize any writes to the searcher reference, couldn't suggestSimilar just start its work by putting searcher in a local variable and use that instead of the field?\n\nI guess concurrency is hard to get right.. \nEirik, could you open a new issue to address SpellChecker's non-thread-safety?  I actually thing simply documenting clearly that it's not thread safe is fine.\nbq. Just a reminder - we need to fix the CHANGES.TXT entry once this is done.\n\nSimon how about you do this, and take this issue (to commit your improvement to throw ACE not NPE)?  Thanks ;)\nbq. Eirik, could you open a new issue to address SpellChecker's non-thread-safety? I actually thing simply documenting clearly that it's not thread safe is fine.\nMike, IMO thread-safety and close() are closely related. If you close the spellchecker you don't want other threads to access the spellchecker anymore. I would keep that inside this issue and rather add close + make it threadsafe in one go.\nSince spellchecker instances are shared between threads already we should rather try to make it thread-safe than documenting it. I see this as a bug though as you really want to share the spellchecker (essentially the searcher) instance instead of opening one for each thread.\nbq.  I would keep that inside this issue and rather add close + make it threadsafe in one go.\n\nOK that sounds good then!\nThis patch adds a close operation to SpellChecker and enables thread-safety.\nI added a testcase for concurrency as well as the close method - comments and review welcome!\nSome feedback on the patch:\n\n  * If you back-port this to 2.9, you can't use any of the\n    java.util.concurrent.*\n\n  * I'm not sure you need a separate SearcherHolder class -- can't you\n    re-use IndexReader's decRef/incRef?\n\n  * You don't need to do this.XXX in most places (maybe you're coming\n    from eg Python? ;) ).\n\n  * Maybe rename \"releaseNewSearcher\" -> \"swapSearcher\"?  (Because it\n    releases the old one and installs the new one).\n\n  * I think there are some thread safety issues -- eg\n    getSearcherHolder isn't sync'd, so, when you incRef\n    this.searcherHolder at the start, but then return\n    this.searcherHolder at the end, it could be two different\n    instances.\n\nbq. If you back-port this to 2.9, you can't use any of the java.util.concurrent.*\nVery good point! - didn't thought about back porting at all.\n\n\nbq. I'm not sure you need a separate SearcherHolder class - can't you re-use IndexReader's decRef/incRef?\nI guess I did not see the simplicity the reader offers - blind due to  java.util.concurrent.* :)\n\nbq. I think there are some thread safety issues.. \nthis is weird - on my dev machine and in the patch it is not synchronized.. on the machine I run the tests it is. Anyway you are right.\n\n\nI changed the code to be compatible with 2.9 using indexReaders.dec/incRef.. will attache in a minute\n\n\n\nupdated patch - this one does not use a Holder class or any java 5 classes for backcompat with 2.9\nLooks good!  Nice and simple.\n\nOnly small concern... you hold the lock while opening the new searcher.  It would be better to open the new searcher without the lock, then only acquire the lock to do the swap; this way any respell requests that come in don't block while the searcher is being opened (because obtainSearcher() needs to get the lock).\nbq. Only small concern... you hold the lock while opening the new searcher....\nI fixed that one - very important for performance though!\n\nI found another issue, write access is not synchronized at all so it is possible to concurrently reindex or at least call indexDictionary() concurrently. In the first place this is not a huge issue as the writer will raise an exception but if the spellIndex is reset while the indexDicitonary is still in progress we could have inconsistencies with searcher and spellindex.\nI added another lock for write operations for now, \nPatch looks good Simon!\nI will commit this tomorrow if nobody objects.\ncommitted in revision 887532\n\nMike, thanks for review. We should backport this change to 2.9 - can you commit that please, I can not though.\nbq. We should backport this change to 2.9 - can you commit that please, I can not though.\n\nAnd, to 3.0.  OK will do...\nMike, I just realized that we need to change the test as it uses java5\nclasses. I will provide you a patch compatible w. 1.4 later.\n\nOn Dec 5, 2009 2:35 PM, \"Michael McCandless (JIRA)\" <jira@apache.org> wrote:\n\n\n   [\nhttps://issues.apache.org/jira/browse/LUCENE-2108?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12786406#action_12786406]\n\nMichael McCandless commented on LUCENE-2108:\n--------------------------------------------\n\nbq. We should backport this change to 2.9 - can you commit that please, I\ncan not though.\n\nAnd, to 3.0.  OK will do...\n\nby SpellChecker internally\n-----------------------------------------------------------------------------------------------------\nLUCENE-2108.patch, LUCENE-2108.patch, LUCENE-2108.patch, LUCENE-2108.patch\n\n--\nThis message is automatically generated by JIRA.\n-\nYou can reply to this email to add a comment to the issue online.\n\n", "issueSearchSentences": ["searcher.close();", "Mike, IMO thread-safety and close() are closely related.", "I can't find any way to close the IndexSearcher (and IndexReader) that", "Shouldn't the new close() method be public?", "close the old searcher, if there was one"], "issueSearchScores": [0.699474573135376, 0.6781275868415833, 0.6740102767944336, 0.6509033441543579, 0.6333561539649963]}
{"aId": 88, "code": "public boolean isGreedy() {\n    return greedy;\n  }", "comment": " A greedy one would first allow the wrapped hit collector to collect current doc and only then throw a TimeExceededException.", "issueId": "LUCENE-1238", "issueStringList": ["intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly tests", "Occasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails.", "e.g.", "with this output:", "{noformat}", "[junit] ------------- Standard Error -----------------", "[junit] Exception in thread \"Thread-97\" junit.framework.AssertionFailedError: no hits found!", "[junit]     at junit.framework.Assert.fail(Assert.java:47)", "[junit]     at junit.framework.Assert.assertTrue(Assert.java:20)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)", "[junit] Exception in thread \"Thread-85\" junit.framework.AssertionFailedError: no hits found!", "[junit]     at junit.framework.Assert.fail(Assert.java:47)", "[junit]     at junit.framework.Assert.assertTrue(Assert.java:20)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)", "[junit] ------------- ---------------- ---------------", "[junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED", "[junit] some threads failed!", "expected:<50> but was:<48>", "[junit] junit.framework.AssertionFailedError: some threads failed!", "expected:<50> but was:<48>", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)", "[junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)", "[junit]", "{noformat}", "Problem either in test or in TimeLimitedCollector.", "fixed typo in summary.", "Problem was in test.", "However fix adds a greediness option to time-limited-collector (TLC):", "A greedy TLC upon timeout would allow the wrapped collector to", "collect current doc and only then throw the timeout exception.", "A non greedy TLC (the default, as before) will immediately throw the exception.", "For the test, setting to greedy allows to require that at least one doc was collected.", "I addition this patch:", "Adds missing javadocs for TLC constructor.", "Increase \"slack\" in timeout requirements in the test.", "This is to prevent further noise in this:", "HLC is required to timeout \"not too soon and not too late\", but in a busy machine", "the \"not too late\" part is problematic to test.", "I considered to removing this part (not too late), but decided to leave it in for now.", "Adds a test for the setGreedy() option.", "All TLC tests pass.", "I intend to commit this later today.", "Committed."], "SplitGT": [" A greedy one would first allow the wrapped hit collector to collect current doc and only then throw a TimeExceededException."], "issueString": "intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly tests\nOccasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:\n\n{noformat}\n   [junit] ------------- Standard Error -----------------\n   [junit] Exception in thread \"Thread-97\" junit.framework.AssertionFailedError: no hits found!\n   [junit]     at junit.framework.Assert.fail(Assert.java:47)\n   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)\n   [junit] Exception in thread \"Thread-85\" junit.framework.AssertionFailedError: no hits found!\n   [junit]     at junit.framework.Assert.fail(Assert.java:47)\n   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)\n   [junit] ------------- ---------------- ---------------\n   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED\n   [junit] some threads failed! expected:<50> but was:<48>\n   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)\n   [junit]\n{noformat}\n\nProblem either in test or in TimeLimitedCollector.\nfixed typo in summary.\nProblem was in test.\n\nHowever fix adds a greediness option to time-limited-collector (TLC):\n* A greedy TLC upon timeout would allow the wrapped collector to \n  collect current doc and only then throw the timeout exception.\n* A non greedy TLC (the default, as before) will immediately throw the exception.\n\nFor the test, setting to greedy allows to require that at least one doc was collected.\n\nI addition this patch:\n* Adds missing javadocs for TLC constructor.\n* Increase \"slack\" in timeout requirements in the test.\n  This is to prevent further noise in this: \n     HLC is required to timeout \"not too soon and not too late\", but in a busy machine \n     the \"not too late\" part is problematic to test.\n     I considered to removing this part (not too late), but decided to leave it in for now.\n* Adds a test for the setGreedy() option.\n\nAll TLC tests pass.\nI intend to commit this later today.\nCommitted. \n", "issueSearchSentences": ["Adds a test for the setGreedy() option.", "For the test, setting to greedy allows to require that at least one doc was collected.", "expected:<50> but was:<48>", "expected:<50> but was:<48>", "Problem was in test."], "issueSearchScores": [0.4935169219970703, 0.37685319781303406, 0.36471354961395264, 0.36471354961395264, 0.3282589316368103]}
{"aId": 90, "code": "protected final boolean incrementBaseToken() throws IOException {\n    stackSize = 0;\n    graphDepth = 0;\n    graphPos = 0;\n    Token oldBase = baseToken;\n    baseToken = nextTokenInStream(baseToken);\n    if (baseToken == null) {\n      return false;\n    }\n    currentGraph.clear();\n    currentGraph.add(baseToken);\n    baseToken.attSource.copyTo(this);\n    recycleToken(oldBase);\n    return true;\n  }", "comment": " Move the root of the graph to the next token in the wrapped TokenStream", "issueId": "LUCENE-8564", "issueStringList": ["Make it easier to iterate over graphs in tokenstreams", "We have a number of TokenFilters that read ahead in the token stream (eg synonyms, shingles) and ideally these would understand token graphs as well as linear streams.", "FixedShingleFilter already has some mechanisms to deal with graphs; this issue is to extract this logic into a GraphTokenStream class that can then be reused by other token filters", "Here is a patch adding a GraphTokenStream class.", "The class wraps an underlying token stream, and then exposes tokens via the following methods:", "incrementBaseToken() : moves the starting point of the graph forwards", "incrementGraphToken() : moves along the currently selected path through the token graph", "incrementGraph() : resets back to the base token, and selects the next path to move along.", "Returns false if all paths have been exhausted", "The patch also reimplements FixedShingleFilter using GraphTokenStream, to illustrate how much easier it is to reason about how things work.", "To protect against misuse, there are hard limits on how far ahead in the stream tokens will be read and cached, and the number of paths through the graph that can be followed from a single base token", "This sounds great \u2013 we need to make it easier to work with graph token streams!", "How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?", "bq.", "How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?", "We'd end up with extra routes through the graph available via incrementGraph()", "Let's imagine a TokenStream that looks like this: z a/b:4 c d/e:2 f g h", "Starting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h", "Then we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e", "Call incrementGraph() again; we get z b g h", "Now that all routes have been exhausted, calling incrementGraph() will return false.", "How many routes are available depends on how far down the graph you move; if in the example above you only advance as far as 'z a c' on the first branch, then incrementGraph() will move directly to the 'a b g' branch.", "I reworked this slightly; we now have GraphTokenFilter as an abstract extension of TokenFilter, rather than using a separate standalone object.", "This makes reset()/end() easier to handle.", "Updated patch with improved end() processing, passes all tests so I think it's good to go?"], "SplitGT": [" Move the root of the graph to the next token in the wrapped TokenStream"], "issueString": "Make it easier to iterate over graphs in tokenstreams\nWe have a number of TokenFilters that read ahead in the token stream (eg synonyms, shingles) and ideally these would understand token graphs as well as linear streams.  FixedShingleFilter already has some mechanisms to deal with graphs; this issue is to extract this logic into a GraphTokenStream class that can then be reused by other token filters\nHere is a patch adding a GraphTokenStream class.  The class wraps an underlying token stream, and then exposes tokens via the following methods:\r\n- incrementBaseToken() : moves the starting point of the graph forwards\r\n- incrementGraphToken() : moves along the currently selected path through the token graph\r\n- incrementGraph() : resets back to the base token, and selects the next path to move along.  Returns false if all paths have been exhausted\r\n\r\nThe patch also reimplements FixedShingleFilter using GraphTokenStream, to illustrate how much easier it is to reason about how things work.\r\n\r\nTo protect against misuse, there are hard limits on how far ahead in the stream tokens will be read and cached, and the number of paths through the graph that can be followed from a single base token\nThis sounds great \u2013 we need to make it easier to work with graph token streams!\r\n\r\nHow does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?\nbq. How does it handle a graph where one of the side paths itself then splits (after a token or two) into its own set of side paths?\r\n\r\nWe'd end up with extra routes through the graph available via incrementGraph()\r\n\r\nLet's imagine a TokenStream that looks like this: z a/b:4 c d/e:2 f g h\r\n\r\nStarting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h\r\nThen we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e\r\nCall incrementGraph() again; we get z b g h\r\nNow that all routes have been exhausted, calling incrementGraph() will return false.\r\n\r\nHow many routes are available depends on how far down the graph you move; if in the example above you only advance as far as 'z a c' on the first branch, then incrementGraph() will move directly to the 'a b g' branch.\nI reworked this slightly; we now have GraphTokenFilter as an abstract extension of TokenFilter, rather than using a separate standalone object.  This makes reset()/end() easier to handle.\nUpdated patch with improved end() processing, passes all tests so I think it's good to go?\n", "issueSearchSentences": ["incrementBaseToken() : moves the starting point of the graph forwards", "incrementGraph() : resets back to the base token, and selects the next path to move along.", "Then we call incrementGraph(); now calling incrementGraphToken() gives us z a c e g h, following the split at d/e", "incrementGraphToken() : moves along the currently selected path through the token graph", "Starting at position z, calling incrementGraphToken() repeatedly will yield the tokenstream z a c d f g h"], "issueSearchScores": [0.700314998626709, 0.6457082629203796, 0.6273037195205688, 0.6042003631591797, 0.5593206882476807]}
{"aId": 94, "code": "private synchronized final float similarity(final String target) {\n    final int m = target.length();\n    final int n = text.length();\n    if (n == 0)  {\n      //we don't have anything to compare.  That means if we just add\n      //the letters for m we get the new word\n      return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length());\n    }\n    if (m == 0) {\n      return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length());\n    }\n\n    final int maxDistance = calculateMaxDistance(m);\n\n    if (maxDistance < Math.abs(m-n)) {\n      //just adding the characters of m to n or vice-versa results in\n      //too many edits\n      //for example \"pre\" length is 3 and \"prefixes\" length is 8.  We can see that\n      //given this optimal circumstance, the edit distance cannot be less than 5.\n      //which is 8-3 or more precisely Math.abs(3-8).\n      //if our maximum edit distance is 4, then we can discard this word\n      //without looking at it.\n      return 0.0f;\n    }\n\n    // init matrix d\n    for (int i = 0; i<=n; ++i) {\n      p[i] = i;\n    }\n\n    // start computing edit distance\n    for (int j = 1; j<=m; ++j) { // iterates through target\n      int bestPossibleEditDistance = m;\n      final char t_j = target.charAt(j-1); // jth character of t\n      d[0] = j;\n\n      for (int i=1; i<=n; ++i) { // iterates through text\n        // minimum of cell to the left+1, to the top+1, diagonally left and up +(0|1)\n        if (t_j != text.charAt(i-1)) {\n          d[i] = Math.min(Math.min(d[i-1], p[i]),  p[i-1]) + 1;\n\t\t} else {\n          d[i] = Math.min(Math.min(d[i-1]+1, p[i]+1),  p[i-1]);\n\t\t}\n        bestPossibleEditDistance = Math.min(bestPossibleEditDistance, d[i]);\n      }\n\n      //After calculating row i, the best possible edit distance\n      //can be found by found by finding the smallest value in a given column.\n      //If the bestPossibleEditDistance is greater than the max distance, abort.\n\n      if (j > maxDistance && bestPossibleEditDistance > maxDistance) {  //equal is okay, but not greater\n        //the closest the target can be to the text is just too far away.\n        //this target is leaving the party early.\n        return 0.0f;\n      }\n\n      // copy current distance counts to 'previous row' distance counts: swap p and d\n      int _d[] = p;\n      p = d;\n      d = _d;\n    }\n\n    // our last action in the above loop was to switch d and p, so p now\n    // actually has the most recent cost counts\n\n    // this will return less than 0.0 when the edit distance is\n    // greater than the number of characters in the shorter word.\n    // but this was the formula that was previously used in FuzzyTermEnum,\n    // so it has not been changed (even though minimumSimilarity must be\n    // greater than 0.0)\n    return 1.0f - ((float)p[n] / (float) (prefix.length() + Math.min(n, m)));\n  }", "comment": " Embedded within this algorithm is a fail-fast Levenshtein distance algorithm. The fail-fast algorithm differs from the standard Levenshtein distance algorithm in that it is aborted if it is discovered that the minimum distance between the words is greater than some threshold.", "issueId": "LUCENE-1183", "issueStringList": ["TRStringDistance uses way too much memory (with patch)", "The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory).", "Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.", "The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up).", "I have reported the new implementation to TRStringDistance.", "new TRStringDistance implementation, as patch and as complete source file.", "It occurs to me that we apparently have two different implementations of Levenshtein, one in spellchecker and one for FuzzyQuery.", "I haven't analyzed them individually to know for sure, but if this is a much better implementation, then we should think about using it for FuzzyQuery, too.", "The FuzzyQuery (FuzzyTermEnum) version claims to have a fast-fail mechanism, too:", "{quote}", "<p>Embedded within this algorithm is a fail-fast Levenshtein distance", "algorithm.", "The fail-fast algorithm differs from the standard Levenshtein", "distance algorithm in that it is aborted if it is discovered that the", "mimimum distance between the words is greater than some threshold.", "<p>", "{quote}", "Cedrik, since you seem to know about these things, would you have time to look at FuzzyTermEnum?", "A 3x speedup there would be great for users, too.", "You caught me while I was finalizing a patch for FuzzyTermEnum... :-) I'll add it to this bug report in the next few minutes.", "Patch for FuzzyTermEnum.", "Also see LUCENE-691 if you are looking at optimizing FuzzyTermEnum.", "Well spotted Karl!", "My version is very similar to LUCENE-691, except I kept some smallish optimisations out the the sake of readability.", "I'll incorporate some of his changes/ideas and publish a new patch.", "Can someone link those 2 issues together in the meantime?", "(There are too many options in the drop-down; don't know which one to choose.)", "New patch for FuzzyTermEnum, incorporating most of LUCENE-691.", "on request by issue reporter", "Committed the TRStringDistance patch -- thank you!", "Committed revision 659016.", "I'll leave the FuzzyTermEnum patch for a later date.", "Is there anything in Bob's FuzzyTermEnum that is not in this patch?", "Anything that you'd want to add, C\u00e9drik?", "All of Bob's FuzzyTermEnum patch is in my patch.", "I only left some smallish optimizations that didn't bring much but did hurt code readability.", "In other words, should you commit my patch, you will have most of (99.9%) LUCENE-691.", "I think this is an important patch for Lucene 2.4, as it brings vast performance improvements in fuzzy search (no hard numbers, sorry).", "Any news on the landing of this patch?", "Now that Lucene 2.9 is out, the vastly better memory usage and speed-up would be a welcome addition to Lucene 3.0's fuzzy search!", "C\u00e9drik, could you update the patch to trunk?", "It sounds like a compelling improvement.", "We should get it in.", "Thanks Michael.", "FuzzyTermEnum.java has not changed for more than 2 years.", "The uploaded patch (FuzzyTermEnum.patch) is still valid for trunk.", "OK I had 2 hunks fail but I managed to apply them.", "Thanks C\u00e9drik!"], "SplitGT": [" Embedded within this algorithm is a fail-fast Levenshtein distance algorithm.", "The fail-fast algorithm differs from the standard Levenshtein distance algorithm in that it is aborted if it is discovered that the minimum distance between the words is greater than some threshold."], "issueString": "TRStringDistance uses way too much memory (with patch)\nThe implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.\n\nThe commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance.\nnew TRStringDistance implementation, as patch and as complete source file.\nIt occurs to me that we apparently have two different implementations of Levenshtein, one in spellchecker and one for FuzzyQuery.  I haven't analyzed them individually to know for sure, but if this is a much better implementation, then we should think about using it for FuzzyQuery, too.  \n\nThe FuzzyQuery (FuzzyTermEnum) version claims to have a fast-fail mechanism, too:\n{quote}\n<p>Embedded within this algorithm is a fail-fast Levenshtein distance\n   * algorithm.  The fail-fast algorithm differs from the standard Levenshtein\n   * distance algorithm in that it is aborted if it is discovered that the\n   * mimimum distance between the words is greater than some threshold.\n   *\n   * <p>\n{quote}\n\nCedrik, since you seem to know about these things, would you have time to look at FuzzyTermEnum?  A 3x speedup there would be great for users, too.\nYou caught me while I was finalizing a patch for FuzzyTermEnum... :-) I'll add it to this bug report in the next few minutes.\nPatch for FuzzyTermEnum.\nAlso see LUCENE-691 if you are looking at optimizing FuzzyTermEnum.\n\n\nWell spotted Karl! My version is very similar to LUCENE-691, except I kept some smallish optimisations out the the sake of readability. I'll incorporate some of his changes/ideas and publish a new patch.\nCan someone link those 2 issues together in the meantime? (There are too many options in the drop-down; don't know which one to choose.)\nNew patch for FuzzyTermEnum, incorporating most of LUCENE-691.\non request by issue reporter\nCommitted the TRStringDistance patch -- thank you!\n\nCommitted revision 659016.\n\n\nI'll leave the FuzzyTermEnum patch for a later date.  Is there anything in Bob's FuzzyTermEnum that is not in this patch?  Anything that you'd want to add, C\u00e9drik?\n\nAll of Bob's FuzzyTermEnum patch is in my patch. I only left some smallish optimizations that didn't bring much but did hurt code readability. In other words, should you commit my patch, you will have most of (99.9%) LUCENE-691.\nI think this is an important patch for Lucene 2.4, as it brings vast performance improvements in fuzzy search (no hard numbers, sorry).\nAny news on the landing of this patch?\nNow that Lucene 2.9 is out, the vastly better memory usage and speed-up would be a welcome addition to Lucene 3.0's fuzzy search!\nC\u00e9drik, could you update the patch to trunk?  It sounds like a compelling improvement.  We should get it in.\nThanks Michael.\nFuzzyTermEnum.java has not changed for more than 2 years. The uploaded patch (FuzzyTermEnum.patch) is still valid for trunk.\nOK I had 2 hunks fail but I managed to apply them.\nThanks C\u00e9drik!\n", "issueSearchSentences": ["mimimum distance between the words is greater than some threshold.", "It sounds like a compelling improvement.", "I have reported the new implementation to TRStringDistance.", "TRStringDistance uses way too much memory (with patch)", "Thanks Michael."], "issueSearchScores": [0.5595747232437134, 0.46320515871047974, 0.38311508297920227, 0.3797283172607422, 0.3756635785102844]}
{"aId": 96, "code": "public static double sumRelativeErrorBound(int numValues) {\n    if (numValues <= 1) {\n      return 0;\n    }\n    // u = unit roundoff in the paper, also called machine precision or machine epsilon\n    double u = Math.scalb(1.0, -52);\n    return (numValues - 1) * u;\n  }", "comment": " This uses formula 3.5 from Higham, Nicholas J. (1993), \"The accuracy of floating point summation\", SIAM Journal on Scientific Computing.", "issueId": "LUCENE-8097", "issueStringList": ["Implement Scorer.maxScore() on disjunctions", "We need to be careful with disjunctions since scores are not always summed up in the same order, which can result in different sums.", "Here is a patch.", "It uses the relative error bound documented in formula 3.5 from Higham, Nicholas J.", "(1993), \"The accuracy of floating point summation\", SIAM Journal on Scientific Computing, which is {{E = (n-1) * u}} if all values are positive, where n is the number of doubles that are summed up and u is the unit roundoff.", "This implies that two sums cannot differ by more than 2*E.", "For the record, we might be able to achieve a better bound since our doubles are actually floats in that case but I don't think it is necessary since the delta that is introduced by this error is generally cancelled by the float cast, and when not it would only increase the max score by 1 ulp."], "SplitGT": [" This uses formula 3.5 from Higham, Nicholas J.", "(1993), \"The accuracy of floating point summation\", SIAM Journal on Scientific Computing."], "issueString": "Implement Scorer.maxScore() on disjunctions\nWe need to be careful with disjunctions since scores are not always summed up in the same order, which can result in different sums.\nHere is a patch. It uses the relative error bound documented in formula 3.5 from Higham, Nicholas J. (1993), \"The accuracy of floating point summation\", SIAM Journal on Scientific Computing, which is {{E = (n-1) * u}} if all values are positive, where n is the number of doubles that are summed up and u is the unit roundoff. This implies that two sums cannot differ by more than 2*E.\nFor the record, we might be able to achieve a better bound since our doubles are actually floats in that case but I don't think it is necessary since the delta that is introduced by this error is generally cancelled by the float cast, and when not it would only increase the max score by 1 ulp.\n", "issueSearchSentences": ["It uses the relative error bound documented in formula 3.5 from Higham, Nicholas J.", "(1993), \"The accuracy of floating point summation\", SIAM Journal on Scientific Computing, which is {{E = (n-1) * u}} if all values are positive, where n is the number of doubles that are summed up and u is the unit roundoff.", "For the record, we might be able to achieve a better bound since our doubles are actually floats in that case but I don't think it is necessary since the delta that is introduced by this error is generally cancelled by the float cast, and when not it would only increase the max score by 1 ulp.", "This implies that two sums cannot differ by more than 2*E.", "We need to be careful with disjunctions since scores are not always summed up in the same order, which can result in different sums."], "issueSearchScores": [0.631812334060669, 0.5822256803512573, 0.5729340314865112, 0.5422468781471252, 0.5000787973403931]}
{"aId": 97, "code": "public static void substituteSystemProperties(Node node) {\n    // loop through child nodes\n    Node child;\n    Node next = node.getFirstChild();\n    while ((child = next) != null) {\n\n      // set next before we change anything\n      next = child.getNextSibling();\n\n      // handle child by node type\n      if (child.getNodeType() == Node.TEXT_NODE) {\n        child.setNodeValue(substituteSystemProperty(child.getNodeValue()));\n      } else if (child.getNodeType() == Node.ELEMENT_NODE) {\n        // handle child elements with recursive call\n        NamedNodeMap attributes = child.getAttributes();\n        for (int i = 0; i < attributes.getLength(); i++) {\n          Node attribute = attributes.item(i);\n          attribute.setNodeValue(substituteSystemProperty(attribute.getNodeValue()));\n        }\n        substituteSystemProperties(child);\n      }\n    }\n  }", "comment": " If the system property is not defined, an empty string is substituted or the default value if provided.", "issueId": "SOLR-79", "issueStringList": ["[PATCH] Using system properties in Solr configuration", "Actually it is not possible to use system properties for configuring the Solr engine.", "There should be a way of referencing system properties from solrconfig.xml, like {$prop.name}.", "The attached patch will provide this feature.", "Patch that allows usage of references to system properties in the solrconfig.xml", "Interesting idea, thanks Alexander!", "If no one has objections, I think it should go in.", "updated patch with test.", "Is there a particular reason that the pattern {$system.prop} was picked over ${system.prop}?", "FWIW, Jetty uses the following xml:", "<SystemProperty name=\"jetty.home\" default=\".", "\"/>/logs/yyyy_mm_dd.jet", "ty.log", "1) i'm not really a fan of the Jetty syntax ... much too verbose for my taste, I would prefer ${system.prop} over {$system.prop} to be consistent with ant and shell variables ... but it would be nice to still support defaults ... isn't ${prop.name:default val} what bash uses?", "2) to be useful, wouldn't we want this to work even if the property wasn't the full value of the tag, ie...", "<listener event=\"postCommit\" class=\"solr.RunExecutableListener\">", "<str name=\"exe\">snapshooter</str>", "<str name=\"dir\">${some.dir.property}/bin</str>", "<bool name=\"wait\">true</bool>", "</listener>", "...if we do this, we should also have some sort of escaping mechanism for completleness, is <tag>$${some.prop}</tag> would not get substitution.", "3) not everything uses Config.getVal ... we would also need similar substitution logic in Config.evaluate, Config.getNode, and the various DOMUtil methods so that it could be used *anywhere* in the solrconfig.xml ... walking all of the NodeTrees to make the substitutions at every level, might get complicated ... maybe there is an easier way of adding a \"visiter\" to the DOM model?", "I took the {$system.prop} pattern from Avalon configuration style.", "After having a look at the ant way, I would also prefer ${system.prop}.", "Furthermore I think it will be very helpful to have something like ${system.prop}/bin.", "Actually it was not my use case, but I'm sure that it is helpful for somebody.", "Borrowing some code from Ant for property substitution (ugly, but avoids reinventing the wheel), attached is a patch that implements property substituion on all attributes and text nodes in the loaded DOM.", "The substituions are done in-place on the DOM thus avoiding having to change code in multiple places.", "Update patch to include necessary build.xml changes and additional test for subtitution in Ant property.", "i haven't tested Erik's patch, but it reads clean to me.", "a couple of minor nits...", "1) it would be good to have some javadocs on substituteSystemProperties(Node)", "2) if the system properties used were \"solr.test.sys.prop1\", etc.", "the patch wouldn't need to modify build.xml ... if we are going to set them explicitly in build.xml, there shold be a comment explaining which test uses them.", "3) at first glance, i'm not sure if this would work, it would be nice to have a test for it...", "s = SolrConfig.config.get(\"propTest/[@attr='prop two']\", \"default\");", "assertEquals(\"prefix-prop one-prop two-suffix\", s);", "4) it looks like it should work, but tests of Config.evaluate, Config.getNode, and the various DOMUtil methods would also be good", "Here's the patch with default value implemented.", "${system.property:default value} - default value is optional, if omitted, \"\" will be used.", "Updated the patch per Hoss' feedback.", "Specifically:", "i haven't tested Erik's patch, but it reads clean to me.", "a couple of minor nits...", "1) Javadoc: done.", "2) Hoss: if the system properties used were \"solr.test.sys.prop1\", etc.", "the patch wouldn't need to modify build.xml ... if we are going to set them explicitly in build.xml, there shold be a comment explaining which test uses them.", "[Erik: I don't understand... are you saying there is a predefined solr.test.sys.prop1 somewhere?", "I didn't spot anything like that, so I've kept with defining a couple of test ones in build.xml", "3)  ye of little faith!", "the doc that the Config object holds has been modified recursively, so all methods querying the doc will see the substituted values.", "I've added a test to show the case you mentioned.", "4) I added similar tests for .evaluate and .getNode.", "All is well.", "I was going to chime in that unset properties should probably not default to an empty string:", "1) so that SOLR-109 would be more feasible", "2) so that we don't silently fail on typos", "...looking at the latest patch, #1 isn't really an issue since $${literal} works ... but especially now that it's possible to specify a default value to use if they property isn't set: trying to use an unset property without a default specified should probably result in an exception ... correct?", "as for my comment about system properties for testing: i was smoking crack thinking that the <syspropertyset> already in the build.xml junit target would magically take care of this for us, forgetting that somewhere the properties actually need to be set.", "I dunno....", "I suppose throwing an exception is fine though that might", "prevent an otherwise functional Solr instance to start up.", "I can see", "it being ok either way on this issue, but I can't envision a case", "where I'd define ${...} bits in my config files and not setting the", "appropriate system properties.", "I'll modify to throw an exception.", "And commit!", ":)", "Erik", "my view is:", "in most cases, a system property not being defined is going to prevent the value from being usable (ie: a blank dataDIr)", "if you want to use a systemp prop in a place where it's acceptible to not have the prop defined at all, then just use ${prop:} to denote that an empty string should be used as the default.", "This feature has been added, complete with the requested exception handling when a property is not defined and no default value provided.", "Very nice, this will make testing easier as well."], "SplitGT": [" If the system property is not defined, an empty string is substituted or the default value if provided."], "issueString": "[PATCH] Using system properties in Solr configuration\nActually it is not possible to use system properties for configuring the Solr engine. There should be a way of referencing system properties from solrconfig.xml, like {$prop.name}.\n\nThe attached patch will provide this feature.\nPatch that allows usage of references to system properties in the solrconfig.xml\nInteresting idea, thanks Alexander!\nIf no one has objections, I think it should go in.\nupdated patch with test.\n\nIs there a particular reason that the pattern {$system.prop} was picked over ${system.prop}?\n\n\n\nFWIW, Jetty uses the following xml:\n\n<SystemProperty name=\"jetty.home\" default=\".\"/>/logs/yyyy_mm_dd.jet\nty.log\n\n1) i'm not really a fan of the Jetty syntax ... much too verbose for my taste, I would prefer ${system.prop} over {$system.prop} to be consistent with ant and shell variables ... but it would be nice to still support defaults ... isn't ${prop.name:default val} what bash uses?\n\n2) to be useful, wouldn't we want this to work even if the property wasn't the full value of the tag, ie...\n\n    <listener event=\"postCommit\" class=\"solr.RunExecutableListener\">\n      <str name=\"exe\">snapshooter</str>\n      <str name=\"dir\">${some.dir.property}/bin</str>\n      <bool name=\"wait\">true</bool>\n    </listener>\n\n...if we do this, we should also have some sort of escaping mechanism for completleness, is <tag>$${some.prop}</tag> would not get substitution.\n\n3) not everything uses Config.getVal ... we would also need similar substitution logic in Config.evaluate, Config.getNode, and the various DOMUtil methods so that it could be used *anywhere* in the solrconfig.xml ... walking all of the NodeTrees to make the substitutions at every level, might get complicated ... maybe there is an easier way of adding a \"visiter\" to the DOM model?\n\nI took the {$system.prop} pattern from Avalon configuration style. After having a look at the ant way, I would also prefer ${system.prop}.\n\nFurthermore I think it will be very helpful to have something like ${system.prop}/bin. Actually it was not my use case, but I'm sure that it is helpful for somebody.\n\nBorrowing some code from Ant for property substitution (ugly, but avoids reinventing the wheel), attached is a patch that implements property substituion on all attributes and text nodes in the loaded DOM.  The substituions are done in-place on the DOM thus avoiding having to change code in multiple places.\nUpdate patch to include necessary build.xml changes and additional test for subtitution in Ant property.\ni haven't tested Erik's patch, but it reads clean to me.\n\na couple of minor nits...\n\n1) it would be good to have some javadocs on substituteSystemProperties(Node)\n\n2) if the system properties used were \"solr.test.sys.prop1\", etc. the patch wouldn't need to modify build.xml ... if we are going to set them explicitly in build.xml, there shold be a comment explaining which test uses them.\n\n3) at first glance, i'm not sure if this would work, it would be nice to have a test for it...\n\n    s = SolrConfig.config.get(\"propTest/[@attr='prop two']\", \"default\");\n    assertEquals(\"prefix-prop one-prop two-suffix\", s);\n\n4) it looks like it should work, but tests of Config.evaluate, Config.getNode, and the various DOMUtil methods would also be good\nHere's the patch with default value implemented.  ${system.property:default value} - default value is optional, if omitted, \"\" will be used.\nUpdated the patch per Hoss' feedback.   Specifically:\n\ni haven't tested Erik's patch, but it reads clean to me.\n\na couple of minor nits...\n\n1) Javadoc: done.\n\n2) Hoss: if the system properties used were \"solr.test.sys.prop1\", etc. the patch wouldn't need to modify build.xml ... if we are going to set them explicitly in build.xml, there shold be a comment explaining which test uses them.  [Erik: I don't understand... are you saying there is a predefined solr.test.sys.prop1 somewhere?  I didn't spot anything like that, so I've kept with defining a couple of test ones in build.xml\n\n3)  ye of little faith!   the doc that the Config object holds has been modified recursively, so all methods querying the doc will see the substituted values.   I've added a test to show the case you mentioned.\n\n4) I added similar tests for .evaluate and .getNode.  All is well.\n\n\n\nI was going to chime in that unset properties should probably not default to an empty string:\n  1) so that SOLR-109 would be more feasible\n  2) so that we don't silently fail on typos\n\n...looking at the latest patch, #1 isn't really an issue since $${literal} works ... but especially now that it's possible to specify a default value to use if they property isn't set: trying to use an unset property without a default specified should probably result in an exception ... correct?\n\n\nas for my comment about system properties for testing: i was smoking crack thinking that the <syspropertyset> already in the build.xml junit target would magically take care of this for us, forgetting that somewhere the properties actually need to be set.\n\n\n\nI dunno.... I suppose throwing an exception is fine though that might  \nprevent an otherwise functional Solr instance to start up.  I can see  \nit being ok either way on this issue, but I can't envision a case  \nwhere I'd define ${...} bits in my config files and not setting the  \nappropriate system properties.\n\nI'll modify to throw an exception.  And commit!  :)\n\n\tErik\n\n\n\n\n\nmy view is:\n  * in most cases, a system property not being defined is going to prevent the value from being usable (ie: a blank dataDIr)\n  * if you want to use a systemp prop in a place where it's acceptible to not have the prop defined at all, then just use ${prop:} to denote that an empty string should be used as the default.\nThis feature has been added, complete with the requested exception handling when a property is not defined and no default value provided.\nVery nice, this will make testing easier as well.\n", "issueSearchSentences": ["1) it would be good to have some javadocs on substituteSystemProperties(Node)", "appropriate system properties.", "as for my comment about system properties for testing: i was smoking crack thinking that the <syspropertyset> already in the build.xml junit target would magically take care of this for us, forgetting that somewhere the properties actually need to be set.", "There should be a way of referencing system properties from solrconfig.xml, like {$prop.name}.", "2) Hoss: if the system properties used were \"solr.test.sys.prop1\", etc."], "issueSearchScores": [0.7602093815803528, 0.6054708361625671, 0.5135048627853394, 0.4962623119354248, 0.48585331439971924]}
{"aId": 98, "code": "public final long getFlushingBytes() {\n    ensureOpen();\n    return docWriter.getFlushingBytes();\n  }", "comment": " Returns the number of bytes currently being flushed", "issueId": "LUCENE-8471", "issueStringList": ["Expose the number of bytes currently being flushed in IndexWriter", "This is already available via the DocumentWriter and flush control.", "Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer.", "Patch.", "I changed tests that were reaching through DocumentWriter and FlushControl to get this value to use the IndexWriter method instead.", "Can we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?", "Sure.", "Should I do the same for activeBytes() and netBytes()?", "I think we should change only `flushBytes` because we only expose it for now.", "+1 LGTM", "+1 Let's maybe mention in javadocs that this is a subset of what #ramBytesUsed returns?", "+1, but could we name it {{getFlushingBytes}} instead?", "{{flushBytes}} sounds like it's going to write bytes to disk or something.", "Updated patch taking into account feedback.", "I'll commit shortly.", "Thanks all!"], "SplitGT": [" Returns the number of bytes currently being flushed"], "issueString": "Expose the number of bytes currently being flushed in IndexWriter\nThis is already available via the DocumentWriter and flush control.\u00a0 Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer.\nPatch.\u00a0 I changed tests that were reaching through DocumentWriter and FlushControl to get this value to use the IndexWriter method instead.\nCan we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?\nSure.\u00a0 Should I do the same for activeBytes() and netBytes()?\nI think we should change only `flushBytes` because we only expose it for now.\n+1 LGTM\n+1 Let's maybe mention in javadocs that this is a subset of what #ramBytesUsed returns?\n+1, but could we name it {{getFlushingBytes}} instead?\u00a0 {{flushBytes}} sounds like it's going to write bytes to disk or something.\nUpdated patch taking into account feedback.\u00a0 I'll commit shortly.\nThanks all!\n", "issueSearchSentences": ["+1, but could we name it {{getFlushingBytes}} instead?", "I think we should change only `flushBytes` because we only expose it for now.", "Can we make `flushBytes` in DocumentsWriterFlushControl a volatile field and its getter without synchronization?", "{{flushBytes}} sounds like it's going to write bytes to disk or something.", "Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer."], "issueSearchScores": [0.6095219850540161, 0.546364426612854, 0.5217363834381104, 0.5180553197860718, 0.5014489889144897]}
{"aId": 101, "code": "@Override\n  public Analyzer getIndexAnalyzer() {\n    return preAnalyzer;\n  }", "comment": " NOTE: If an index analyzer is specified in the schema, it will be ignored.", "issueId": "SOLR-4619", "issueStringList": ["Improve PreAnalyzedField query analysis", "PreAnalyzed field extends plain FieldType and mistakenly uses the DefaultAnalyzer as query analyzer, and doesn't allow for customization via <analyzer> schema elements.", "Instead it should extend TextField and support all query analysis supported by that type.", "Comments by John Berryman, copied from SOLR-1535:", "{quote}", "Ah, I see.", "This is a bit lower level than I was thinking.", "Still useful, but different.", "I was thinking about having PreAnalyzedField extend directly from TextField rather than from FieldType, and then be able to build up whatever analysis chain that you want in the usual TextField sense.", "Query analysis would proceed as with a normal TextField, but index analysis would smart detect whether this input was already parsed or not.", "If the input was not parsed, then it would go through the normal analysis.", "On the other hand, if the input was already parsed, then the token stream would go straight into the index (the assumption being that someone upstream understands what they're doing).", "This way, in the SolrJ client you could build up some extra functionality so that the PreAnalyzedTextFields would be parsed client side and sent to Solr.", "In my current application, we have one Solr and N-indexers on different machines.", "The setup described here would take a big load off of Solr.", "The other benefit of this setup is that query analysis proceeds as it always does.", "I don't understand how someone would search over a PreAnalyzed field as it currently stands, without a bit of extra work/custom code on the client.", "One pitfall to my idea is that you'd have to create a similar PreAnalyzedIntField, PreAnalyzedLocationField, PreAnalyzedDateField etc.", "I wish Java had mixins or multiple inheritance.", "{quote}", "bq.", "having PreAnalyzedField extend directly from TextField rather than from FieldType", "Done in the patch above.", "This means you can specify query analyzers.", "bq.", "index analysis would smart detect whether this input was already parsed or not", "Well, this should be possible with the pluggable parsers that this field supports.", "Since the formats can be vastly different, depending on the parserImpl, I'm not sure if we could come up with a generic detection mechanism...", "FYI, Andrzej, I see you have set 4.2.1 as the fix version - Mark Miller said on #lucene earlier today about cutting a 4.2.1 release: \"i may go tonight or tomorrow depending\".", "Wow... thanks for being so proactive here.", "I'll need to look at this in more detail later.", "Removing 4.2.1 from Fix version - this apparently needs more discussion.", "Instead of having a special pre-analyzed field type, I think it makes more sense to introduce smarts into Solr's DocumentBuilder that sees a pre-analyzed field type value (e.g.", "a \"Field\" or some TokenStream of some sort) and instead of calling createField() on the field type, it simply populates the document.", "There are details to be worked out for sure, but I think this is superior to having a special field type that is pre-analyzed when other field types are not.", "Arguably *all* fields should be able to be pre-analyzed.", "David: hmm, I'm not sure about that ... this would complicate the processing of all field types in order to support a use case that is very specific.", "If you throw in the ability to support different serializations then the detection of whether a field content is in a pre-analyzed format or not is not that simple anymore.", "Do you have a use case in mind that would require switching on the fly between the regular and pre-analyzed formats?", "John: please test the patch that I attached.", "If this is what you had in mind then I'd like to commit it soon, as it's a clear improvement over the current functionality.", "I guess we see this very differently.", "I'm not arguing for any \"switching on the fly\" of anything, even if incidentally what I describe makes that possible.", "Your design couples the ability to pre-analyze with the choice of field type, and I think that coupling need not exist.", "It's already creating problems like this very issue (SOLR-4619).", "Query analysis wouldn't need any improving, it shouldn't have anything to do with wether the field data was analyzed for indexing before it got to Solr or not.", "bq.", "this would complicate the processing of all field types", "Sure, but it's not much.", "I've already been poring over DocumentBuilder (as part of SOLR-4329) and I think I know what's involved.", "It needs to examine the field value: if it's \"Field\" then it gets added right to the document.", "A Solr UpdateRequestProcessor could convert the external serialization of the token stream from JSON/Avro/whatever to Field.", "I like [~dsmiley]s generalized approach.", "Could you perhaps throw up a patch for discussion?", "Code matters more than words, for sure.", "I already have more work than I can handle at the moment and I'm sorry I can't contribute the implementation I describe right now.", "David, thanks for explaining - I see your point, and I like it too.", "bq.", "It needs to examine the field value: if it's \"Field\" then it gets added right to the document.", "Even if we push this functionality to an UpdateRequestProcessor it still needs to know when (not) to convert the input String to a Field.", "Do you have some thoughts about this?", "In UpdateRequestProcessor we deal with SolrInputDocument-s in the context of available schema and solrconfig - so there are a few options how to determine the need for conversion.", "# It could be based on the document itself (tricky, in the light of multiple serialization formats).", "# OR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.", "# OR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.", "# And finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.", "bq.", "1.", "It could be based on the document itself (tricky, in the light of multiple serialization formats).", "I don't understand.", "bq.", "2.", "OR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.", "If it were generalized, i.e.", "per-field map of metadata, then I rather like it.", "Though it'd take a fair amount of work I think to fully realize this (e.g.", "update SolrJ & XML & JSON input formats), and it might also make URP's into more of a full-fledged pipeline but I think such things are better done external to Solr.", "bq.", "3.", "OR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.", "I don't like this, as it ties the choice of pre-analysis to the schema which I think is unnecessary coupling just as it is to the field type.", "bq.", "4.", "And finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.", "I don't see it as \"mixing schema\" unless you simply mean to say that fields are referred to outside of the schema.", "But heck, that's inevitable as fields are already referred to all over solrconfig.xml.", "It's unrealistic to expect the names of fields in one's schema to not exist outside of schema.xml -- the app needs to know too :-)", "One option would be to pass in a pseudo field {{\\_pre-analyzed\\_}} (leading and trailing underscore) with a list of field names that are pre-analyzed.", "The sender of the data is certainly aware of which fields are pre-analyzed as it had to pre-analyze them, so it can simply communicate that.", "So it looks to me like the least controversial option is to put the list of preanalyzed fields in solrconfig in the specification of the URP.", "The trick with a \"magic\" field name sounds useful too - it would allow overriding the list of fields on a per-document basis.", "This could be also achieved by passing the list of fields via SolrParams - although it would affect all documents in a given update request.", "Anyway, I think these are good ideas worth trying.", "I'll start working on a patch.", "Thanks for the comments!", "I came to conclusion that this is really a different issue, worth pursuing on its own, so I created SOLR-4648.", "The improvements in analysis that I mentioned in the original description, and the patch, should be applied regardless of SOLR-4648.", "Patch that brings Andrzej's patch up to date with trunk, and adds tests for query-time functionality.", "I had assumed that {{PreAnalyzedField}}-s would use the {{PreAnalyzedTokenizer}} at query time, but that is not (currently) the case: instead {{FieldType.DefaultAnalyzer}} is used.", "This patch changes the behavior when no analyzer is specified to instead use {{PreAnalyzedTokenizer}}.", "However, there is a chicken-and-egg interaction between {{PreAnalyzedTokenizer}} and {{QueryBuilder.createFieldQuery()}}, which aborts before performing any tokenization if the supplied analyzer's attribute factory doesn't contain a {{TermToBytesRefAttribute}}.", "But {{PreAnalyzedTokenizer}} doesn't have any attributes defined until the input stream is consumed, in {{reset()}}.", "[~rcmuir] added a comment as part of LUCENE-5388 to {{PreAnalyzedTokenizer}}'s ctor, where {{AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY}} is set as the attribute factory rather than the default packed implementation: \"we don't pack attributes: since we are used for (de)serialization and dont want bloat.\"", "This patch moves the {{stream.reset()}} call in {{QueryBuilder.createFieldQuery()}} in front of the {{TermToBytesRefAttribute}} check, so that {{PreAnalyzedTokenizer}} (and other tokenizers that don't have a pre-added set of attributes) has a chance to populate its attributes, and also moves the {{addAttribute(PositionIncrementAttribute.class)}} call to after the {{TermToBytesRefAttribute}} check, since that won't be needed if no tokenization will be performed.", "An alternate approach to fix the chicken-and-egg problem might be to have {{PreAnalyzedTokenizer}} always include a dummy {{TermToBytesRefAttribute}} implementation, and then remove it when {{reset()}} is called, but that seems hackish.", "I haven't run the full tests yet with this patch, but the included query-time {{PreAnalyzedField}} tests succeed.", "I welcome feedback.", "{quote}", "But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()", "{quote}", "Right, thats a bug really.", "According to TokenStream's class javadocs:", "{quote}", "The workflow of the new TokenStream API is as follows:", "1.", "Instantiation of TokenStream/TokenFilters which add/get attributes to/from the AttributeSource.", "2.", "The consumer calls reset().", "3.", "The consumer retrieves attributes from the stream and stores local references to all attributes it wants to access.", "{quote}", "So we have consumers (such as QueryBuilder) doing stuff out of order: if they do step 3 before they do step 2.", "My question is, can we detect this in tests?", "If MockAnalyzer can enforce it, it is easier to fix it consistently everywhere.", "One idea is if MockTokenizer deferred initializing its attributes until reset()?", "Its not going to be the best (we need to tie it into its state machine logic somehow for that), but it might be an easy step.", "Also, majority of TokenFilters (which basically also serve as consumers too), are doing step 3 before step 2 today.", "Most of them are just assigning to final variables in their constructor.", "So something is off: we gotta go one of two ways.", "Either fix the documentation to swap step 3 before step 2, and fix this TokenStream to somehow provide attributes before reset(), or we make a massive change to tons of tokenizers (making them more complex and less efficient).", "But I think we have to do something, at least we should fix the docs to be clear, they need to reflect reality.", "Massive change doesn't seem warranted.", "{quote}", "bq.", "But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()", "Right, thats a bug really.", "{quote}", "bq.", "fix this TokenStream to somehow provide attributes before reset()", "Since the input reader must be consumed before the attributes can be provided, the tokenizer must somehow have access to the input reader prior to reset().", "The most likely place is setReader(), but Tokenizer.setReader() is final.", "A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader().", "I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.", "bq.", "A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader().", "I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.", "I was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream():  A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes.", "{quote}", "bq.", "A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader().", "I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.", "I was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream(): A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes.", "{quote}", "Patch implementing the idea, splitting reader consumption out from reset() into its own method: decodeInput().", "This method first removes all attributes from PreAnalyzedTokenizer's AttributeSource, then adds needed ones as a side effect of parsing the input.", "There is a kludge here: because TokenStreamComponents.setReader() doesn't throw an exception, PreAnalyzedAnalyzer overrides createComponents() to create a TokenStreamComponents instance that catches and stores exceptions encountered during reader consumption with the stream's PreAnalyzedTokenizer instance, whose reset() method will then throw the stored exception, if any.", "With this patch, PreAnalyzedAnalyzer can be reused; previously PreAnalyzedTokenizer reuse would ignore new input and re-emit tokens deserialized from the initial input.", "With this patch, PreAnalyzedField analysis works like this:", "# If a query analyzer is specified in the schema then it will be used at query time.", "# If an analyzer is specified in the schema with no type (i.e., it is neither of \"index\" nor \"query\" type), then this analyzer will be used for query parsing, but will be ignored at index time.", "# If only an analyzer of \"index\" type is specified in the schema, then this analyzer will be used for query parsing, but will be ignored at index time.", "This patch adds a new method removeAllAttributes() to AttributeSource, to support reuse of token streams with variable attributes, like PreAnalyzedTokenizer.", "I think it's ready to go.", "Updated patch, fixes PreAnalyzedFieldTest.testInvalidJson() to properly initialize its PreAnalyzedField, and to call reset() on the token streams created with the invalid JSON snippets, so that the exceptions triggered by the invalid JSON and stored during token stream creation are appropriately thrown.", "Also tests that PreAnalyzedAnalyzer can be reused with valid input after having been fed invalid input.", "I plan on committing this later today if there are no objections.", "Committed to trunk and branch_5x.", "I opened LUCENE-6987 to address the TokenStream workflow documentation."], "SplitGT": [" NOTE: If an index analyzer is specified in the schema, it will be ignored."], "issueString": "Improve PreAnalyzedField query analysis\nPreAnalyzed field extends plain FieldType and mistakenly uses the DefaultAnalyzer as query analyzer, and doesn't allow for customization via <analyzer> schema elements.\n\nInstead it should extend TextField and support all query analysis supported by that type.\nComments by John Berryman, copied from SOLR-1535:\n\n{quote}\nAh, I see. This is a bit lower level than I was thinking. Still useful, but different. I was thinking about having PreAnalyzedField extend directly from TextField rather than from FieldType, and then be able to build up whatever analysis chain that you want in the usual TextField sense. Query analysis would proceed as with a normal TextField, but index analysis would smart detect whether this input was already parsed or not. If the input was not parsed, then it would go through the normal analysis. On the other hand, if the input was already parsed, then the token stream would go straight into the index (the assumption being that someone upstream understands what they're doing).\n\nThis way, in the SolrJ client you could build up some extra functionality so that the PreAnalyzedTextFields would be parsed client side and sent to Solr. In my current application, we have one Solr and N-indexers on different machines. The setup described here would take a big load off of Solr. The other benefit of this setup is that query analysis proceeds as it always does. I don't understand how someone would search over a PreAnalyzed field as it currently stands, without a bit of extra work/custom code on the client.\n\nOne pitfall to my idea is that you'd have to create a similar PreAnalyzedIntField, PreAnalyzedLocationField, PreAnalyzedDateField etc. I wish Java had mixins or multiple inheritance.\n{quote}\nbq. having PreAnalyzedField extend directly from TextField rather than from FieldType\n\nDone in the patch above. This means you can specify query analyzers.\n\nbq. index analysis would smart detect whether this input was already parsed or not\n\nWell, this should be possible with the pluggable parsers that this field supports. Since the formats can be vastly different, depending on the parserImpl, I'm not sure if we could come up with a generic detection mechanism...\n\nFYI, Andrzej, I see you have set 4.2.1 as the fix version - Mark Miller said on #lucene earlier today about cutting a 4.2.1 release: \"i may go tonight or tomorrow depending\".\nWow... thanks for being so proactive here. I'll need to look at this in more detail later.\nRemoving 4.2.1 from Fix version - this apparently needs more discussion.\nInstead of having a special pre-analyzed field type, I think it makes more sense to introduce smarts into Solr's DocumentBuilder that sees a pre-analyzed field type value (e.g. a \"Field\" or some TokenStream of some sort) and instead of calling createField() on the field type, it simply populates the document.  There are details to be worked out for sure, but I think this is superior to having a special field type that is pre-analyzed when other field types are not.  Arguably *all* fields should be able to be pre-analyzed.\nDavid: hmm, I'm not sure about that ... this would complicate the processing of all field types in order to support a use case that is very specific. If you throw in the ability to support different serializations then the detection of whether a field content is in a pre-analyzed format or not is not that simple anymore. Do you have a use case in mind that would require switching on the fly between the regular and pre-analyzed formats?\n\nJohn: please test the patch that I attached. If this is what you had in mind then I'd like to commit it soon, as it's a clear improvement over the current functionality.\nI guess we see this very differently.  I'm not arguing for any \"switching on the fly\" of anything, even if incidentally what I describe makes that possible.  Your design couples the ability to pre-analyze with the choice of field type, and I think that coupling need not exist.  It's already creating problems like this very issue (SOLR-4619).  Query analysis wouldn't need any improving, it shouldn't have anything to do with wether the field data was analyzed for indexing before it got to Solr or not.\n\nbq.  this would complicate the processing of all field types\n\nSure, but it's not much.  I've already been poring over DocumentBuilder (as part of SOLR-4329) and I think I know what's involved.  It needs to examine the field value: if it's \"Field\" then it gets added right to the document.  A Solr UpdateRequestProcessor could convert the external serialization of the token stream from JSON/Avro/whatever to Field.\nI like [~dsmiley]s generalized approach. Could you perhaps throw up a patch for discussion?\nCode matters more than words, for sure.  I already have more work than I can handle at the moment and I'm sorry I can't contribute the implementation I describe right now.\nDavid, thanks for explaining - I see your point, and I like it too.\n\nbq. It needs to examine the field value: if it's \"Field\" then it gets added right to the document.\n\nEven if we push this functionality to an UpdateRequestProcessor it still needs to know when (not) to convert the input String to a Field. Do you have some thoughts about this?\n\nIn UpdateRequestProcessor we deal with SolrInputDocument-s in the context of available schema and solrconfig - so there are a few options how to determine the need for conversion.\n\n# It could be based on the document itself (tricky, in the light of multiple serialization formats).\n# OR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.\n# OR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.\n# And finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.\nbq. 1. It could be based on the document itself (tricky, in the light of multiple serialization formats).\n\nI don't understand.\n\nbq. 2. OR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.\n\nIf it were generalized, i.e. per-field map of metadata, then I rather like it.  Though it'd take a fair amount of work I think to fully realize this (e.g. update SolrJ & XML & JSON input formats), and it might also make URP's into more of a full-fledged pipeline but I think such things are better done external to Solr.\n\nbq. 3. OR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.\n\nI don't like this, as it ties the choice of pre-analysis to the schema which I think is unnecessary coupling just as it is to the field type.\n\nbq. 4. And finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.\n\nI don't see it as \"mixing schema\" unless you simply mean to say that fields are referred to outside of the schema.  But heck, that's inevitable as fields are already referred to all over solrconfig.xml.  It's unrealistic to expect the names of fields in one's schema to not exist outside of schema.xml -- the app needs to know too :-)\n\nOne option would be to pass in a pseudo field {{\\_pre-analyzed\\_}} (leading and trailing underscore) with a list of field names that are pre-analyzed.  The sender of the data is certainly aware of which fields are pre-analyzed as it had to pre-analyze them, so it can simply communicate that.\nSo it looks to me like the least controversial option is to put the list of preanalyzed fields in solrconfig in the specification of the URP. The trick with a \"magic\" field name sounds useful too - it would allow overriding the list of fields on a per-document basis. This could be also achieved by passing the list of fields via SolrParams - although it would affect all documents in a given update request.\n\nAnyway, I think these are good ideas worth trying. I'll start working on a patch. Thanks for the comments!\nI came to conclusion that this is really a different issue, worth pursuing on its own, so I created SOLR-4648. The improvements in analysis that I mentioned in the original description, and the patch, should be applied regardless of SOLR-4648.\nPatch that brings Andrzej's patch up to date with trunk, and adds tests for query-time functionality.\n\nI had assumed that {{PreAnalyzedField}}-s would use the {{PreAnalyzedTokenizer}} at query time, but that is not (currently) the case: instead {{FieldType.DefaultAnalyzer}} is used.  This patch changes the behavior when no analyzer is specified to instead use {{PreAnalyzedTokenizer}}.\n\nHowever, there is a chicken-and-egg interaction between {{PreAnalyzedTokenizer}} and {{QueryBuilder.createFieldQuery()}}, which aborts before performing any tokenization if the supplied analyzer's attribute factory doesn't contain a {{TermToBytesRefAttribute}}.  But {{PreAnalyzedTokenizer}} doesn't have any attributes defined until the input stream is consumed, in {{reset()}}. [~rcmuir] added a comment as part of LUCENE-5388 to {{PreAnalyzedTokenizer}}'s ctor, where {{AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY}} is set as the attribute factory rather than the default packed implementation: \"we don't pack attributes: since we are used for (de)serialization and dont want bloat.\"\n\nThis patch moves the {{stream.reset()}} call in {{QueryBuilder.createFieldQuery()}} in front of the {{TermToBytesRefAttribute}} check, so that {{PreAnalyzedTokenizer}} (and other tokenizers that don't have a pre-added set of attributes) has a chance to populate its attributes, and also moves the {{addAttribute(PositionIncrementAttribute.class)}} call to after the {{TermToBytesRefAttribute}} check, since that won't be needed if no tokenization will be performed.\n\nAn alternate approach to fix the chicken-and-egg problem might be to have {{PreAnalyzedTokenizer}} always include a dummy {{TermToBytesRefAttribute}} implementation, and then remove it when {{reset()}} is called, but that seems hackish.\n\nI haven't run the full tests yet with this patch, but the included query-time {{PreAnalyzedField}} tests succeed.\n\nI welcome feedback.\n{quote}\n But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()\n{quote}\n\nRight, thats a bug really. According to TokenStream's class javadocs:\n\n{quote}\n The workflow of the new TokenStream API is as follows:\n\n 1.  Instantiation of TokenStream/TokenFilters which add/get attributes to/from the AttributeSource.\n 2.  The consumer calls reset().\n 3.  The consumer retrieves attributes from the stream and stores local references to all attributes it wants to access. \n{quote}\n\nSo we have consumers (such as QueryBuilder) doing stuff out of order: if they do step 3 before they do step 2.\n\nMy question is, can we detect this in tests? If MockAnalyzer can enforce it, it is easier to fix it consistently everywhere. One idea is if MockTokenizer deferred initializing its attributes until reset()? Its not going to be the best (we need to tie it into its state machine logic somehow for that), but it might be an easy step.\n\nAlso, majority of TokenFilters (which basically also serve as consumers too), are doing step 3 before step 2 today. Most of them are just assigning to final variables in their constructor.\n\nSo something is off: we gotta go one of two ways. Either fix the documentation to swap step 3 before step 2, and fix this TokenStream to somehow provide attributes before reset(), or we make a massive change to tons of tokenizers (making them more complex and less efficient).\n\nBut I think we have to do something, at least we should fix the docs to be clear, they need to reflect reality.\nMassive change doesn't seem warranted.\n\n{quote}\nbq. But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()\nRight, thats a bug really.\n{quote}\nbq. fix this TokenStream to somehow provide attributes before reset()\n\nSince the input reader must be consumed before the attributes can be provided, the tokenizer must somehow have access to the input reader prior to reset().  The most likely place is setReader(), but Tokenizer.setReader() is final.\n\nA new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader().  I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.\n\nbq. A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader(). I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.\n\nI was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream():  A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes.\n{quote}\nbq. A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader(). I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.\n\nI was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream(): A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes.\n{quote}\n\nPatch implementing the idea, splitting reader consumption out from reset() into its own method: decodeInput().  This method first removes all attributes from PreAnalyzedTokenizer's AttributeSource, then adds needed ones as a side effect of parsing the input.\n\nThere is a kludge here: because TokenStreamComponents.setReader() doesn't throw an exception, PreAnalyzedAnalyzer overrides createComponents() to create a TokenStreamComponents instance that catches and stores exceptions encountered during reader consumption with the stream's PreAnalyzedTokenizer instance, whose reset() method will then throw the stored exception, if any.\n\nWith this patch, PreAnalyzedAnalyzer can be reused; previously PreAnalyzedTokenizer reuse would ignore new input and re-emit tokens deserialized from the initial input.\n\nWith this patch, PreAnalyzedField analysis works like this: \n# If a query analyzer is specified in the schema then it will be used at query time.\n# If an analyzer is specified in the schema with no type (i.e., it is neither of \"index\" nor \"query\" type), then this analyzer will be used for query parsing, but will be ignored at index time.\n# If only an analyzer of \"index\" type is specified in the schema, then this analyzer will be used for query parsing, but will be ignored at index time.\n\nThis patch adds a new method removeAllAttributes() to AttributeSource, to support reuse of token streams with variable attributes, like PreAnalyzedTokenizer.\n\nI think it's ready to go.\nUpdated patch, fixes PreAnalyzedFieldTest.testInvalidJson() to properly initialize its PreAnalyzedField, and to call reset() on the token streams created with the invalid JSON snippets, so that the exceptions triggered by the invalid JSON and stored during token stream creation are appropriately thrown.  Also tests that PreAnalyzedAnalyzer can be reused with valid input after having been fed invalid input. \nI plan on committing this later today if there are no objections.\nCommitted to trunk and branch_5x.\n\nI opened LUCENE-6987 to address the TokenStream workflow documentation.\n", "issueSearchSentences": ["This means you can specify query analyzers.", "# If only an analyzer of \"index\" type is specified in the schema, then this analyzer will be used for query parsing, but will be ignored at index time.", "# If an analyzer is specified in the schema with no type (i.e., it is neither of \"index\" nor \"query\" type), then this analyzer will be used for query parsing, but will be ignored at index time.", "I don't understand how someone would search over a PreAnalyzed field as it currently stands, without a bit of extra work/custom code on the client.", "# If a query analyzer is specified in the schema then it will be used at query time."], "issueSearchScores": [0.628316342830658, 0.5868644118309021, 0.5524525046348572, 0.5404394865036011, 0.5396292209625244]}
{"aId": 102, "code": "public boolean isOptimized() {\n    for (int i = 0; i < readers.size(); i++) {\n      if (!((IndexReader)readers.get(i)).isOptimized()) {\n        return false;\n      }\n    }\n    \n    // all subindexes are optimized\n    return true;\n  }", "comment": " Checks recursively if all subindexes are optimized", "issueId": "LUCENE-832", "issueStringList": ["NPE when calling isCurrent() on a ParallellReader", "As demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE.", "Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here.", "Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly?", "At the very least this behavior should be documented so others know what to expect.", "[junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR", "[junit] null", "[junit] java.lang.NullPointerException", "[junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)", "[junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)", "[junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)", "[junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)", "Index: src/test/org/apache/lucene/index/TestParallelReader.java", "===================================================================", "src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)", "+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)", "@@ -135,6 +135,15 @@", "assertEquals(docParallel.get(\"f4\"), docSingle.get(\"f4\"));", "}", "}", "+", "+  public void testIsCurrent() throws IOException {", "+    Directory dir1 = getDir1();", "+    Directory dir2 = getDir2();", "+    ParallelReader pr = new ParallelReader();", "+    pr.add(IndexReader.open(dir1));", "+    pr.add(IndexReader.open(dir2));", "+    assertTrue(pr.isCurrent());", "+  }", "Fiels 1-4 indexed together:", "private Searcher single() throws IOException {", "shades of LUCENE-781 here.", "This patch fixes ParallelReader similar to LUCENE-781:", "ParallelReader.getVersion() now throws an", "UnsupportedOperationException.", "ParallelReader.isOptimized() now checks if all underlying", "indexes are optimized and returns true in such a case.", "ParallelReader.isCurrent() now checks if all underlying", "IndexReaders are up to date and returns true in such a case.", "Additional tests in TestParallelReader to test these methods.", "All tests pass.", "I'm planning to commit this soon...", "Committed."], "SplitGT": [" Checks recursively if all subindexes are optimized"], "issueString": "NPE when calling isCurrent() on a ParallellReader\nAs demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.\n\n\n    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)\n    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)\n    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)\n    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)\n\n\n\nIndex: src/test/org/apache/lucene/index/TestParallelReader.java\n===================================================================\n--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)\n+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)\n@@ -135,6 +135,15 @@\n       assertEquals(docParallel.get(\"f4\"), docSingle.get(\"f4\"));\n     }\n   }\n+  \n+  public void testIsCurrent() throws IOException {\n+    Directory dir1 = getDir1();\n+    Directory dir2 = getDir2();\n+    ParallelReader pr = new ParallelReader();\n+    pr.add(IndexReader.open(dir1));\n+    pr.add(IndexReader.open(dir2));\n+    assertTrue(pr.isCurrent());\n+  }\n \n   // Fiels 1-4 indexed together:\n   private Searcher single() throws IOException {\n\nshades of LUCENE-781 here.\nThis patch fixes ParallelReader similar to LUCENE-781:\n\n   * ParallelReader.getVersion() now throws an\n     UnsupportedOperationException.\n\t \n   * ParallelReader.isOptimized() now checks if all underlying\n     indexes are optimized and returns true in such a case.\n\t \n   * ParallelReader.isCurrent() now checks if all underlying\n     IndexReaders are up to date and returns true in such a case.\n\t \n   * Additional tests in TestParallelReader to test these methods.\n\t \n\nAll tests pass. I'm planning to commit this soon...\nCommitted.\n", "issueSearchSentences": ["ParallelReader.isOptimized() now checks if all underlying", "IndexReaders are up to date and returns true in such a case.", "indexes are optimized and returns true in such a case.", "Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly?", "ParallelReader.isCurrent() now checks if all underlying"], "issueSearchScores": [0.7705115079879761, 0.6718986630439758, 0.6568495035171509, 0.6020550727844238, 0.5140084624290466]}
{"aId": 103, "code": "public void unregisterCore(String collection) {\n    AtomicBoolean reconstructState = new AtomicBoolean(false);\n    collectionWatches.compute(collection, (k, v) -> {\n      if (v == null)\n        return null;\n      if (v.coreRefCount > 0)\n        v.coreRefCount--;\n      if (v.canBeRemoved()) {\n        watchedCollectionStates.remove(collection);\n        lazyCollectionStates.put(collection, new LazyCollectionRef(collection));\n        reconstructState.set(true);\n        return null;\n      }\n      return v;\n    });\n    if (reconstructState.get()) {\n      synchronized (getUpdateLock()) {\n        constructState();\n      }\n    }\n  }", "comment": " Not a public API.", "issueId": "SOLR-8323", "issueStringList": ["Add CollectionWatcher API to ZkStateReader", "An API to watch for changes to collection state would be a generally useful thing, both internally and for client use.", "Patch outlining the basic idea.", "This adds two new interfaces, CollectionStateWatcher and CollectionStatePredicate.", "The first can be registered for a particular collection with ZkStateReader and is called when the state of that collection changes (as determined by the internal watcher of that collection's state.json node).", "The second is used in a new ZkStateReader.waitForState() method, and is called on a state change to see if the state of a collection matches a predicate.", "There are also forwarding methods on CloudSolrClient for use by SolrJ clients, and a couple of helper methods on DocCollection and Replica to easily check for collection liveness.", "The new interfaces lend themselves nicely to use as Java 8 functional interfaces, and the TestCollectionStateWatchers test demonstrate both lambdas and method references here.", "This should make it easy to replace some of the helper methods (eg waitForThingsToLevelOut, waitForRecoveriesToFinish) in our tests with methods available to SolrJ.", "A caveat: this is only implemented for collections with their own state.json.", "I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.", "bq.", "waitForThingsToLevelOut", "That one is pretty test specific.", "bq.", "I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.", "Someone should remove stateformat=1 for Solr 6 in an ideal world.", "bq.", "Someone should remove stateformat=1 for Solr 6 in an ideal world", "Absolutely.", "Maybe this new API should just go into trunk for now?", "If Solr 6 is coming early in the new year it makes sense to start adding things that don't need to worry about back-compatibility.", "Updated patch, using the SolrCloudTestCase from SOLR-8758.", "This has required a couple of tweaks to the collection-watching code in ZkStateReader, to allow for watching of non-existent collections.", "Why is DocCollection.isFullyActive() static?", "bq.", "stateWatchers.putIfAbsent(collection, Collections.synchronizedList(new ArrayList<>()));", "You want computeIfAbsent() here to avoid the allocations.", "If waitForState() exits with the TimeoutException, the watcher never gets removed.", "There is a fundamental problem with how interestingCollections is getting managed now; there are external controls on that set, but now it's mixed up with the CollectionStateWatcher API.", "As an example, CollectionStateWatcher adds but never removes; and an external caller could call removeZkWatcher on a collection that there's a listener for.", "The way the code is structured with setCreationWatch and refreshAndWatch doesn't make sense to me.", "Why in the heck are they recursive?", "I don't think you need all this.", "I suspect what you really want is to move the call to notifyStateWatchers() and handle it more intelligently to not fire events if the state hasn't actually changed.", "Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages.", "Thanks for the review, Scott!", "Here's an update patch.", "bq.", "Why is DocCollection.isFullyActive() static?", "Because the DocCollection passed to onStateChanged() may be null if the collection doesn't exist, or has been deleted.", "bq.", "If waitForState() exits with the TimeoutException, the watcher never gets removed.", "Fixed.", "bq.", "Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages", "Done, thanks - that's considerably simpler.", "bq.", "There is a fundamental problem with how interestingCollections is getting managed now", "I've restructured this entirely.", "Watches keep track of a) how many cores they have interested in them, and b) how many state watchers there are.", "Changes to a CollectionWatch state are always done inside a ConcurrentHashMap.compute() method to keep them atomic.", "This simplifies the watch handling in ZKController as well, and removes the abstraction leak where external objects controlled when to remove watches.", "I haven't forgotten this one, going to give the new patch a look this week (today or tomorrow)", "Looking at this now.", "BTW, a Github PR might actually make this way easier....", "I like the scheme of reference counting the ZkController core references", "Could collectionWatches and interestingCollections be unified into a single thing?", "collectionWatches.keySet should always be equal to interestingCollections, so I don't a reason to have both", "nit: make the static type of collectionWatchers be ConcurrentMap?", "Conveys intent better and plays nicer in IDE.", "unregisterCore needs a better guard against under-referencing, since it can be called from the outside.", "A caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.", "Might even be advisable to throw an exception here on under reference.", "{code}", "LOG.info(\"Deleting data for [{}]\", coll);", "notifyStateWatchers(coll, newState);", "{code}", "newState is always null (IDE warning) so maybe just pass in null", "{code}", "if (watchers.size() == 0)", "return;", "{code}", "No need to early exit here, the loop will do it anyway", "In notifyStateWatchers you can avoid some copies but just re-assigning the instance variable to a new empty set, and taking ownership of the existing set to fire events on.", "In getStateWatchers() you probably still want to wrap in a compute function to avoid weird race conditions and memory-visibility problems.", "In particular there's absolutely no ordering guarantees on the reference to watch.stateWatchers", "fetchCollectionState() expectExists parameter doesn't make sense to me...", "I would have thought that if a non-null watcher is passed in, you always want to setup an exists watch if the node doesn't exist.", "And if a null watcher is passed in, calling exists() is a waste of energy.", "registerCore/ unregisterCore should probably retain the previous doc:", "getStateWatchers() could return null vs. empty set to differentiate between whether or not the collection is being watched, which would improve the precision of test assertions.", "I did get one failure on a test run:", "\"Did not see a fully active cluster after 30 seconds\"", "But second run it passed.", "Wow, thanks for the very thorough review Scott!", "Here's an updated patch.", "bq.", "Could collectionWatches and interestingCollections be unified into a single thing?", "Unfortunately not, as it's needed to detect collections which have migrated from state format 1 to state format 2.", "There's almost certainly a nicer way of doing that, though - maybe in a follow-up issue?", "bq.", "make the static type of collectionWatchers be ConcurrentMap?", "I disagree here - we don't use any of the concurrent methods, so I think just using Map is fine?", "bq.", "unregisterCore needs a better guard against under-referencing", "Added.", "I don't think throwing an exception is necessary, although maybe we should log a warning in this case?", "bq.", "newState is always null", "changed", "bq.", "No need to early exit here", "changed", "bq.", "In notifyStateWatchers you can avoid some copies...", "I think this ends up as a wash, given that you may end up creating multiple HashSets?", "And we're only copying references, after all.", "bq.", "In getStateWatchers() you probably still want to wrap in a compute function...", "Compute() doesn't help here, I don't think?", "And given that it's a test-only method, I'm not too concerned about accuracy.", "I've made it return a copy rather than return the original set, which should stop weird things happening to it once it's been returned, though.", "bq.", "fetchCollectionState() expectExists parameter doesn't make sense to me", "Again, this is due to state format 1 - a collection state might be in clusterstate.json, so the collection-specific state might not exist.", "I agree about the null watcher though, and have added a check around the exists call for that.", "bq.", "registerCore/ unregisterCore should probably retain the previous doc:", "Added back", "bq.", "getStateWatchers() could return null vs. empty set", "Nice idea, added.", "I've also added an explicit test for state format 1 collections, and updated the code so that it actually works :)", "Don't suppose I could convince you to open a github PR?", "Would make it much easier to review.", ":D", "Pull request opened, review away!", "I see that you've already committed some changes to the way legacy collections are dealt with, so we may well be able to remove the 'interestingcollections' list - will give it a go.", "[~dragonsinth] any comments on the pull request?", "I'd like to get this in soon, as it will make it easier to clean up a bunch of tests.", "Sorry!", "I'll look at it today.", "Been swamped with other stuff.", ":(", "Pardon the distraction to the fine work going on here but I'd like to possibly emulate this code review process on other issue(s).", "Is it necessary to create a branch on some other/personal repo and then issue a pull request (as was done here I see) or is it possible for someone to review commits to a branch on our repo/mirror?", "I'm thinking SOLR-5750 -- https://github.com/apache/lucene-solr/commits/solr-5750   (feel free to make a comment to test).", "Pretty sure you can create pull requests from branches within the same repository, so there's no need to have your own clone if you don't want one.", "Final patch.", "I think this is ready!", "I already LGTM'd the github PR, I don't think I need to look at the patch file?", "Thanks for all the reviewing Scott!", "Now on to SOLR-9056 :)"], "SplitGT": [" Not a public API."], "issueString": "Add CollectionWatcher API to ZkStateReader\nAn API to watch for changes to collection state would be a generally useful thing, both internally and for client use.\nPatch outlining the basic idea.\n\nThis adds two new interfaces, CollectionStateWatcher and CollectionStatePredicate.  The first can be registered for a particular collection with ZkStateReader and is called when the state of that collection changes (as determined by the internal watcher of that collection's state.json node).  The second is used in a new ZkStateReader.waitForState() method, and is called on a state change to see if the state of a collection matches a predicate.  There are also forwarding methods on CloudSolrClient for use by SolrJ clients, and a couple of helper methods on DocCollection and Replica to easily check for collection liveness.\n\nThe new interfaces lend themselves nicely to use as Java 8 functional interfaces, and the TestCollectionStateWatchers test demonstrate both lambdas and method references here.\n\nThis should make it easy to replace some of the helper methods (eg waitForThingsToLevelOut, waitForRecoveriesToFinish) in our tests with methods available to SolrJ.\n\nA caveat: this is only implemented for collections with their own state.json.  I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.\nbq. waitForThingsToLevelOut\n\nThat one is pretty test specific.\n\nbq.  I think it should be relatively easy to extend it to stateformat=1 collections as well if people think that's worth it.\n\nSomeone should remove stateformat=1 for Solr 6 in an ideal world.\nbq. Someone should remove stateformat=1 for Solr 6 in an ideal world\n\nAbsolutely.  Maybe this new API should just go into trunk for now?  If Solr 6 is coming early in the new year it makes sense to start adding things that don't need to worry about back-compatibility.\nUpdated patch, using the SolrCloudTestCase from SOLR-8758.\n\nThis has required a couple of tweaks to the collection-watching code in ZkStateReader, to allow for watching of non-existent collections.\n- Why is DocCollection.isFullyActive() static?\n\nbq. stateWatchers.putIfAbsent(collection, Collections.synchronizedList(new ArrayList<>()));\n\n - You want computeIfAbsent() here to avoid the allocations.\n\n- If waitForState() exits with the TimeoutException, the watcher never gets removed.\n\n- There is a fundamental problem with how interestingCollections is getting managed now; there are external controls on that set, but now it's mixed up with the CollectionStateWatcher API.  As an example, CollectionStateWatcher adds but never removes; and an external caller could call removeZkWatcher on a collection that there's a listener for.\n\n- The way the code is structured with setCreationWatch and refreshAndWatch doesn't make sense to me. Why in the heck are they recursive?  I don't think you need all this.  I suspect what you really want is to move the call to notifyStateWatchers() and handle it more intelligently to not fire events if the state hasn't actually changed.  Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages.\nThanks for the review, Scott!  Here's an update patch.\n\nbq. Why is DocCollection.isFullyActive() static?\n\nBecause the DocCollection passed to onStateChanged() may be null if the collection doesn't exist, or has been deleted.\n\nbq. If waitForState() exits with the TimeoutException, the watcher never gets removed.\n\nFixed.\n\nbq. Basically, you want to call notifyStateWatchers() from within updateWatchedCollection() exactly at the 3 points we're emitting log messages\n\nDone, thanks - that's considerably simpler.\n\nbq. There is a fundamental problem with how interestingCollections is getting managed now\n\nI've restructured this entirely.  Watches keep track of a) how many cores they have interested in them, and b) how many state watchers there are.  Changes to a CollectionWatch state are always done inside a ConcurrentHashMap.compute() method to keep them atomic.  This simplifies the watch handling in ZKController as well, and removes the abstraction leak where external objects controlled when to remove watches.\nI haven't forgotten this one, going to give the new patch a look this week (today or tomorrow)\nLooking at this now.  BTW, a Github PR might actually make this way easier....\nI like the scheme of reference counting the ZkController core references\nCould collectionWatches and interestingCollections be unified into a single thing?\ncollectionWatches.keySet should always be equal to interestingCollections, so I don't a reason to have both\nnit: make the static type of collectionWatchers be ConcurrentMap?  Conveys intent better and plays nicer in IDE.\nunregisterCore needs a better guard against under-referencing, since it can be called from the outside.\nA caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.\nMight even be advisable to throw an exception here on under reference.\n{code}\nLOG.info(\"Deleting data for [{}]\", coll);\nnotifyStateWatchers(coll, newState);\n{code}\n\nnewState is always null (IDE warning) so maybe just pass in null\n{code}\n    if (watchers.size() == 0)\n      return;\n{code}\n\nNo need to early exit here, the loop will do it anyway\nIn notifyStateWatchers you can avoid some copies but just re-assigning the instance variable to a new empty set, and taking ownership of the existing set to fire events on.\nIn getStateWatchers() you probably still want to wrap in a compute function to avoid weird race conditions and memory-visibility problems.  In particular there's absolutely no ordering guarantees on the reference to watch.stateWatchers\nfetchCollectionState() expectExists parameter doesn't make sense to me... I would have thought that if a non-null watcher is passed in, you always want to setup an exists watch if the node doesn't exist.  And if a null watcher is passed in, calling exists() is a waste of energy.\nregisterCore/ unregisterCore should probably retain the previous doc:\n\n/** This is not a public API. Only used by ZkController */\n\ngetStateWatchers() could return null vs. empty set to differentiate between whether or not the collection is being watched, which would improve the precision of test assertions.\nI did get one failure on a test run:\n\"Did not see a fully active cluster after 30 seconds\"\n\nBut second run it passed.\nWow, thanks for the very thorough review Scott!  Here's an updated patch.\n\nbq. Could collectionWatches and interestingCollections be unified into a single thing?\n\nUnfortunately not, as it's needed to detect collections which have migrated from state format 1 to state format 2.  There's almost certainly a nicer way of doing that, though - maybe in a follow-up issue?\n\nbq. make the static type of collectionWatchers be ConcurrentMap?\n\nI disagree here - we don't use any of the concurrent methods, so I think just using Map is fine?\n\nbq. unregisterCore needs a better guard against under-referencing\n\nAdded.  I don't think throwing an exception is necessary, although maybe we should log a warning in this case?\n\nbq. newState is always null\n\nchanged\n\nbq. No need to early exit here\n\nchanged\n\nbq. In notifyStateWatchers you can avoid some copies...\n\nI think this ends up as a wash, given that you may end up creating multiple HashSets?  And we're only copying references, after all.\n\nbq. In getStateWatchers() you probably still want to wrap in a compute function...\n\nCompute() doesn't help here, I don't think?  And given that it's a test-only method, I'm not too concerned about accuracy.  I've made it return a copy rather than return the original set, which should stop weird things happening to it once it's been returned, though.\n\nbq. fetchCollectionState() expectExists parameter doesn't make sense to me\n\nAgain, this is due to state format 1 - a collection state might be in clusterstate.json, so the collection-specific state might not exist.  I agree about the null watcher though, and have added a check around the exists call for that.\n\nbq. registerCore/ unregisterCore should probably retain the previous doc:\n\nAdded back\n\nbq. getStateWatchers() could return null vs. empty set\n\nNice idea, added.\n\nI've also added an explicit test for state format 1 collections, and updated the code so that it actually works :)\nDon't suppose I could convince you to open a github PR?  Would make it much easier to review. :D\nPull request opened, review away!  I see that you've already committed some changes to the way legacy collections are dealt with, so we may well be able to remove the 'interestingcollections' list - will give it a go.\n[~dragonsinth] any comments on the pull request?  I'd like to get this in soon, as it will make it easier to clean up a bunch of tests.\nSorry! I'll look at it today. Been swamped with other stuff. :(\nPardon the distraction to the fine work going on here but I'd like to possibly emulate this code review process on other issue(s).  Is it necessary to create a branch on some other/personal repo and then issue a pull request (as was done here I see) or is it possible for someone to review commits to a branch on our repo/mirror?  I'm thinking SOLR-5750 -- https://github.com/apache/lucene-solr/commits/solr-5750   (feel free to make a comment to test).\nPretty sure you can create pull requests from branches within the same repository, so there's no need to have your own clone if you don't want one.\nFinal patch.  I think this is ready!\nI already LGTM'd the github PR, I don't think I need to look at the patch file?\nThanks for all the reviewing Scott!  Now on to SOLR-9056 :)\n", "issueSearchSentences": ["unregisterCore needs a better guard against under-referencing, since it can be called from the outside.", "unregisterCore needs a better guard against under-referencing", "A caller could call unregisterCore enough times to make coreRefCount negative, offsetting a positive stateWatchers.size() and prematurely removing.", "As an example, CollectionStateWatcher adds but never removes; and an external caller could call removeZkWatcher on a collection that there's a listener for.", "I see that you've already committed some changes to the way legacy collections are dealt with, so we may well be able to remove the 'interestingcollections' list - will give it a go."], "issueSearchScores": [0.694825291633606, 0.684318482875824, 0.6673475503921509, 0.5502804517745972, 0.5389702320098877]}
{"aId": 105, "code": "public static void setAllowDocsOutOfOrder(boolean allow) {\n    allowDocsOutOfOrder = allow;\n  }", "comment": " Indicates whether hit docs may be collected out of docid order. In other words, with this setting, HitCollector#collect(int,float) might beinvoked first for docid N and only later for docid N-1. Being static, this setting is system wide. If docs out of order are allowed scoring might be faster for certain queries (disjunction queries with less than 32 prohibited terms). This setting has no effect for other queries.", "issueId": "LUCENE-730", "issueStringList": ["Restore top level disjunction performance", "This patch restores the performance of top level disjunctions.", "The introduction of BooleanScorer2 had impacted this as reported", "on java-user on 21 Nov 2006 by Stanislav Jordanov.", "This patches BooleanScorer2 to use BooleanScorer in the score(HitCollector) method.", "This also patches BooleanScorer to accept a minimum number of optional matchers.", "The patch also disables some test code: the use of checkSkipTo in QueryUtils", "caused a test failure in TestBoolean2 with the above changes.", "I think this", "could be expected because of the changed document scoring order", "for top level disjunction queries.", "At the moment I don't know how to resolve this.", "With the complete patch, all tests pass here.", "Just a quick note that I contacted Stanislav Jordanov about Paul's patch here.", "Stanislav only used BooleanScorer.setUseScorer14() and that restored performance for him, but he did not try this patch (and won't be doing that as he's not working with Lucene at the moment).", "Paul, what is special about the number 32 here (BooleanScorer2):", "+    if ((requiredScorers.size() == 0) &&", "+        prohibitedScorers.size() < 32) {", "+      // fall back to BooleanScorer, scores documents somewhat out of order", "+      BooleanScorer bs = new BooleanScorer(getSimilarity(), minNrShouldMatch);", "Why can we use BooleanScorer if there are less than 32 prohibited clauses, but not otherwise?", "Thanks.", "32 is the max number of required + prohibited clauses in the orig BooleanScorer (because it uses an int as a bitfield for each document in the current id range being considered).", "Further to Yonik's answer, I have not done any tests with prohibited scorers comparing BooleanScorer and BooleanScorer2.", "It is quite possible that using skipTo() on any prohibited scorer (via BooleanScorer2) is generally faster than using BooleanScorer.", "Prohibited clauses in queries are quite seldom, so it is going to be difficult to find out whether a smaller value than 32 would be generally optimal.", "I've committed this (changed a few minor things in the patch)...without benchmarking BS vs. BS2 with < 32 prohibited clauses.", "Hm, if I exposed that 32 as a static setter method, then one could easily benchmark and compare BS vs. BS2 with Doron's contrib/benchmark.", "As discussed on java-dev the default behavior of BooleanScorer should be to return the documents in order, because there are people who rely in their apps on that.", "Docs out of order should only be allowed if BooleanQuery.setUseScorer14(true) is set explicitly.", "With this patch the old BooleanScorer is only used if BooleanQuery.setUseScorer14(true) is set.", "It also enables the tests in QueryUtils again that check if the docs are returned in order.", "All tests pass.", "The patch applies cleanly here, all core tests pass.", "And I like the allowDocsOutOfOrder approach.", "Thanks for reviewing, Paul!", "I will commit this soon if nobody objects...", "No objection, only some remarks.", "One bigger issue:", "The latest patch defaults to docs in order above performance,", "but my personal taste is to have performance by default.", "And some smaller ones:", "One could still adapt QueryUtills to take the possibility", "of docs out of order into account.", "Some performance tests with prohibited scorers could still", "be needed to find out which of the boolean scorers does better", "on them.", "Two comments:", "With this patch the class BooleanWeight is not", "in (direct) use anymore - it is extended by", "BooleanWeight2 and then only the latter is used,", "and creates either Scorer2 or Scorer.", "We could", "get rid of BolleanWeight2, and have a single", "class BooleanWeight.", "Javadocs for useScorer14 methods:", "Indicates that 1.4 BooleanScorer should be used.", "Being static, This setting is system wide.", "Scoring in 1.4 mode may be faster.", "But note that unlike the default behavior, it does", "not guarantee that docs are collected in docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "public static void setUseScorer14(boolean use14) {", "Whether 1.4 BooleanScorer should be used.", "@see #setUseScorer14(boolean)", "public static boolean getUseScorer14() {", "> The latest patch defaults to docs in order above performance,", "> but my personal taste is to have performance by default.", "I think it makes more sense to \"default\" to the most consistent rigidly defined behavior (docs in order), since that behavior will work (by definition) for any caller regardless of whether the caller expects the docs in order or not.", "people who find performance lacking can then assess their needs and make a conscious choice to change the setting, and see if it actually improves performance in their use cases.", "(ie: \"avoid premature optimization\" and all that)", "> The latest patch defaults to docs in order above performance,", "> but my personal taste is to have performance by default.", "I agree with Hoss here.", "IMO allowing docs out of order is a big", "API change.", "I think if people switch to 2.2 they just want it", "to work as before without having to add special settings.", "If", "they need better performance for certain types of queries and", "they know that their application can deal with docs out of order", "they can enable the faster scoring.", "So my vote is +1 for docs in order by default.", "> Some performance tests with prohibited scorers could still", "> be needed to find out which of the boolean scorers does better", "> on them.", "That'd be helpful.", "However, I'm currently working on some other", "issues.", "Maybe you or others would have some time to run those", "tests?", "> With this patch the class BooleanWeight is not", "> in (direct) use anymore - it is extended by", "> BooleanWeight2 and then only the latter is used,", "> and creates either Scorer2 or Scorer.", "We could", "> get rid of BolleanWeight2, and have a single", "> class BooleanWeight.", "Agree.", "Will do.", "> Javadocs for useScorer14 methods:", "This is good!", "Thanks Doron, I will add the javadocs", "to my patch.", "New patch with the following changes:", "Removes BooleanWeight2", "Javadocs for useScorer14 methods provided by Doron", "(Is the patch reversed?", "It did not apply at the first attempt,", "probably because my working copy is not the same as the trunk.)", "After ant clean, the boolean tests still pass here:", "ant -Dtestcase='TestBool*' test-core", "A slight improvement for the javadocs of BooleanQuery.java.", "In the javadocs of the scorer() method it is indicated that a BooleanScorer2", "will always be used, so it is better to mention here that BooleanScorer2", "delegates to a 1.4 scorer in some cases:", "Indicates that BooleanScorer2 will delegate", "the scoring to a 1.4 BooleanScorer", "for most queries without required clauses.", "Being static, this setting is system wide.", "Scoring in 1.4 mode may be faster.", "But note that unlike the default behavior, it does", "not guarantee that docs are collected in docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "public static void setUseScorer14(boolean use14) {", "useScorer14 = use14;", "}", "> A slight improvement for the javadocs of BooleanQuery.java.", "> In the javadocs of the scorer() method it is indicated that a BooleanScorer2", "> will always be used, so it is better to mention here that BooleanScorer2", "> delegates to a 1.4 scorer in some cases:", "Maybe we should just deprecate the useScorer14 methods and add new methods", "allowDocsOutOfOrder.", "That should be easier to understand for the users.", "And probably most users don't know (or don't care about) the differences", "between BooleanScorer and BooleanScorer2 anyway.", "New patch that deprecates the useScorer14 methods and adds new", "methods:", "Indicates whether hit docs may be collected out of docid", "order.", "In other words, with this setting,", "{@link HitCollector#collect(int,float)} might be", "invoked first for docid N and only later for docid N-1.", "Being static, this setting is system wide.", "If docs out of order are allowed scoring might be faster", "for certain queries (disjunction queries with less than", "32 prohibited terms).", "This setting has no effect for", "other queries.", "public static void setAllowDocsOutOfOrder(boolean allow);", "Whether hit docs may be collected out of docid order.", "@see #setAllowDocsOutOfOrder(boolean)", "public static boolean getAllowDocsOutOfOrder();", "I think this is easier to understand for the users because it", "tells them what they need to know (docs in or out of order)", "and hides technical details (BooleanScorer vs. BooleanScorer2).", "All tests pass.", "I just committed the latest patch.", "Thanks everyone!"], "SplitGT": [" Indicates whether hit docs may be collected out of docid order.", "In other words, with this setting, HitCollector#collect(int,float) might beinvoked first for docid N and only later for docid N-1.", "Being static, this setting is system wide.", "If docs out of order are allowed scoring might be faster for certain queries (disjunction queries with less than 32 prohibited terms).", "This setting has no effect for other queries."], "issueString": "Restore top level disjunction performance\nThis patch restores the performance of top level disjunctions. \nThe introduction of BooleanScorer2 had impacted this as reported\non java-user on 21 Nov 2006 by Stanislav Jordanov.\n\nThis patches BooleanScorer2 to use BooleanScorer in the score(HitCollector) method.\nThis also patches BooleanScorer to accept a minimum number of optional matchers.\n\nThe patch also disables some test code: the use of checkSkipTo in QueryUtils\ncaused a test failure in TestBoolean2 with the above changes. I think this\ncould be expected because of the changed document scoring order\nfor top level disjunction queries.\nAt the moment I don't know how to resolve this.\n\nWith the complete patch, all tests pass here.\n\nJust a quick note that I contacted Stanislav Jordanov about Paul's patch here.  Stanislav only used BooleanScorer.setUseScorer14() and that restored performance for him, but he did not try this patch (and won't be doing that as he's not working with Lucene at the moment).\n\nPaul, what is special about the number 32 here (BooleanScorer2):\n\n+    if ((requiredScorers.size() == 0) &&\n+        prohibitedScorers.size() < 32) {\n+      // fall back to BooleanScorer, scores documents somewhat out of order\n+      BooleanScorer bs = new BooleanScorer(getSimilarity(), minNrShouldMatch);\n\nWhy can we use BooleanScorer if there are less than 32 prohibited clauses, but not otherwise?  Thanks.\n\n32 is the max number of required + prohibited clauses in the orig BooleanScorer (because it uses an int as a bitfield for each document in the current id range being considered).\nFurther to Yonik's answer, I have not done any tests with prohibited scorers comparing BooleanScorer and BooleanScorer2.\n\nIt is quite possible that using skipTo() on any prohibited scorer (via BooleanScorer2) is generally faster than using BooleanScorer. Prohibited clauses in queries are quite seldom, so it is going to be difficult to find out whether a smaller value than 32 would be generally optimal.\n\n\n\nI've committed this (changed a few minor things in the patch)...without benchmarking BS vs. BS2 with < 32 prohibited clauses.\n\nHm, if I exposed that 32 as a static setter method, then one could easily benchmark and compare BS vs. BS2 with Doron's contrib/benchmark.\n\nAs discussed on java-dev the default behavior of BooleanScorer should be to return the documents in order, because there are people who rely in their apps on that. Docs out of order should only be allowed if BooleanQuery.setUseScorer14(true) is set explicitly.\nWith this patch the old BooleanScorer is only used if BooleanQuery.setUseScorer14(true) is set. It also enables the tests in QueryUtils again that check if the docs are returned in order.\n\nAll tests pass.\nThe patch applies cleanly here, all core tests pass.\nAnd I like the allowDocsOutOfOrder approach.\n\nThanks for reviewing, Paul!\n\nI will commit this soon if nobody objects...\nNo objection, only some remarks.\n\nOne bigger issue:\n\nThe latest patch defaults to docs in order above performance,\nbut my personal taste is to have performance by default.\n\nAnd some smaller ones:\n\nOne could still adapt QueryUtills to take the possibility\nof docs out of order into account.\n\nSome performance tests with prohibited scorers could still\nbe needed to find out which of the boolean scorers does better\non them.\n\nTwo comments: \n\nWith this patch the class BooleanWeight is not\nin (direct) use anymore - it is extended by \nBooleanWeight2 and then only the latter is used, \nand creates either Scorer2 or Scorer. We could \nget rid of BolleanWeight2, and have a single \nclass BooleanWeight.\n\nJavadocs for useScorer14 methods:\n  /**\n   * Indicates that 1.4 BooleanScorer should be used.\n   * Being static, This setting is system wide.\n   * Scoring in 1.4 mode may be faster.\n   * But note that unlike the default behavior, it does \n   * not guarantee that docs are collected in docid\n   * order. In other words, with this setting, \n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1. \n   */\n  public static void setUseScorer14(boolean use14) {\n\n  /**\n   * Whether 1.4 BooleanScorer should be used.\n   * @see #setUseScorer14(boolean)\n   */\n  public static boolean getUseScorer14() {\n\n\n> The latest patch defaults to docs in order above performance,\n> but my personal taste is to have performance by default.\n\nI think it makes more sense to \"default\" to the most consistent rigidly defined behavior (docs in order), since that behavior will work (by definition) for any caller regardless of whether the caller expects the docs in order or not.\n\npeople who find performance lacking can then assess their needs and make a conscious choice to change the setting, and see if it actually improves performance in their use cases.\n\n(ie: \"avoid premature optimization\" and all that)\n> The latest patch defaults to docs in order above performance,\n> but my personal taste is to have performance by default.\n\nI agree with Hoss here. IMO allowing docs out of order is a big\nAPI change. I think if people switch to 2.2 they just want it\nto work as before without having to add special settings. If \nthey need better performance for certain types of queries and \nthey know that their application can deal with docs out of order\nthey can enable the faster scoring. \nSo my vote is +1 for docs in order by default.\n\n> Some performance tests with prohibited scorers could still\n> be needed to find out which of the boolean scorers does better\n> on them. \n\nThat'd be helpful. However, I'm currently working on some other\nissues. Maybe you or others would have some time to run those\ntests?\n> With this patch the class BooleanWeight is not\n> in (direct) use anymore - it is extended by \n> BooleanWeight2 and then only the latter is used, \n> and creates either Scorer2 or Scorer. We could \n> get rid of BolleanWeight2, and have a single \n> class BooleanWeight.\n\nAgree. Will do.\n\n> Javadocs for useScorer14 methods:\n\nThis is good! Thanks Doron, I will add the javadocs\nto my patch.\nNew patch with the following changes:\n\n- Removes BooleanWeight2\n- Javadocs for useScorer14 methods provided by Doron\n(Is the patch reversed? It did not apply at the first attempt,\nprobably because my working copy is not the same as the trunk.)\nAfter ant clean, the boolean tests still pass here:\nant -Dtestcase='TestBool*' test-core\n\nA slight improvement for the javadocs of BooleanQuery.java.\nIn the javadocs of the scorer() method it is indicated that a BooleanScorer2\nwill always be used, so it is better to mention here that BooleanScorer2\ndelegates to a 1.4 scorer in some cases:\n\n  /**\n   * Indicates that BooleanScorer2 will delegate\n   * the scoring to a 1.4 BooleanScorer\n   * for most queries without required clauses.\n   * Being static, this setting is system wide.\n   * Scoring in 1.4 mode may be faster.\n   * But note that unlike the default behavior, it does\n   * not guarantee that docs are collected in docid\n   * order. In other words, with this setting,\n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1.\n   */\n  public static void setUseScorer14(boolean use14) {\n    useScorer14 = use14;\n  }\n\n> A slight improvement for the javadocs of BooleanQuery.java.\n> In the javadocs of the scorer() method it is indicated that a BooleanScorer2\n> will always be used, so it is better to mention here that BooleanScorer2\n> delegates to a 1.4 scorer in some cases:\n\nMaybe we should just deprecate the useScorer14 methods and add new methods\nallowDocsOutOfOrder. That should be easier to understand for the users. \nAnd probably most users don't know (or don't care about) the differences\nbetween BooleanScorer and BooleanScorer2 anyway.\nNew patch that deprecates the useScorer14 methods and adds new\nmethods:\n\n  /**\n   * Indicates whether hit docs may be collected out of docid\n   * order. In other words, with this setting, \n   * {@link HitCollector#collect(int,float)} might be\n   * invoked first for docid N and only later for docid N-1.\n   * Being static, this setting is system wide.\n   * If docs out of order are allowed scoring might be faster\n   * for certain queries (disjunction queries with less than\n   * 32 prohibited terms). This setting has no effect for \n   * other queries.\n   */\n  public static void setAllowDocsOutOfOrder(boolean allow);\n  \n  /**\n   * Whether hit docs may be collected out of docid order.\n   * @see #setAllowDocsOutOfOrder(boolean)\n   */\n  public static boolean getAllowDocsOutOfOrder();\n  \n\nI think this is easier to understand for the users because it \ntells them what they need to know (docs in or out of order) \nand hides technical details (BooleanScorer vs. BooleanScorer2).\n\nAll tests pass.\n\nI just committed the latest patch. Thanks everyone!\n", "issueSearchSentences": ["public static void setAllowDocsOutOfOrder(boolean allow);", "@see #setAllowDocsOutOfOrder(boolean)", "public static boolean getAllowDocsOutOfOrder();", "allowDocsOutOfOrder.", "And I like the allowDocsOutOfOrder approach."], "issueSearchScores": [0.9663220643997192, 0.7950327396392822, 0.7056070566177368, 0.7028650045394897, 0.6860175132751465]}
{"aId": 106, "code": "private int initPhrasePositions() throws IOException {\n    int end = Integer.MIN_VALUE;\n    \n    // no repeats at all (most common case is also the simplest one)\n    if (checkedRepeats && !hasRepeats) {\n      // build queue from list\n      pq.clear();\n      for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max\n        pp.firstPosition();\n        if (pp.position > end) {\n          end = pp.position;\n        }\n        pq.add(pp);         // build pq from list\n      }\n      return end;\n    }\n    \n    //printPositions(System.err, \"Init: 1: Bef position\");\n    \n    // position the pp's\n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max  \n      pp.firstPosition();\n    }\n    \n    //printPositions(System.err, \"Init: 2: Aft position\");\n    \n    // one time initialization for this scorer (done only for the first candidate doc)\n    if (!checkedRepeats) {\n      checkedRepeats = true;\n      ArrayList<PhrasePositions> ppsA = new ArrayList<PhrasePositions>();\n      PhrasePositions dummyPP = new PhrasePositions(null, -1, -1);\n      // check for repeats\n      for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max\n        if (pp.nextRepeating != null) {\n          continue; // a repetition of an earlier pp\n        }\n        ppsA.add(pp);\n        int tpPos = tpPos(pp);\n        for (PhrasePositions prevB=pp, pp2=pp.next; pp2!= min; pp2=pp2.next) {\n          if (\n              pp2.nextRepeating != null  // already detected as a repetition of an earlier pp\n              || pp.offset == pp2.offset // not a repetition: the two PPs are originally in same offset in the query! \n              || tpPos(pp2) != tpPos) {  // not a repetition\n            continue; \n          }\n          // a repetition\n          hasRepeats = true;\n          prevB.nextRepeating = pp2;  // add pp2 to the repeats linked list\n          pp2.nextRepeating = dummyPP; // allows not to handle the last pp in a sub-list\n          prevB = pp2;\n        }\n      }\n      if (hasRepeats) {\n        // clean dummy markers\n        for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max\n          if (pp.nextRepeating == dummyPP) {\n            pp.nextRepeating = null;\n          }\n        }\n      }\n      nrPps = ppsA.toArray(new PhrasePositions[0]);\n      pq = new PhraseQueue(nrPps.length);\n    }\n    \n    //printPositions(System.err, \"Init: 3: Aft check-repeats\");\n    \n    // with repeats must advance some repeating pp's so they all start with differing tp's\n    if (hasRepeats) {\n      for (PhrasePositions pp: nrPps) {\n        if ((end=advanceRepeats(pp, end)) == Integer.MIN_VALUE) {\n          return Integer.MIN_VALUE; // ran out of a term -- done (no valid matches in current doc)\n        }\n      }\n    }\n    \n    //printPositions(System.err, \"Init: 4: Aft advance-repeats\");\n    \n    // build queue from non repeating pps \n    pq.clear();\n    for (PhrasePositions pp: nrPps) {\n      if (pp.position > end) {\n        end = pp.position;\n      }\n      pq.add(pp);\n    }\n    \n    return end;\n  }", "comment": " Insert to pq only non repeating PPs, or PPs that are the first in a repeating group.", "issueId": "LUCENE-3215", "issueStringList": ["SloppyPhraseScorer sometimes computes Infinite freq", "reported on user list:", "http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query", "test case", "the problem in this case, is it computes a 'matchLength' of -1.", "then the default impl of sloppyFreq divides by zero, because its defined as:", "{noformat}", "return 1.0f / (distance + 1);", "{noformat}", "those lyrics are entertaining, but here is a more concise test.", "It seems to me the root cause of the bug is having position 'holes' (e.g.", "stopwords), especially across duplicated terms...", "For example, if you disable position increments in queryparser it won't trigger the bug.", "Here's a hack patch...", "I tried writing some correctness tests here but its *really* hard for me to visualize what the freq should even be for these \"sloppy repeating phrase queries with holes\".", "An update on this...", "This is not related to LUCENE-3142 - the latter was fixed but this one still fails.", "The patch fix which 'abs' the distance indeed avoids the infinite score problem, but I was not 100% comfortable with it - how can the distance be none positive?", "Digging into it shows a wrong assumption in SloppyPhraseScorer:", "{code}", "private int initPhrasePositions() throws IOException {", "int end = 0;", "{code}", "The initial value of end assumes that all positions will be nonnegative.", "But this is wrong, as PP position is computed as", "{code}", "position = postings.nextPosition() - offset", "{code}", "So, whenever the query term appears in the doc in a position smaller than its offset in the query, the computed position is negative.", "The correct initialization for end is therefore:", "{code}", "private int initPhrasePositions() throws IOException {", "int end = Integer.MIN_VALUE;", "{code}", "You would expect this bug to surfaced sooner...", "Anyhow, for the 3 tests that Robert added, this only resolve testInfiniteFreq1() but the other two tests still fail, investigating...", "OK I think I have a fix for this.", "While looking at it, I realized that PhraseScorer (the one that used to base both Exact&Sloppy phrase scorers but now is the base of only sloppy phrase scorer) is way too complicated and inefficient.", "All those sort calls after each matching doc can be avoided.", "So I am modifying PhraseScorer to not have a phrase-queue at all - just the sorted linked list, which is always kept sorted by advancing last beyond first.", "Last is renamed to 'min' and first is renamed to 'max'.", "Making the list cyclic allows more efficient manipulation of it.", "With this, SloppyPhraseScorer is modified to maintain its own phrase queue.", "The queue size is set at the first candidate document.", "In order to handle repetitions (Same term in different query offsets) it will contain only some of the pps: those that either have no repetitions, or are the first (lower query offset) in a repeating group.", "A linked list of repeating pps was added: so PhrasePositions has a new member: nextRepeating.", "Detection of repeating pps and creation of that list is done once per scorer: at the first candidate doc.", "For solving the bugs reported here, in addition to the initiation of 'end' as explained in previous comment, advanceRepeatingPPs now also update two values:", "end, in case one of the repeating pps is far ahead (larger)", "position of the first pp in a repeating list (the one that is in the queue - in case the repeating pp is far behind (smaller).", "This can happen when there are holes in the query, as position = tpPOs - offset.", "It fixes the problem of false negative distances which caused this bug.", "It is tricky: relies on that PhrasePositions.nextPosition() ignores pp.position and just call positions.nextPosition().", "But it is correct, as the modified position is used to replace pp in the queue.", "Last, I think that the test added with holes had one wrong assert: It added four docs:", "drug drug", "drug druggy drug", "drug druggy druggy drug", "drug druggy drug druggy drug", "defined this query (number is the offset):", "drug(1) drug(3)", "and expected that with slop=1 the first doc would not be found.", "I think it should be found, as the slop operates in both directions.", "So modified the query to: drug(1) drug(3)", "Patch to follow.", "Attached patch is based on r1166541 - before recent changes to scorers.", "Will merge with recent changes tomorrow or so.", "All tests pass.", "I believe that sloppy scoring performance should improve with this change but did not check this.", "Updated patch for current trunk r1172055.", "Previous patch still produces NANs and infinite scores with holes.", "Updated patch is fixing this, by updating END (before computing the new match-length) also for pp (not only for its repeats).", "I plan to commit this soon.", "Fixed", "r1173961 - trunk", "r1174002 - 3x", "Prior to committing I compared the performance of sloppy phrase queries with/out repeats for large documents with many candidate matches and did not see the anticipated speedup, though, at least, no degradations as well."], "SplitGT": [" Insert to pq only non repeating PPs, or PPs that are the first in a repeating group."], "issueString": "SloppyPhraseScorer sometimes computes Infinite freq\nreported on user list:\nhttp://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query\n\ntest case\nthe problem in this case, is it computes a 'matchLength' of -1.\nthen the default impl of sloppyFreq divides by zero, because its defined as:\n{noformat}\nreturn 1.0f / (distance + 1);\n{noformat}\n\nthose lyrics are entertaining, but here is a more concise test.\nIt seems to me the root cause of the bug is having position 'holes' (e.g. stopwords), especially across duplicated terms...\n\nFor example, if you disable position increments in queryparser it won't trigger the bug.\nHere's a hack patch... \n\nI tried writing some correctness tests here but its *really* hard for me to visualize what the freq should even be for these \"sloppy repeating phrase queries with holes\".\n\nAn update on this...\n\nThis is not related to LUCENE-3142 - the latter was fixed but this one still fails.\n\nThe patch fix which 'abs' the distance indeed avoids the infinite score problem, but I was not 100% comfortable with it - how can the distance be none positive?\n\nDigging into it shows a wrong assumption in SloppyPhraseScorer:\n\n{code}\n    private int initPhrasePositions() throws IOException {\n        int end = 0;\n{code}\n\nThe initial value of end assumes that all positions will be nonnegative.\nBut this is wrong, as PP position is computed as \n\n{code}\n  position = postings.nextPosition() - offset\n{code}\n\nSo, whenever the query term appears in the doc in a position smaller than its offset in the query, the computed position is negative. The correct initialization for end is therefore:\n\n{code}\n    private int initPhrasePositions() throws IOException {\n        int end = Integer.MIN_VALUE;\n{code}\n\nYou would expect this bug to surfaced sooner...\n\nAnyhow, for the 3 tests that Robert added, this only resolve testInfiniteFreq1() but the other two tests still fail, investigating...\nOK I think I have a fix for this.\n\nWhile looking at it, I realized that PhraseScorer (the one that used to base both Exact&Sloppy phrase scorers but now is the base of only sloppy phrase scorer) is way too complicated and inefficient. All those sort calls after each matching doc can be avoided. \n\nSo I am modifying PhraseScorer to not have a phrase-queue at all - just the sorted linked list, which is always kept sorted by advancing last beyond first. Last is renamed to 'min' and first is renamed to 'max'. Making the list cyclic allows more efficient manipulation of it. \n\nWith this, SloppyPhraseScorer is modified to maintain its own phrase queue. The queue size is set at the first candidate document. In order to handle repetitions (Same term in different query offsets) it will contain only some of the pps: those that either have no repetitions, or are the first (lower query offset) in a repeating group. A linked list of repeating pps was added: so PhrasePositions has a new member: nextRepeating.\n\nDetection of repeating pps and creation of that list is done once per scorer: at the first candidate doc.\n\nFor solving the bugs reported here, in addition to the initiation of 'end' as explained in previous comment, advanceRepeatingPPs now also update two values:\n- end, in case one of the repeating pps is far ahead (larger)\n- position of the first pp in a repeating list (the one that is in the queue - in case the repeating pp is far behind (smaller). This can happen when there are holes in the query, as position = tpPOs - offset. It fixes the problem of false negative distances which caused this bug. It is tricky: relies on that PhrasePositions.nextPosition() ignores pp.position and just call positions.nextPosition(). But it is correct, as the modified position is used to replace pp in the queue.\n\nLast, I think that the test added with holes had one wrong assert: It added four docs:\n- drug drug\n- drug druggy drug\n- drug druggy druggy drug\n- drug druggy drug druggy drug\n\ndefined this query (number is the offset):\n- drug(1) drug(3)\n\nand expected that with slop=1 the first doc would not be found.\nI think it should be found, as the slop operates in both directions.\nSo modified the query to: drug(1) drug(3)\n\nPatch to follow.\nAttached patch is based on r1166541 - before recent changes to scorers. Will merge with recent changes tomorrow or so. All tests pass.\nI believe that sloppy scoring performance should improve with this change but did not check this.\nUpdated patch for current trunk r1172055.\nPrevious patch still produces NANs and infinite scores with holes.\nUpdated patch is fixing this, by updating END (before computing the new match-length) also for pp (not only for its repeats).\n\nI plan to commit this soon.\nFixed\n- r1173961 - trunk\n- r1174002 - 3x\n\nPrior to committing I compared the performance of sloppy phrase queries with/out repeats for large documents with many candidate matches and did not see the anticipated speedup, though, at least, no degradations as well.\n", "issueSearchSentences": ["private int initPhrasePositions() throws IOException {", "private int initPhrasePositions() throws IOException {", "It is tricky: relies on that PhrasePositions.nextPosition() ignores pp.position and just call positions.nextPosition().", "A linked list of repeating pps was added: so PhrasePositions has a new member: nextRepeating.", "position of the first pp in a repeating list (the one that is in the queue - in case the repeating pp is far behind (smaller)."], "issueSearchScores": [0.8352369070053101, 0.8352369070053101, 0.6320685148239136, 0.6160099506378174, 0.570008397102356]}
{"aId": 109, "code": "public static Query createJoinQuery(String joinField,\n                                      Query fromQuery,\n                                      Query toQuery,\n                                      IndexSearcher searcher,\n                                      ScoreMode scoreMode,\n                                      MultiDocValues.OrdinalMap ordinalMap) throws IOException {\n    IndexReader indexReader = searcher.getIndexReader();\n    int numSegments = indexReader.leaves().size();\n    final long valueCount;\n    if (numSegments == 0) {\n      return new MatchNoDocsQuery();\n    } else if (numSegments == 1) {\n      // No need to use the ordinal map, because there is just one segment.\n      ordinalMap = null;\n      LeafReader leafReader = searcher.getIndexReader().leaves().get(0).reader();\n      SortedDocValues joinSortedDocValues = leafReader.getSortedDocValues(joinField);\n      if (joinSortedDocValues != null) {\n        valueCount = joinSortedDocValues.getValueCount();\n      } else {\n        return new MatchNoDocsQuery();\n      }\n    } else {\n      if (ordinalMap == null) {\n        throw new IllegalArgumentException(\"OrdinalMap is required, because there is more than 1 segment\");\n      }\n      valueCount = ordinalMap.getValueCount();\n    }\n\n    Query rewrittenFromQuery = searcher.rewrite(fromQuery);\n    if (scoreMode == ScoreMode.None) {\n      GlobalOrdinalsCollector globalOrdinalsCollector = new GlobalOrdinalsCollector(joinField, ordinalMap, valueCount);\n      searcher.search(fromQuery, globalOrdinalsCollector);\n      return new GlobalOrdinalsQuery(globalOrdinalsCollector.getCollectorOrdinals(), joinField, ordinalMap, toQuery, rewrittenFromQuery, indexReader);\n    }\n\n    GlobalOrdinalsWithScoreCollector globalOrdinalsWithScoreCollector;\n    switch (scoreMode) {\n      case Total:\n        globalOrdinalsWithScoreCollector = new GlobalOrdinalsWithScoreCollector.Sum(joinField, ordinalMap, valueCount);\n        break;\n      case Max:\n        globalOrdinalsWithScoreCollector = new GlobalOrdinalsWithScoreCollector.Max(joinField, ordinalMap, valueCount);\n        break;\n      case Avg:\n        globalOrdinalsWithScoreCollector = new GlobalOrdinalsWithScoreCollector.Avg(joinField, ordinalMap, valueCount);\n        break;\n      default:\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"Score mode %s isn't supported.\", scoreMode));\n    }\n    searcher.search(fromQuery, globalOrdinalsWithScoreCollector);\n    return new GlobalOrdinalsWithScoreQuery(globalOrdinalsWithScoreCollector, joinField, ordinalMap, toQuery, rewrittenFromQuery, indexReader);\n  }", "comment": " This join has certain restrictions and requirements: 1) A document can only refer to one other document. 3) There must be a single sorted doc values join field used by both the \"from\" and \"to\" documents.", "issueId": "LUCENE-6352", "issueStringList": ["Add global ordinal based query time join", "Global ordinal based query time join as an alternative to the current query time join.", "The implementation is faster for subsequent joins between reopens, but requires an OrdinalMap to be built.", "This join has certain restrictions and requirements:", "A document can only refer to on other document.", "(but can be referred by one or more documents)", "A type field must exist on all documents and each document must be categorized to a type.", "This is to distingues between the \"from\" and \"to\" side.", "There must be a single sorted doc values field use by both the \"from\" and \"to\" documents.", "By encoding join into a single doc values field it is trival to build an ordinals map from it.", "Attached draft patch.", "Attached a new version of the global ordinal based query time join:", "Added support for the max, total and avg score mode.", "Added more tests.", "Attached a new version:", "added hascode/equal/extractTerms methods to the query impls.", "added optimization for in the case an index has only 1 segment", "updated to the latest two phase iterator changes.", "Thanks Martijn!", "I had a look at the patch it looks very clean, I like it.", "{code}", "Query rewrittenFromQuery = fromQuery.rewrite(indexReader); (JoinUtil.java)", "{code}", "I think you should rather call searcher.rewrite(fromQuery) here, which will take care of rewriting until rewrite returns 'this'.", "{code}", "final float[][] blocks = new float[Integer.MAX_VALUE / arraySize][];", "{code}", "Instead of allocating based on Integer.MAX_VALUE, maybe it should use the number of unique values?", "ie.", "'(int) (((long) valueCount + arraySize - 1) / arraySize)' ?", "{code}", "return new ComplexExplanation(true, score, \"Score based on join value \" + joinValue.utf8ToString());", "{code}", "I don't think it is safe to convert to a string as we have no idea whether the value represents an utf8 string?", "In BaseGlobalOrdinalScorer, you are caching the current doc ID, maybe we should not?", "When I worked on approximations, caching the current doc ID proved to be quite error-prone and it was often better to just call approximation.docID() when the current doc ID was needed.", "Another thing I'm wondering about is the equals/hashCode impl of this global ordinal query: since documents that match depend on what happens in other segments, this query cannot be cached per segment.", "So maybe it should include the current IndexReader in its equals/hashCode comparison in order to work correctly with query caches?", "In the read-only case, this would still allow this query to be cached since the current reader never changes while in the read/write case this query will unlikely be cached given that the query cache will notice that it does not get reused?", "Adrien, Thanks for taking a look at it!", "I updated the patch and applied all the comments, but the comment about the explain.", "I wonder if we can check whether a BytesRef is valid utf8 and then convert it?", "Otherwise just use the BytesRef directly.", "I like the explain to be somewhat useful and this is the best I can think of right now.", "Or maybe we could just document that this feature expects that the join field stores utf8 string values?", "Yes, that makes sense.", "I updated the join util jdocs to reflect this.", "Another patch, forgot to change the size of the int[][] array in GlobalOrdinalsWithScoreCollector.Occurrences", "Just had another look at the patch and found two issues:", "Occurrences still allocates blocks using MAX_VALUE instead of the number of docs per segment", "Scores allocates using '(valueCount + arraySize - 1) / arraySize' but I think we need to cast to a long before the addition and then back to an int after the division in order to avoid overflows if the doc count in the segment is greater than MAX_VALUE - arraySize.", "So this would be: '(int) (((long) valueCount + arraySize - 1) / arraySize)'", "Otherwise +1 to commit!", "This is interesting usage of two-phase iteration.", "Good point about the potential overflow issue!", "I changed the patch to avoid this.", "I'll commit this shortly to trunk and 5x."], "SplitGT": [" This join has certain restrictions and requirements: 1) A document can only refer to one other document.", "3) There must be a single sorted doc values join field used by both the \"from\" and \"to\" documents."], "issueString": "Add global ordinal based query time join \nGlobal ordinal based query time join as an alternative to the current query time join. The implementation is faster for subsequent joins between reopens, but requires an OrdinalMap to be built.\n\nThis join has certain restrictions and requirements:\n* A document can only refer to on other document. (but can be referred by one or more documents)\n* A type field must exist on all documents and each document must be categorized to a type. This is to distingues between the \"from\" and \"to\" side.\n* There must be a single sorted doc values field use by both the \"from\" and \"to\" documents. By encoding join into a single doc values field it is trival to build an ordinals map from it.\nAttached draft patch.\nAttached a new version of the global ordinal based query time join:\n* Added support for the max, total and avg score mode.\n* Added more tests.\nAttached a new version:\n* added hascode/equal/extractTerms methods to the query impls.\n* added optimization for in the case an index has only 1 segment\n* updated to the latest two phase iterator changes.\nThanks Martijn! I had a look at the patch it looks very clean, I like it.\n\n{code}\nQuery rewrittenFromQuery = fromQuery.rewrite(indexReader); (JoinUtil.java)\n{code}\n\nI think you should rather call searcher.rewrite(fromQuery) here, which will take care of rewriting until rewrite returns 'this'.\n\n{code}\nfinal float[][] blocks = new float[Integer.MAX_VALUE / arraySize][];\n{code}\n\nInstead of allocating based on Integer.MAX_VALUE, maybe it should use the number of unique values? ie. '(int) (((long) valueCount + arraySize - 1) / arraySize)' ?\n\n{code}\nreturn new ComplexExplanation(true, score, \"Score based on join value \" + joinValue.utf8ToString());\n{code}\n\nI don't think it is safe to convert to a string as we have no idea whether the value represents an utf8 string?\n\nIn BaseGlobalOrdinalScorer, you are caching the current doc ID, maybe we should not? When I worked on approximations, caching the current doc ID proved to be quite error-prone and it was often better to just call approximation.docID() when the current doc ID was needed.\n\nAnother thing I'm wondering about is the equals/hashCode impl of this global ordinal query: since documents that match depend on what happens in other segments, this query cannot be cached per segment. So maybe it should include the current IndexReader in its equals/hashCode comparison in order to work correctly with query caches? In the read-only case, this would still allow this query to be cached since the current reader never changes while in the read/write case this query will unlikely be cached given that the query cache will notice that it does not get reused?\nAdrien, Thanks for taking a look at it!\n\nI updated the patch and applied all the comments, but the comment about the explain.  I wonder if we can check whether a BytesRef is valid utf8 and then convert it? Otherwise just use the BytesRef directly. I like the explain to be somewhat useful and this is the best I can think of right now.\nOr maybe we could just document that this feature expects that the join field stores utf8 string values?\nYes, that makes sense. I updated the join util jdocs to reflect this.\nAnother patch, forgot to change the size of the int[][] array in GlobalOrdinalsWithScoreCollector.Occurrences\nJust had another look at the patch and found two issues:\n - Occurrences still allocates blocks using MAX_VALUE instead of the number of docs per segment\n - Scores allocates using '(valueCount + arraySize - 1) / arraySize' but I think we need to cast to a long before the addition and then back to an int after the division in order to avoid overflows if the doc count in the segment is greater than MAX_VALUE - arraySize. So this would be: '(int) (((long) valueCount + arraySize - 1) / arraySize)'\n\nOtherwise +1 to commit! This is interesting usage of two-phase iteration.\n\nGood point about the potential overflow issue! I changed the patch to avoid this. I'll commit this shortly to trunk and 5x.\n", "issueSearchSentences": ["Global ordinal based query time join as an alternative to the current query time join.", "Or maybe we could just document that this feature expects that the join field stores utf8 string values?", "By encoding join into a single doc values field it is trival to build an ordinals map from it.", "Add global ordinal based query time join", "Attached a new version of the global ordinal based query time join:"], "issueSearchScores": [0.6186809539794922, 0.5454418063163757, 0.5348269939422607, 0.5295124053955078, 0.5264813899993896]}
{"aId": 112, "code": "public int traversalSize()\r\n  {\r\n    return Integer.MAX_VALUE;\r\n  }", "comment": " Specify the number of hits to traverse.", "issueId": "LUCENE-837", "issueStringList": ["contrib/benchmark QueryMaker and Task Refactorings", "Introduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.", "Add in a new QueryMaker for reading queries from a file that is specified in the properties.", "Patch shortly, and if no concerns, will commit tomorrow or Wed.", "The patch I have is slightly broader than just QueryMaker, as it also refactors some common code in the Search*TravTask.", "See changes.txt in contrib/benchmark.", "Patch applies in contrib/benchmark, NOT Lucene root!", "Does some refactoring of QueryMaker implementations to share some common code and introduces AbstractQueryMaker as parent class.", "SearchTravRetTask now extends SearchTravTask.", "ReadTask now implements traversalSize() method to allow us to specify number of hits to traverse (SearchTravTask now supports a command parameter).", "This will allow us to simulate user scenario of traversing 10 docs at a time.", "Added new FileBasedQueryMaker which allows for the specification of a file or resource containing a list of queries that can be parsed by the QP, assuming one per line.", "The changes looks good.", "I integrated in my changes:", "Modified query-maker generation for read related tasks to make further read tasks addition simpler and safer.", "Changed Taks' setParams() to throw UnsupportedOperationException if that task does not suppot command line param.", "Improved javadoc to specify all properties command line params currently supported.", "Refactored ReportTasks so that it is easy/possible now to create new report tasks.", "If you feel comfortable with committng them together, that would be great.", "Or, if you prefer to commit yours first, I'll reintegrate mine later.", "The changes are supposed to allow you now to easily add new report tasks.", "I also modified the micro-standard.alg to use the new traverse params.", "(I didn't try the new file based query makers.)", "Attached file (benchmark-more-updates.patch) contains the changes from benchmark-updates.patch.", "committed revision 520890", "more updates coming shortly.", "I will attach patch, but am also going to commit.", "Here's my changes.", "Am going to commit shortly", "Committed field-selector-bench.patch on revision 521569", "Hi, I like the new field selector stuff.", "Few comments:", "copyright notice missing in the new ***Selector class", "The bytes storing in BasicDocMaker - perhaps better to extract the bytes in", "BasicDocMaker, - just before the closing '}' of", "if (docData.body!=null && docData.body.length()>0) {", "just taking bdy.getBytes(\"UTF-8\").", "This way this too would respect the doc size limitation parameter.", "This would actually allow all doc makers to support this, and you can undo the changes to DocData and to ReutersDocMaker.", "Hope I got it right?", "Would you like to add the new task to list in TestPerfTasksParse, with no parameters and with some parameters?", "(it should be anyhow parse-tested now automatically, but with no params.)", "the meaning of the new reuters.doc.maker.store.bytes property is: also store the", "entire doc content as a raw bytes field, right?", "(It was not clear to from the javadoc.)", "Regards,", "Doron"], "SplitGT": [" Specify the number of hits to traverse."], "issueString": "contrib/benchmark QueryMaker and Task Refactorings\nIntroduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.\n\nAdd in a new QueryMaker for reading queries from a file that is specified in the properties.\n\nPatch shortly, and if no concerns, will commit tomorrow or Wed.\nThe patch I have is slightly broader than just QueryMaker, as it also refactors some common code in the Search*TravTask.\n\nSee changes.txt in contrib/benchmark.\n\nPatch applies in contrib/benchmark, NOT Lucene root!\n\nDoes some refactoring of QueryMaker implementations to share some common code and introduces AbstractQueryMaker as parent class.\n\nSearchTravRetTask now extends SearchTravTask.\n\nReadTask now implements traversalSize() method to allow us to specify number of hits to traverse (SearchTravTask now supports a command parameter).  This will allow us to simulate user scenario of traversing 10 docs at a time.\n\nAdded new FileBasedQueryMaker which allows for the specification of a file or resource containing a list of queries that can be parsed by the QP, assuming one per line.\nThe changes looks good. \nI integrated in my changes:\n\n- Modified query-maker generation for read related tasks to make further read tasks addition simpler and safer.\n- Changed Taks' setParams() to throw UnsupportedOperationException if that task does not suppot command line param.\n- Improved javadoc to specify all properties command line params currently supported.\n- Refactored ReportTasks so that it is easy/possible now to create new report tasks.\n\nIf you feel comfortable with committng them together, that would be great. Or, if you prefer to commit yours first, I'll reintegrate mine later. The changes are supposed to allow you now to easily add new report tasks. \n\nI also modified the micro-standard.alg to use the new traverse params.\n(I didn't try the new file based query makers.)\n\nAttached file (benchmark-more-updates.patch) contains the changes from benchmark-updates.patch.\ncommitted revision 520890\nmore updates coming shortly.  I will attach patch, but am also going to commit.\nHere's my changes.  Am going to commit shortly\nCommitted field-selector-bench.patch on revision 521569\nHi, I like the new field selector stuff.\n\nFew comments:\n- copyright notice missing in the new ***Selector class\n- The bytes storing in BasicDocMaker - perhaps better to extract the bytes in \n   BasicDocMaker, - just before the closing '}' of \n      if (docData.body!=null && docData.body.length()>0) {\n  just taking bdy.getBytes(\"UTF-8\").\n  This way this too would respect the doc size limitation parameter.\n  This would actually allow all doc makers to support this, and you can undo the changes to DocData and to ReutersDocMaker.\n  Hope I got it right?\n- Would you like to add the new task to list in TestPerfTasksParse, with no parameters and with some parameters?\n  (it should be anyhow parse-tested now automatically, but with no params.)\n- the meaning of the new reuters.doc.maker.store.bytes property is: also store the \n  entire doc content as a raw bytes field, right? (It was not clear to from the javadoc.)\n\nRegards,\nDoron\n", "issueSearchSentences": ["This way this too would respect the doc size limitation parameter.", "ReadTask now implements traversalSize() method to allow us to specify number of hits to traverse (SearchTravTask now supports a command parameter).", "Regards,", "(It was not clear to from the javadoc.)", "committed revision 520890"], "issueSearchScores": [0.44201475381851196, 0.42298251390457153, 0.3118477463722229, 0.28931018710136414, 0.25867605209350586]}
{"aId": 114, "code": "public void setValue(Object v, Float b) {\n    boost = (b==null) ? 1.0f : b.floatValue();\n\n    if( v instanceof Object[] ) {\n      Object[] arr = (Object[])v;\n      Collection<Object> c = new ArrayList<Object>( arr.length );\n      for( Object o : arr ) {\n        c.add( o );\n      }\n      value = c;\n    }\n    else {\n      value = v;\n    }\n  }", "comment": " Set the value for a field.", "issueId": "SOLR-280", "issueStringList": ["slightly more efficient SolrDocument implementation", "Following discussion in SOLR-272", "This implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.", "The API changes slightly in that:", "getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.", "getFirstValue( name ) returns a single value for the field.", "This is intended to make things easier for client applications.", "We could go further and store boosted values as:", "class BoostedValue {", "float boost;", "Object value;", "}", "but I think that makes the implementation to convoluted.", "If we went this route, we would need to check each value before passing it back to the client.", "I think one should be able to simply set a field value without a copy being made:", "public Object setField(String name, Collection value)  {", "return _fields.put(name, value);", "}", "One area I'm concerned about performance is on the output side of things.", "If we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.", "Just something to keep in mind.", "Another thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)", "This is a new implementation where SolrInputDocument *does not* extend SolrDocument.", "This way each can be optimized for how they are most frequently used.", "This adds:", "public class SolrInputField", "{", "final String name;", "float boost = 1.0f;", "Object value = null;", "}", "and SolrInputDocument keeps a Map<String,SolrInputField>", "This still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later."], "SplitGT": [" Set the value for a field."], "issueString": "slightly more efficient SolrDocument implementation\nFollowing discussion in SOLR-272\n\nThis implementation stores fields as a Map<String,Object> rather then a Map<String,Collection<Object>>.  The API changes slightly in that:\n\n getFieldValue( name ) returns a Collection if there are more then one fields and a Object if there is only one.\n\ngetFirstValue( name ) returns a single value for the field.  This is intended to make things easier for client applications.\nWe could go further and store boosted values as:\n\nclass BoostedValue {\n  float boost;\n  Object value;\n} \n\nbut I think that makes the implementation to convoluted.  If we went this route, we would need to check each value before passing it back to the client.\nI think one should be able to simply set a field value without a copy being made:\n \npublic Object setField(String name, Collection value)  {\n  return _fields.put(name, value);\n}\n\nOne area I'm concerned about performance is on the output side of things.\nIf we added an extension point to manipulate documents before they are written to the response, it makes sense to use SolrDocument there rather than Lucene's Document.... but we may be constructing 100 in the course of a response.  Just something to keep in mind.\n\nAnother thing to keep in mind is that if it complicates things having SolrInputDocument inherit from SolrDocument, that relationship isn't needed (is it?)\n\nThis is a new implementation where SolrInputDocument *does not* extend SolrDocument.  This way each can be optimized for how they are most frequently used.  \n\nThis adds:\n\npublic class SolrInputField \n{\n  final String name;\n  float boost = 1.0f;\n  Object value = null;\n}\n\nand SolrInputDocument keeps a Map<String,SolrInputField>\n\nThis still handles the distinctness bit in SolrInputDocument -- there may be a way to put the SOLR-139 logic elsewhere but i'll tackle that later.\n\n", "issueSearchSentences": ["class BoostedValue {", "I think one should be able to simply set a field value without a copy being made:", "We could go further and store boosted values as:", "Object value;", "public Object setField(String name, Collection value)  {"], "issueSearchScores": [0.4560704827308655, 0.45025861263275146, 0.4431016445159912, 0.4265044033527374, 0.42068547010421753]}
{"aId": 115, "code": "public static void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {\n    if ((shift & ~0x3f) != 0)  // ensure shift is 0..63\n      throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");\n    int nChars = (((63-shift)*37)>>8) + 1;    // i/7 is the same as (i*37)>>8 for i in 0..63\n    bytes.offset = 0;\n    bytes.length = nChars+1;   // one extra for the byte that contains the shift info\n    if (bytes.bytes.length < bytes.length) {\n      bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max\n    }\n    bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n  }", "comment": " This is method is used by NumericTokenStream.", "issueId": "LUCENE-4690", "issueStringList": ["Optimize NumericUtils.", "*ToPrefixCoded(), add versions that don't hash", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).", "edit: Uwe pointed out they were used in one place.", "Other places still don't need it.", "Current code:", "{code}", "public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "if (shift>63 || shift<0)", "throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");", "int hash, nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;", "if (bytes.bytes.length < bytes.length) {", "bytes.grow(NumericUtils.BUF_SIZE_LONG);", "}", "bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "calculate hash", "for (int i = 1; i < bytes.length; i++) {", "hash = 31*hash + bytes.bytes[i];", "}", "return hash;", "}", "{code}", "Proposed code template (i.e.", "for all of the *ToPrefixCoded methods):", "{code}", "public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "assert (shift & ~0x3f) == 0;  // ensure shift is 0..63", "int nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;   // one extra for the byte that contains the shift info", "if (bytes.bytes.length < bytes.length) {", "bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max", "}", "bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "}", "{code}", "Some of the changes:", "Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.", "Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.", "replace grow with simple allocation.", "1. grow over-allocates all the time.", "Most of the time (like here) it's wasted space.", "2. grow copies the previous buffer when allocating a bigger buffer.", "This is wasted/unneeded here.", "removes hash code calculation", "An additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.", "One equivalent of i/7 is (i*37)>>8 for i in 0..63.", "This only takes 4 cycles.", "Curious, how do you know that and/or measure that?", "bq.", "Curious, how do you know that and/or measure that?", "The number of cycles?", "It's all documented in various places.", "Of course one needs a good sense of what assembly a compiler/hotspot will emit.", "Integer multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).", "http://gmplib.org/~tege/x86-timing.pdf", "http://www.agner.org/optimize/instruction_tables.pdf", "bq.", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "{code:java}", "@Override", "public int fillBytesRef() {", "try {", "assert valueSize == 64 || valueSize == 32;", "return (valueSize == 64) ?", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "} catch (IllegalArgumentException iae) {", "return empty token before first or after last", "bytes.length = 0;", "return 0;", "}", "}", "{code}", "Other comments:", "The masking away of invalid shifts is a no-go to me.", "This leads to unexpected behaviour.", "A agree grow() does not need to be used for this stuff.", "We can simply reallocate, as we know size exactly.", "By the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.", "bq.", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "Ahh, I didn't see it because the use of the value is on a separate line from the method call.", "Makes it hard to find.", "Here's a patch that doubles the performance of NumericUtils.", "*ToPrefixCoded", "Seems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)", "Thanks, Yonik!"], "SplitGT": [" This is method is used by NumericTokenStream."], "issueString": "Optimize NumericUtils.*ToPrefixCoded(), add versions that don't hash\nAs far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).  If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).\n\nedit: Uwe pointed out they were used in one place.  Other places still don't need it.\nCurrent code:\n{code}\n  public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {\n    if (shift>63 || shift<0)\n      throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");\n    int hash, nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;\n    if (bytes.bytes.length < bytes.length) {\n      bytes.grow(NumericUtils.BUF_SIZE_LONG);\n    }\n    bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n    // calculate hash\n    for (int i = 1; i < bytes.length; i++) {\n      hash = 31*hash + bytes.bytes[i];\n    }\n    return hash;\n  }\n{code}\n\nProposed code template (i.e. for all of the *ToPrefixCoded methods):\n{code}\n  public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {\n    assert (shift & ~0x3f) == 0;  // ensure shift is 0..63\n    int nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;   // one extra for the byte that contains the shift info\n    if (bytes.bytes.length < bytes.length) {\n      bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max\n    }\n    bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n  }\n{code}\n\nSome of the changes:\n - Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.  Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.\n - replace grow with simple allocation.\n   1. grow over-allocates all the time.  Most of the time (like here) it's wasted space.\n   2. grow copies the previous buffer when allocating a bigger buffer.  This is wasted/unneeded here.\n - removes hash code calculation\nAn additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.  One equivalent of i/7 is (i*37)>>8 for i in 0..63.  This only takes 4 cycles.\nCurious, how do you know that and/or measure that?\nbq. Curious, how do you know that and/or measure that?\n\nThe number of cycles?  It's all documented in various places.  Of course one needs a good sense of what assembly a compiler/hotspot will emit.\nInteger multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).\n\nhttp://gmplib.org/~tege/x86-timing.pdf\nhttp://www.agner.org/optimize/instruction_tables.pdf\nbq. As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).\n\nThe return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\n{code:java}\n    @Override\n    public int fillBytesRef() {\n      try {\n        assert valueSize == 64 || valueSize == 32;\n        return (valueSize == 64) ? \n          NumericUtils.longToPrefixCoded(value, shift, bytes) :\n          NumericUtils.intToPrefixCoded((int) value, shift, bytes);\n      } catch (IllegalArgumentException iae) {\n        // return empty token before first or after last\n        bytes.length = 0;\n        return 0;\n      }\n    }\n{code}\n\nOther comments:\n- The masking away of invalid shifts is a no-go to me. This leads to unexpected behaviour.\n- A agree grow() does not need to be used for this stuff. We can simply reallocate, as we know size exactly.\nBy the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.\nbq. The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\nAhh, I didn't see it because the use of the value is on a separate line from the method call.  Makes it hard to find.\nHere's a patch that doubles the performance of NumericUtils.*ToPrefixCoded\n\nSeems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)\nThanks, Yonik!\n", "issueSearchSentences": ["public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);"], "issueSearchScores": [0.9126536846160889, 0.9107717871665955, 0.8014967441558838, 0.7287958860397339, 0.5933851599693298]}
{"aId": 118, "code": "public void setPhraseLimit (int phraseLimit) { this.phraseLimit = phraseLimit; }", "comment": " The default is 5000.", "issueId": "LUCENE-3234", "issueStringList": ["Provide limit on phrase analysis in FastVectorHighlighter", "With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.", "If one is willing to accept", "less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.", "This is analogous to the Highlighter limit on the number of characters to analyze.", "The patch includes an artifical test case that shows > 1000x speedup.", "In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.", "Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.", "With phraseLimit = -1, you get the existing FVH behavior.", "At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.", "I like this tradeoff Mike, thanks!", "should we consider setting some kind of absurd default like 10,000 to really prevent some pathological cases with huge documents?", "We could document in CHANGES.txt that if you want the old behavior, set it to -1 or Integer.MAX_VALUE (I think we can use this here?", "offsets are ints?)", "Yes, although a smaller number might be fine.", "Maybe Koji will comment: I don't completely understand the scaling here, but it seemed to me that I had a case with around 2000 occurrences of a term that lead to a 15-20 sec evaluation time on my desktop.", "The max value will be an int, sire, although I think the number is going to scale like positions, not offsets FWIW.", "yeah, you are right.. but seeing as how positions are ints too, I think it might be easier to do Integer.MAX_VALUE versus the -1 parameter.", "Yes, that makes sense to me - default to 5000, say, and set explicitly to either MAX_VALUE or -1 to get the unlimited behavior (I prefer to allow -1 since otherwise you should probably treat it as an error).", "Do you want me to change the patch, or should I just leave that to the committer?", "You can change it if you don't mind.", "However, I think I agree it would be good to figure out if there is an n^2 here.", "This might have some affect on what the default value should be... ideally there is some way we could fix the n^2.", "Is there a way to turn your test case into a benchmark, or do you have a separate benchmark (the example you mentioned where it blows up really bad).", "This could help in looking at what's going on.", "I don't think I can share the test documents I have - they belong to someone else.", "I can look at trying to make something bad happen with the wikipedia data, but I'm curious why a benchmark is preferable to a test case?", "oh thats ok, i just meant a little tiny benchmark, hitting the nasty case that we might think might be n^2.", "If the little test case does that... then that will work, just wasn't sure if it did.", "either way just something to look at in the profiler, etc.", "I did go back and look at the original case that made me worried; in that case the \"bad\" document is 650K, and the matched term occurs 23000 times in it.", "The search still finishes in 24 sec or so on my desktop, which isn't too bad I guess, considering.", "After looking at that and measuring the change in the test case in the patch as the number of terms increase, I don't think there actually is an n^2 - just linear, but the growth is still enough that the patch has value.", "The test case in the patch is closely targeted at the method which takes all the time when you have large numbers of matching terms in a single document.", "Mike, thank you for your continuous interest to FVH!", "Can you add the parameter for Solr, with an appropriate default value if you would like.", "I don't know assertTrue test in testManyRepeatedTerms() is ok, for JENKINS?", "Added solr parameter hl.phraseLimit (default=5000)", "Koji - I'm not sure what the issue w/assertTrue is?", "It looked to me as if the test case ultimately inherits from org.junit.Assert, which defines the method?", "Is there a different version of junit on Jenkins without that method?", "Oh I see, I think i'm nervous about testRepeatedTerms too.", "Maybe we can comment it out and just mention its more of a benchmark?", "The problem could be that the test is timing-based... in general a machine could suddenly get busy at any time,", "especially since we run many tests in parallel, so I'm worried it could intermittently fail.", "Sure - the test is fragile.", "It was just meant to illustrate the use case; not really a good unit test for regression.", "The last patch has it commented.", "I am not sure how much it is related to this issue but there was", "a similar issue in Lucene.Net.", "https://issues.apache.org/jira/browse/LUCENENET-350", "Updated patch attached.", "I added CHANGES.txt entries for Lucene and Solr, used Integer.MAX_VALUE for the default and added @param for phraseLimit in the new constructor javadoc.", "Will commit soon.", "Oops, wrong patch.", "This one is correct.", "trunk: Committed revision 1139995.", "3x: Committed revision 1139997.", "Thanks, Mike!", "Thank you, Koji - it's nice to have my first patch committed!", "um - one little comment; since you made the default be MAX_VALUE, there is a javadoc comment that should be updated which says it is 5000.", "Thank you again for checking the commit, Mike!", "The javadoc has been fixed."], "SplitGT": [" The default is 5000."], "issueString": "Provide limit on phrase analysis in FastVectorHighlighter\nWith larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept\nless-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.\n\nThe patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.\n\nWith phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.\n\nI like this tradeoff Mike, thanks!\n\nshould we consider setting some kind of absurd default like 10,000 to really prevent some pathological cases with huge documents?\nWe could document in CHANGES.txt that if you want the old behavior, set it to -1 or Integer.MAX_VALUE (I think we can use this here? offsets are ints?)\nYes, although a smaller number might be fine.  Maybe Koji will comment: I don't completely understand the scaling here, but it seemed to me that I had a case with around 2000 occurrences of a term that lead to a 15-20 sec evaluation time on my desktop.  The max value will be an int, sire, although I think the number is going to scale like positions, not offsets FWIW.\n\nyeah, you are right.. but seeing as how positions are ints too, I think it might be easier to do Integer.MAX_VALUE versus the -1 parameter.\n\nYes, that makes sense to me - default to 5000, say, and set explicitly to either MAX_VALUE or -1 to get the unlimited behavior (I prefer to allow -1 since otherwise you should probably treat it as an error).  Do you want me to change the patch, or should I just leave that to the committer?\nYou can change it if you don't mind. However, I think I agree it would be good to figure out if there is an n^2 here. This might have some affect on what the default value should be... ideally there is some way we could fix the n^2.\n\nIs there a way to turn your test case into a benchmark, or do you have a separate benchmark (the example you mentioned where it blows up really bad). This could help in looking at what's going on.\n\nI don't think I can share the test documents I have - they belong to someone else.  I can look at trying to make something bad happen with the wikipedia data, but I'm curious why a benchmark is preferable to a test case? \noh thats ok, i just meant a little tiny benchmark, hitting the nasty case that we might think might be n^2.\nIf the little test case does that... then that will work, just wasn't sure if it did.\n\neither way just something to look at in the profiler, etc.\nI did go back and look at the original case that made me worried; in that case the \"bad\" document is 650K, and the matched term occurs 23000 times in it.  The search still finishes in 24 sec or so on my desktop, which isn't too bad I guess, considering.\n\nAfter looking at that and measuring the change in the test case in the patch as the number of terms increase, I don't think there actually is an n^2 - just linear, but the growth is still enough that the patch has value. The test case in the patch is closely targeted at the method which takes all the time when you have large numbers of matching terms in a single document.\nMike, thank you for your continuous interest to FVH! Can you add the parameter for Solr, with an appropriate default value if you would like. I don't know assertTrue test in testManyRepeatedTerms() is ok, for JENKINS?\nAdded solr parameter hl.phraseLimit (default=5000)\n\nKoji - I'm not sure what the issue w/assertTrue is?  It looked to me as if the test case ultimately inherits from org.junit.Assert, which defines the method?   Is there a different version of junit on Jenkins without that method?\nOh I see, I think i'm nervous about testRepeatedTerms too.\nMaybe we can comment it out and just mention its more of a benchmark?\n\nThe problem could be that the test is timing-based... in general a machine could suddenly get busy at any time,\nespecially since we run many tests in parallel, so I'm worried it could intermittently fail.\nSure - the test is fragile.  It was just meant to illustrate the use case; not really a good unit test for regression.  The last patch has it commented.\nI am not sure how much it is related to this issue but there was\na similar issue in Lucene.Net.\nhttps://issues.apache.org/jira/browse/LUCENENET-350\n\n\nUpdated patch attached. I added CHANGES.txt entries for Lucene and Solr, used Integer.MAX_VALUE for the default and added @param for phraseLimit in the new constructor javadoc. Will commit soon.\nOops, wrong patch. This one is correct.\ntrunk: Committed revision 1139995.\n3x: Committed revision 1139997.\n\nThanks, Mike!\nThank you, Koji - it's nice to have my first patch committed!\n\num - one little comment; since you made the default be MAX_VALUE, there is a javadoc comment that should be updated which says it is 5000.\nThank you again for checking the commit, Mike! The javadoc has been fixed.\n", "issueSearchSentences": ["At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.", "Added solr parameter hl.phraseLimit (default=5000)", "With phraseLimit = -1, you get the existing FVH behavior.", "In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.", "Provide limit on phrase analysis in FastVectorHighlighter"], "issueSearchScores": [0.6310808658599854, 0.584428071975708, 0.5600566267967224, 0.46632781624794006, 0.4442625641822815]}
{"aId": 120, "code": "public void close() throws IOException {\n    searcher.close();\n    searcher = null;\n  }", "comment": " Close the IndexSearcher used by this SpellChecker.", "issueId": "LUCENE-2108", "issueStringList": ["SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally", "I can't find any way to close the IndexSearcher (and IndexReader) that", "is being used by SpellChecker internally.", "I've worked around this issue by keeping a single SpellChecker open", "for each index, but I'd really like to be able to close it and", "reopen it on demand without leaking file descriptors.", "Could we add a close() method to SpellChecker that will close the", "IndexSearcher and null the reference to it?", "And perhaps add some code", "that reopens the searcher if the reference to it is null?", "Or would", "that break thread safety of SpellChecker?", "The attached patch adds a close method but leaves it to the user to", "call setSpellIndex to reopen the searcher if desired.", "Patch that adds a close method to SpellChecker.", "The method calls close on the searcher used and then nulls the reference so that a new IndexSearcher will be created by the next call to setSpellIndex", "Shouldn't the new close() method be public?", "Haha, this is why I said the patch should be \"pretty\" trivial, instead of just \"trivial\" :-)", "Yes, it should certainly be private.", "No idea how that happend.", "Must have been sleeping at the keyboad.", "Note that you said \"private\" again ;)  I'm starting to wonder if you are not human!", "Is this a turing test?", "OK, ok, I'll make it public, and port back to the 3.0 branch!", "Dude, you have be to a human to make mistakes as stupid as these!", "(pubic void close, public void close, public void close...)", "bq.", "Dude, you have be to a human to make mistakes as stupid as these!", "Good point :)", "Thanks Eirik!", "Mike / Eirik,", "If you set the searcher to null you might risk a NPE if suggestSimilar() or other methods are called afterwards.", "I would like to see something like ensureOpen() which throws an AlreadyClosedException  or something similar.", "I will upload a suggestion in a second but need to run so tread it just as a suggestion.", "Simon", "Something like that would be more appropriate IMO", "Simon,", "Yes, that sound excactly like what I was thinking when I said \"some code", "that reopens the searcher if the reference to it is null\".", "I just didn't include it in my patch because I couldn't figure out how to do it properly.", "I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?", "Reopening to get the AlreadyClosedException in there...", "Well not exactly.", "Simon's suggestion was just to throw an AlreadyClosedException instead of a NullPointerException which is probably ok and definitely easier.", "bq.", "I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?", "this class is not threadsafe anyway.", "If you look at this snippet:", "{code}", "close the old searcher, if there was one", "if (searcher != null) {", "searcher.close();", "}", "searcher = new IndexSearcher(this.spellIndex, true);", "{code}", "there could be a race if you concurrently reindex or set a new dictionary.", "IMO this should either be documented or made threadsafe.", "The close method should invalidate the spellchecker - it should not be possible to use a already closed Spellchecker.", "The searcher should be somehow ref counted so that if there is a searcher still in use you can concurrently reindex / add a new dictionary to ensure that the same searcher is used throughout suggestSimilar().", "I will take care of it once I get back tomorrow.", "Just a reminder - we need to fix the CHANGES.TXT entry once this is done.", "Mike,", "Please account for my demonstrated stupidity when considering this suggestion for thread safety policy / goals:", "1) Concurrent invocations of  suggestSimilar() should not interfere with each other.", "2) An invocation of any of the write methods (setSpellIndex, clearIndex, indexDictionary) should not interfere with aleady invoked suggestSimilar", "3) All calls to write methods should be serialized (We could probably synchronize these methods?)", "If we synchronize any writes to the searcher reference, couldn't suggestSimilar just start its work by putting searcher in a local variable and use that instead of the field?", "I guess concurrency is hard to get right..", "Eirik, could you open a new issue to address SpellChecker's non-thread-safety?", "I actually thing simply documenting clearly that it's not thread safe is fine.", "bq.", "Just a reminder - we need to fix the CHANGES.TXT entry once this is done.", "Simon how about you do this, and take this issue (to commit your improvement to throw ACE not NPE)?", "Thanks ;)"], "SplitGT": [" Close the IndexSearcher used by this SpellChecker."], "issueString": "SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally\nI can't find any way to close the IndexSearcher (and IndexReader) that\nis being used by SpellChecker internally.\n\nI've worked around this issue by keeping a single SpellChecker open\nfor each index, but I'd really like to be able to close it and\nreopen it on demand without leaking file descriptors.\n\nCould we add a close() method to SpellChecker that will close the\nIndexSearcher and null the reference to it? And perhaps add some code\nthat reopens the searcher if the reference to it is null? Or would\nthat break thread safety of SpellChecker?\n\nThe attached patch adds a close method but leaves it to the user to\ncall setSpellIndex to reopen the searcher if desired.\nPatch that adds a close method to SpellChecker. The method calls close on the searcher used and then nulls the reference so that a new IndexSearcher will be created by the next call to setSpellIndex\nShouldn't the new close() method be public?\nHaha, this is why I said the patch should be \"pretty\" trivial, instead of just \"trivial\" :-)\n\nYes, it should certainly be private. No idea how that happend. Must have been sleeping at the keyboad.\nNote that you said \"private\" again ;)  I'm starting to wonder if you are not human!  Is this a turing test?\n\nOK, ok, I'll make it public, and port back to the 3.0 branch!\n\nDude, you have be to a human to make mistakes as stupid as these!\n\n(pubic void close, public void close, public void close...)\nbq. Dude, you have be to a human to make mistakes as stupid as these!\n\nGood point :)\n\n\nThanks Eirik!\nMike / Eirik,\n\nIf you set the searcher to null you might risk a NPE if suggestSimilar() or other methods are called afterwards. I would like to see something like ensureOpen() which throws an AlreadyClosedException  or something similar. I will upload a suggestion in a second but need to run so tread it just as a suggestion.\n\nSimon \nSomething like that would be more appropriate IMO\nSimon,\n\nYes, that sound excactly like what I was thinking when I said \"some code\nthat reopens the searcher if the reference to it is null\".\n\nI just didn't include it in my patch because I couldn't figure out how to do it properly.\n\nI'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?\n\n\nReopening to get the AlreadyClosedException in there...\nWell not exactly. Simon's suggestion was just to throw an AlreadyClosedException instead of a NullPointerException which is probably ok and definitely easier.\nbq. I'd assume ensureOpen needs to be synchronized some way so that two threads can't open IndexSearchers concurrently?\n\nthis class is not threadsafe anyway. If you look at this snippet:\n{code}\n // close the old searcher, if there was one\nif (searcher != null) {\n  searcher.close();\n}\nsearcher = new IndexSearcher(this.spellIndex, true);\n{code}\nthere could be a race if you concurrently reindex or set a new dictionary. IMO this should either be documented or made threadsafe. The close method should invalidate the spellchecker - it should not be possible to use a already closed Spellchecker.\nThe searcher should be somehow ref counted so that if there is a searcher still in use you can concurrently reindex / add a new dictionary to ensure that the same searcher is used throughout suggestSimilar(). \n\nI will take care of it once I get back tomorrow.\nJust a reminder - we need to fix the CHANGES.TXT entry once this is done. \nMike,\n\nPlease account for my demonstrated stupidity when considering this suggestion for thread safety policy / goals:\n\n1) Concurrent invocations of  suggestSimilar() should not interfere with each other.\n2) An invocation of any of the write methods (setSpellIndex, clearIndex, indexDictionary) should not interfere with aleady invoked suggestSimilar\n3) All calls to write methods should be serialized (We could probably synchronize these methods?)\n\nIf we synchronize any writes to the searcher reference, couldn't suggestSimilar just start its work by putting searcher in a local variable and use that instead of the field?\n\nI guess concurrency is hard to get right.. \nEirik, could you open a new issue to address SpellChecker's non-thread-safety?  I actually thing simply documenting clearly that it's not thread safe is fine.\nbq. Just a reminder - we need to fix the CHANGES.TXT entry once this is done.\n\nSimon how about you do this, and take this issue (to commit your improvement to throw ACE not NPE)?  Thanks ;)\n", "issueSearchSentences": ["searcher.close();", "I can't find any way to close the IndexSearcher (and IndexReader) that", "Shouldn't the new close() method be public?", "(pubic void close, public void close, public void close...)", "close the old searcher, if there was one"], "issueSearchScores": [0.6979692578315735, 0.6622896194458008, 0.6497251987457275, 0.636307954788208, 0.6336966156959534]}
{"aId": 121, "code": "public LiveIndexWriterConfig setMergePolicy(MergePolicy mergePolicy) {\n    if (mergePolicy == null) {\n      throw new IllegalArgumentException(\"mergePolicy must not be null\");\n    }\n    this.mergePolicy = mergePolicy;\n    return this;\n  }", "comment": " Any merges in flight or any merges already registered by the previous MergePolicy are notaffected.", "issueId": "LUCENE-5883", "issueStringList": ["Move MergePolicy to LiveIndexWriterConfig", "Since LUCENE-5711, MergePolicy is no longer wired to an IndexWriter instance.", "Therefore it can be moved to be a live setting on IndexWriter, which will allow apps to plug-in an MP on a live IW instance, without closing/reopening the writer.", "See for example LUCENE-5526 - instead of adding MP to forceMerge, apps could change the MP before calling forceMerge, with e.g.", "an UpgradeIndexMergePolicy.", "I think we can also make MergeScheduler a live setting, though I currently don't see the benefits of doing that, so I'd rather not touch it now.", "Patch moves MergePolicy to LiveIndexWriterConfig.", "Note though that at the moment I removed the calls to MegePolicy.close() from within IndexWriter as I think it's wrong to do that.", "I also asked a question on the mailing list: http://markmail.org/message/psoohjaye3l3ejxl", "+1", "Maybe note in the javadocs that the change only takes effect for subsequent merge selection, i.e.", "any merges in flight or any merges already registered by the previous MergePolicy \"win\".", "Thanks Mike, I improved the jdocs.", "This patch also removes Closeable from MergePolicy as discussed on the mailing list.", "BTW, the way I implemented it is that top-level API methods grab the current MergePolicy on the IWC and pass it down to methods like mergeMiddle(), commitInternal() etc., so that if the MP changes in the middle, nothing weird happens.", "E.g.", "if a merge was registered, but midway the MP changes and its CFS settings are different ...", "I think it's safer and it's not important to have THE most recent MP affect already registered merges.", "If it wasn't clear, I think this is ready to commit.", "+1, thanks Shai!", "+1, very good!", "I think  LUCENE-5526 is not obsolete by that, but much easier to implement.", "I still would like to pass a MergePolicy to forceMerge, just for the forced merging.", "On the other hand, you can change merge policy, forceMerge() without waiting, and change back.", "This is possible, because the merge policy does not change for running merges...", "Indeed.", "With this patch it would mean to pass the given MP or config.getMP().", "We can discuss on LUCENE-5526 if it's needed at all though, I mean if it's OK to add another API to IW, when the app can change the MP, call forceMerge, change it back.", "But let's discuss there.", "Committed to trunk and 4x."], "SplitGT": [" Any merges in flight or any merges already registered by the previous MergePolicy are notaffected."], "issueString": "Move MergePolicy to LiveIndexWriterConfig\nSince LUCENE-5711, MergePolicy is no longer wired to an IndexWriter instance. Therefore it can be moved to be a live setting on IndexWriter, which will allow apps to plug-in an MP on a live IW instance, without closing/reopening the writer. See for example LUCENE-5526 - instead of adding MP to forceMerge, apps could change the MP before calling forceMerge, with e.g. an UpgradeIndexMergePolicy.\n\nI think we can also make MergeScheduler a live setting, though I currently don't see the benefits of doing that, so I'd rather not touch it now.\nPatch moves MergePolicy to LiveIndexWriterConfig. Note though that at the moment I removed the calls to MegePolicy.close() from within IndexWriter as I think it's wrong to do that. I also asked a question on the mailing list: http://markmail.org/message/psoohjaye3l3ejxl\n+1\n\nMaybe note in the javadocs that the change only takes effect for subsequent merge selection, i.e. any merges in flight or any merges already registered by the previous MergePolicy \"win\".\nThanks Mike, I improved the jdocs. This patch also removes Closeable from MergePolicy as discussed on the mailing list.\nBTW, the way I implemented it is that top-level API methods grab the current MergePolicy on the IWC and pass it down to methods like mergeMiddle(), commitInternal() etc., so that if the MP changes in the middle, nothing weird happens. E.g. if a merge was registered, but midway the MP changes and its CFS settings are different ... I think it's safer and it's not important to have THE most recent MP affect already registered merges.\nIf it wasn't clear, I think this is ready to commit.\n+1, thanks Shai!\n+1, very good!\n\nI think  LUCENE-5526 is not obsolete by that, but much easier to implement. I still would like to pass a MergePolicy to forceMerge, just for the forced merging.\n\nOn the other hand, you can change merge policy, forceMerge() without waiting, and change back. This is possible, because the merge policy does not change for running merges...\nIndeed. With this patch it would mean to pass the given MP or config.getMP(). We can discuss on LUCENE-5526 if it's needed at all though, I mean if it's OK to add another API to IW, when the app can change the MP, call forceMerge, change it back. But let's discuss there.\nCommitted to trunk and 4x.\n", "issueSearchSentences": ["Move MergePolicy to LiveIndexWriterConfig", "Patch moves MergePolicy to LiveIndexWriterConfig.", "I still would like to pass a MergePolicy to forceMerge, just for the forced merging.", "On the other hand, you can change merge policy, forceMerge() without waiting, and change back.", "Since LUCENE-5711, MergePolicy is no longer wired to an IndexWriter instance."], "issueSearchScores": [0.7607520818710327, 0.6856290698051453, 0.632084846496582, 0.612084150314331, 0.6017829179763794]}
{"aId": 122, "code": "public static int intToPrefixCoded(final int val, final int shift, final BytesRef bytes) {\n    intToPrefixCodedBytes(val, shift, bytes);\n    return bytes.hashCode();\n  }", "comment": " This is method is used by NumericTokenStream.", "issueId": "LUCENE-4690", "issueStringList": ["Optimize NumericUtils.", "*ToPrefixCoded(), add versions that don't hash", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).", "edit: Uwe pointed out they were used in one place.", "Other places still don't need it.", "Current code:", "{code}", "public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "if (shift>63 || shift<0)", "throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");", "int hash, nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;", "if (bytes.bytes.length < bytes.length) {", "bytes.grow(NumericUtils.BUF_SIZE_LONG);", "}", "bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "calculate hash", "for (int i = 1; i < bytes.length; i++) {", "hash = 31*hash + bytes.bytes[i];", "}", "return hash;", "}", "{code}", "Proposed code template (i.e.", "for all of the *ToPrefixCoded methods):", "{code}", "public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "assert (shift & ~0x3f) == 0;  // ensure shift is 0..63", "int nChars = (63-shift)/7 + 1;", "bytes.offset = 0;", "bytes.length = nChars+1;   // one extra for the byte that contains the shift info", "if (bytes.bytes.length < bytes.length) {", "bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max", "}", "bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);", "long sortableBits = val ^ 0x8000000000000000L;", "sortableBits >>>= shift;", "while (nChars > 0) {", "Store 7 bits per byte for compatibility", "with UTF-8 encoding of terms", "bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);", "sortableBits >>>= 7;", "}", "}", "{code}", "Some of the changes:", "Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.", "Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.", "replace grow with simple allocation.", "1. grow over-allocates all the time.", "Most of the time (like here) it's wasted space.", "2. grow copies the previous buffer when allocating a bigger buffer.", "This is wasted/unneeded here.", "removes hash code calculation", "An additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.", "One equivalent of i/7 is (i*37)>>8 for i in 0..63.", "This only takes 4 cycles.", "Curious, how do you know that and/or measure that?", "bq.", "Curious, how do you know that and/or measure that?", "The number of cycles?", "It's all documented in various places.", "Of course one needs a good sense of what assembly a compiler/hotspot will emit.", "Integer multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).", "http://gmplib.org/~tege/x86-timing.pdf", "http://www.agner.org/optimize/instruction_tables.pdf", "bq.", "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "{code:java}", "@Override", "public int fillBytesRef() {", "try {", "assert valueSize == 64 || valueSize == 32;", "return (valueSize == 64) ?", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "} catch (IllegalArgumentException iae) {", "return empty token before first or after last", "bytes.length = 0;", "return 0;", "}", "}", "{code}", "Other comments:", "The masking away of invalid shifts is a no-go to me.", "This leads to unexpected behaviour.", "A agree grow() does not need to be used for this stuff.", "We can simply reallocate, as we know size exactly.", "By the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.", "bq.", "The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():", "Ahh, I didn't see it because the use of the value is on a separate line from the method call.", "Makes it hard to find.", "Here's a patch that doubles the performance of NumericUtils.", "*ToPrefixCoded", "Seems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)", "Thanks, Yonik!"], "SplitGT": [" This is method is used by NumericTokenStream."], "issueString": "Optimize NumericUtils.*ToPrefixCoded(), add versions that don't hash\nAs far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).  If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself).\n\nedit: Uwe pointed out they were used in one place.  Other places still don't need it.\nCurrent code:\n{code}\n  public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {\n    if (shift>63 || shift<0)\n      throw new IllegalArgumentException(\"Illegal shift value, must be 0..63\");\n    int hash, nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;\n    if (bytes.bytes.length < bytes.length) {\n      bytes.grow(NumericUtils.BUF_SIZE_LONG);\n    }\n    bytes.bytes[0] = (byte) (hash = (SHIFT_START_LONG + shift));\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n    // calculate hash\n    for (int i = 1; i < bytes.length; i++) {\n      hash = 31*hash + bytes.bytes[i];\n    }\n    return hash;\n  }\n{code}\n\nProposed code template (i.e. for all of the *ToPrefixCoded methods):\n{code}\n  public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {\n    assert (shift & ~0x3f) == 0;  // ensure shift is 0..63\n    int nChars = (63-shift)/7 + 1;\n    bytes.offset = 0;\n    bytes.length = nChars+1;   // one extra for the byte that contains the shift info\n    if (bytes.bytes.length < bytes.length) {\n      bytes.bytes = new byte[NumericUtils.BUF_SIZE_LONG];  // use the max\n    }\n    bytes.bytes[0] = (byte)(SHIFT_START_LONG + shift);\n    long sortableBits = val ^ 0x8000000000000000L;\n    sortableBits >>>= shift;\n    while (nChars > 0) {\n      // Store 7 bits per byte for compatibility\n      // with UTF-8 encoding of terms\n      bytes.bytes[nChars--] = (byte)(sortableBits & 0x7f);\n      sortableBits >>>= 7;\n    }\n  }\n{code}\n\nSome of the changes:\n - Setting bytes.length to be larger than the current contained array temporarily puts BytesRef into an invalid state.  Calling any BytesRef methods (like grow) while it is in that invalid state is suspect.\n - replace grow with simple allocation.\n   1. grow over-allocates all the time.  Most of the time (like here) it's wasted space.\n   2. grow copies the previous buffer when allocating a bigger buffer.  This is wasted/unneeded here.\n - removes hash code calculation\nAn additional cool little hack... even though it's much improved, IDIV still has a latency of 18-42 cycles on a core2 processor.  One equivalent of i/7 is (i*37)>>8 for i in 0..63.  This only takes 4 cycles.\nCurious, how do you know that and/or measure that?\nbq. Curious, how do you know that and/or measure that?\n\nThe number of cycles?  It's all documented in various places.  Of course one needs a good sense of what assembly a compiler/hotspot will emit.\nInteger multiply has been 3 cycles for quite a while for both Intel and AMD, and shifts have been a single cycle (after the ill-fated P4).\n\nhttp://gmplib.org/~tege/x86-timing.pdf\nhttp://www.agner.org/optimize/instruction_tables.pdf\nbq. As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).\n\nThe return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\n{code:java}\n    @Override\n    public int fillBytesRef() {\n      try {\n        assert valueSize == 64 || valueSize == 32;\n        return (valueSize == 64) ? \n          NumericUtils.longToPrefixCoded(value, shift, bytes) :\n          NumericUtils.intToPrefixCoded((int) value, shift, bytes);\n      } catch (IllegalArgumentException iae) {\n        // return empty token before first or after last\n        bytes.length = 0;\n        return 0;\n      }\n    }\n{code}\n\nOther comments:\n- The masking away of invalid shifts is a no-go to me. This leads to unexpected behaviour.\n- A agree grow() does not need to be used for this stuff. We can simply reallocate, as we know size exactly.\nBy the way, your patch above would corrumpt the IAE case in fillBytesRef used by indexer.\nbq. The return value (the hash) is used by NumericTokenStreeam#NumericTermAttribute.fillBytesRef():\n\nAhh, I didn't see it because the use of the value is on a separate line from the method call.  Makes it hard to find.\nHere's a patch that doubles the performance of NumericUtils.*ToPrefixCoded\n\nSeems good to me to have non-hashing versions (these versions exist for unicodeutil for terms already for similar purposes)\nThanks, Yonik!\n", "issueSearchSentences": ["public static int longToPrefixCoded(final long val, final int shift, final BytesRef bytes) {", "public void longToPrefixCodedBytes(final long val, final int shift, final BytesRef bytes) {", "NumericUtils.intToPrefixCoded((int) value, shift, bytes);", "NumericUtils.longToPrefixCoded(value, shift, bytes) :", "*ToPrefixCoded"], "issueSearchScores": [0.8763465881347656, 0.874834418296814, 0.7649760842323303, 0.7478858828544617, 0.6070547699928284]}
{"aId": 123, "code": "public void optimize(int maxNumSegments) throws CorruptIndexException, IOException {\n    optimize(maxNumSegments, true);\n  }", "comment": " Optimize the index down to <= maxNumSegments.", "issueId": "LUCENE-982", "issueStringList": ["Create new method optimize(int maxNumSegments) in IndexWriter", "Spinning this out from the discussion in LUCENE-847.", "I think having a way to \"slightly optimize\" your index would be useful", "for many applications.", "The current optimize() call is very expensive for large indices", "because it always optimizes fully down to 1 segment.", "If we add a new", "method which instead is allowed to stop optimizing once it has <=", "maxNumSegments segments in the index, this would allow applications to", "eg optimize down to say <= 10 segments after doing a bunch of updates.", "This should be a nice compromise of gaining good speedups of searching", "while not spending the full (and typically very high) cost of", "optimizing down to a single segment.", "Since LUCENE-847 is now formalizing an API for decoupling merge policy", "from IndexWriter, if we want to add this new optimize method we need", "to take it into account in LUCENE-847.", "+1", "sounds like a great idea.", "One heuristic that has been quite useful for us is to skip optimizing segments that occupy some fixed fraction of the index.", "The remainder of the segments are optimized as usual (the heuristic can be applied recursively).", "70% is a decent number.", "Attached patch to implement new optimize(int maxNumSegments).", "I fixed LogMergePolicy to respect the maxNumSegments arg.", "When", "optimizing, we first concurrently do every mergeFactor section of", "segment merges (going back from the tail).", "Then, for the final partial (< mergeFactor segments) merge, we pick", "contiguous segments that are the smallest net size, as long as the", "resulting merged segment is not > 2X larger than the segment to its", "left (to prevent creating a lopsided index over time).", "Mike,", "I looked at your patch and I'm wondering if it wouldn't make more", "sense to limit the overall size of the segments (MB and/or num docs)", "involved in a merge rather than the number of segments?", "{quote}", "I looked at your patch and I'm wondering if it wouldn't make more", "sense to limit the overall size of the segments (MB and/or num docs)", "involved in a merge rather than the number of segments?", "{quote}", "Thanks for reviewing :)", "I think that's a good idea!", "But I'm torn on which is \"better\" as a first step.", "If we limit by size, then the benefit is, even as your index grows", "very large, the cost of optimizing is constant once you hit the max", "segment size.", "You keep your optimize cost down.", "But, then, your searches will get slower and slower as your index", "grows since these large segments never get merged (actually you'd have", "to set maxMergeDocs as well so normal merging wouldn't merge them).", "But limiting by segment count, I think you keep you search costs", "lower, at the expense of higher and higher optimize costs as your", "index gets larger.", "I think people optimize because they want to pay a high cost, once,", "now, in order to have fast[er] searches.", "So by limiting segment count", "during optimizing, we still leave the increasing cost (as your index", "grows) on the optimize() call.", "I think we should eventually do both?", "The good news is with the ability to customize MergePolicy, anyone can", "customize what it means to \"optimize\" an index just by implementing", "their own MergePolicy.", "{quote}", "I think people optimize because they want to pay a high cost, once,", "now, in order to have fast[er] searches.", "So by limiting segment count", "during optimizing, we still leave the increasing cost (as your index", "grows) on the optimize() call.", "{quote}", "Yeah good point.", "I understand the usecase for maxNumSegments.", "{quote}", "I think we should eventually do both?", "{quote}", "+1", "OK I plan to commit this in a day or two.", "I just committed this."], "SplitGT": [" Optimize the index down to <= maxNumSegments."], "issueString": "Create new method optimize(int maxNumSegments) in IndexWriter\nSpinning this out from the discussion in LUCENE-847.\n\nI think having a way to \"slightly optimize\" your index would be useful\nfor many applications.\n\nThe current optimize() call is very expensive for large indices\nbecause it always optimizes fully down to 1 segment.  If we add a new\nmethod which instead is allowed to stop optimizing once it has <=\nmaxNumSegments segments in the index, this would allow applications to\neg optimize down to say <= 10 segments after doing a bunch of updates.\nThis should be a nice compromise of gaining good speedups of searching\nwhile not spending the full (and typically very high) cost of\noptimizing down to a single segment.\n\nSince LUCENE-847 is now formalizing an API for decoupling merge policy\nfrom IndexWriter, if we want to add this new optimize method we need\nto take it into account in LUCENE-847.\n\n+1\n sounds like a great idea.\nOne heuristic that has been quite useful for us is to skip optimizing segments that occupy some fixed fraction of the index.  The remainder of the segments are optimized as usual (the heuristic can be applied recursively).  70% is a decent number.\nAttached patch to implement new optimize(int maxNumSegments).\n\nI fixed LogMergePolicy to respect the maxNumSegments arg.  When\noptimizing, we first concurrently do every mergeFactor section of\nsegment merges (going back from the tail).\n\nThen, for the final partial (< mergeFactor segments) merge, we pick\ncontiguous segments that are the smallest net size, as long as the\nresulting merged segment is not > 2X larger than the segment to its\nleft (to prevent creating a lopsided index over time).\n\nMike,\n\nI looked at your patch and I'm wondering if it wouldn't make more\nsense to limit the overall size of the segments (MB and/or num docs)\ninvolved in a merge rather than the number of segments?\n\n{quote}\nI looked at your patch and I'm wondering if it wouldn't make more\nsense to limit the overall size of the segments (MB and/or num docs)\ninvolved in a merge rather than the number of segments?\n{quote}\n\nThanks for reviewing :)\n\nI think that's a good idea!\n\nBut I'm torn on which is \"better\" as a first step.\n\nIf we limit by size, then the benefit is, even as your index grows\nvery large, the cost of optimizing is constant once you hit the max\nsegment size.  You keep your optimize cost down.\n\nBut, then, your searches will get slower and slower as your index\ngrows since these large segments never get merged (actually you'd have\nto set maxMergeDocs as well so normal merging wouldn't merge them).\n\nBut limiting by segment count, I think you keep you search costs\nlower, at the expense of higher and higher optimize costs as your\nindex gets larger.\n\nI think people optimize because they want to pay a high cost, once,\nnow, in order to have fast[er] searches.  So by limiting segment count\nduring optimizing, we still leave the increasing cost (as your index\ngrows) on the optimize() call.\n\nI think we should eventually do both?\n\nThe good news is with the ability to customize MergePolicy, anyone can\ncustomize what it means to \"optimize\" an index just by implementing\ntheir own MergePolicy.\n\n{quote}\nI think people optimize because they want to pay a high cost, once,\nnow, in order to have fast[er] searches. So by limiting segment count\nduring optimizing, we still leave the increasing cost (as your index\ngrows) on the optimize() call.\n{quote}\nYeah good point. I understand the usecase for maxNumSegments.\n\n{quote}\nI think we should eventually do both?\n{quote}\n+1\nOK I plan to commit this in a day or two.\nI just committed this.\n", "issueSearchSentences": ["Create new method optimize(int maxNumSegments) in IndexWriter", "Attached patch to implement new optimize(int maxNumSegments).", "maxNumSegments segments in the index, this would allow applications to", "I understand the usecase for maxNumSegments.", "The current optimize() call is very expensive for large indices"], "issueSearchScores": [0.8278374671936035, 0.8262981176376343, 0.7010204792022705, 0.6962132453918457, 0.6862434148788452]}
{"aId": 127, "code": "private void partitionTLDprefixesBySuffixLength() {\n    TLDsBySuffixLength.add(new TreeSet<>());            // initialize set for zero-suffix TLDs\n    for (SortedMap.Entry<String,Boolean> entry : processedTLDsLongestFirst.entrySet()) {\n      String TLD = entry.getKey();\n      if (entry.getValue()) {\n        // System.out.println(\"Skipping already processed: \" + TLD);\n        continue;\n      }\n      // System.out.println(\"Adding zero-suffix TLD: \" + TLD);\n      TLDsBySuffixLength.get(0).add(TLD);\n      for (int suffixLength = 1 ; (TLD.length() - suffixLength) >= 2 ; ++suffixLength) {\n        String TLDprefix = TLD.substring(0, TLD.length() - suffixLength);\n        if (false == processedTLDsLongestFirst.containsKey(TLDprefix)) {\n          // System.out.println(\"Ignoring non-TLD prefix: \" + TLDprefix);\n          break;                                        // shorter prefixes can be ignored\n        }\n        if (processedTLDsLongestFirst.get(TLDprefix)) {\n          // System.out.println(\"Skipping already processed prefix: \" + TLDprefix);\n          break;                                        // shorter prefixes have already been processed \n        }\n\n        processedTLDsLongestFirst.put(TLDprefix, true); // mark as processed\n        if (TLDsBySuffixLength.size() == suffixLength)\n          TLDsBySuffixLength.add(new TreeSet<>());\n        SortedSet<String> TLDbucket = TLDsBySuffixLength.get(suffixLength);\n        TLDbucket.add(TLDprefix);\n        // System.out.println(\"Adding TLD prefix of \" + TLD + \" with suffix length \" + suffixLength + \": \" + TLDprefix);\n      }\n    }\n  }", "comment": " Partition TLDs by whether they are prefixes of other TLDs and then by suffix length.", "issueId": "LUCENE-8278", "issueStringList": ["UAX29URLEmailTokenizer is not detecting some tokens as URL type", "We are using the UAX29URLEmailTokenizer so we can use the token types in our plugins.", "However, I noticed that the tokenizer is not detecting certain URLs as <URL> but <ALPHANUM> instead.", "Examples that are not working:", "example.com is <ALPHANUM>", "example.net is <ALPHANUM>", "But:", "https://example.com is <URL>", "as is https://example.net", "Examples that work:", "example.ch is <URL>", "example.co.uk is <URL>", "example.nl is <URL>", "I have checked this JIRA, and could not find an issue.", "I have tested this on Lucene (Solr) 6.4.1 and 7.3.", "Could someone confirm my findings and advise what I could do to (help) resolve this issue?", "Confirming there is an issue, but I don't think the spellings of \"example.com\" and \"example.net\" are the problem though; more likely this is related to an end-of-input issue.", "This test added to {{TestUAX29URLEmailTokenizer}} fails for me:", "{noformat}", "public void testExampleURLs() throws Exception {", "Analyzer analyzer = new Analyzer() {", "@Override protected TokenStreamComponents createComponents(String fieldName) {", "return new TokenStreamComponents(new UAX29URLEmailTokenizer(newAttributeFactory()));", "}};", "A trailing space allows these to succeed", "BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com \", new String[]{\"example.com\"}, new String[]{\"<URL>\"});", "BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net \", new String[]{\"example.net\"}, new String[]{\"<URL>\"});", "These fail", "BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com\", new String[]{\"example.com\"}, new String[]{\"<URL>\"});", "BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net\", new String[]{\"example.net\"}, new String[]{\"<URL>\"});", "}", "{noformat}", "So there is an issue here with no-scheme end-of-input URLs not being recognized as type {{<URL>}}.", "Hmm, \"example.co\" and \"example.info\" in the above test succeed, so the problem here *is* somehow related to TLD spelling.", "Thank you for confirming this issue Steve.", "We run Lucene/Solr 6.6 on our production servers, and we also found this workaround to append a whitespace to the token to work on this version.", "However, this workaround is no longer working on Lucene 7.3.0.", "I'll see if I can fix this...", "I ran a test to check all TLDs appended to \"example.", "\", and 169 out of 1543 possible TLDs have this problem:", "{code:java}", "\"accountants\", \"ads\", \"aeg\", \"afl\", \"aig\", \"aol\", \"art\", \"audio\", \"autos\", \"aws\", \"axa\", \"bar\", \"bbc\", \"bet\",", "\"bid\", \"bingo\", \"bms\", \"bnl\", \"bom\", \"boo\", \"bot\", \"box\", \"bzh\", \"cab\", \"cal\", \"cam\", \"camp\", \"car\", \"care\",", "\"careers\", \"cat\", \"cfa\", \"citic\", \"com\", \"coupons\", \"crs\", \"cruises\", \"deals\", \"dev\", \"dog\", \"dot\", \"eco\",", "\"esq\", \"eus\", \"fans\", \"fit\", \"foo\", \"fox\", \"frl\", \"fund\", \"gal\", \"games\", \"gdn\", \"gea\", \"gifts\", \"gle\",", "\"gmo\", \"goog\", \"hkt\", \"htc\", \"ing\", \"int\", \"ist\", \"itv\", \"jmp\", \"jot\", \"kia\", \"kpn\", \"krd\", \"lat\", \"law\",", "\"loans\", \"ltd\", \"man\", \"map\", \"markets\", \"med\", \"men\", \"mlb\", \"mma\", \"moe\", \"mov\", \"msd\", \"mtn\", \"nab\",", "\"nec\", \"new\", \"news\", \"nfl\", \"ngo\", \"now\", \"nra\", \"pay\", \"pet\", \"phd\", \"photos\", \"ping\", \"pnc\", \"pro\",", "\"prof\", \"pru\", \"pwc\", \"red\", \"reisen\", \"ren\", \"reviews\", \"run\", \"rwe\", \"sap\", \"sas\", \"sbi\", \"sca\", \"ses\",", "\"sew\", \"ski\", \"soy\", \"srl\", \"stc\", \"taxi\", \"tci\", \"tdk\", \"thd\", \"tjx\", \"top\", \"trv\", \"tvs\", \"vet\", \"vig\",", "\"vin\", \"wine\", \"works\", \"aco\", \"aigo\", \"arte\", \"bbt\", \"bio\", \"biz\", \"bmw\", \"book\", \"call\", \"cars\", \"cfd\",", "\"food\", \"gap\", \"gmx\", \"ink\", \"joy\", \"kim\", \"ltda\", \"menu\", \"meo\", \"mls\", \"moi\", \"mom\", \"mtr\", \"net\", \"nrw\",", "\"pink\", \"prod\", \"rent\", \"sapo\", \"sbs\", \"scb\", \"sex\", \"sexy\", \"skin\", \"sky\", \"srt\", \"vip\"", "{code}", "In each of the above cases I've looked at, there is a TLD that is a prefix that is shorter by one letter (see the [branch_7x TLD regex|https://git1-us-west.apache.org/repos/asf?p=lucene-solr.git;a=blob;f=lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro;hb=refs/heads/branch_7x]).", "Not sure if all such TLDs have this problem; I'll look.", "Also, on branch_7x anyway (from which 7.3.0 was cut a few months ago), appending a space to the input still works around the problem for me, so I can't reproduce the non-working workaround that you say is a problem with 7.3.0, [~drjz].", "bq.", "Not sure if all such TLDs have this problem; I'll look.", "Yes, the problematic TLDs are exactly the set of those for which there exists a one-letter-shorter prefix.", "I suspect this behavior is caused as a side-effect of the fix for LUCENE-5391.", "I've attached a fully-regenerated patch (which is why it's so big...) against the master branch for a fix I cooked up.", "In this change, the TLD macro generator partitions TLDs by whether they are prefixes of other TLDs, and by suffix length, and then the grammar tries the longest TLDs first, falling back one suffix char at a time.", "Currently there are only 3 buckets:", "# None of the TLDs is a 1-character-shorter prefix of another TLD", "# Each TLD is a prefix of another TLD by 1 character", "# Each TLD is a prefix of another TLD by 2 characters", "The TLD macro generator does not hard code the number of buckets, so it should be able to handle future TLD prefixes with suffixes of more than 2 characters.", "I've added a test for {{example.TLD}} URLs at end-of-input for all TLDs, and it passes, as do all other tests in the analyzers-common module.", "FYI, the fix here was complicated by the fact that JFlex doesn't support end-of-input assertion (like Java's {{\\z}}) as part of a lexical rule: the {{<<EOF>>}} rule can't be combined with a regex, and zero-length lookahead assertions must match at least one character.", "[~drjz], can you test this in your context?", "Here's the output from the TLD macro generator ({{ant gen-tlds}}) with the patch:", "{noformat}", "gen-tlds:", "[java] Found 1541 TLDs in IANA Root Zone Database at http://www.internic.net/zones/root.zone", "[java] Wrote TLD macros to '/Users/sarowe/git/lucene-solr-3/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro':", "[java]                       ASCIITLD: 1420 TLDs", "[java]     ASCIITLDprefix_1CharSuffix:  109 TLDs", "[java]     ASCIITLDprefix_2CharSuffix:   12 TLDs", "[java]                          Total: 1541 TLDs", "{noformat}", "I plan on committing this tomorrow if I don't get any feedback before then.", "Hi Steve, sorry for the late response.", "I will check this tomorrow.", "Thanks for picking up this bug report!", "bq.", "I will check this tomorrow.", "Any luck [~drjz]?", "I think I have tested the patch:", "{code:java}", "patch -p1 -i LUCENE-8278.patch", "patching file lucene/analysis/common/build.xml", "patching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro", "patching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java", "patching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex", "patching file lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java", "patching file lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java", "{code}", "then ant compile", "Started Solr and created a core with a fieldType:", "{code:java}", "<fieldType name=\"urlEmail\" class=\"solr.TextField\">", "<analyzer>", "<tokenizer class=\"solr.UAX29URLEmailTokenizerFactory\"/>", "</analyzer>", "</fieldType>{code}", "Then tested in the Solr Admin but didn't see a difference, but perhaps I missed something.", "bq.", "Started Solr [...] Then tested in the Solr Admin but didn't see a difference", "How did you start Solr?", "You must first run {{ant server}} to put the modified {{lucene-analyzers-common}} jar into {{solr/server/}}, which will then be used when you run {{bin/solr start}}.", "I made a Solr configset with an analyzer containing only a {{solr.UAX29URLEmailTokenizerFactory}} tokenizer, then ran Solr on master both without the patch: !unpatched.png!", "and with the patch: !patched.png!.", "[~drjz]: I think you're just having issues running the modified code.", "Committing shortly."], "SplitGT": [" Partition TLDs by whether they are prefixes of other TLDs and then by suffix length."], "issueString": "UAX29URLEmailTokenizer is not detecting some tokens as URL type\nWe are using the UAX29URLEmailTokenizer so we can use the token types in our plugins.\r\n\r\nHowever, I noticed that the tokenizer is not detecting certain URLs as <URL> but <ALPHANUM> instead.\r\n\r\nExamples that are not working:\r\n * example.com is <ALPHANUM>\r\n * example.net is <ALPHANUM>\r\n\r\nBut:\r\n * https://example.com is <URL>\r\n * as is https://example.net\r\n\r\nExamples that work:\r\n * example.ch is <URL>\r\n * example.co.uk is <URL>\r\n * example.nl is <URL>\r\n\r\nI have checked this JIRA, and could not find an issue. I have tested this on Lucene (Solr) 6.4.1 and 7.3.\r\n\r\nCould someone confirm my findings and advise what I could do to (help) resolve this issue?\nConfirming there is an issue, but I don't think the spellings of \"example.com\" and \"example.net\" are the problem though; more likely this is related to an end-of-input issue.  This test added to {{TestUAX29URLEmailTokenizer}} fails for me:\r\n  \r\n{noformat}\r\n  public void testExampleURLs() throws Exception {\r\n    Analyzer analyzer = new Analyzer() {\r\n      @Override protected TokenStreamComponents createComponents(String fieldName) {\r\n        return new TokenStreamComponents(new UAX29URLEmailTokenizer(newAttributeFactory()));\r\n      }};\r\n\r\n    // A trailing space allows these to succeed\r\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com \", new String[]{\"example.com\"}, new String[]{\"<URL>\"});\r\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net \", new String[]{\"example.net\"}, new String[]{\"<URL>\"});\r\n    \r\n    // These fail\r\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com\", new String[]{\"example.com\"}, new String[]{\"<URL>\"});\r\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net\", new String[]{\"example.net\"}, new String[]{\"<URL>\"});\r\n  }\r\n{noformat}\r\n\r\nSo there is an issue here with no-scheme end-of-input URLs not being recognized as type {{<URL>}}.\nHmm, \"example.co\" and \"example.info\" in the above test succeed, so the problem here *is* somehow related to TLD spelling. \nThank you for confirming this issue Steve. We run Lucene/Solr 6.6 on our production servers, and we also found this workaround to append a whitespace to the token to work on this version. However, this workaround is no longer working on Lucene 7.3.0. I'll see if I can fix this...\nI ran a test to check all TLDs appended to \"example.\", and 169 out of 1543 possible TLDs have this problem:\r\n\r\n{code:java}\r\n\"accountants\", \"ads\", \"aeg\", \"afl\", \"aig\", \"aol\", \"art\", \"audio\", \"autos\", \"aws\", \"axa\", \"bar\", \"bbc\", \"bet\",\r\n\"bid\", \"bingo\", \"bms\", \"bnl\", \"bom\", \"boo\", \"bot\", \"box\", \"bzh\", \"cab\", \"cal\", \"cam\", \"camp\", \"car\", \"care\", \r\n\"careers\", \"cat\", \"cfa\", \"citic\", \"com\", \"coupons\", \"crs\", \"cruises\", \"deals\", \"dev\", \"dog\", \"dot\", \"eco\", \r\n\"esq\", \"eus\", \"fans\", \"fit\", \"foo\", \"fox\", \"frl\", \"fund\", \"gal\", \"games\", \"gdn\", \"gea\", \"gifts\", \"gle\", \r\n\"gmo\", \"goog\", \"hkt\", \"htc\", \"ing\", \"int\", \"ist\", \"itv\", \"jmp\", \"jot\", \"kia\", \"kpn\", \"krd\", \"lat\", \"law\", \r\n\"loans\", \"ltd\", \"man\", \"map\", \"markets\", \"med\", \"men\", \"mlb\", \"mma\", \"moe\", \"mov\", \"msd\", \"mtn\", \"nab\", \r\n\"nec\", \"new\", \"news\", \"nfl\", \"ngo\", \"now\", \"nra\", \"pay\", \"pet\", \"phd\", \"photos\", \"ping\", \"pnc\", \"pro\", \r\n\"prof\", \"pru\", \"pwc\", \"red\", \"reisen\", \"ren\", \"reviews\", \"run\", \"rwe\", \"sap\", \"sas\", \"sbi\", \"sca\", \"ses\", \r\n\"sew\", \"ski\", \"soy\", \"srl\", \"stc\", \"taxi\", \"tci\", \"tdk\", \"thd\", \"tjx\", \"top\", \"trv\", \"tvs\", \"vet\", \"vig\", \r\n\"vin\", \"wine\", \"works\", \"aco\", \"aigo\", \"arte\", \"bbt\", \"bio\", \"biz\", \"bmw\", \"book\", \"call\", \"cars\", \"cfd\", \r\n\"food\", \"gap\", \"gmx\", \"ink\", \"joy\", \"kim\", \"ltda\", \"menu\", \"meo\", \"mls\", \"moi\", \"mom\", \"mtr\", \"net\", \"nrw\", \r\n\"pink\", \"prod\", \"rent\", \"sapo\", \"sbs\", \"scb\", \"sex\", \"sexy\", \"skin\", \"sky\", \"srt\", \"vip\"\r\n{code}\r\n\r\nIn each of the above cases I've looked at, there is a TLD that is a prefix that is shorter by one letter (see the [branch_7x TLD regex|https://git1-us-west.apache.org/repos/asf?p=lucene-solr.git;a=blob;f=lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro;hb=refs/heads/branch_7x]).  Not sure if all such TLDs have this problem; I'll look.\r\n\r\nAlso, on branch_7x anyway (from which 7.3.0 was cut a few months ago), appending a space to the input still works around the problem for me, so I can't reproduce the non-working workaround that you say is a problem with 7.3.0, [~drjz].\nbq.  Not sure if all such TLDs have this problem; I'll look.\r\n\r\nYes, the problematic TLDs are exactly the set of those for which there exists a one-letter-shorter prefix.\nI suspect this behavior is caused as a side-effect of the fix for LUCENE-5391.\nI've attached a fully-regenerated patch (which is why it's so big...) against the master branch for a fix I cooked up.  In this change, the TLD macro generator partitions TLDs by whether they are prefixes of other TLDs, and by suffix length, and then the grammar tries the longest TLDs first, falling back one suffix char at a time.  Currently there are only 3 buckets: \r\n\r\n# None of the TLDs is a 1-character-shorter prefix of another TLD\r\n# Each TLD is a prefix of another TLD by 1 character\r\n# Each TLD is a prefix of another TLD by 2 characters\r\n\r\nThe TLD macro generator does not hard code the number of buckets, so it should be able to handle future TLD prefixes with suffixes of more than 2 characters. \r\n\r\nI've added a test for {{example.TLD}} URLs at end-of-input for all TLDs, and it passes, as do all other tests in the analyzers-common module.\r\n\r\nFYI, the fix here was complicated by the fact that JFlex doesn't support end-of-input assertion (like Java's {{\\z}}) as part of a lexical rule: the {{<<EOF>>}} rule can't be combined with a regex, and zero-length lookahead assertions must match at least one character.\r\n\r\n[~drjz], can you test this in your context?\nHere's the output from the TLD macro generator ({{ant gen-tlds}}) with the patch:\r\n\r\n{noformat}\r\ngen-tlds:\r\n     [java] Found 1541 TLDs in IANA Root Zone Database at http://www.internic.net/zones/root.zone\r\n     [java] Wrote TLD macros to '/Users/sarowe/git/lucene-solr-3/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro':\r\n     [java]                       ASCIITLD: 1420 TLDs\r\n     [java]     ASCIITLDprefix_1CharSuffix:  109 TLDs\r\n     [java]     ASCIITLDprefix_2CharSuffix:   12 TLDs\r\n     [java]                          Total: 1541 TLDs\r\n{noformat}\nI plan on committing this tomorrow if I don't get any feedback before then.\nHi Steve, sorry for the late response. I will check this tomorrow. Thanks for picking up this bug report! \nbq. I will check this tomorrow.\r\n\r\nAny luck [~drjz]?\nI think I have tested the patch:\r\n{code:java}\r\npatch -p1 -i LUCENE-8278.patch \r\npatching file lucene/analysis/common/build.xml\r\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro\r\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java\r\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex\r\npatching file lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java\r\npatching file lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java\r\n{code}\r\n\u00a0\r\n\r\nthen ant compile\r\n\r\nStarted Solr and created a core with a fieldType:\r\n{code:java}\r\n<fieldType name=\"urlEmail\" class=\"solr.TextField\">\r\n\u00a0\u00a0\u00a0\u00a0\u00a0 <analyzer>\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <tokenizer class=\"solr.UAX29URLEmailTokenizerFactory\"/>\r\n\u00a0\u00a0\u00a0\u00a0\u00a0 </analyzer>\r\n</fieldType>{code}\r\nThen tested in the Solr Admin but didn't see a difference, but perhaps I missed something.\nbq. Started Solr [...] Then tested in the Solr Admin but didn't see a difference\r\n\r\nHow did you start Solr?  You must first run {{ant server}} to put the modified {{lucene-analyzers-common}} jar into {{solr/server/}}, which will then be used when you run {{bin/solr start}}.\nI made a Solr configset with an analyzer containing only a {{solr.UAX29URLEmailTokenizerFactory}} tokenizer, then ran Solr on master both without the patch: !unpatched.png! and with the patch: !patched.png!.\r\n\r\n[~drjz]: I think you're just having issues running the modified code.\r\n\r\nCommitting shortly.\n", "issueSearchSentences": ["In this change, the TLD macro generator partitions TLDs by whether they are prefixes of other TLDs, and by suffix length, and then the grammar tries the longest TLDs first, falling back one suffix char at a time.", "Yes, the problematic TLDs are exactly the set of those for which there exists a one-letter-shorter prefix.", "In each of the above cases I've looked at, there is a TLD that is a prefix that is shorter by one letter (see the [branch_7x TLD regex|https://git1-us-west.apache.org/repos/asf?p=lucene-solr.git;a=blob;f=lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro;hb=refs/heads/branch_7x]).", "\", and 169 out of 1543 possible TLDs have this problem:", "Not sure if all such TLDs have this problem; I'll look."], "issueSearchScores": [0.6366192102432251, 0.6162740588188171, 0.5275192856788635, 0.524219274520874, 0.5221132040023804]}
{"aId": 128, "code": "final DocumentsWriterPerThread checkoutLargestNonPendingWriter() {\n    ThreadState largestNonPendingWriter = findLargestNonPendingWriter();\n    if (largestNonPendingWriter != null) {\n      // we only lock this very briefly to swap it's DWPT out - we don't go through the DWPTPool and it's free queue\n      largestNonPendingWriter.lock();\n      try {\n        synchronized (this) {\n          try {\n            if (largestNonPendingWriter.isInitialized() == false) {\n              return nextPendingFlush();\n            } else {\n              return checkout(largestNonPendingWriter, largestNonPendingWriter.isFlushPending() == false);\n            }\n          } finally {\n            updateStallState();\n          }\n        }\n      } finally {\n        largestNonPendingWriter.unlock();\n      }\n    }\n    return null;\n  }", "comment": " Returns the largest non-pending flushable DWPT or null if there is none.", "issueId": "LUCENE-8068", "issueStringList": ["Allow IndexWriter to write a single DWPT to disk", "Today we IW can only flush a DWPT to disk if an external resource calls flush()  or refreshes a NRT reader or if a DWPT is selected as flush pending.", "Yet, the latter has the problem that it always ties up an indexing thread and if flush / NRT refresh is called a whole bunch of indexing threads is tied up.", "If IW could offer a simple `flushNextBuffer()` method that synchronously flushes the next pending or biggest active buffer to disk memory could be controlled in a more fine granular fashion from outside of the IW.", "This is for instance useful if more than one IW (shards) must be maintained in a single JVM / system.", "here is an initial patch", "added some more javadocs and shared more code in `DocumentsWriterFlushControl` [~mikemccand] can you take a look a this?", "This is a really nice idea!", "It gives more granular control over moving IW's RAM buffer to disk, and lets other threads (than the indexing threads) participate in flushing.", "Can you mark the new method as {{@lucene.experimental}}?", "There's no guarantee it flushes that largest (most heap consuming) DWPT right?", "I think that's fine.", "It looks like it first tries to find any DWPT already marked for flush, and failing that, it then finds the largest one.", "What if the largest one is currently still indexing a document (via another thread)?", "Do we wait (on the {{lock}} call) for that one document to finish?", "Or are we only iterating over DWPTs not currently indexing a document?", "But then is there a starvation risk?", "Is there a (small) concurrency risk that a flush (via another thread) is called right after you first asked for next pending DWPT, got null, then tried to find the largest non-pending DWPT, but the concurrent flush has now marked them all pending?", "I think it's fine if so; maybe explain in the javadocs that this is just \"best effort\"?", "Should the new method maybe return a boolean indicating whether it actually wrote a segment?", "Maybe instead of using \"documents writer per thread\" and \"writer per thread buffer\" in the javadocs, just refer to them as the per-thread in memory segments?", "Small typo here (\"non\" -> \"none\"):", "{noformat}", "+   * Returns the largest non-pending flushable DWPT or <code>null</code> if there is non.", "{noformat}", "Maybe assert that {{freeList.remove}} returned true here?", "{noformat}", "+  ThreadState getAndLock(ThreadState state) {", "+    synchronized (this) {", "+      freeList.remove(state);", "+    }", "+    state.lock();", "+    return state;", "+  }", "+", "{noformat}", "thanks [~mikemccand] I applied your feedback and simplified the locking part in the DWPTThreadPool.", "can you take another look", "+1, new patch looks great, except still has \"non\" typo :)", "new patch with fixed typo and a changes entry.", "I will commit this later today or tomorrow unless anybody objects."], "SplitGT": [" Returns the largest non-pending flushable DWPT or null if there is none."], "issueString": "Allow IndexWriter to write a single DWPT to disk\nToday we IW can only flush a DWPT to disk if an external resource calls flush()  or refreshes a NRT reader or if a DWPT is selected as flush pending. Yet, the latter has the problem that it always ties up an indexing thread and if flush / NRT refresh is called a whole bunch of indexing threads is tied up. If IW could offer a simple `flushNextBuffer()` method that synchronously flushes the next pending or biggest active buffer to disk memory could be controlled in a more fine granular fashion from outside of the IW. This is for instance useful if more than one IW (shards) must be maintained in a single JVM / system. \nhere is an initial patch\nadded some more javadocs and shared more code in `DocumentsWriterFlushControl` [~mikemccand] can you take a look a this?\nThis is a really nice idea!  It gives more granular control over moving IW's RAM buffer to disk, and lets other threads (than the indexing threads) participate in flushing.\r\n\r\nCan you mark the new method as {{@lucene.experimental}}?\r\n\r\nThere's no guarantee it flushes that largest (most heap consuming) DWPT right?  I think that's fine.  It looks like it first tries to find any DWPT already marked for flush, and failing that, it then finds the largest one.\r\n\r\nWhat if the largest one is currently still indexing a document (via another thread)?  Do we wait (on the {{lock}} call) for that one document to finish?  Or are we only iterating over DWPTs not currently indexing a document?  But then is there a starvation risk?\r\n\r\nIs there a (small) concurrency risk that a flush (via another thread) is called right after you first asked for next pending DWPT, got null, then tried to find the largest non-pending DWPT, but the concurrent flush has now marked them all pending?  I think it's fine if so; maybe explain in the javadocs that this is just \"best effort\"?\r\n\r\nShould the new method maybe return a boolean indicating whether it actually wrote a segment?\r\n\r\nMaybe instead of using \"documents writer per thread\" and \"writer per thread buffer\" in the javadocs, just refer to them as the per-thread in memory segments?\r\n\r\nSmall typo here (\"non\" -> \"none\"):\r\n\r\n{noformat}\r\n+   * Returns the largest non-pending flushable DWPT or <code>null</code> if there is non.\r\n{noformat}\r\n\r\nMaybe assert that {{freeList.remove}} returned true here?\r\n\r\n{noformat}\r\n+  ThreadState getAndLock(ThreadState state) {\r\n+    synchronized (this) {\r\n+      freeList.remove(state);\r\n+    }\r\n+    state.lock();\r\n+    return state;\r\n+  }\r\n+\r\n{noformat}\r\n\nthanks [~mikemccand] I applied your feedback and simplified the locking part in the DWPTThreadPool. can you take another look\n+1, new patch looks great, except still has \"non\" typo :)\nnew patch with fixed typo and a changes entry. I will commit this later today or tomorrow unless anybody objects.\n", "issueSearchSentences": ["Is there a (small) concurrency risk that a flush (via another thread) is called right after you first asked for next pending DWPT, got null, then tried to find the largest non-pending DWPT, but the concurrent flush has now marked them all pending?", "Maybe instead of using \"documents writer per thread\" and \"writer per thread buffer\" in the javadocs, just refer to them as the per-thread in memory segments?", "It looks like it first tries to find any DWPT already marked for flush, and failing that, it then finds the largest one.", "What if the largest one is currently still indexing a document (via another thread)?", "thanks [~mikemccand] I applied your feedback and simplified the locking part in the DWPTThreadPool."], "issueSearchScores": [0.6343637704849243, 0.5474517345428467, 0.5453816652297974, 0.5429487228393555, 0.5061953067779541]}
{"aId": 129, "code": "public String[] list() {\n    return directory.list(new IndexFileNameFilter());\n  }", "comment": " Returns an array of strings, one for each Lucene index file in the directory.", "issueId": "LUCENE-638", "issueStringList": ["Can't put non-index files (e.g.", "CVS, SVN directories) in a Lucene index directory", "Lucene won't tolerate foreign files in its index directories.", "This makes it impossible to keep an index in a CVS or Subversion repository.", "For instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called \".svn\".", "java.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn", "(Is a directory)", "at java.io.RandomAccessFile.open(Native Method)", "at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)", "at", "org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)", "at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)", "at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)", "at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)", "at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)", "What exactly does your code look like?", "Something else must be wrong because I use an index that's committed to CVS without problems (using Lucene 2.0).", "Here's an example.", "Compile and run this with lucene-core-2.0.0.jar in the classpath.", "The RAMDirectory class uses the Directory.list() method to create the index input in the constructor.", "The implementation of FSDirectory returns all containing files of the directory including non lucene files and also all containing directories.", "In my opinion this method should not return non lucene files and directories for consistency.", "The problem does only appear if you create a RamDirectory from a existing index.", "here is the patch for the trunk:", "(I also added a try / finally block in the rename() method to close all streams in the case of an exception on the input stream the output stream would remain open.)", "All test passed, almost no tests in the store package :).", "Index: FSDirectory.java", "===================================================================", "FSDirectory.java    (revision 426479)", "+++ FSDirectory.java    (working copy)", "@@ -211,9 +211,9 @@", "}", "}", "/** Returns an array of strings, one for each file in the directory.", "*/", "+  /** Returns an array of strings, one for each lucnene index file in the directory.", "*/", "public String[] list() {", "return directory.list();", "+    return directory.list(new IndexFileNameFilter());", "}", "@@ -296,20 +296,23 @@", "throw newExc;", "}", "finally {", "if (in != null) {", "try {", "in.close();", "} catch (IOException e) {", "throw new RuntimeException(\"Cannot close input stream: \" + e.toString(), e);", "+    try{", "+          if (in != null) {", "+            try {", "+              in.close();", "+            } catch (IOException e) {", "+              throw new RuntimeException(\"Cannot close input stream: \" + e.toString(), e);", "+            }", "}", "}", "if (out != null) {", "try {", "out.close();", "} catch (IOException e) {", "throw new RuntimeException(\"Cannot close output stream: \" + e.toString(), e);", "+    }finally{", "+          if (out != null) {", "+            try {", "+              out.close();", "+            } catch (IOException e) {", "+              throw new RuntimeException(\"Cannot close output stream: \" + e.toString(), e);", "+            }", "}", "}", "+    }", "}", "}", "}", "Thanks, this has now been fixed in trunk."], "SplitGT": [" Returns an array of strings, one for each Lucene index file in the directory."], "issueString": "Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directory\nLucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.\n\nFor instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called \".svn\".\n\njava.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn\n(Is a directory)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n        at\norg.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)\n        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)\n        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)\n        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)\n        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)\nWhat exactly does your code look like? Something else must be wrong because I use an index that's committed to CVS without problems (using Lucene 2.0).\n\nHere's an example.  Compile and run this with lucene-core-2.0.0.jar in the classpath.\nThe RAMDirectory class uses the Directory.list() method to create the index input in the constructor. The implementation of FSDirectory returns all containing files of the directory including non lucene files and also all containing directories. \nIn my opinion this method should not return non lucene files and directories for consistency. \nThe problem does only appear if you create a RamDirectory from a existing index.\n\nhere is the patch for the trunk: \n(I also added a try / finally block in the rename() method to close all streams in the case of an exception on the input stream the output stream would remain open.)\nAll test passed, almost no tests in the store package :).\n\n\nIndex: FSDirectory.java\n===================================================================\n--- FSDirectory.java    (revision 426479) \n+++ FSDirectory.java    (working copy) \n@@ -211,9 +211,9 @@\n     }\n   }\n \n-  /** Returns an array of strings, one for each file in the directory. */ \n+  /** Returns an array of strings, one for each lucnene index file in the directory. */ \n   public String[] list() {\n-    return directory.list(); \n+    return directory.list(new IndexFileNameFilter()); \n   }\n \n   /** Returns true iff a file with the given name exists. */\n@@ -296,20 +296,23 @@\n         throw newExc;\n       }\n       finally {\n-        if (in != null) { \n-          try { \n-            in.close(); \n-          } catch (IOException e) { \n-            throw new RuntimeException(\"Cannot close input stream: \" + e.toString(), e); \n+    try{       \n+          if (in != null) { \n+            try { \n+              in.close(); \n+            } catch (IOException e) { \n+              throw new RuntimeException(\"Cannot close input stream: \" + e.toString(), e); \n+            } \n           }\n-        } \n-        if (out != null) { \n-          try { \n-            out.close(); \n-          } catch (IOException e) { \n-            throw new RuntimeException(\"Cannot close output stream: \" + e.toString(), e); \n+    }finally{ \n+          if (out != null) { \n+            try { \n+              out.close(); \n+            } catch (IOException e) { \n+              throw new RuntimeException(\"Cannot close output stream: \" + e.toString(), e); \n+            } \n           }\n-        } \n+    } \n       }\n     }\n   }\nThanks, this has now been fixed in trunk.\n\n", "issueSearchSentences": ["public String[] list() {", "+    return directory.list(new IndexFileNameFilter());", "return directory.list();", "The RAMDirectory class uses the Directory.list() method to create the index input in the constructor.", "In my opinion this method should not return non lucene files and directories for consistency."], "issueSearchScores": [0.7885395884513855, 0.7873740792274475, 0.7595915794372559, 0.5269357562065125, 0.4824102520942688]}
{"aId": 132, "code": "public void setDateResolution(String fieldName, DateTools.Resolution dateResolution) {\n    if (fieldName == null) {\n      throw new IllegalArgumentException(\"Field cannot be null.\");\n    }\n\n    if (fieldToDateResolution == null) {\n      // lazily initialize HashMap\n      fieldToDateResolution = new HashMap();\n    }\n\n    fieldToDateResolution.put(fieldName, dateResolution);\n  }", "comment": " Sets the date resolution used by RangeQueries for a specific field.", "issueId": "LUCENE-732", "issueStringList": ["Support DateTools in QueryParser", "The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values.", "However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated.", "In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.", "This patch replaces the use of DateField in QueryParser by DateTools.", "Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:", "Sets the default date resolution used by RangeQueries for fields for which no", "specific date resolutions has been set.", "Field specific resolutions can be set", "with {@link #setDateResolution(String, DateTools.Resolution)}.", "@param dateResolution the default date resolution to set", "public void setDateResolution(DateTools.Resolution dateResolution);", "Sets the date resolution used by RangeQueries for a specific field.", "@param field field for which the date resolution is to be set", "@param dateResolution date resolution to set", "public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);", "(I also added the corresponding getter methods).", "Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.", "The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY.", "Please let me know if you think we should use a different resolution as default.", "I extended TestQueryParser to test this new feature.", "All unit tests pass.", "I'm not sure if most people use DateTools already, as it has just been added in Lucene 1.9.", "Maybe you could consider an option (yes, yet another option isn't nice, I know)?", "Otherwise we need to properly document how to continue using DateField, i.e.", "by extending QueryParser and overwriting this method I guess.", "cleanest way to be backwards compatible would be to not have an initial default Resolution, and use DateField if no Resolution can be found for the field specified.", "Existing clients would still get DateField for all dates, clients that add a call to setDateResolution(\"foo\", BAR); would get a DateTools formated query with resolution BAR for field foo, but other pre-existing fields would still use DateField, and clients that call setDateResolution(BAR); would allways get a DateTools formatted query, either at resolution BAR or at some other resolution if they also use the two arg setDateResolution", "Actually it is documented in the QueryParser how to use a different format for dates:", "...", "feature also assumes that your index uses the {@link DateTools} class to store dates.", "If you use a different format and you still want QueryParser", "to turn local dates in range queries into valid queries you need to create your own", "query parser that inherits QueryParser and overwrites", "{@link #getRangeQuery(String, String, String, boolean)}.", "...", "And the javadoc of DateField says:", "Deprecated.", "If you build a new index, use DateTools instead.", "This class is included for use with existing indices and will be removed in a future release.", "So the question is in how many future releases we want to support DateField.", "If we still want to support it in 2.1 I agree to Hoss that his suggestion would be a nice and clean way to do that and I can easily change the patch accordingly.", "It avoids having another option in QueryParser.", "This new patch contains the changes suggested by Hoss:", "by default the QueryParser uses DateField to format dates for all fields", "if a date resolution has been set using void setDateResolution(DateTools.Resolution) then DateTools is used for all fields with the given resolution", "if a field specific date resolution has been set using setDateResolution(String, DateTools.Resolution) then DateTools is used for the given field with the given date resolution; other fields are not affected", "So with this patch the behaviour of QueryParser does not change unless either setDateResolution(DateTools.Resolution) or setDateResolution(String, DateTools.Resolution) have been called.", "Changed the summary to better reflect the second version of the patch.", "+1 for queryparser_datetools2.patch", "the only nitpick i have is with the class level javadocs...", "<p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g.", "<tt>date:[6/1/2005 TO 6/4/2005]</tt>", "produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04.", "Note", "that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.", "By default a date is formatted using the deprecated {@link DateField} for compatibility reasons.", "To use the new {@link DateTools} to format dates, a {@link DateTools.Resolution} has to be set.</p>", "<p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)", "...the word \"format\" is used to mean two very differnet things in this paragraph, we should clean that up so it's clera that Locale determines the DateFormat QP uses when trying to parse input  in [a TO b] syntax, while either DateField or DateTools are used to create the \"indexed\" value queried against.", "You're right Hoss, the word \"format\" is used ambiguously in the javadoc.", "We could change it to", "<p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g.", "<tt>date:[6/1/2005 TO 6/4/2005]</tt>", "produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04.", "Note", "that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.", "By default a date is converted into a search term using the deprecated {@link DateField} for compatibility reasons.", "To use the new {@link DateTools} to convert dates, a {@link DateTools.Resolution} has to be set.</p>", "<p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)}", "or {@link #setDateResolution(String, DateTools.Resolution)}.", "The former sets the default date resolution for all fields, whereas the latter can", "be used to set field specific date resolutions.", "Field specific date resolutions take, if set, precedence over the default date resolution.", "</p>", "<p>If you use neither {@link DateField} nor {@link DateTools} in your index, you can create your own", "query parser that inherits QueryParser and overwrites {@link #getRangeQuery(String, String, String, boolean)} to", "use a different method for date conversion.", "</p>", "Sounds better?", "Do you want me to create another patch that includes this javadoc?", "I've commited queryparser_datetools2.patch with two changes...", "1) revised the class level javadocs based on Michael's last comment.", "2) reinstated the old TestQueryParser.testDateRange() as testLegacyDateRange() to attempt to future proof against breaking backwards compatibility."], "SplitGT": [" Sets the date resolution used by RangeQueries for a specific field."], "issueString": "Support DateTools in QueryParser\nThe QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.\n\nThis patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:\n\n  /**\n   * Sets the default date resolution used by RangeQueries for fields for which no\n   * specific date resolutions has been set. Field specific resolutions can be set\n   * with {@link #setDateResolution(String, DateTools.Resolution)}.\n   *  \n   * @param dateResolution the default date resolution to set\n   */\n  public void setDateResolution(DateTools.Resolution dateResolution);\n  \n  /**\n   * Sets the date resolution used by RangeQueries for a specific field.\n   *  \n   * @param field field for which the date resolution is to be set \n   * @param dateResolution date resolution to set\n   */\n  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);\n\n(I also added the corresponding getter methods).\n\nNow the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.\nThe initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. \n\nPlease let me know if you think we should use a different resolution as default.\n\nI extended TestQueryParser to test this new feature.\n\nAll unit tests pass.\n\nI'm not sure if most people use DateTools already, as it has just been added in Lucene 1.9. Maybe you could consider an option (yes, yet another option isn't nice, I know)? Otherwise we need to properly document how to continue using DateField, i.e. by extending QueryParser and overwriting this method I guess.\n\ncleanest way to be backwards compatible would be to not have an initial default Resolution, and use DateField if no Resolution can be found for the field specified.  Existing clients would still get DateField for all dates, clients that add a call to setDateResolution(\"foo\", BAR); would get a DateTools formated query with resolution BAR for field foo, but other pre-existing fields would still use DateField, and clients that call setDateResolution(BAR); would allways get a DateTools formatted query, either at resolution BAR or at some other resolution if they also use the two arg setDateResolution\nActually it is documented in the QueryParser how to use a different format for dates:\n\n   ...\n    * feature also assumes that your index uses the {@link DateTools} class to store dates.\n    * If you use a different format and you still want QueryParser\n    * to turn local dates in range queries into valid queries you need to create your own\n    * query parser that inherits QueryParser and overwrites\n    * {@link #getRangeQuery(String, String, String, boolean)}.\n   ...\n\nAnd the javadoc of DateField says:\n\n   Deprecated. If you build a new index, use DateTools instead. This class is included for use with existing indices and will be removed in a future release.\n\nSo the question is in how many future releases we want to support DateField. If we still want to support it in 2.1 I agree to Hoss that his suggestion would be a nice and clean way to do that and I can easily change the patch accordingly. It avoids having another option in QueryParser.\nThis new patch contains the changes suggested by Hoss:\n- by default the QueryParser uses DateField to format dates for all fields\n- if a date resolution has been set using void setDateResolution(DateTools.Resolution) then DateTools is used for all fields with the given resolution\n- if a field specific date resolution has been set using setDateResolution(String, DateTools.Resolution) then DateTools is used for the given field with the given date resolution; other fields are not affected\n\nSo with this patch the behaviour of QueryParser does not change unless either setDateResolution(DateTools.Resolution) or setDateResolution(String, DateTools.Resolution) have been called.\nChanged the summary to better reflect the second version of the patch.\n+1 for queryparser_datetools2.patch\n\nthe only nitpick i have is with the class level javadocs...\n\n  * <p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g. <tt>date:[6/1/2005 TO 6/4/2005]</tt>\n  * produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04. Note\n  * that the format of the accepted input depends on {@link #setLocale(Locale) the locale}.\n  * By default a date is formatted using the deprecated {@link DateField} for compatibility reasons.\n  * To use the new {@link DateTools} to format dates, a {@link DateTools.Resolution} has to be set.</p>\n  * <p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)\n\n\n...the word \"format\" is used to mean two very differnet things in this paragraph, we should clean that up so it's clera that Locale determines the DateFormat QP uses when trying to parse input  in [a TO b] syntax, while either DateField or DateTools are used to create the \"indexed\" value queried against.\nYou're right Hoss, the word \"format\" is used ambiguously in the javadoc. We could change it to\n\n * <p>In {@link RangeQuery}s, QueryParser tries to detect date values, e.g. <tt>date:[6/1/2005 TO 6/4/2005]</tt>\n * produces a range query that searches for \"date\" fields between 2005-06-01 and 2005-06-04. Note\n * that the format of the accepted input depends on {@link #setLocale(Locale) the locale}. \n * By default a date is converted into a search term using the deprecated {@link DateField} for compatibility reasons.\n * To use the new {@link DateTools} to convert dates, a {@link DateTools.Resolution} has to be set.</p> \n * <p>The date resolution that shall be used for RangeQueries can be set using {@link #setDateResolution(DateTools.Resolution)}\n * or {@link #setDateResolution(String, DateTools.Resolution)}. The former sets the default date resolution for all fields, whereas the latter can\n * be used to set field specific date resolutions. Field specific date resolutions take, if set, precedence over the default date resolution.\n * </p>\n * <p>If you use neither {@link DateField} nor {@link DateTools} in your index, you can create your own\n * query parser that inherits QueryParser and overwrites {@link #getRangeQuery(String, String, String, boolean)} to\n * use a different method for date conversion.\n * </p> \n\nSounds better? Do you want me to create another patch that includes this javadoc?\nI've commited queryparser_datetools2.patch with two changes...\n\n1) revised the class level javadocs based on Michael's last comment.\n2) reinstated the old TestQueryParser.testDateRange() as testLegacyDateRange() to attempt to future proof against breaking backwards compatibility.\n", "issueSearchSentences": ["public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);", "if a field specific date resolution has been set using setDateResolution(String, DateTools.Resolution) then DateTools is used for the given field with the given date resolution; other fields are not affected", "public void setDateResolution(DateTools.Resolution dateResolution);", "with {@link #setDateResolution(String, DateTools.Resolution)}.", "if a date resolution has been set using void setDateResolution(DateTools.Resolution) then DateTools is used for all fields with the given resolution"], "issueSearchScores": [0.9222235679626465, 0.8144252896308899, 0.7913637161254883, 0.7846596837043762, 0.7845984101295471]}
{"aId": 133, "code": "public long softUpdateDocuments(Term term, Iterable<? extends Iterable<? extends IndexableField>> docs, Field... softDeletes) throws IOException {\n    if (term == null) {\n      throw new IllegalArgumentException(\"term must not be null\");\n    }\n    if (softDeletes == null || softDeletes.length == 0) {\n      throw new IllegalArgumentException(\"at least one soft delete must be present\");\n    }\n    return updateDocuments(DocumentsWriterDeleteQueue.newNode(buildDocValuesUpdate(term, softDeletes, false)), docs);\n  }", "comment": " One use of this API is to retain older versions of documents instead of replacing them. The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.", "issueId": "LUCENE-8200", "issueStringList": ["Allow doc-values to be updated atomically together with a document", "Today we can only update a document by deleting all previously indexed documents for the given term.", "In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)", "This change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted.", "This is a spinnoff from LUCENE-8198", "here is a github commit for reference https://github.com/s1monw/lucene-solr/commit/ff6bd484590a02d169b9259a465ce70069f83e82", "the public IW api looks good and simple.", "There's a missing space in the javadocs \"flush may happen only afterthe add\"", "I didn't review any of the impl details or tests.", "+1, patch looks great; I left a minor comment on the github commit.", "Amazing how little code the change requires, and it's a nice approach for soft deletes.", "thanks [~mikemccand] I attached a new patch.", "I am aiming to push this tomorrow unless anybody objects.", "One think that I realized is that we can't utilize this for index sorting since fields that participate in the sort can't be updated.", "But I think that is not a major issue.", "//cc [~rcmuir]", "Cool feature!", "I like the name \"softUpdateDocuments\" for this.", "Maybe the CHANGES.txt entry should make reference to the particular method so user's can easily get to further explanatory javadocs.", "I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.", "For example:", "bq.", "One use of this API is to retain older versions of documents instead of replacing them.", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.", "The patch did not apply for me {{git apply ../patches/LUCENE-8200.patch}} ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "ok sure.", "> I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.", "For example:", "I can do that.", "> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.", "my repo is aligned with whatever is on  _https://git-wip-us.apache.org/repos/asf/lucene-solr.git_ it has not been forked from a specific mirror on github which doesn't matter at all here.", "I don't really understand why you bring this up like a second time here.", "it's unrelated.", "{quote}my repo is aligned with whatever is on _[https://git-wip-us.apache.org/repos/asf/lucene-solr.git_] it has not been forked from a specific mirror on github which doesn't matter at all here.", "I don't really understand why you bring this up like a second time here.", "it's unrelated.", "{quote}", "1st time was just an unrelated question; sorry if asking it annoyed you.", "2nd time is perhaps in retrospect misattributed.", "I thought that your repo shared no lineage with ASF's but now I think I'm mistaken.", "If it didn't share lineage (which was my thinking at the time I commented), it would explain why \"git apply ...\" didn't work.", "But it's not that.", "It didn't work when I tried to apply it in IntelliJ either.", "A mystery; ah well.", "Does \"git apply ...\" work for anyone else here for this patch?", "My usual method is to apply patches directly in IntelliJ but when that failed I went to the CLI and found it failed there too, so I'm stumped.", "Raw patches are harder to review as thoroughly as one can in an IDE (of course).", "[~dsmiley] I updated the patch and uploaded it.", "I think the previous patch had 2 commits in it.", "Sorry for the confusion.", "I applied it with _patch -p1 < ../LUCENE-8200.patch_ just fine", "That was it; thanks.", "IntelliJ likes this one, and so does {{git apply ...}}", "thanks everybody"], "SplitGT": [" One use of this API is to retain older versions of documents instead of replacing them.", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time."], "issueString": " Allow doc-values to be updated atomically together with a document\nToday we can only update a document by deleting all previously indexed documents for the given term. In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)\r\n    \r\nThis change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted. This is a spinnoff from LUCENE-8198\r\n\nhere is a github commit for reference https://github.com/s1monw/lucene-solr/commit/ff6bd484590a02d169b9259a465ce70069f83e82\nthe public IW api looks good and simple.\r\n\r\nThere's a missing space in the javadocs \"flush may happen only afterthe add\"\r\n\r\nI didn't review any of the impl details or tests.\n+1, patch looks great; I left a minor comment on the github commit.\u00a0 Amazing how little code the change requires, and it's a nice approach for soft deletes.\nthanks [~mikemccand] I attached a new patch. I am aiming to push this tomorrow unless anybody objects. One think that I realized is that we can't utilize this for index sorting since fields that participate in the sort can't be updated. But I think that is not a major issue. //cc [~rcmuir]\nCool feature!  I like the name \"softUpdateDocuments\" for this.  Maybe the CHANGES.txt entry should make reference to the particular method so user's can easily get to further explanatory javadocs.\r\n\r\nI suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is.  For example:\r\nbq. One use of this API is to retain older versions of documents instead of replacing them. The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time.\r\n\r\nThe patch did not apply for me {{git apply ../patches/LUCENE-8200.patch}} ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\n> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\r\nok sure.\r\n\r\n> I suggest the javadocs more explicitly give the user an idea of what this might be used for, since it seems a bit abstract as-is. For example:\r\nI can do that.\r\n\r\n> The patch did not apply for me git apply ../patches/LUCENE-8200.patch ... probably because your GH repository does not have the same git lineage as the official ASF repo that is mirrored/forked elsewhere.\r\n\r\nmy repo is aligned with whatever is on  _https://git-wip-us.apache.org/repos/asf/lucene-solr.git_ it has not been forked from a specific mirror on github which doesn't matter at all here. I don't really understand why you bring this up like a second time here. it's unrelated. \r\n\n{quote}my repo is aligned with whatever is on _[https://git-wip-us.apache.org/repos/asf/lucene-solr.git_] it has not been forked from a specific mirror on github which doesn't matter at all here. I don't really understand why you bring this up like a second time here. it's unrelated.\r\n{quote}\r\n1st time was just an unrelated question; sorry if asking it annoyed you.\r\n2nd time is perhaps in retrospect misattributed.  I thought that your repo shared no lineage with ASF's but now I think I'm mistaken.  If it didn't share lineage (which was my thinking at the time I commented), it would explain why \"git apply ...\" didn't work.  But it's not that.  It didn't work when I tried to apply it in IntelliJ either.  A mystery; ah well.  Does \"git apply ...\" work for anyone else here for this patch?  My usual method is to apply patches directly in IntelliJ but when that failed I went to the CLI and found it failed there too, so I'm stumped.  Raw patches are harder to review as thoroughly as one can in an IDE (of course).\n[~dsmiley] I updated the patch and uploaded it. I think the previous patch had 2 commits in it. Sorry for the confusion. I applied it with _patch -p1 < ../LUCENE-8200.patch_ just fine\nThat was it; thanks. IntelliJ likes this one, and so does {{git apply ...}}\nthanks everybody\n", "issueSearchSentences": ["Today we can only update a document by deleting all previously indexed documents for the given term.", "This change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted.", "I like the name \"softUpdateDocuments\" for this.", "In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s)", "The existing documents can be updated to reflect they are no longer current while atomically adding new documents at the same time."], "issueSearchScores": [0.7710763216018677, 0.7291295528411865, 0.6615508198738098, 0.6598093509674072, 0.6161084175109863]}
