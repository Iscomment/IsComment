{"aId": 1, "code": "static void removeTablePermissions(Configuration conf, byte[] tableName, byte[] column)\n      throws IOException{\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Removing permissions of removed column \" + Bytes.toString(column) +\n                \" from table \"+ Bytes.toString(tableName));\n    }\n\n    HTable acls = null;\n    try {\n      acls = new HTable(conf, ACL_TABLE_NAME);\n\n      Scan scan = new Scan();\n      scan.addFamily(ACL_LIST_FAMILY);\n\n      String columnName = Bytes.toString(column);\n      scan.setFilter(new QualifierFilter(CompareOp.EQUAL, new RegexStringComparator(\n                     String.format(\"(%s%s%s)|(%s%s)$\",\n                     ACL_KEY_DELIMITER, columnName, ACL_KEY_DELIMITER,\n                     ACL_KEY_DELIMITER, columnName))));\n\n      Set<byte[]> qualifierSet = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);\n      ResultScanner scanner = acls.getScanner(scan);\n      try {\n        for (Result res : scanner) {\n          for (byte[] q : res.getFamilyMap(ACL_LIST_FAMILY).navigableKeySet()) {\n            qualifierSet.add(q);\n          }\n        }\n      } finally {\n        scanner.close();\n      }\n\n      if (qualifierSet.size() > 0) {\n        Delete d = new Delete(tableName);\n        for (byte[] qualifier : qualifierSet) {\n          d.deleteColumns(ACL_LIST_FAMILY, qualifier);\n        }\n        acls.delete(d);\n      }\n    } finally {\n      if (acls != null) acls.close();\n    }\n  }", "comment": " Remove specified table column from the _acl_ table.", "issueId": "HBASE-5385", "issueStringList": ["Delete table/column should delete stored permissions on -acl- table", "Deleting the table or a column does not cascade to the stored permissions at the -acl- table.", "We should also remove those permissions, otherwise, it can be a security leak, where freshly created tables contain permissions from previous same-named tables.", "We might also want to ensure, upon table creation, that no entries are already stored at the -acl- table.", "Remove a table from _acl_ is straightforward, but remove a column from it is not as easy.", "The _acl_ table has table name as key, and has one column family that contains user rights.", "{code}", "tablename -> user -> rights", "tablename -> user,family -> rights", "tablename -> user,family,qualifier -> rights", "{code}", "To remove a table column from the _acl_ we need to remove the table rows where the qualifier contains ',family'.", "Any thoughts on how to implement that?", "Adding a Delete Filter?", "Perform a Scan with QualifierFilter to remove a column from the _acl_ table.", "Looks good.", "Can we add:", "1.", "Audit logging AccessController.AUDITLOG", "2.", "On preCreateTable and preAddColumn, ensure that the acl table is empty for the table / column.", "We might still have residual acl entries if smt goes wrong.", "If so, we should refuse creating a table by throwing a kind of access control exception.", "Andrew, any comments?", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12525101/HBASE-5385-v1.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//console", "This message is automatically generated.", "{quote}", "On preCreateTable and preAddColumn, ensure that the acl table is empty for the table / column.", "We might still have residual acl entries if smt goes wrong.", "If so, we should refuse creating a table by throwing a kind of access control exception.", "{quote}", "Currently there's no check on grant to see if the table/family/qualifier exist.", "Maybe we can open another jira for this, to implement the exists check on grant and verify in all pre* if there's nothing left.", "+1 looks good.", "bq.", "Maybe we can open another jira for this, to implement the exists check on grant and verify in all pre* if there's nothing left.", "This is a good idea since it's a different problem scope than this jira.", "patch rebased, HBASE-5732 went in.", "(Also opened HBASE-5947, for the pre/post check for empty acl)", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12526613/HBASE-5385-v2.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 27 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.replication.TestReplication", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//console", "This message is automatically generated.", "Patch v3 puts scanner.close() in finally block.", "@Matteo:", "Now that HBASE-5342 went in, can you rebase patch v3 :-) ?", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12526619/5385-v3.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 27 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.TestDrainingServer", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//console", "This message is automatically generated.", "rebase after HBASE-5342 merge", "Patch v3 integrated to trunk.", "Thanks for the patch, Matteo.", "Thanks for the review, Andy.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12526622/HBASE-5385-v3.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "1 tests included.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 31 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.TestDrainingServer", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//console", "This message is automatically generated.", "Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #2 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/2/])", "HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)", "Result = FAILURE", "tedyu :", "Files :", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java", "Integrated in HBase-TRUNK #2876 (See [https://builds.apache.org/job/HBase-TRUNK/2876/])", "HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)", "Result = FAILURE", "tedyu :", "Files :", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java"], "SplitGT": [" Remove specified table column from the _acl_ table."], "issueString": "Delete table/column should delete stored permissions on -acl- table  \nDeleting the table or a column does not cascade to the stored permissions at the -acl- table. We should also remove those permissions, otherwise, it can be a security leak, where freshly created tables contain permissions from previous same-named tables. We might also want to ensure, upon table creation, that no entries are already stored at the -acl- table. \nRemove a table from _acl_ is straightforward, but remove a column from it is not as easy.\n\nThe _acl_ table has table name as key, and has one column family that contains user rights.\n{code}\ntablename -> user -> rights\ntablename -> user,family -> rights\ntablename -> user,family,qualifier -> rights\n{code}\n\nTo remove a table column from the _acl_ we need to remove the table rows where the qualifier contains ',family'.\n\nAny thoughts on how to implement that? Adding a Delete Filter?\nPerform a Scan with QualifierFilter to remove a column from the _acl_ table.\nLooks good. Can we add:\n1. Audit logging AccessController.AUDITLOG\n2. On preCreateTable and preAddColumn, ensure that the acl table is empty for the table / column. We might still have residual acl entries if smt goes wrong. If so, we should refuse creating a table by throwing a kind of access control exception. \n\nAndrew, any comments? \n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12525101/HBASE-5385-v1.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n     \n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1693//console\n\nThis message is automatically generated.\n{quote}\nOn preCreateTable and preAddColumn, ensure that the acl table is empty for the table / column. We might still have residual acl entries if smt goes wrong. If so, we should refuse creating a table by throwing a kind of access control exception.\n{quote}\n\nCurrently there's no check on grant to see if the table/family/qualifier exist. \nMaybe we can open another jira for this, to implement the exists check on grant and verify in all pre* if there's nothing left.\n+1 looks good. \n\nbq. Maybe we can open another jira for this, to implement the exists check on grant and verify in all pre* if there's nothing left.\n\nThis is a good idea since it's a different problem scope than this jira.\npatch rebased, HBASE-5732 went in.\n\n(Also opened HBASE-5947, for the pre/post check for empty acl)\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12526613/HBASE-5385-v2.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 27 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.replication.TestReplication\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1856//console\n\nThis message is automatically generated.\nPatch v3 puts scanner.close() in finally block.\n@Matteo:\nNow that HBASE-5342 went in, can you rebase patch v3 :-) ?\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12526619/5385-v3.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 27 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.TestDrainingServer\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1859//console\n\nThis message is automatically generated.\nrebase after HBASE-5342 merge\nPatch v3 integrated to trunk.\n\nThanks for the patch, Matteo.\n\nThanks for the review, Andy.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12526622/HBASE-5385-v3.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 31 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.TestDrainingServer\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1860//console\n\nThis message is automatically generated.\nIntegrated in HBase-TRUNK-on-Hadoop-2.0.0 #2 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/2/])\n    HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)\n\n     Result = FAILURE\ntedyu : \nFiles : \n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java\n\nIntegrated in HBase-TRUNK #2876 (See [https://builds.apache.org/job/HBase-TRUNK/2876/])\n    HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)\n\n     Result = FAILURE\ntedyu : \nFiles : \n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java\n\n", "issueSearchSentences": ["We should also remove those permissions, otherwise, it can be a security leak, where freshly created tables contain permissions from previous same-named tables.", "Deleting the table or a column does not cascade to the stored permissions at the -acl- table.", "Delete table/column should delete stored permissions on -acl- table", "HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)", "HBASE-5385 Delete table/column should delete stored permissions on -acl- table (Matteo Bertozi) (Revision 1337512)"], "issueSearchScores": [0.7499351501464844, 0.7347300052642822, 0.7190932631492615, 0.6983642578125, 0.6983642578125]}
{"aId": 5, "code": "private synchronized void snapshotEnabledTable(SnapshotDescription snapshot)\n      throws HBaseSnapshotException {\n    TakeSnapshotHandler handler;\n    try {\n      handler = new EnabledTableSnapshotHandler(snapshot, master, this);\n      this.executorService.submit(handler);\n      this.handler = handler;\n    } catch (IOException e) {\n      // cleanup the working directory by trying to delete it from the fs.\n      Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);\n      try {\n        if (this.master.getMasterFileSystem().getFileSystem().delete(workingDir, true)) {\n          LOG.warn(\"Couldn't delete working directory (\" + workingDir + \" for snapshot:\"\n              + snapshot);\n        }\n      } catch (IOException e1) {\n        LOG.warn(\"Couldn't delete working directory (\" + workingDir + \" for snapshot:\" + snapshot);\n      }\n      // fail the snapshot\n      throw new SnapshotCreationException(\"Could not build snapshot handler\", e, snapshot);\n    }\n  }", "comment": " Take a snapshot of a enabled table.", "issueId": "HBASE-6864", "issueStringList": ["Online snapshots scaffolding", "Basic scaffolding for taking a snapshot of an offline table.", "This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.", "This has been subsumed by HBASE-7208", "Ok, I'm going to reopen this and put a patch up for this.", "review here.", "https://reviews.apache.org/r/8390/", "found plumbing bug, will flag for review when ready.", "bug fixed, ready for review again.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12561818/pre-hbase-6864.v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 87 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3626//console", "This message is automatically generated.", "reviewboard here: https://reviews.apache.org/r/8390/", "Updates from reviews", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562697/pre-hbase-6864.v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3761//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562815/pre-hbase-6864.v4.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3787//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562872/pre-hbase-6864.v5.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3798//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12563000/pre-hbase-6864.v6.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3814//console", "This message is automatically generated.", "I've committed v7 to the online-snapshot branch.", "Thanks for the reviews Jesse, Stack and Matteo."], "SplitGT": [" Take a snapshot of a enabled table."], "issueString": "Online snapshots scaffolding\nBasic scaffolding for taking a snapshot of an offline table. This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.\nThis has been subsumed by HBASE-7208\nOk, I'm going to reopen this and put a patch up for this.\nreview here. https://reviews.apache.org/r/8390/\nfound plumbing bug, will flag for review when ready.\nbug fixed, ready for review again.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12561818/pre-hbase-6864.v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 87 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3626//console\n\nThis message is automatically generated.\nreviewboard here: https://reviews.apache.org/r/8390/\nUpdates from reviews\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562697/pre-hbase-6864.v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3761//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562815/pre-hbase-6864.v4.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3787//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562872/pre-hbase-6864.v5.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3798//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12563000/pre-hbase-6864.v6.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3814//console\n\nThis message is automatically generated.\nI've committed v7 to the online-snapshot branch.  Thanks for the reviews Jesse, Stack and Matteo.\n", "issueSearchSentences": ["Basic scaffolding for taking a snapshot of an offline table.", "I've committed v7 to the online-snapshot branch.", "This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.", "Online snapshots scaffolding", "against trunk revision ."], "issueSearchScores": [0.6452211141586304, 0.4712814390659332, 0.46203339099884033, 0.39497876167297363, 0.385634183883667]}
{"aId": 6, "code": "static Map<byte[], String> createFamilyCompressionMap(Configuration conf) {\n    Map<byte[], String> compressionMap = new TreeMap<byte[], String>(Bytes.BYTES_COMPARATOR);\n    String compressionConf = conf.get(COMPRESSION_CONF_KEY, \"\");\n    for (String familyConf : compressionConf.split(\"&\")) {\n      String[] familySplit = familyConf.split(\"=\");\n      if (familySplit.length != 2) {\n        continue;\n      }\n      \n      try {\n        compressionMap.put(URLDecoder.decode(familySplit[0], \"UTF-8\").getBytes(),\n            URLDecoder.decode(familySplit[1], \"UTF-8\"));\n      } catch (UnsupportedEncodingException e) {\n        // will not happen with UTF-8 encoding\n        throw new AssertionError(e);\n      }\n    }\n    return compressionMap;\n  }", "comment": " Run inside the task to deserialize column family to compression algorithm map from the configuration.", "issueId": "HBASE-3474", "issueStringList": ["HFileOutputFormat to use column family's compression algorithm", "HFileOutputFormat  currently creates HFile writer's using a compression algorithm set as configuration \"hbase.hregion.max.filesize\" with default as no compression.", "The code does not take into account the compression algorithm configured for the table's column family.", "As a result bulk uploaded tables are not compressed until a major compaction is run on them.", "This could be fixed by using the column family descriptors while creating HFile writers.", "HFileOutputFormat uses HColumnDescriptor's of column families in the HTable and creates writers with corresponding compression algorithms.", "Patch that uses the column descriptors for setting compression.", "Removed stray prints from the patch", "Thanks for taking this on, Ashish.", "A few comments on the patch:", "for createFamilyCompressionMap, specify in the javadoc that this is run inside the task to deserialize the map back out of the Configuration", "In javadoc for configureCompression, I don't think there's any reason to include the @param table and @param conf since they're kind of obvious", "in createFamilyCompressionMap, 'compression' is misspelled in one var name", "The encoding you've chosen for this config value isn't the best, since we currently do support ',' in a column family name.", "What about encoding the map like a query string in a URL?", "foo=bar&baz=blah - and using URLEncoder/URLDecoder to escape the =s and &s that might be in there.", "Incorporated Todd's comments on javadoc and use of URL encoder for serializing family compression map.", "Hey Ashish.", "This is looking good.", "A few more comments before I think it's ready to commit:", "If you make configureCompression and createFamilyCompressionMap package-private instead of private, you could add a nice unit test for them.", "You can use Mockito to make a mock HTable that returns the HColumnDescriptors you want for the sake of the test - if you need a hand with this you can catch me on #hbase IRC", "Probably a good idea to put \"families.compression\" in a constant, and maybe change it to be more obviously scoped - ie something like \"hbase.hfileoutputformat.families.compression\"", "When you upload a patch, you should take the diff from the root of the SVN trunk/ - so that it applies with \"patch -p0\" from inside trunk/.", "Thanks!", "Will use a constant for the config key.", "The test will take me some time because I am not familiar with mockito.", "Will put out a patch as soon as I get my head around creating the unit test.", "Let me know if you need a hand with the test, happy to help.", "Mockito is strange at first but once you get the hang of it, it's pretty nice", "Added unit tests for", "1. compression serialization deserialization to job config", "2. testing that the hfile writers use the config correctly.", "Minor issues", "1.", "The test parses out HFile.Reader.toString() to retrieve the compression algorithm used on an HFile.", "One alternative is to add getCompressionAlgo() to HFile.Reader.", "2.", "The testColumnFamilyCompression create a mapred job just to get hold of an usable writer.", "Not sure if this is the best thing to do.", "Hi Ashish.", "I took your patch and cleaned up the unit tests a bit to share some of the mocking code.", "I also added a {{getCompressionAlgorithm}} call to {{HFile.Reader}} as you suggested.", "I removed one of the unit tests since it seemed to be entirely subsumed by the other one.", "Also updated the unit tests to check cases like 0-CF and 1-CF.", "Can you check over this updated patch and make sure it looks good to you?", "Hi Todd, Thanks for re-factoring the tests.", "They look better now.", "Will try to be more careful in looking for code reuse opportunities.", "I just removed some unused imports from the test file.", "My IDE marks unused imports as warnings :)", "@Ashish So, are you +1 on committing your patch (w/ Todd cleanup)?", "+1 on committing the patch with Todd's cleanup.", "Committed Todds fixup of Ashish's patch.", "Thank you for the patch Ashish."], "SplitGT": [" Run inside the task to deserialize column family to compression algorithm map from the configuration."], "issueString": "HFileOutputFormat to use column family's compression algorithm\nHFileOutputFormat  currently creates HFile writer's using a compression algorithm set as configuration \"hbase.hregion.max.filesize\" with default as no compression. The code does not take into account the compression algorithm configured for the table's column family.  As a result bulk uploaded tables are not compressed until a major compaction is run on them. This could be fixed by using the column family descriptors while creating HFile writers.\nHFileOutputFormat uses HColumnDescriptor's of column families in the HTable and creates writers with corresponding compression algorithms.\nPatch that uses the column descriptors for setting compression.\nRemoved stray prints from the patch\nThanks for taking this on, Ashish. A few comments on the patch:\n- for createFamilyCompressionMap, specify in the javadoc that this is run inside the task to deserialize the map back out of the Configuration\n- In javadoc for configureCompression, I don't think there's any reason to include the @param table and @param conf since they're kind of obvious\n- in createFamilyCompressionMap, 'compression' is misspelled in one var name\n- The encoding you've chosen for this config value isn't the best, since we currently do support ',' in a column family name. What about encoding the map like a query string in a URL? foo=bar&baz=blah - and using URLEncoder/URLDecoder to escape the =s and &s that might be in there.\nIncorporated Todd's comments on javadoc and use of URL encoder for serializing family compression map.\nHey Ashish. This is looking good. A few more comments before I think it's ready to commit:\n- If you make configureCompression and createFamilyCompressionMap package-private instead of private, you could add a nice unit test for them. You can use Mockito to make a mock HTable that returns the HColumnDescriptors you want for the sake of the test - if you need a hand with this you can catch me on #hbase IRC\n- Probably a good idea to put \"families.compression\" in a constant, and maybe change it to be more obviously scoped - ie something like \"hbase.hfileoutputformat.families.compression\"\n- When you upload a patch, you should take the diff from the root of the SVN trunk/ - so that it applies with \"patch -p0\" from inside trunk/.\n\nThanks!\nWill use a constant for the config key. The test will take me some time because I am not familiar with mockito. Will put out a patch as soon as I get my head around creating the unit test.\nLet me know if you need a hand with the test, happy to help. Mockito is strange at first but once you get the hang of it, it's pretty nice\nAdded unit tests for \n1. compression serialization deserialization to job config\n2. testing that the hfile writers use the config correctly.\n\nMinor issues\n-------------\n\n1. The test parses out HFile.Reader.toString() to retrieve the compression algorithm used on an HFile. One alternative is to add getCompressionAlgo() to HFile.Reader. \n2. The testColumnFamilyCompression create a mapred job just to get hold of an usable writer. Not sure if this is the best thing to do.\nHi Ashish. I took your patch and cleaned up the unit tests a bit to share some of the mocking code. I also added a {{getCompressionAlgorithm}} call to {{HFile.Reader}} as you suggested.\n\nI removed one of the unit tests since it seemed to be entirely subsumed by the other one. Also updated the unit tests to check cases like 0-CF and 1-CF.\n\nCan you check over this updated patch and make sure it looks good to you?\nHi Todd, Thanks for re-factoring the tests. They look better now. Will try to be more careful in looking for code reuse opportunities. I just removed some unused imports from the test file. My IDE marks unused imports as warnings :)   \n@Ashish So, are you +1 on committing your patch (w/ Todd cleanup)?\n+1 on committing the patch with Todd's cleanup.\nCommitted Todds fixup of Ashish's patch.  Thank you for the patch Ashish.\n", "issueSearchSentences": ["for createFamilyCompressionMap, specify in the javadoc that this is run inside the task to deserialize the map back out of the Configuration", "If you make configureCompression and createFamilyCompressionMap package-private instead of private, you could add a nice unit test for them.", "in createFamilyCompressionMap, 'compression' is misspelled in one var name", "In javadoc for configureCompression, I don't think there's any reason to include the @param table and @param conf since they're kind of obvious", "Incorporated Todd's comments on javadoc and use of URL encoder for serializing family compression map."], "issueSearchScores": [0.7383835315704346, 0.6746751070022583, 0.6698713898658752, 0.5723817944526672, 0.571236789226532]}
{"aId": 8, "code": "public static void fixMetaHoleOnlineAndAddReplicas(Configuration conf,\n      HRegionInfo hri, Collection<ServerName> servers, int numReplicas) throws IOException {\n    HTable meta = new HTable(conf, TableName.META_TABLE_NAME);\n    Put put = MetaTableAccessor.makePutFromRegionInfo(hri);\n    if (numReplicas > 1) {\n      Random r = new Random();\n      ServerName[] serversArr = servers.toArray(new ServerName[servers.size()]);\n      for (int i = 1; i < numReplicas; i++) {\n        ServerName sn = serversArr[r.nextInt(serversArr.length)];\n        // the column added here is just to make sure the master is able to\n        // see the additional replicas when it is asked to assign. The\n        // final value of these columns will be different and will be updated\n        // by the actual regionservers that start hosting the respective replicas\n        MetaTableAccessor.addLocation(put, sn, sn.getStartcode(), i);\n      }\n    }\n    meta.put(put);\n    meta.close();\n  }", "comment": " Puts the specified HRegionInfo into META with replica related columns", "issueId": "HBASE-10674", "issueStringList": ["HBCK should be updated to do replica related checks", "HBCK should be updated to have a check for whether the replicas are assigned to the right machines (default and non-default replicas ideally should not be in the same server if there is more than one server in the cluster and such scenarios).", "[~jmhsieh] suggested this in HBASE-10362.", "This is a WIP patch.", "It addresses the issue to do with plugging hdfs holes (when a hdfs hole is plugged, the old region is closed and a new region is created; this patch adds closing/opening for the corresponding replicas).", "This patch addresses the cases where some replicas are not deployed in the cluster, or excess replicas are deployed in the cluster.", "It also deals with plugging meta holes with the appropriate replica qualifiers.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12658715/10674-1.2.txt", "against trunk revision .", "ATTACHMENT ID: 12658715", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 3 new or modified tests.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 2.0.3) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 site{color}.", "The mvn site goal succeeds with this patch.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction", "org.apache.hadoop.hbase.migration.TestNamespaceUpgrade", "org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas", "org.apache.hadoop.hbase.regionserver.TestRegionReplicas", "org.apache.hadoop.hbase.client.TestReplicasClient", "org.apache.hadoop.hbase.master.TestRestartCluster", "org.apache.hadoop.hbase.TestRegionRebalancing", "org.apache.hadoop.hbase.TestIOFencing", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//console", "This message is automatically generated.", "Is this still work in progress or ready to review?", "Yes,[~posix4e] ready to review.", "OK I have no commit bit, that being said.....", "What's going on here, can we make it into a function?", "This has gotten pretty hairy", "+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java", "@@ -2949,6 +2949,7 @@ public class HBaseFsck extends Configured {", "} else {", "m = new MetaEntry(hri, sn, ts, null, null);", "}", "HbckInfo previous = regionInfoMap.get(hri.getEncodedName());", "if (previous == null) {", "regionInfoMap.put(hri.getEncodedName(), new HbckInfo(m));", "....", "TODO What does first online hri mean?", "Maybe a javadoc?", "return FSUtils.getTableName(tableDir);", "} else {", "return the info from the first online hri", "for (OnlineEntry e : deployedEntries) {", "return e.hri.getTable();", "}", "This is weirdly formatted.", "Maybe run an autoformat", "++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.ja", "@@ -186,12 +186,14 @@ public class HBaseFsckRepair {", "Puts the specified HRegionInfo into META with replica related columns", "public static void fixMetaHoleOnlineAndAddReplicas(Configuration conf,", "HRegionInfo hri, Collection<ServerName> servers, int numReplicas) throws", "Woops accidently hit add", "....", "if (numReplicas > 1) {", "Random r = new Random();", "Shouldn't this be?", "ServerName[] serversArr = servers.toArray(new ServerName[0]);", "+      ServerName[] serversArr = servers.toArray(new ServerName[servers.size()])", "You have a spare todo around", "a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java", "+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java", "@@ -301,6 +301,7 @@ public class TestHBaseFsck {", "@param metaRow  if true remove region's row from META", "@param hdfs if true remove region's dir in HDFS", "@param regionInfoOnly if true remove a region dir's .regioninfo file", "@param replicaId TODO", "private void deleteRegion(Configuration conf, final HTableDescriptor htd,", "Also I noticed A commented out line in HBaseFsck", "@@ -3514,6 +3517,7 @@ public class HBaseFsck extends Configured {", "check to see if the existence of this region matches the region in M", "for (HRegionInfo r:regions) {", "HbckInfo hbi = hbck.getOrCreateInfo(r.getEncodedName());", "if (!RegionReplicaUtil.isDefaultReplica(r)) hbi.setSkipChecks(true)", "hbi.addServer(r, rsinfo);", "}", "{noformat}", "+    Result r = meta.get(get);", "+    RegionLocations rl = MetaTableAccessor.getRegionLocations(r);", "+    if (rl == null) return;", "{noformat}", "Should this be an error instead of silently returning?", "{noformat}", "+          for (HRegionLocation h : rl.getRegionLocations()) {", "+            if (h == null || h.getRegionInfo() == null) {", "+              continue;", "+            }", "{noformat}", "And this should be an error as well, especially if h.getRegionInfo() is null?", "Nice tests.", "Just when I was thinking \"what about this case?\"", "I find a test for it.", "Questions aside, LGTM, +1.", "Agreed, my vote does't count but +1", "Oooh one other thing", "in HBaseFsck.java", "Puts the specified HRegionInfo into META.", "public static void fixMetaHoleOnline(Configuration conf,", "HRegionInfo hri) throws IOException {", "HTable meta = new HTable(conf, TableName.META_TABLE_NAME);", "MetaTableAccessor.addRegionToMeta(meta, hri);", "meta.close();", "}", "no longer seems to be used", "Committed.", "Thanks [~posix4e] & [~ndimiduk] for reviews.", "What I committed with the review comments incorporated.", "FAILURE: Integrated in HBase-TRUNK #5368 (See [https://builds.apache.org/job/HBase-TRUNK/5368/])", "HBASE-10674.", "HBCK should be updated to do replica related checks (ddas: rev 8562752143f57744ac522ace1a2b196c45b428d4)", "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java", "hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java", "hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java", "hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java", "Addendum fixes compilation.", "FAILURE: Integrated in HBase-TRUNK #5369 (See [https://builds.apache.org/job/HBase-TRUNK/5369/])", "HBASE-10674 HBCK should be updated to do replica related checks Addendum (tedyu: rev ab757169a19be668e7a7d2e52c1da01d4810e0c2)", "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java"], "SplitGT": [" Puts the specified HRegionInfo into META with replica related columns"], "issueString": "HBCK should be updated to do replica related checks\nHBCK should be updated to have a check for whether the replicas are assigned to the right machines (default and non-default replicas ideally should not be in the same server if there is more than one server in the cluster and such scenarios). [~jmhsieh] suggested this in HBASE-10362.\nThis is a WIP patch. It addresses the issue to do with plugging hdfs holes (when a hdfs hole is plugged, the old region is closed and a new region is created; this patch adds closing/opening for the corresponding replicas).\nThis patch addresses the cases where some replicas are not deployed in the cluster, or excess replicas are deployed in the cluster. It also deals with plugging meta holes with the appropriate replica qualifiers.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12658715/10674-1.2.txt\n  against trunk revision .\n  ATTACHMENT ID: 12658715\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction\n                  org.apache.hadoop.hbase.migration.TestNamespaceUpgrade\n                  org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas\n                  org.apache.hadoop.hbase.regionserver.TestRegionReplicas\n                  org.apache.hadoop.hbase.client.TestReplicasClient\n                  org.apache.hadoop.hbase.master.TestRestartCluster\n                  org.apache.hadoop.hbase.TestRegionRebalancing\n                  org.apache.hadoop.hbase.TestIOFencing\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/10229//console\n\nThis message is automatically generated.\nIs this still work in progress or ready to review?\nYes,[~posix4e] ready to review.\nOK I have no commit bit, that being said.....\n\nWhat's going on here, can we make it into a function?  This has gotten pretty hairy \n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java\n@@ -2949,6 +2949,7 @@ public class HBaseFsck extends Configured {\n             } else {\n               m = new MetaEntry(hri, sn, ts, null, null);\n             }\n\n             HbckInfo previous = regionInfoMap.get(hri.getEncodedName());\n             if (previous == null) {\n               regionInfoMap.put(hri.getEncodedName(), new HbckInfo(m));\n....\n/TODO What does first online hri mean? Maybe a javadoc?\n         return FSUtils.getTableName(tableDir);\n       } else {\n         // return the info from the first online hri\n         for (OnlineEntry e : deployedEntries) {\n           return e.hri.getTable();\n         }\n\n\nThis is weirdly formatted. Maybe run an autoformat\n++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.ja\n@@ -186,12 +186,14 @@ public class HBaseFsckRepair {\n    * Puts the specified HRegionInfo into META with replica related columns\n    */\n   public static void fixMetaHoleOnlineAndAddReplicas(Configuration conf,\n-      HRegionInfo hri, Collection<ServerName> servers, int numReplicas) throws \n\nWoops accidently hit add\n\n....\n   if (numReplicas > 1) {\n       Random r = new Random();\n\nShouldn't this be?\n-      ServerName[] serversArr = servers.toArray(new ServerName[0]);\n+      ServerName[] serversArr = servers.toArray(new ServerName[servers.size()])\n\nYou have a spare todo around\n---- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n@@ -301,6 +301,7 @@ public class TestHBaseFsck {\n    * @param metaRow  if true remove region's row from META\n    * @param hdfs if true remove region's dir in HDFS\n    * @param regionInfoOnly if true remove a region dir's .regioninfo file\n    * @param replicaId TODO\n    */\n   private void deleteRegion(Configuration conf, final HTableDescriptor htd,\n\nAlso I noticed A commented out line in HBaseFsck\n\n@@ -3514,6 +3517,7 @@ public class HBaseFsck extends Configured {\n         // check to see if the existence of this region matches the region in M\n         for (HRegionInfo r:regions) {\n           HbckInfo hbi = hbck.getOrCreateInfo(r.getEncodedName());\n           //if (!RegionReplicaUtil.isDefaultReplica(r)) hbi.setSkipChecks(true)\n           hbi.addServer(r, rsinfo);\n         }\n\n{noformat}\n+    Result r = meta.get(get);\n+    RegionLocations rl = MetaTableAccessor.getRegionLocations(r);\n+    if (rl == null) return;\n{noformat}\n\nShould this be an error instead of silently returning?\n\n{noformat}\n+          for (HRegionLocation h : rl.getRegionLocations()) {\n+            if (h == null || h.getRegionInfo() == null) {\n+              continue;\n+            }\n{noformat}\n\nAnd this should be an error as well, especially if h.getRegionInfo() is null? \n\nNice tests. Just when I was thinking \"what about this case?\" I find a test for it.\n\nQuestions aside, LGTM, +1.\nAgreed, my vote does't count but +1\nOooh one other thing\n\nin HBaseFsck.java\n  /**\n   * Puts the specified HRegionInfo into META.\n   */\n  public static void fixMetaHoleOnline(Configuration conf,\n      HRegionInfo hri) throws IOException {\n    HTable meta = new HTable(conf, TableName.META_TABLE_NAME);\n    MetaTableAccessor.addRegionToMeta(meta, hri);\n    meta.close();\n  }\n\nno longer seems to be used\nCommitted. Thanks [~posix4e] & [~ndimiduk] for reviews.\nWhat I committed with the review comments incorporated.\nFAILURE: Integrated in HBase-TRUNK #5368 (See [https://builds.apache.org/job/HBase-TRUNK/5368/])\nHBASE-10674. HBCK should be updated to do replica related checks (ddas: rev 8562752143f57744ac522ace1a2b196c45b428d4)\n* hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java\n* hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java\n\nAddendum fixes compilation.\nFAILURE: Integrated in HBase-TRUNK #5369 (See [https://builds.apache.org/job/HBase-TRUNK/5369/])\nHBASE-10674 HBCK should be updated to do replica related checks Addendum (tedyu: rev ab757169a19be668e7a7d2e52c1da01d4810e0c2)\n* hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n\n", "issueSearchSentences": ["public static void fixMetaHoleOnlineAndAddReplicas(Configuration conf,", "HRegionInfo hri, Collection<ServerName> servers, int numReplicas) throws", "public static void fixMetaHoleOnline(Configuration conf,", "HBCK should be updated to have a check for whether the replicas are assigned to the right machines (default and non-default replicas ideally should not be in the same server if there is more than one server in the cluster and such scenarios).", "Puts the specified HRegionInfo into META with replica related columns"], "issueSearchScores": [0.7504163980484009, 0.719990611076355, 0.6418057680130005, 0.5884629487991333, 0.578744113445282]}
{"aId": 9, "code": "protected void verifyFileIntegrity(byte[] persistentChecksum, String algorithm)\n    throws IOException {\n    byte[] calculateChecksum = calculateChecksum(algorithm);\n    if (!Bytes.equals(persistentChecksum, calculateChecksum)) {\n      throw new IOException(\"Mismatch of checksum! The persistent checksum is \" +\n      Bytes.toString(persistentChecksum) + \", but the calculate checksum is \" +\n        Bytes.toString(calculateChecksum));\n    }\n  }", "comment": " Verify cache files's integrity", "issueId": "HBASE-23017", "issueStringList": ["[Forward-port] Verify the file integrity in persistent IOEngine", "Verify the persistent cache file integrity before retrieve from persistence file.", "HBASE-22890  is ready to go in now.. Pls change the master patch also accordingly.", "PR can only be applied to master and branch-2.", "There're conflicts both in branch-2.2 and branch-2.1.", "Please provide a patch or a new PR for it.", "Ping[~anoop.hbase], should we add this feature in 2.2.z and 2.1.z.", "I see the fixed version targets to 2.3.0 and 3.0.0 only.", "It should be fine for only 2.3.0 and master"], "SplitGT": [" Verify cache files's integrity"], "issueString": "[Forward-port] Verify the file integrity in persistent IOEngine\nVerify the persistent cache file integrity before retrieve from persistence file.\nHBASE-22890  is ready to go in now.. Pls change the master patch also accordingly.\nPR can only be applied to master and branch-2.\r\n\r\nThere're conflicts both in branch-2.2 and branch-2.1. \r\n\r\nPlease provide a patch or a new PR for it. \r\n\nPing[~anoop.hbase], should we add this feature in 2.2.z and 2.1.z.\r\n\r\nI see the fixed version targets to 2.3.0 and 3.0.0 only.\nIt should be fine for only 2.3.0 and master\n", "issueSearchSentences": ["[Forward-port] Verify the file integrity in persistent IOEngine", "Verify the persistent cache file integrity before retrieve from persistence file.", "There're conflicts both in branch-2.2 and branch-2.1.", "Ping[~anoop.hbase], should we add this feature in 2.2.z and 2.1.z.", "HBASE-22890  is ready to go in now.. Pls change the master patch also accordingly."], "issueSearchScores": [0.5053857564926147, 0.49006202816963196, 0.2855268120765686, 0.20020738244056702, 0.1879047453403473]}
{"aId": 10, "code": "private synchronized void snapshotDisabledTable(SnapshotDescription snapshot)\n      throws HBaseSnapshotException {\n\n    // set the snapshot to be a disabled snapshot, since the client doesn't know about that\n    snapshot = snapshot.toBuilder().setType(Type.DISABLED).build();\n\n    DisabledTableSnapshotHandler handler;\n    try {\n      handler = new DisabledTableSnapshotHandler(snapshot, this.master);\n      this.executorService.submit(handler);\n      this.handler = handler;\n    } catch (IOException e) {\n      // cleanup the working directory by trying to delete it from the fs.\n      Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);\n      try {\n        if (this.master.getMasterFileSystem().getFileSystem().delete(workingDir, true)) {\n          LOG.error(\"Couldn't delete working directory (\" + workingDir + \" for snapshot:\"\n              + snapshot);\n        }\n      } catch (IOException e1) {\n        LOG.error(\"Couldn't delete working directory (\" + workingDir + \" for snapshot:\" + snapshot);\n      }\n      // fail the snapshot\n      throw new SnapshotCreationException(\"Could not build snapshot handler\", e, snapshot);\n    }\n  }", "comment": " Take a snapshot of a disabled table.", "issueId": "HBASE-6864", "issueStringList": ["Online snapshots scaffolding", "Basic scaffolding for taking a snapshot of an offline table.", "This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.", "This has been subsumed by HBASE-7208", "Ok, I'm going to reopen this and put a patch up for this.", "review here.", "https://reviews.apache.org/r/8390/", "found plumbing bug, will flag for review when ready.", "bug fixed, ready for review again.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12561818/pre-hbase-6864.v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 87 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3626//console", "This message is automatically generated.", "reviewboard here: https://reviews.apache.org/r/8390/", "Updates from reviews", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562697/pre-hbase-6864.v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3761//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562815/pre-hbase-6864.v4.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3787//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12562872/pre-hbase-6864.v5.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3798//console", "This message is automatically generated.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12563000/pre-hbase-6864.v6.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 88 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3814//console", "This message is automatically generated.", "I've committed v7 to the online-snapshot branch.", "Thanks for the reviews Jesse, Stack and Matteo."], "SplitGT": [" Take a snapshot of a disabled table."], "issueString": "Online snapshots scaffolding\nBasic scaffolding for taking a snapshot of an offline table. This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.\nThis has been subsumed by HBASE-7208\nOk, I'm going to reopen this and put a patch up for this.\nreview here. https://reviews.apache.org/r/8390/\nfound plumbing bug, will flag for review when ready.\nbug fixed, ready for review again.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12561818/pre-hbase-6864.v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 87 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3626//console\n\nThis message is automatically generated.\nreviewboard here: https://reviews.apache.org/r/8390/\nUpdates from reviews\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562697/pre-hbase-6864.v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3761//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562815/pre-hbase-6864.v4.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3787//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12562872/pre-hbase-6864.v5.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3798//console\n\nThis message is automatically generated.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12563000/pre-hbase-6864.v6.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 88 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3814//console\n\nThis message is automatically generated.\nI've committed v7 to the online-snapshot branch.  Thanks for the reviews Jesse, Stack and Matteo.\n", "issueSearchSentences": ["Basic scaffolding for taking a snapshot of an offline table.", "I've committed v7 to the online-snapshot branch.", "This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.", "Online snapshots scaffolding", "against trunk revision ."], "issueSearchScores": [0.6458706855773926, 0.4464191794395447, 0.4377914071083069, 0.3981935381889343, 0.3859456181526184]}
{"aId": 14, "code": "protected boolean includeRegionInSplit(final byte[] startKey, final byte [] endKey) {\n    return true;\n  }", "comment": " all regions are included).", "issueId": "HBASE-2302", "issueStringList": ["Optimize M-R by bulk excluding regions - less InputSplit-s  to avoid traffic on region servers when performing M-R on a subset of the table", "TableInputFormatBase , creates a InputSplit per region.", "Given that  the keys are sorted - sometimes -  it might be needed to perform M-R on a subset of the keyset ( regions ) .", "Adding a provision to filter the regions when generating InputSplits might be useful .", "The granularity of exclusion is per-region-wise.", "A RowFilter might still be needed during a Scan on a separate region, but that is a separate issue altogether.", "Methodology: Add a way to prune the keyset before generating inputsplits , by default - all sets are returned indicating all regions are included that could be overridden as necessary, depending on the higher-level logic.", "add a method TableInputFormatBase.java#getPrunedKeys , where the user can prune the list of keys on which M-R needs to be applied to reduce the number of InputSplit .", "Very non-intrusive, but probably not the most API friendly.", "TODO: unit tests as necessary", "Overriddable method - includeRegionInSplit(final byte[] startKey, final byte [] endKey) - added.", "This does not break any compatibility - so would be a nice-to-have for 0.20.4 .", "Committed branch and trunk.", "Thanks for the patch Kay Kay."], "SplitGT": [" all regions are included)."], "issueString": "Optimize M-R by bulk excluding regions - less InputSplit-s  to avoid traffic on region servers when performing M-R on a subset of the table\nTableInputFormatBase , creates a InputSplit per region. Given that  the keys are sorted - sometimes -  it might be needed to perform M-R on a subset of the keyset ( regions ) . Adding a provision to filter the regions when generating InputSplits might be useful . \n\nThe granularity of exclusion is per-region-wise. A RowFilter might still be needed during a Scan on a separate region, but that is a separate issue altogether. \n\nMethodology: Add a way to prune the keyset before generating inputsplits , by default - all sets are returned indicating all regions are included that could be overridden as necessary, depending on the higher-level logic.  \n\n*  add a method TableInputFormatBase.java#getPrunedKeys , where the user can prune the list of keys on which M-R needs to be applied to reduce the number of InputSplit . \n\nVery non-intrusive, but probably not the most API friendly. \n\nTODO: unit tests as necessary \nOverriddable method - includeRegionInSplit(final byte[] startKey, final byte [] endKey) - added. \n\nThis does not break any compatibility - so would be a nice-to-have for 0.20.4 . \nCommitted branch and trunk.  Thanks for the patch Kay Kay.\n", "issueSearchSentences": ["Overriddable method - includeRegionInSplit(final byte[] startKey, final byte [] endKey) - added.", "Given that  the keys are sorted - sometimes -  it might be needed to perform M-R on a subset of the keyset ( regions ) .", "A RowFilter might still be needed during a Scan on a separate region, but that is a separate issue altogether.", "TableInputFormatBase , creates a InputSplit per region.", "Adding a provision to filter the regions when generating InputSplits might be useful ."], "issueSearchScores": [0.9169175624847412, 0.4846612513065338, 0.4742218852043152, 0.4655180275440216, 0.44830265641212463]}
{"aId": 17, "code": "public void setWriteBufferSize(long writeBufferSize) throws IOException {\n    this.writeBufferSize = writeBufferSize;\n    if(currentWriteBufferSize > writeBufferSize) {\n      flushCommits();\n    }\n  }", "comment": " Set the size of the buffer in bytes. If the new size is lower than the current size of data in the write buffer, the buffer is flushed.", "issueId": "HBASE-1770", "issueStringList": ["HTable.setWriteBufferSize does not flush the writeBuffer when its size is set to a value lower than its current size.", "When setting the size of the write buffer to a value lower than the current size of data in the write buffer, the content of the write buffer should be flushed so it does not occupy in memory more than its new size for an extended period of time.", "Adds call to flushCommits if the new size of the write buffer is lower than the current size of the data in the buffer.", "Side effect is that setWriteBufferSize now throws IOException as propagated from flushCommits.", "Patch looks good.", "Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically.", "Can fix on commit.", "Should we put this into 0.20 branch and trunk?", "Indeed my intent was to write 'calls flushCommits', not 'call flushCommits'.", "+1 on adding to branch and trunk", "Committed to trunk and branch.", "Thanks for the patch, Mathias."], "SplitGT": [" Set the size of the buffer in bytes.", "If the new size is lower than the current size of data in the write buffer, the buffer is flushed."], "issueString": "HTable.setWriteBufferSize does not flush the writeBuffer when its size is set to a value lower than its current size.\nWhen setting the size of the write buffer to a value lower than the current size of data in the write buffer, the content of the write buffer should be flushed so it does not occupy in memory more than its new size for an extended period of time.\n\nAdds call to flushCommits if the new size of the write buffer is lower than the current size of the data in the buffer.\n\nSide effect is that setWriteBufferSize now throws IOException as propagated from flushCommits.\nPatch looks good.  Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically.  Can fix on commit.\n\nShould we put this into 0.20 branch and trunk?\nIndeed my intent was to write 'calls flushCommits', not 'call flushCommits'.\n+1 on adding to branch and trunk\nCommitted to trunk and branch.\n\nThanks for the patch, Mathias.\n", "issueSearchSentences": ["Side effect is that setWriteBufferSize now throws IOException as propagated from flushCommits.", "HTable.setWriteBufferSize does not flush the writeBuffer when its size is set to a value lower than its current size.", "Adds call to flushCommits if the new size of the write buffer is lower than the current size of the data in the buffer.", "When setting the size of the write buffer to a value lower than the current size of data in the write buffer, the content of the write buffer should be flushed so it does not occupy in memory more than its new size for an extended period of time.", "Indeed my intent was to write 'calls flushCommits', not 'call flushCommits'."], "issueSearchScores": [0.9036999344825745, 0.8126232624053955, 0.7952723503112793, 0.7474778890609741, 0.5882527828216553]}
{"aId": 18, "code": "public void setCaching(int caching) {\n    this.caching = caching;\n  }", "comment": " Set the number of rows for caching that will be passed to scanners.", "issueId": "HBASE-1759", "issueStringList": ["Ability to specify scanner caching on a per-scan basis", "I think that clients should have the ability to configure the scanner caching setting on a per-scan basis via the org.apache.hadoop.hbase.client.Scan object.", "I propose adding a new caching property to the Scan class which would override the HTable.scannerCaching property if set.", "This would turn the HTable.scannerCaching property into more of a default setting.", "The code inside ClientScanner (an inner-class implementation of ResultScanner) would look like this:", "{code}", "Use the caching from the Scan.", "If not set, use the default cache setting for this table.", "if (this.scan.getCaching() > 0) {", "this.caching = this.scan.getCaching();", "} else {", "this.caching = HTable.this.scannerCaching;", "}", "{code}", "Note that currently the only option for per-scan scanner caching configuration is to modify the state of HTable for each Scan.", "This could lead to confusion when HTables are pooled since the pool would potentially end up with many HTables all configured differently.", "I will attach a patch.", "I'm looking forward to hearing your comments.", "Patch against trunk (at approx.", "2009-08-11 11:12 AM PST).", "HTable change looks good.", "Scan class comment seems like too much detail... Maybe just a tiny blurb and link to the method... then put detailed explanation in the method javadoc.", "Also, wording is a little confusing \"To set the number of rows that will be fetched when calling next on a scanner if it is not served from memory, execute...\", not sure what you mean if it is not served from memory... Can guess but I think it's confusing for a new user to see this information in the top of the Scan javadoc.", "How about just, to modify scanner caching for just this scan, use this method... in the method javadoc, put the detailed stuff, and be a little more explicit about what you mean by serving from memory (I guess, you would explain what scanner caching is briefly).", "(nitpick, there is unnecessary re-ordering of imports... you can leave it, things should be properly ordered, but as you know, we like our patches as short as possible)", "Actual code all looks quite good.", "I'm +1 on this for reasons you outlined, with doc changes I described.", "Thanks Ken!", ":)", "Jon, I copied the comments from the <description> element for the hbase.client.scanner.caching property in hbase-default.xml :)", "I'll submit another patch with the doc changes you are suggesting.", "New patch submitted with less wordy javadoc comments for Scan and a little more wordy javadocs comments for setCaching method as suggested by Jonathan.", "+1 Patch looks good.", "Any other committers want to weigh in before commit?", "Yes, I was just looking at this.", "+1.", "Potentially very useful.", "+1 Patch looks good to me.", "Committed.", "Thanks Ken.", "Committed to 0.21 trunk and 0.20 branch.", "Commited to 0.21 trunk and 0.20 branch"], "SplitGT": [" Set the number of rows for caching that will be passed to scanners."], "issueString": "Ability to specify scanner caching on a per-scan basis\nI think that clients should have the ability to configure the scanner caching setting on a per-scan basis via the org.apache.hadoop.hbase.client.Scan object.  I propose adding a new caching property to the Scan class which would override the HTable.scannerCaching property if set.  This would turn the HTable.scannerCaching property into more of a default setting.\n\nThe code inside ClientScanner (an inner-class implementation of ResultScanner) would look like this:\n{code}\n      // Use the caching from the Scan.  If not set, use the default cache setting for this table.\n      if (this.scan.getCaching() > 0) {\n        this.caching = this.scan.getCaching();\n      } else {\n        this.caching = HTable.this.scannerCaching;\n      }\n{code}\n\nNote that currently the only option for per-scan scanner caching configuration is to modify the state of HTable for each Scan.  This could lead to confusion when HTables are pooled since the pool would potentially end up with many HTables all configured differently.\n\nI will attach a patch.  I'm looking forward to hearing your comments.\nPatch against trunk (at approx. 2009-08-11 11:12 AM PST).\nHTable change looks good.\n\nScan class comment seems like too much detail... Maybe just a tiny blurb and link to the method... then put detailed explanation in the method javadoc.  Also, wording is a little confusing \"To set the number of rows that will be fetched when calling next on a scanner if it is not served from memory, execute...\", not sure what you mean if it is not served from memory... Can guess but I think it's confusing for a new user to see this information in the top of the Scan javadoc.\n\nHow about just, to modify scanner caching for just this scan, use this method... in the method javadoc, put the detailed stuff, and be a little more explicit about what you mean by serving from memory (I guess, you would explain what scanner caching is briefly).\n\n(nitpick, there is unnecessary re-ordering of imports... you can leave it, things should be properly ordered, but as you know, we like our patches as short as possible)\n\nActual code all looks quite good.  I'm +1 on this for reasons you outlined, with doc changes I described.\n\nThanks Ken!  :)\nJon, I copied the comments from the <description> element for the hbase.client.scanner.caching property in hbase-default.xml :)\nI'll submit another patch with the doc changes you are suggesting.\nNew patch submitted with less wordy javadoc comments for Scan and a little more wordy javadocs comments for setCaching method as suggested by Jonathan.\n+1 Patch looks good.  Any other committers want to weigh in before commit?\nYes, I was just looking at this. +1. Potentially very useful.\n+1 Patch looks good to me.\nCommitted.  Thanks Ken.\nCommitted to 0.21 trunk and 0.20 branch.\nCommited to 0.21 trunk and 0.20 branch\n", "issueSearchSentences": ["If not set, use the default cache setting for this table.", "I propose adding a new caching property to the Scan class which would override the HTable.scannerCaching property if set.", "Use the caching from the Scan.", "This would turn the HTable.scannerCaching property into more of a default setting.", "this.caching = HTable.this.scannerCaching;"], "issueSearchScores": [0.6027528643608093, 0.5051497220993042, 0.4699017405509949, 0.45397982001304626, 0.4436503052711487]}
{"aId": 22, "code": "public List<ServerName> getExcludedServersForSystemTable() {\n    return getExcludedServersForSystemTable(false);\n  }", "comment": " For system table, we must assign regions to a server with highest version.", "issueId": "HBASE-22923", "issueStringList": ["hbase:meta is assigned to localhost when we downgrade the hbase version", "When we downgrade the hbase version\uff08rsgroup enable\uff09, we found that the hbase:meta table could not be assigned.", "{code:java}", "master.AssignmentManager: Failed assignment of hbase:meta,,1.1588230740 to localhost,1,1, trying to assign elsewhere instead; try=1 of 10 java.io.IOException: Call to localhost/127.0.0.1:1 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:1", "{code}", "hbase group list:", "HBASE_META group\uff08hbase:meta and other system tables\uff09", "default group", "1.Down grade all servers in HBASE_META first", "2.higher version servers is in default", "3.hbase:meta assigned to localhost", "For system table, we assign them to a server with highest version.", "AssignmentManager#getExcludedServersForSystemTable", "But did not consider the rsgroup.", "When the nodes of system table group are all in the list of exclude nodes,\u00a0can we assign the system table across the group?", "bq.1.Down grade all servers in HBASE_META first", "Is this order imp?", "The versions in nodes of system group can be kept higher only while panning this upgrade/downgrade?", "Can u explain why this sequence was a must?", "Here is what can reproduce this easily (and it can happen unknowingly in production):", "# Create 'system' RSGroup.", "Move all system tables to this RSGroup.", "# Now we have system tables in system RSGroup and all other tables in default RSGroup.", "# Bring up new RegionServer on higher version.", "Since it's IP address is not yet known to master, it will be added to 'default' RSGroup by default (or let's say unknowingly one of default RSGroup's RegionServer is restarted and brought to higher version during rolling upgrade).", "# One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.", "And we will get the same error as reported by [~wenbang]\u00a0as per Jira description.", "Just had a quick glance, and it seems that we have purposefully kept RSGroupInfoManager (together with it's coproc endpoint) away from hbase-server (hbase-rsgroup not reachable) in branch-1 and branch-2, and it is again moved back to hbase-server module in trunk.", "[~zhangduo]\u00a0[~zghao] does this mean branch-1 and 2 have no way for AM to interact with RSGroup APIs directly?", "If we have a way, it would be better for AM to identify that any RS brought up with higher version than rest of RS should belong to 'system' RSGroup (for meta to move) and if it doesn't belong to system RSGroup, then do not move meta region to it regardless of the version difference.", "[~anoop.hbase]", "[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement: _All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules.", "This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.", "So that's the historical reason for the separation, and I assume for compatibility guideline reasons hbase-rsgroups was only merged into the core modules in master branch for HBase 3.", "Anyway, in your scenario, this seems like the key problem:", "{quote}Bring up new RegionServer on higher version.\u00a0\u00a0[...]", "One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.", "{quote}", "This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables, thus changing how we do rolling upgrades, thus changing the requirements for this behavior.", "At the very least we can make it optional.", "{quote}[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement:\u00a0_All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules.", "This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.", "{quote}", "I see, makes sense.", "{quote}This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables", "{quote}", "Agree, this behaviour is in line with any incompatible changes: user table region hosted on higher version RS can face issues connecting to meta hosted on lower version RS due to various reasons: coproc endpoint or schema incompatibilities.", "{quote}At the very least we can make it optional.", "{quote}", "Yeah that's better idea.", "In fact, we can introduce config for min version required to move system tables to.", "That would be much better.", "This way, operator would be willing to live with meta and other system table regions continue to live on lower versioned RS.", "In major case, we would want to define the value as next major version (assuming meta schema and coproc changes introduced in major version).", "For instance, setting min version to \"2.0.0\" would mean during upgrade from 1.6.0 -> 1.6.1 -> 1.7.0 would not have to worry about moving system regions to higher versioned RS but anytime we upgrade to HBase 2, AM will assign system table regions to higher versioned RS.", "Will come up with PR soon.", "Thank you\u00a0[~apurtell]!", "Thanks for the reviews [~apurtell] [~bharathv].", "Results for branch branch-1", "[build #142 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(x) {color:red}-1 general checks{color}", "For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk7 checks{color}", "For more information [see jdk7 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK7_Nightly_Build_Report/]", "(x) {color:red}-1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details."], "SplitGT": [" For system table, we must assign regions to a server with highest version."], "issueString": "hbase:meta is assigned to localhost when we downgrade the hbase version\nWhen we downgrade the hbase version\uff08rsgroup enable\uff09, we found that the hbase:meta table could not be assigned.\r\n{code:java}\r\nmaster.AssignmentManager: Failed assignment of hbase:meta,,1.1588230740 to localhost,1,1, trying to assign elsewhere instead; try=1 of 10 java.io.IOException: Call to localhost/127.0.0.1:1 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:1\r\n{code}\r\nhbase group list:\r\n\r\n\u00a0 HBASE_META group\uff08hbase:meta and other system tables\uff09\r\n\r\n\u00a0 default group\r\n\r\n1.Down grade all servers in HBASE_META first\r\n\r\n2.higher version servers is in default\r\n\r\n3.hbase:meta assigned to localhost\r\n\r\nFor system table, we assign them to a server with highest version.\r\n\r\nAssignmentManager#getExcludedServersForSystemTable\r\n\r\nBut did not consider the rsgroup.\nWhen the nodes of system table group are all in the list of exclude nodes,\u00a0can we assign the system table across the group?\nbq.1.Down grade all servers in HBASE_META first\r\nIs this order imp? The versions in nodes of system group can be kept higher only while panning this upgrade/downgrade?\r\nCan u explain why this sequence was a must?\nHere is what can reproduce this easily (and it can happen unknowingly in production):\r\n # Create 'system' RSGroup. Move all system tables to this RSGroup.\r\n # Now we have system tables in system RSGroup and all other tables in default RSGroup.\r\n # Bring up new RegionServer on higher version. Since it's IP address is not yet known to master, it will be added to 'default' RSGroup by default (or let's say unknowingly one of default RSGroup's RegionServer is restarted and brought to higher version during rolling upgrade).\r\n # One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup. And we will get the same error as reported by [~wenbang]\u00a0as per Jira description.\r\n\r\n\u00a0\r\n\r\nJust had a quick glance, and it seems that we have purposefully kept RSGroupInfoManager (together with it's coproc endpoint) away from hbase-server (hbase-rsgroup not reachable) in branch-1 and branch-2, and it is again moved back to hbase-server module in trunk.\r\n\r\n[~zhangduo]\u00a0[~zghao] does this mean branch-1 and 2 have no way for AM to interact with RSGroup APIs directly? If we have a way, it would be better for AM to identify that any RS brought up with higher version than rest of RS should belong to 'system' RSGroup (for meta to move) and if it doesn't belong to system RSGroup, then do not move meta region to it regardless of the version difference.\r\n\r\n[~anoop.hbase]\n[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement: _All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules. This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.\u00a0\r\n\r\nSo that's the historical reason for the separation, and I assume for compatibility guideline reasons hbase-rsgroups was only merged into the core modules in master branch for HBase 3.\u00a0\r\n\r\nAnyway, in your scenario, this seems like the key problem:\r\n{quote}Bring up new RegionServer on higher version.\u00a0\u00a0[...]\u00a0One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.\u00a0\r\n\r\n{quote}\r\nThis behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables, thus changing how we do rolling upgrades, thus changing the requirements for this behavior. At the very least we can make it optional.\u00a0\n{quote}[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement:\u00a0_All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules. This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.\u00a0\r\n{quote}\r\nI see, makes sense.\r\n{quote}This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables\r\n{quote}\r\nAgree, this behaviour is in line with any incompatible changes: user table region hosted on higher version RS can face issues connecting to meta hosted on lower version RS due to various reasons: coproc endpoint or schema incompatibilities.\r\n{quote}At the very least we can make it optional.\r\n{quote}\r\nYeah that's better idea. In fact, we can introduce config for min version required to move system tables to. That would be much better. This way, operator would be willing to live with meta and other system table regions continue to live on lower versioned RS. In major case, we would want to define the value as next major version (assuming meta schema and coproc changes introduced in major version). For instance, setting min version to \"2.0.0\" would mean during upgrade from 1.6.0 -> 1.6.1 -> 1.7.0 would not have to worry about moving system regions to higher versioned RS but anytime we upgrade to HBase 2, AM will assign system table regions to higher versioned RS.\r\n\r\nWill come up with PR soon. Thank you\u00a0[~apurtell]!\nThanks for the reviews [~apurtell] [~bharathv].\nResults for branch branch-1\n\t[build #142 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(x) {color:red}-1 general checks{color}\n-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//General_Nightly_Build_Report/]\n\n\n(/) {color:green}+1 jdk7 checks{color}\n-- For more information [see jdk7 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK7_Nightly_Build_Report/]\n\n\n(x) {color:red}-1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n", "issueSearchSentences": ["AssignmentManager#getExcludedServersForSystemTable", "For system table, we assign them to a server with highest version.", "When the nodes of system table group are all in the list of exclude nodes,\u00a0can we assign the system table across the group?", "# Now we have system tables in system RSGroup and all other tables in default RSGroup.", "This way, operator would be willing to live with meta and other system table regions continue to live on lower versioned RS."], "issueSearchScores": [0.7873160243034363, 0.5218108892440796, 0.4394035339355469, 0.4319223463535309, 0.40215742588043213]}
{"aId": 29, "code": "public static int getMobValueLength(Cell cell) {\n    return Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), Bytes.SIZEOF_INT);\n  }", "comment": " Gets the mob value length from the mob ref cell.", "issueId": "HBASE-11646", "issueStringList": ["Handle the MOB in compaction", "In the updated MOB design however, admins can set CF level thresholds that would force cell values > the threshold to use the MOB write path instead of the traditional path.", "There are two cases where mobs need to interact with this threshold", "1) How do we handle the case when the threshold size is changed?", "2) Today, you can bulkload hfiles that contain MOBs.", "These cells will work as normal inside hbase.", "Unfortunately the cells with MOBs in them will never benefit form the MOB write path.", "The proposal here is to modify compaction in mob enabled cf's such that the threshold value is honored with compactions.", "This handles case #1 -- elements that should be moved out of the normal hfiles get 'compacted' into refs and mob hfiles, and values that should be pulled into the cf get derefed and written out wholy in the compaction.", "For case #2, we can maintain the same behavior and compaction would move data into the mob writepath/lifecycle.", "reorganized the description to make it easier to follow", "[~jmhsieh] Thanks Jon for the descriptions.", "for the case#1, if a threshold is set larger, do we need to save the mob data back to the HBase?", "Or just leave them in the mob files?", "The most important thing is that hbase serves the correct data regardless of whether we move data into or out of the mob write path.", "My gut says we'd want to put a new copy of the mob them back into normal hfiles if the threshold is changed -- having it only go one way and being irreversible seems inconsistent.", "Can you come up with a case where we wouldn't want or expect the copy back into hfiles behavior?", "Thanks.", "I don't think I have.", "I agree with the bi-direction move.", "The threshold might be changed.", "After that, some cells in the mob files need to be added back to HBase store file, some cells in the HBase store files needs to be added to mob files.", "These are done in the HBase compaction.", "And it's implemented in this patch.", "note -- the patch is being reviewed here https://reviews.apache.org/r/24736/", "Refine the patch according to Jon, Ram and Anoop's comments and improvements.", "Thanks a lot.", "Upload the latest patch, please help review and comment.", "Thanks.", "Refactor the code after changing the mob flag and threshold in the hcd from string to byte[].", "Update the patch to refine the class import in the Compactor.", "Update the patch after the value size of a reference cell in a mob-enabled column is changed from a long to an int.", "Upload the latest patch (V6).", "Refine the code and comments, add methods to parse the value of mob ref cell.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12668311/HBASE-11646-V6.diff", "against trunk revision .", "ATTACHMENT ID: 12668311", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/10859//console", "This message is automatically generated.", "Update the patch according Anoop's comments in RB.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12669376/HBASE-11646-V7.diff", "against trunk revision .", "ATTACHMENT ID: 12669376", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 2 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/10942//console", "This message is automatically generated.", "+1", "I +1'ed in review board.", "Will commit to hbase-11339 pending clean test run.", "Did a minor tweak and ran", "{code}", "mvn clean test -Dtest=Test*Compact*", "{code}", "It passed.", "Commimtted to hbase-11339 branch.", "thanks jingcheng and thanks for the reviews ram and anoop.", "I cannot seem to commit currently.", "will try again later and close issue then.", "attached th v8 version I tried to commit.", "Update the patch (V9) according to Jon and Ram's comments in RB.", "1.", "!Bytes.equals(emptyBytes, value) -> valueLength!=0", "2.", "Add comments to describe why we have to case the Store to HMobStore in DefaultMobCompactor.", "I've commited v10 -- it has the same minor tweek v8 had.", "Thanks jingcheng, ram and anoop."], "SplitGT": [" Gets the mob value length from the mob ref cell."], "issueString": "Handle the MOB in compaction\nIn the updated MOB design however, admins can set CF level thresholds that would force cell values > the threshold to use the MOB write path instead of the traditional path.  There are two cases where mobs need to interact with this threshold\n\n1) How do we handle the case when the threshold size is changed?\n2) Today, you can bulkload hfiles that contain MOBs.  These cells will work as normal inside hbase.  Unfortunately the cells with MOBs in them will never benefit form the MOB write path.\n\nThe proposal here is to modify compaction in mob enabled cf's such that the threshold value is honored with compactions.  This handles case #1 -- elements that should be moved out of the normal hfiles get 'compacted' into refs and mob hfiles, and values that should be pulled into the cf get derefed and written out wholy in the compaction.  For case #2, we can maintain the same behavior and compaction would move data into the mob writepath/lifecycle.\nreorganized the description to make it easier to follow\n[~jmhsieh] Thanks Jon for the descriptions.\nfor the case#1, if a threshold is set larger, do we need to save the mob data back to the HBase? Or just leave them in the mob files?\nThe most important thing is that hbase serves the correct data regardless of whether we move data into or out of the mob write path.  My gut says we'd want to put a new copy of the mob them back into normal hfiles if the threshold is changed -- having it only go one way and being irreversible seems inconsistent.  Can you come up with a case where we wouldn't want or expect the copy back into hfiles behavior?\nThanks. I don't think I have. I agree with the bi-direction move.\nThe threshold might be changed.\nAfter that, some cells in the mob files need to be added back to HBase store file, some cells in the HBase store files needs to be added to mob files. These are done in the HBase compaction. And it's implemented in this patch.\nnote -- the patch is being reviewed here https://reviews.apache.org/r/24736/\nRefine the patch according to Jon, Ram and Anoop's comments and improvements. Thanks a lot.\nUpload the latest patch, please help review and comment. Thanks.\nRefactor the code after changing the mob flag and threshold in the hcd from string to byte[].\nUpdate the patch to refine the class import in the Compactor.\nUpdate the patch after the value size of a reference cell in a mob-enabled column is changed from a long to an int.\nUpload the latest patch (V6). Refine the code and comments, add methods to parse the value of mob ref cell.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12668311/HBASE-11646-V6.diff\n  against trunk revision .\n  ATTACHMENT ID: 12668311\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/10859//console\n\nThis message is automatically generated.\nUpdate the patch according Anoop's comments in RB.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12669376/HBASE-11646-V7.diff\n  against trunk revision .\n  ATTACHMENT ID: 12669376\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/10942//console\n\nThis message is automatically generated.\n+1\nI +1'ed in review board.  Will commit to hbase-11339 pending clean test run.\nDid a minor tweak and ran \n{code}\n mvn clean test -Dtest=Test*Compact*\n{code}\n\nIt passed.  Commimtted to hbase-11339 branch.  thanks jingcheng and thanks for the reviews ram and anoop.\n\n\nI cannot seem to commit currently.  will try again later and close issue then.\nattached th v8 version I tried to commit.\nUpdate the patch (V9) according to Jon and Ram's comments in RB.\n1. !Bytes.equals(emptyBytes, value) -> valueLength!=0\n2. Add comments to describe why we have to case the Store to HMobStore in DefaultMobCompactor.\nI've commited v10 -- it has the same minor tweek v8 had.  Thanks jingcheng, ram and anoop.\n", "issueSearchSentences": ["Update the patch after the value size of a reference cell in a mob-enabled column is changed from a long to an int.", "!Bytes.equals(emptyBytes, value) -> valueLength!=0", "Refine the code and comments, add methods to parse the value of mob ref cell.", "The proposal here is to modify compaction in mob enabled cf's such that the threshold value is honored with compactions.", "Unfortunately the cells with MOBs in them will never benefit form the MOB write path."], "issueSearchScores": [0.5440307855606079, 0.3394605219364166, 0.3054528832435608, 0.29980701208114624, 0.2749418020248413]}
{"aId": 30, "code": "HRegionFileSystem(final Configuration conf, final FileSystem fs,\n      final Path tableDir, final HRegionInfo regionInfo) {\n    this.fs = fs;\n    this.conf = conf;\n    this.tableDir = tableDir;\n    this.regionInfo = regionInfo;\n  }", "comment": " Create a view to the on-disk region", "issueId": "HBASE-7807", "issueStringList": ["Introduce HRegionFileSystem and move region fs related code", "Add a new HRegionFileSystem class that allows to create an on-disk region without needs to instantiate an HRegion.", "This is the first refactor step, the next step is moving file creation and retrival inside this class.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12568750/HBASE-7807-v0.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 18 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 5 warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.master.TestZKBasedOpenCloseRegion", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//console", "This message is automatically generated.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12570803/HBASE-7807-v1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 24 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//console", "This message is automatically generated.", "Can you add an /r/?", "Thanks", "review board here: https://reviews.apache.org/r/9589/", "left some minor comments", "+1", "v3 rebase against latest trunk (no changes)", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12571179/HBASE-7807-v3.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 26 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//console", "This message is automatically generated.", "Integrated in HBase-TRUNK #3904 (See [https://builds.apache.org/job/HBase-TRUNK/3904/])", "HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)", "Result = FAILURE", "mbertozzi :", "Files :", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java", "Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #423 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/423/])", "HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)", "Result = FAILURE", "mbertozzi :", "Files :", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java"], "SplitGT": [" Create a view to the on-disk region"], "issueString": "Introduce HRegionFileSystem and move region fs related code\nAdd a new HRegionFileSystem class that allows to create an on-disk region without needs to instantiate an HRegion.\nThis is the first refactor step, the next step is moving file creation and retrival inside this class.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12568750/HBASE-7807-v0.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 18 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 5 warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.master.TestZKBasedOpenCloseRegion\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4403//console\n\nThis message is automatically generated.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12570803/HBASE-7807-v1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 24 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4528//console\n\nThis message is automatically generated.\nCan you add an /r/? Thanks\nreview board here: https://reviews.apache.org/r/9589/\nleft some minor comments\n+1\nv3 rebase against latest trunk (no changes)\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12571179/HBASE-7807-v3.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 26 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4574//console\n\nThis message is automatically generated.\nIntegrated in HBase-TRUNK #3904 (See [https://builds.apache.org/job/HBase-TRUNK/3904/])\n    HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)\n\n     Result = FAILURE\nmbertozzi : \nFiles : \n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java\n\nIntegrated in HBase-TRUNK-on-Hadoop-2.0.0 #423 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/423/])\n    HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)\n\n     Result = FAILURE\nmbertozzi : \nFiles : \n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java\n\n", "issueSearchSentences": ["HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)", "HBASE-7807 Introduce HRegionFileSystem and move region fs related code (Revision 1451025)", "Introduce HRegionFileSystem and move region fs related code", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java"], "issueSearchScores": [0.7394129037857056, 0.7394129037857056, 0.7224938869476318, 0.710206925868988, 0.710206925868988]}
{"aId": 31, "code": "public Collection<HServerInfo> getServerInfo() {\n    return Collections.unmodifiableCollection(liveServerInfo);\n  }", "comment": " Returns detailed region server information: A list of HServerInfo, containing server load and resource usage statistics as HServerLoad, containing per-region statistics as HServerLoad.RegionLoad.", "issueId": "HBASE-1454", "issueStringList": ["HBaseAdmin.getClusterStatus", "Somewhat like Hadoop's JobClient.getClusterStatus().", "Provides status information to clients:", "The names of region servers in the cluster.", "The average cluster load.", "The number of regions deployed on the cluster.", "The number of requests since last report.", "Detailed region server loading and resource usage information: A list of HServerInfo, containing load information as HServerLoad, containing per-region load information as HServerLoad.RegionLoad.", "Patch withdrawn.", "Working on a version -2 with shell tool support.", "+1", "Thats nice Andrew.", "Committed to trunk."], "SplitGT": [" Returns detailed region server information: A list of HServerInfo, containing server load and resource usage statistics as HServerLoad, containing per-region statistics as HServerLoad.RegionLoad."], "issueString": "HBaseAdmin.getClusterStatus\nSomewhat like Hadoop's JobClient.getClusterStatus().\n\nProvides status information to clients:\n\n* The names of region servers in the cluster.\n* The average cluster load.\n* The number of regions deployed on the cluster.\n* The number of requests since last report.\n* Detailed region server loading and resource usage information: A list of HServerInfo, containing load information as HServerLoad, containing per-region load information as HServerLoad.RegionLoad. \nPatch withdrawn. Working on a version -2 with shell tool support. \n+1\n\nThats nice Andrew.\nCommitted to trunk.\n", "issueSearchSentences": ["Detailed region server loading and resource usage information: A list of HServerInfo, containing load information as HServerLoad, containing per-region load information as HServerLoad.RegionLoad.", "HBaseAdmin.getClusterStatus", "Somewhat like Hadoop's JobClient.getClusterStatus().", "The names of region servers in the cluster.", "Provides status information to clients:"], "issueSearchScores": [0.44304007291793823, 0.396573930978775, 0.37768763303756714, 0.29321184754371643, 0.2565779983997345]}
{"aId": 34, "code": "public static void deleteMyEphemeralNodeOnDisk() {\n    String fileName = getMyEphemeralNodeFileName();\n\n    if (fileName != null) {\n      new File(fileName).delete();\n    }\n  }", "comment": " delete the znode file", "issueId": "HBASE-5926", "issueStringList": ["Delete the master znode after a master crash", "This is the continuation of the work done in HBASE-5844.", "But we can't apply exactly the same strategy: for the region server, there is a znode per region server, while for the master & backup master there is a single znode for both.", "So if we apply the same strategy as for a regionserver, we may have this scenario:", "1) Master starts", "2) Backup master starts", "3) Master dies", "4) ZK detects it", "5) Backup master receives the update from ZK", "6) Backup master creates the new master node and become the main master", "7) Previous master script continues", "8) Previous master script deletes the master node in ZK", "9) => issue: we deleted the node just created by the new master", "This should not happen often (usually the znode will be deleted soon enough), but it can happen.", "the race condition is decreased to a production-acceptable minimum imho.", "We do a compare & delete in the java code, so the race condition is now: between the comparison and the delete, we fail if, and only if: the session expires and the master node is deleted and the master backup recreates the node.", "That's unlikely.", "I cannot find CleanZnode class in the patch.", "{code}", "+    \"Usage: Master [opts] start|stop|cleanZNode\\n\" +", "{code}", "Please add document for cleanZNode command.", "{code}", "+   * delete the znode master if its content is same to the parameter", "{code}", "'znode master' -> 'master znode', 'same to' -> 'same as'", "{code}", "+    } catch (KeeperException ignore) {", "+    } catch (IOException ignore) {", "+    }", "{code}", "I would expect some logging for above cases.", "v8.", "with Ted's comments taken into account.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12527816/5926.v8.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.replication.TestReplication", "org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//console", "This message is automatically generated.", "These tests run ok locally.", "Please add javadoc for the new class:", "{code}", "+public class CleanZnode {", "{code}", "readMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't.", "What was the reason ?", "{code}", "+   * Get the name of the file used to store the znode", "{code}", "Please add ' contents' at the end of the above.", "{code}", "+  public static int cleanZNode(Configuration conf) {", "+    conf.setInt(\"zookeeper.recovery.retry\", 0);", "{code}", "Should the setting be restored before exiting the above method ?", "bq.", "javadoc", "done.", "bq.", "readMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't.", "What was the reason ?", "When we write we ignore the results (i.e.", "we don't stop the master or the region server if we can't store the znode, we just continue).", "When we read, we're interested in the exception: the pattern in HMasterCommandLine is to return -1 on error.", "bq.", "Please add ' contents' at the end of the above.", "ok.", "bq.", "Should the setting be restored before exiting the above method ?", "I now clone the conf.", "bq.", "I now clone the conf.", "That is safer, avoiding race condition w.r.t.", "the original conf.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12527860/5926.v9.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//console", "This message is automatically generated.", "@Stack:", "Can you comment on the latest patch ?", "CleanZNode should be in the tool package or in zookeeper (probably the latter since thats its dependencies)  ... hmmm.. but hangon, I see why you have it here... because its used by master and regionserver packages.", "Thats fine.", "Its in the right place I'd say.", "You should look at the javadoc that is created from your src.", "Its going to be a jumble.", "Check it out.", "You need a little bit of html in there at least for your list of strategy dependencies.", "What is the filecontent?", "We don't need any, right?", "The name of the file is enough?", "This should be boolean rather than int?", "Or is it returned to shell?", "If so, should say so in the comment: \"+   * @return if done returns 0 else -1.\"", "Is CleanZNode a good name?", "How about ZNodeCleaner or ZNodeClearer or CrashZNodeCleaner?", "I think in HMasterCommandLine, should be start|stop|clear so it fits format of the other commands.", "In MasterAddressTracker, can you get the znode sequence id and only delete if the sequence id  matches?", "bq.", "You should look at the javadoc that is created from your src.", "Its going to be a jumble.", "Check it out.", "You need a little bit of html in there at least for your list of strategy dependencies.", "Done.", "bq.", "What is the filecontent?", "We don't need any, right?", "The name of the file is enough?", "We need the content.", "For the regionserver, the content is the znode path.", "For the master it's the full ServerName (stringified).", "bq.", "This should be boolean rather than int?", "Or is it returned to shell?", "If so, should say so in the comment: \"+ * @return if done returns 0 else -1.\"", "Done.", "bq.", "Is CleanZNode a good name?", "How about ZNodeCleaner or ZNodeClearer or CrashZNodeCleaner?", "Renamed to ZNodeClearer", "bq.", "I think in HMasterCommandLine, should be start|stop|clear so it fits format of the other commands.", "Done.", "bq.", "In MasterAddressTracker, can you get the znode sequence id and only delete if the sequence id matches?", "We store the full ServerName so if there is a restart we will see it.", "But maybe you're speaking about the znode version?", "Because I looked at the zk api, and with the version we could remove totally the race condition...", "bq.", "We need the content.", "For the regionserver, the content is the znode path.", "For the master it's the full ServerName (stringified).", "Does it have to be full path?", "Can it not just be the node name?", "Thats safe to put in the fs?", "Master servername stringified should be safe in the fs too.", "Yeah, version.", "Yes, it could be the node name only.", "Does it make a difference?", "To me, they can both be safely written in the fs.", "I check the version in v11, so there is no race condition at all now.", "@N:", "Did you forget to include ZNodeClearer class in the latest patch ?", "{code}", "+    } catch (KeeperException e) {", "+      LOG.info(\"Can't get or delete the master znode\", e);", "+    } catch (DeserializationException e) {", "+      LOG.info(\"Can't get or delete the master znode\", e);", "+    }", "{code}", "I think the above log should be at WARN level.", "Its fine putting it in file.", "And what Ted said.", "Thanks N.", "v13 should do it...", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12528018/5926.v13.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "1 core tests.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//console", "This message is automatically generated.", "I think it's ok, I don't have this locally...", "Minor comments:", "{code}", "+ * servers.", "It allows to delete immediately the znode when the master or the regions server crash.", "{code}", "'regions server crash' -> 'region server crashes'", "{code}", "+ * The region server / master write a specific file when they start / become main master.", "When they", "{code}", "'write a specific' -> 'writes a specific'.", "'they start / become' -> 'it starts / becomes'", "I think using 'they' is confusing because region server and master have different roles.", "{code}", "+    \" clear  Delete the master znode in ZooKeeper after a master crash\\n \"+", "{code}", "'master crash' -> 'master crashes'", "+1 on patch v14", "I'll fix Ted comments on commit.", "Looks like N already addressed Ted's review comments.", "Great.", "Applied to trunk.", "Thanks for the patch N.", "Integrated in HBase-TRUNK #2899 (See [https://builds.apache.org/job/HBase-TRUNK/2899/])", "HBASE-5926 Delete the master znode after a master crash (Revision 1340185)", "Result = FAILURE", "stack :", "Files :", "hbase/trunk/bin/hbase-daemon.sh", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java", "hbase/trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java", "ZNodeClearer.java isn't in source repo.", "1 overall.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12528104/5926.v14.patch", "against trunk revision .", "+1 @author.", "The patch does not contain any @author tags.", "+1 tests included.", "The patch appears to include 3 new or modified tests.", "+1 hadoop23.", "The patch compiles against the hadoop 0.23.x profile.", "+1 javadoc.", "The javadoc tool did not generate any warning messages.", "+1 javac.", "The applied patch does not increase the total number of javac compiler warnings.", "1 findbugs.", "The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.", "+1 release audit.", "The applied patch does not increase the total number of release audit warnings.", "+1 core tests.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//console", "This message is automatically generated.", "Fixed.", "Thanks Ted.", "Integrated in HBase-TRUNK #2900 (See [https://builds.apache.org/job/HBase-TRUNK/2900/])", "HBASE-5926 Delete the master znode after a master crash (Revision 1340200)", "Result = SUCCESS", "stack :", "Files :", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java", "Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #10 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/10/])", "HBASE-5926 Delete the master znode after a master crash (Revision 1340200)", "HBASE-5926 Delete the master znode after a master crash (Revision 1340185)", "Result = FAILURE", "stack :", "Files :", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java", "stack :", "Files :", "hbase/trunk/bin/hbase-daemon.sh", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java", "hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java", "hbase/trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java"], "SplitGT": [" delete the znode file"], "issueString": "Delete the master znode after a master crash\nThis is the continuation of the work done in HBASE-5844.\nBut we can't apply exactly the same strategy: for the region server, there is a znode per region server, while for the master & backup master there is a single znode for both.\n\nSo if we apply the same strategy as for a regionserver, we may have this scenario:\n1) Master starts\n2) Backup master starts\n3) Master dies\n4) ZK detects it\n5) Backup master receives the update from ZK\n6) Backup master creates the new master node and become the main master\n7) Previous master script continues\n8) Previous master script deletes the master node in ZK\n9) => issue: we deleted the node just created by the new master\n\nThis should not happen often (usually the znode will be deleted soon enough), but it can happen.\nthe race condition is decreased to a production-acceptable minimum imho. We do a compare & delete in the java code, so the race condition is now: between the comparison and the delete, we fail if, and only if: the session expires and the master node is deleted and the master backup recreates the node. That's unlikely. \nI cannot find CleanZnode class in the patch.\n{code}\n+    \"Usage: Master [opts] start|stop|cleanZNode\\n\" +\n{code}\nPlease add document for cleanZNode command.\n{code}\n+   * delete the znode master if its content is same to the parameter\n{code}\n'znode master' -> 'master znode', 'same to' -> 'same as'\n{code}\n+    } catch (KeeperException ignore) {\n+    } catch (IOException ignore) {\n+    }\n{code}\nI would expect some logging for above cases.\n\nv8. with Ted's comments taken into account.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12527816/5926.v8.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.replication.TestReplication\n                  org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1909//console\n\nThis message is automatically generated.\nThese tests run ok locally.\nPlease add javadoc for the new class:\n{code}\n+public class CleanZnode {\n{code}\n\nreadMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't. What was the reason ?\n{code}\n+   * Get the name of the file used to store the znode\n{code}\nPlease add ' contents' at the end of the above.\n{code}\n+  public static int cleanZNode(Configuration conf) {\n+    conf.setInt(\"zookeeper.recovery.retry\", 0);\n{code}\nShould the setting be restored before exiting the above method ?\nbq. javadoc\ndone.\n\nbq. readMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't. What was the reason ?\nWhen we write we ignore the results (i.e. we don't stop the master or the region server if we can't store the znode, we just continue). When we read, we're interested in the exception: the pattern in HMasterCommandLine is to return -1 on error.\n\nbq. Please add ' contents' at the end of the above.\nok.\n\nbq. Should the setting be restored before exiting the above method ?\nI now clone the conf.\n\nbq. I now clone the conf.\nThat is safer, avoiding race condition w.r.t. the original conf.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12527860/5926.v9.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1912//console\n\nThis message is automatically generated.\n@Stack:\nCan you comment on the latest patch ?\nCleanZNode should be in the tool package or in zookeeper (probably the latter since thats its dependencies)  ... hmmm.. but hangon, I see why you have it here... because its used by master and regionserver packages.  Thats fine.  Its in the right place I'd say.\n\nYou should look at the javadoc that is created from your src.  Its going to be a jumble.  Check it out.  You need a little bit of html in there at least for your list of strategy dependencies.\n\nWhat is the filecontent?  We don't need any, right?  The name of the file is enough?\n\nThis should be boolean rather than int?  Or is it returned to shell?  If so, should say so in the comment: \"+   * @return if done returns 0 else -1.\"\n\nIs CleanZNode a good name?  How about ZNodeCleaner or ZNodeClearer or CrashZNodeCleaner?\n\nI think in HMasterCommandLine, should be start|stop|clear so it fits format of the other commands.\n\nIn MasterAddressTracker, can you get the znode sequence id and only delete if the sequence id  matches?\n\nbq. You should look at the javadoc that is created from your src. Its going to be a jumble. Check it out. You need a little bit of html in there at least for your list of strategy dependencies.\nDone.\n\nbq. What is the filecontent? We don't need any, right? The name of the file is enough?\nWe need the content. For the regionserver, the content is the znode path. For the master it's the full ServerName (stringified).\n\nbq. This should be boolean rather than int? Or is it returned to shell? If so, should say so in the comment: \"+ * @return if done returns 0 else -1.\"\nDone.\n\nbq. Is CleanZNode a good name? How about ZNodeCleaner or ZNodeClearer or CrashZNodeCleaner?\nRenamed to ZNodeClearer \n\nbq. I think in HMasterCommandLine, should be start|stop|clear so it fits format of the other commands.\nDone.\n\nbq. In MasterAddressTracker, can you get the znode sequence id and only delete if the sequence id matches?\nWe store the full ServerName so if there is a restart we will see it. But maybe you're speaking about the znode version? Because I looked at the zk api, and with the version we could remove totally the race condition...\n\nbq. We need the content. For the regionserver, the content is the znode path. For the master it's the full ServerName (stringified).\n\nDoes it have to be full path?  Can it not just be the node name?  Thats safe to put in the fs?  Master servername stringified should be safe in the fs too.\n\nYeah, version.\nYes, it could be the node name only. Does it make a difference? To me, they can both be safely written in the fs.\nI check the version in v11, so there is no race condition at all now.\n\n@N:\nDid you forget to include ZNodeClearer class in the latest patch ?\n{code}\n+    } catch (KeeperException e) {\n+      LOG.info(\"Can't get or delete the master znode\", e);\n+    } catch (DeserializationException e) {\n+      LOG.info(\"Can't get or delete the master znode\", e);\n+    }\n{code}\nI think the above log should be at WARN level.\nIts fine putting it in file.  And what Ted said.  Thanks N.\nv13 should do it...\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12528018/5926.v13.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n     -1 core tests.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1927//console\n\nThis message is automatically generated.\nI think it's ok, I don't have this locally...\nMinor comments:\n{code}\n+ * servers. It allows to delete immediately the znode when the master or the regions server crash.\n{code}\n'regions server crash' -> 'region server crashes'\n{code}\n+ * The region server / master write a specific file when they start / become main master. When they\n{code}\n'write a specific' -> 'writes a specific'. 'they start / become' -> 'it starts / becomes'\nI think using 'they' is confusing because region server and master have different roles.\n{code}\n+    \" clear  Delete the master znode in ZooKeeper after a master crash\\n \"+\n{code}\n'master crash' -> 'master crashes'\n\n+1 on patch v14\nI'll fix Ted comments on commit.\nLooks like N already addressed Ted's review comments.  Great.\n\nApplied to trunk.  Thanks for the patch N.\nIntegrated in HBase-TRUNK #2899 (See [https://builds.apache.org/job/HBase-TRUNK/2899/])\n    HBASE-5926 Delete the master znode after a master crash (Revision 1340185)\n\n     Result = FAILURE\nstack : \nFiles : \n* /hbase/trunk/bin/hbase-daemon.sh\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java\n* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java\n\nZNodeClearer.java isn't in source repo.\n-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12528104/5926.v14.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 hadoop23.  The patch compiles against the hadoop 0.23.x profile.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 33 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/1933//console\n\nThis message is automatically generated.\nFixed.  Thanks Ted.\nIntegrated in HBase-TRUNK #2900 (See [https://builds.apache.org/job/HBase-TRUNK/2900/])\n    HBASE-5926 Delete the master znode after a master crash (Revision 1340200)\n\n     Result = SUCCESS\nstack : \nFiles : \n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java\n\nIntegrated in HBase-TRUNK-on-Hadoop-2.0.0 #10 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/10/])\n    HBASE-5926 Delete the master znode after a master crash (Revision 1340200)\nHBASE-5926 Delete the master znode after a master crash (Revision 1340185)\n\n     Result = FAILURE\nstack : \nFiles : \n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java\n\nstack : \nFiles : \n* /hbase/trunk/bin/hbase-daemon.sh\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java\n* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java\n\n", "issueSearchSentences": ["readMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't.", "readMyEphemeralNodeOnDisk() throws IOException but writeMyEphemeralNodeOnDisk() doesn't.", "It allows to delete immediately the znode when the master or the regions server crash.", "This should not happen often (usually the znode will be deleted soon enough), but it can happen.", "9) => issue: we deleted the node just created by the new master"], "issueSearchScores": [0.651874303817749, 0.651874303817749, 0.545403778553009, 0.5161924958229065, 0.49059703946113586]}
{"aId": 35, "code": "public Map<String, Pair<ServerName, List<ServerName>>> getInconsistentRegions() {\n    // Need synchronized here, as this \"snapshot\" may be changed after checking.\n    rwLock.readLock().lock();\n    try {\n      return this.inconsistentRegionsSnapshot;\n    } finally {\n      rwLock.readLock().unlock();\n    }\n  }", "comment": " Found the inconsistent regions. Master thought this region opened, but no regionserver reported it. Master thought this region opened on Server1, but regionserver reported Server2 case 3. More than one regionservers reported opened this region", "issueId": "HBASE-22709", "issueStringList": ["Add a chore thread in master to do hbck checking and display results in 'HBCK Report' page", "See\u00a0HBASE-21965.", "There may have two type of failed splited/merged regions.", "One is\u00a0orphan region on filesystem.", "Another one is\u00a0unassigned daughter regions or merged regions.", "Plan to add a seperate hbck.jsp for them.", "Will add more hbck problems to this UI in future.", "A basic patch.", "Refactor HBaseFsck and add a new HbckChecker chore in master.", "Will add ui later.", "Let me move the refactor work to a new issue.", "| (x) *{color:red}-1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated.", "{color} |", "| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  7s{color} | {color:red} HBASE-22709 does not apply to master.", "Rebase required?", "Wrong Branch?", "See https://yetus.apache.org/documentation/in-progress/precommit-patchnames for help.", "{color} |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| JIRA Issue | HBASE-22709 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12975392/HBASE-22709.master.001.patch |", "| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/664/console |", "| Powered by | Apache Yetus 0.9.0 http://yetus.apache.org |", "This message was automatically generated.", "Looks good.", "It runs once an hour?", "Can operator ask to run it?", "[~zghaobac]", "You see note on end of https://issues.apache.org/jira/browse/HBASE-21745?focusedCommentId=16889062&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16889062 ?", "I'm working on a patch to beef up CatalogJanitor so when it scans on its five minute interval, it reports on holes, overlaps, and unknown servers.", "Was going to have it manufacture a Report.", "The Report could show on your hbck UI page.", "Was thinking of adding it to ClusterMetrics.", "bq.", "It runs once an hour?", "Yes.", "It need to scan all hdfs dir.", "So I thought no need to run it frequently.", "bq.", "Can operator ask to run it?", "Good idea.", "bq.", "I'm working on a patch to beef up CatalogJanitor so when it scans on its five minute interval, it reports on holes, overlaps, and unknown servers.", "Great.", "The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.", "[~zghaobac]", "bq.", "The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.", "Yes.", "The work here and in HBASE-22723 compliments each other.", "I can dump the hbase:meta report into whereever the new hbck UI page ends up.", "Did you see suggestion of an 'adoption service' in HBASE-27145?", "We'd ask the Master to 'adopt' orphans found in HDFS (by your work here for instance).", "bq.", "Did you see suggestion of an 'adoption service' in HBASE-27145?", "We'd ask the Master to 'adopt' orphans found in HDFS (by your work here for instance).", "I thought we only fix it by HBCK2 now...", "The master UI shows the orphan regions and user use HBCK2 to fix them.", "I found a bug for \"Problematic Regions\": the rsReports didn't remove the dead server's report.", "Will fix this in this issue.", "As I plan to move the \"Problematic Regions\" UI to a new hbck.jsp.", "bq.", "I thought we only fix it by HBCK2 now...", "The master UI shows the orphan regions and user use HBCK2 to fix them.", "HBCK2 will ask the Master to adopt 'orphans', yes.", "This facility will find the orphans.", "We maybe should make it so HBCK2 can ask the Master for list of orphans (JSON listing?)", "Will this overload master?", "We have already put lots of functions into master...", "bq.", "Will this overload master?", "We have already put lots of functions into master...", "Master is in charge, the final arbitor.", "How is it overloaded?", "You mean running adoption ?", "It'd be some fs work, mostly moving I'd hope.", "Will be conscious.", "bq.", "Will this overload master?", "We have already put lots of functions into master...", "It only need to list the hdfs directory.", "And no need to scan meta.", "And the rsReports is in-memory.", "For the region hole or overlap problem, it will be done in CatalogJanitor.", "It already scan meta now.", "I thought it is fine to add more checking thing in CatalogJanitor.", "bq.", "We maybe should make it so HBCK2 can ask the Master for list of orphans (JSON listing?)", "This can be done in UI?", "a link to get the JSON listing.", "I thought no need to add a API for this...", "I think stack means that we could just send a 'fix' command to HBCK2 and it will poll the list of orphan regions and then fix them.", "But I think this maybe a bit dangerous?", "If the balancer is still running, we may introduce more inconsistencies if we do this automatically...", "bq.", "But I think this maybe a bit dangerous?", "If the balancer is still running, we may introduce more inconsistencies if we do this automatically...", "Yes, the check result is not absolutely right.", "The \"fix\" thing shoule be better done by user manually.", "And for the inconsistent regions, current HBCK2 is enough to fix them.", "Case1.", "Master thought this region opened, but no regionserver reported it.", "====>  Fix: Use assign cmd.", "Case2.", "Master thought this region opened on Server1, but regionserver reported Server2.", "====> Fix: need to check the server is still exist.", "If not, schedule SCP for it.", "If exist, restart Server2 and Server1.", "Case3.", "More than one regionservers reported opened this region", "====> Fix: restart the regionservers.", "bq.", "But I think this maybe a bit dangerous?", "If the balancer is still running, we may introduce more inconsistencies if we do this automatically...", "I hear you.", "I think no RIT and no balancer when an 'adoption' runs.", "hbase:meta for the table we are importing into should be healthy too  -- no holes and no overlaps so whatever the content of the orphans, there is a place for the adopted content to go.", "Maybe put Master into  'maintenance mode' (disables balancer and lets see, maybe we can quiesce other aspects of Master too.", "[~busbey] reminded me of HBASE-21073 today too...", "Need to take a look at it).", "bq.", "The \"fix\" thing shoule be better done by user manually.", "Yes, hbck2 asks the master to adopt a list of directories/regions.", "And as you/[~zghaobac] says, Master has all state.", "Ping [~stack] for reviewing.", "Pushed to branch-2.0+.", "Thanks [~stack] for reviewing.", "Results for branch branch-2", "[build #2121 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//General_Nightly_Build_Report/]", "(x) {color:red}-1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(/) {color:green}+1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}", "Results for branch master", "[build #1278 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1278/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(x) {color:red}-1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}", "Results for branch branch-2.0", "[build #1799 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(x) {color:red}-1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "Results for branch branch-2.1", "[build #1418 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(x) {color:red}-1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}", "Results for branch branch-2.2", "[build #471 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//General_Nightly_Build_Report/]", "(x) {color:red}-1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(x) {color:red}-1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}"], "SplitGT": [" Found the inconsistent regions.", "Master thought this region opened, but no regionserver reported it.", "Master thought this region opened on Server1, but regionserver reported Server2 case 3.", "More than one regionservers reported opened this region"], "issueString": "Add a chore thread in master to do hbck checking and display results in 'HBCK Report' page\nSee\u00a0HBASE-21965. There may have two type of failed splited/merged regions. One is\u00a0orphan region on filesystem. Another one is\u00a0unassigned daughter regions or merged regions. Plan to add a seperate hbck.jsp for them. Will add more hbck problems to this UI in future.\nA basic patch. Refactor HBaseFsck and add a new HbckChecker chore in master. Will add ui later.\nLet me move the refactor work to a new issue.\n| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |\r\n| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  7s{color} | {color:red} HBASE-22709 does not apply to master. Rebase required? Wrong Branch? See https://yetus.apache.org/documentation/in-progress/precommit-patchnames for help. {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| JIRA Issue | HBASE-22709 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12975392/HBASE-22709.master.001.patch |\r\n| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/664/console |\r\n| Powered by | Apache Yetus 0.9.0 http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n\nLooks good. It runs once an hour? Can operator ask to run it? [~zghaobac]\r\n\r\nYou see note on end of https://issues.apache.org/jira/browse/HBASE-21745?focusedCommentId=16889062&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16889062 ?\r\n\r\nI'm working on a patch to beef up CatalogJanitor so when it scans on its five minute interval, it reports on holes, overlaps, and unknown servers. Was going to have it manufacture a Report. The Report could show on your hbck UI page. Was thinking of adding it to ClusterMetrics.\r\n\r\n\nbq.  It runs once an hour?\r\nYes. It need to scan all hdfs dir. So I thought no need to run it frequently.\r\nbq. Can operator ask to run it?\r\nGood idea.\r\n\r\nbq. I'm working on a patch to beef up CatalogJanitor so when it scans on its five minute interval, it reports on holes, overlaps, and unknown servers. \r\nGreat. The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.\n[~zghaobac]\r\nbq. The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.\r\n\r\nYes. The work here and in HBASE-22723 compliments each other. I can dump the hbase:meta report into whereever the new hbck UI page ends up.\r\n\r\nDid you see suggestion of an 'adoption service' in HBASE-27145? We'd ask the Master to 'adopt' orphans found in HDFS (by your work here for instance).\nbq. Did you see suggestion of an 'adoption service' in HBASE-27145? We'd ask the Master to 'adopt' orphans found in HDFS (by your work here for instance).\r\nI thought we only fix it by HBCK2 now... The master UI shows the orphan regions and user use HBCK2 to fix them. \nI found a bug for \"Problematic Regions\": the rsReports didn't remove the dead server's report. Will fix this in this issue. As I plan to move the \"Problematic Regions\" UI to a new hbck.jsp.\nbq. I thought we only fix it by HBCK2 now... The master UI shows the orphan regions and user use HBCK2 to fix them.\r\n\r\nHBCK2 will ask the Master to adopt 'orphans', yes. This facility will find the orphans. We maybe should make it so HBCK2 can ask the Master for list of orphans (JSON listing?)\nWill this overload master? We have already put lots of functions into master...\nbq. Will this overload master? We have already put lots of functions into master...\r\n\r\nMaster is in charge, the final arbitor. How is it overloaded? You mean running adoption ? It'd be some fs work, mostly moving I'd hope. Will be conscious.\r\n\nbq. Will this overload master? We have already put lots of functions into master...\r\nIt only need to list the hdfs directory. And no need to scan meta. And the rsReports is in-memory.\r\n\r\nFor the region hole or overlap problem, it will be done in CatalogJanitor. It already scan meta now. I thought it is fine to add more checking thing in CatalogJanitor.\nbq. We maybe should make it so HBCK2 can ask the Master for list of orphans (JSON listing?)\r\nThis can be done in UI? a link to get the JSON listing. I thought no need to add a API for this...\nI think stack means that we could just send a 'fix' command to HBCK2 and it will poll the list of orphan regions and then fix them.\r\n\r\nBut I think this maybe a bit dangerous? If the balancer is still running, we may introduce more inconsistencies if we do this automatically...\nbq. But I think this maybe a bit dangerous? If the balancer is still running, we may introduce more inconsistencies if we do this automatically...\r\nYes, the check result is not absolutely right. The \"fix\" thing shoule be better done by user manually.\r\n\r\nAnd for the inconsistent regions, current HBCK2 is enough to fix them.\r\nCase1. Master thought this region opened, but no regionserver reported it.\r\n====>  Fix: Use assign cmd.\r\nCase2. Master thought this region opened on Server1, but regionserver reported Server2.\r\n====> Fix: need to check the server is still exist. If not, schedule SCP for it. If exist, restart Server2 and Server1.\r\nCase3. More than one regionservers reported opened this region\r\n====> Fix: restart the regionservers.\r\n\r\n\nbq. But I think this maybe a bit dangerous? If the balancer is still running, we may introduce more inconsistencies if we do this automatically...\r\n\r\nI hear you. I think no RIT and no balancer when an 'adoption' runs. hbase:meta for the table we are importing into should be healthy too  -- no holes and no overlaps so whatever the content of the orphans, there is a place for the adopted content to go. Maybe put Master into  'maintenance mode' (disables balancer and lets see, maybe we can quiesce other aspects of Master too. [~busbey] reminded me of HBASE-21073 today too... Need to take a look at it).\r\n\r\nbq. The \"fix\" thing shoule be better done by user manually.\r\n\r\nYes, hbck2 asks the master to adopt a list of directories/regions. And as you/[~zghaobac] says, Master has all state.\r\n\r\n\nPing [~stack] for reviewing.\nPushed to branch-2.0+. Thanks [~stack] for reviewing.\nResults for branch branch-2\n\t[build #2121 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//General_Nightly_Build_Report/]\n\n\n\n\n(x) {color:red}-1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(/) {color:green}+1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2121//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\nResults for branch master\n\t[build #1278 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1278/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//General_Nightly_Build_Report/]\n\n\n\n\n(/) {color:green}+1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(x) {color:red}-1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1278//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\nResults for branch branch-2.0\n\t[build #1799 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//General_Nightly_Build_Report/]\n\n\n\n\n(/) {color:green}+1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(x) {color:red}-1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.0/1799//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\nResults for branch branch-2.1\n\t[build #1418 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//General_Nightly_Build_Report/]\n\n\n\n\n(/) {color:green}+1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(x) {color:red}-1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1418//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\nResults for branch branch-2.2\n\t[build #471 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//General_Nightly_Build_Report/]\n\n\n\n\n(x) {color:red}-1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(x) {color:red}-1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/471//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\n", "issueSearchSentences": ["The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.", "The patch here only handle the orphan regions on RegionServer or on HDFS and the inconsistent regions.", "Master thought this region opened on Server1, but regionserver reported Server2.", "And the rsReports is in-memory.", "====> Fix: restart the regionservers."], "issueSearchScores": [0.4134151339530945, 0.4134151339530945, 0.4058554768562317, 0.373442679643631, 0.3724684715270996]}
{"aId": 37, "code": "public boolean isSet(_Fields field) {\n    if (field == null) {\n      throw new IllegalArgumentException();\n    }\n\n    switch (field) {\n    case TABLE:\n      return isSetTable();\n    case ROW:\n      return isSetRow();\n    case COLUMNS:\n      return isSetColumns();\n    case VALUES:\n      return isSetValues();\n    }\n    throw new IllegalStateException();\n  }", "comment": " Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise", "issueId": "HBASE-10960", "issueStringList": ["Enhance HBase Thrift 1 to include \"append\" and \"checkAndPut\" operations", "Both append, and checkAndPut functionalities are available in Thrift 2 interface, but not in Thrift.", "So, adding the support for these functionalities in Thrift1 too.", "Attaching the patch for adding the functionalities and correpsonding unit tests.", "Can you post on review board?", "Added the patch to the review board (https://reviews.apache.org/r/20361/).", "[~lhofhansl], [~saint.ack@gmail.com],[~apurtell] - the current patch is is close -- you all want this in 0.94/0.96/0.98?", "Unless there is a specific ask from somebody to have this 0.94 and this 100% backwards compatible (which it looks like it is), I'd say we should pass on this for 0.94.", "Pass, unless someone is asking for it for 0.96.", "Attaching +1'ed patch for hadoopqa to test.", "Since this only affects thrift, I ran TestThriftServer with the patch applied and it passed.", "Thanks for the patch Srikanth, committing to trunk.", "I think this commit broke the trunk build", "{noformat}", "[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hbase-thrift: Compilation failure: Compilation failure:", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:[81,47] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: package org.apache.hadoop.hbase.thrift.generated", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[629,30] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: interface Iface", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:[1493,30] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class HBaseHandler", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java:[40,47] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: package org.apache.hadoop.hbase.thrift.generated", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java:[215,40] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class ThriftUtilities", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[741,23] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: interface AsyncIface", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3666,23] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class AsyncClient", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3674,14] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_call", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3675,25] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_call", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[1951,30] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class Client", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[1957,28] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class Client", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53476,11] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53553,6] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53580,11] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53587,33] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53544,98] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53564,26] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53613,21] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_args", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53765,36] error: cannot find symbol", "[ERROR] symbol:   class TAppend", "[ERROR] location: class append_argsStandardScheme", "[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53824,30] error: cannot find symbol", "[ERROR] -> [Help 1]", "{noformat}", "[~jmhsieh]", "The attached patch has changes for introducing new file TAppend.java, but unfortunately this got missed in trunk https://github.com/apache/hbase/commit/ac9928f53c1552696ad9995b25de811475712717 and hence the build broke.", "Can you please take a look into it?", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12642010/hbase-10960.v3.patch", "against trunk revision .", "ATTACHMENT ID: 12642010", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 4 new or modified tests.", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 8 warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 lineLengths{color}.", "The patch introduces the following lines longer than 100:", "+      append_call method_call = new append_call(append, resultHandler, this, ___protocolFactory, ___transport);", "+      checkAndPut_call method_call = new checkAndPut_call(tableName, row, column, value, mput, attributes, resultHandler, this, ___protocolFactory, ___transport);", "+          result.success = iface.checkAndPut(args.tableName, args.row, args.column, args.value, args.mput, args.attributes);", "+    private static final Map<Class<?", "extends IScheme>, SchemeFactory> schemes = new HashMap<Class<?", "extends IScheme>, SchemeFactory>();", "+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them.", "*/", "+        if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!", "\");", "+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */", "+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {", "+        // check for required fields of primitive type, which can't be checked in the validate method", "+    private static final Map<Class<?", "extends IScheme>, SchemeFactory> schemes = new HashMap<Class<?", "extends IScheme>, SchemeFactory>();", "{color:green}+1 site{color}.", "The mvn site goal succeeds with this patch.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//console", "This message is automatically generated.", "Committed missing file, verified compilation.", "Thanks Srikanth.", "Sorry about that.", "Thanks for the fix.", "FAILURE: Integrated in HBase-TRUNK #5118 (See [https://builds.apache.org/job/HBase-TRUNK/5118/])", "HBASE-10960 Enhance HBase Thrift 1 to include \"append\" and \"checkAndPut\" operations (Srikanth Srungarapu) (jmhsieh: rev 1590152)", "hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java", "hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java", "hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java", "hbase/trunk/hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift", "hbase/trunk/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java"], "SplitGT": [" Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise"], "issueString": "Enhance HBase Thrift 1 to include \"append\" and \"checkAndPut\" operations\nBoth append, and checkAndPut functionalities are available in Thrift 2 interface, but not in Thrift. So, adding the support for these functionalities in Thrift1 too.\nAttaching the patch for adding the functionalities and correpsonding unit tests.\nCan you post on review board?\nAdded the patch to the review board (https://reviews.apache.org/r/20361/).\n[~lhofhansl], [~saint.ack@gmail.com],[~apurtell] - the current patch is is close -- you all want this in 0.94/0.96/0.98?\nUnless there is a specific ask from somebody to have this 0.94 and this 100% backwards compatible (which it looks like it is), I'd say we should pass on this for 0.94.\n\nPass, unless someone is asking for it for 0.96.\nAttaching +1'ed patch for hadoopqa to test.\nSince this only affects thrift, I ran TestThriftServer with the patch applied and it passed.  \n\nThanks for the patch Srikanth, committing to trunk.\nI think this commit broke the trunk build\n\n{noformat}\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hbase-thrift: Compilation failure: Compilation failure:\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:[81,47] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: package org.apache.hadoop.hbase.thrift.generated\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[629,30] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: interface Iface\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java:[1493,30] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class HBaseHandler\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java:[40,47] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: package org.apache.hadoop.hbase.thrift.generated\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java:[215,40] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class ThriftUtilities\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[741,23] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: interface AsyncIface\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3666,23] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class AsyncClient\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3674,14] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_call\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[3675,25] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_call\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[1951,30] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class Client\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[1957,28] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class Client\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53476,11] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53553,6] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53580,11] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53587,33] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53544,98] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53564,26] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53613,21] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_args\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53765,36] error: cannot find symbol\n[ERROR] symbol:   class TAppend\n[ERROR] location: class append_argsStandardScheme\n[ERROR] /usr/src/Hadoop/hbase/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:[53824,30] error: cannot find symbol\n[ERROR] -> [Help 1]\n\n{noformat}\n[~jmhsieh]\nThe attached patch has changes for introducing new file TAppend.java, but unfortunately this got missed in trunk https://github.com/apache/hbase/commit/ac9928f53c1552696ad9995b25de811475712717 and hence the build broke. Can you please take a look into it?\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12642010/hbase-10960.v3.patch\n  against trunk revision .\n  ATTACHMENT ID: 12642010\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified tests.\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 8 warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 lineLengths{color}.  The patch introduces the following lines longer than 100:\n    +      append_call method_call = new append_call(append, resultHandler, this, ___protocolFactory, ___transport);\n+      checkAndPut_call method_call = new checkAndPut_call(tableName, row, column, value, mput, attributes, resultHandler, this, ___protocolFactory, ___transport);\n+          result.success = iface.checkAndPut(args.tableName, args.row, args.column, args.value, args.mput, args.attributes);\n+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */\n+        if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");\n+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */\n+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {\n+        // check for required fields of primitive type, which can't be checked in the validate method\n+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n\n  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/9402//console\n\nThis message is automatically generated.\nCommitted missing file, verified compilation. Thanks Srikanth. \nSorry about that.  Thanks for the fix.\n\n\nFAILURE: Integrated in HBase-TRUNK #5118 (See [https://builds.apache.org/job/HBase-TRUNK/5118/])\nHBASE-10960 Enhance HBase Thrift 1 to include \"append\" and \"checkAndPut\" operations (Srikanth Srungarapu) (jmhsieh: rev 1590152)\n* /hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java\n* /hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java\n* /hbase/trunk/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java\n* /hbase/trunk/hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift\n* /hbase/trunk/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java\n\n", "issueSearchSentences": ["+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */", "+        if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!", "+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them.", "+          result.success = iface.checkAndPut(args.tableName, args.row, args.column, args.value, args.mput, args.attributes);", "+        // check for required fields of primitive type, which can't be checked in the validate method"], "issueSearchScores": [0.5143749117851257, 0.46930640935897827, 0.40406110882759094, 0.3719812333583832, 0.31761929392814636]}
{"aId": 38, "code": "public long incrementColumnValue(final byte [] row, final byte [] family, \n      final byte [] qualifier, final long amount)\n  throws IOException {\n    return incrementColumnValue(row, family, qualifier, amount, true);\n  }", "comment": " Atomically increments a column value. If the column value already exists and is not a big-endian long, this could throw an exception.", "issueId": "HBASE-1563", "issueStringList": ["incrementColumnValue does not write to WAL", "Incrementing never writes to the WAL.", "Under failure scenarios, you will lose all increments since the last flush.", "Do we want to expose the option to the client as to whether to write to WAL or not?", "This patch adds a writeToWAL argument and appends to the edit log on every increment, however the boolean is not exposed to the client in this patch and is default true.", "Wanted to discuss with others before going further.", "Thoughts?", "IMHO, it's fine to expose the boolean to the client but there should be a client side API alternative which does not expose the boolean which internally sets writeToWAL as true.", "Everybody will be happy.", "Let's just do it.", "+1 patch coming...", "Please review.", "Why not have old incrementColumnValue call the new one with a flag set to 'true'?", "Code is fine.", "This portion of the javadoc:", "bq.", "If the column value isn't long-like, this could throw an exception.", "I understand what you are saying, but I think this may just confuse users.", "Can you consider an alternate wording?", "2 things:", "in the client code you build a 'get' that isnt used.", "we are writing to the WAL _after_ the increment, this may be the only way to do it effectively, but we should also think about this carefully.", "@Ryan I think the Get is just some old leftover when it was used to be sent over that way and should now be removed.", "Removes unnecessary Get in client code", "Removed duplicate server calls in the client code, default just calls full w/ true", "Replaced increment javadoc with \"Atomically increments a column value.", "If the column value already exists and is not a big-endian long, this could throw an exception.\"", "Is that more clear?", "@Ryan I got most the way through splitting up Store.incrementColumnValue logic so we figure out what we have to do first, then write to wal, then perform the insert.", "The issue is when we increment in-place in the memcache.", "If we truly don't want to write until the wal append, we'd have to make a copy of the KV, perform the increment on it, and then either increment the original memcache value or swap.", "This more or less negates the in-place increment optimization.", "Thoughts?", "Patch looks good except for the identified weirdness where we write to WAL after the update.", "To do this properly you'd need to row lock and make a copy.", "Is that too onerous a price to pay?", "Ryan notes that old code did row lock and was fast.", "So, we should go ahead and pull the KV from memcache, copy it, increment it, add to WAL, then insert back into memcache?", "It won't be too slow?", "Maybe only do this if write to wal is enabled?", "Why is there a race if a row lock is outstanding Ryan?", "We are already getting a row lock, there is no race condition that I can see.", "I'd really like the increment-in-place over making a new KV each time we increment.", "The downside is if the regionserver dies in the brief window after the update and before the WAL append, we lose a single increment.", "If it dies on either side, then nothing would change.", "The upside is we don't allocate a new KV each time.", "That's what I'm after with increments, doing them in place and not doing a new insertion each time (and new allocation).", "I have an internal ID assignment system that uses custom patches on 0.19.", "Was hoping to drop our custom patches and use this out of the box, but if we copy the KV each time I'll likely patch it to not do that :)  I'd rather not lose any increments but the window is very small and you lose at most a single increment.", "If a KV is 100 bytes, and I increment just 1M times in a day, i've unnecessarily allocated 100MB (and the worst kind, lots of small 100 byte allocations that stick around for a bit).", "We still satisfy the property of \"Once the increment has returned successfully, it is safe\".", "This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).", "bq.", "We still satisfy the property of \"Once the increment has returned successfully, it is safe\".", "This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).", "This makes sense to me.", "a row lock is only for writing.", "The race happens when someone is reading the value you are incrementing.", "They could copy a partially updated view into the RPC buffer, and the user could see something odd and unexpected (values going down for example).", "If we used a ConcurrentSkipListSet, we can't put a duplicate, which requires us to temporarily remove the KeyValue, then put it back, opens a hole whereby we can look for a value and it not be there.", "So to support not-in-place modification we need to also move to a ConcurrentSkipListMap in memcache.", "me man, i love OSS", "Someone please review 1577 -- it converts memcache to CSLM from CSLS.", "So, interesting, need to operate on copy so readers don't see partially changed KV.", "That makes sense.", "For now, let's just make the copy and make sure things are safe.", "In addition to 1577.", "Good stuff ryan.", "Stack, you've brought your improper usage of /me onto the web now!", "Will post patch tomorrow", "New patch that copies the existing KV instead of incrementing in-place.", "This is now broken.", "The CSLMap has the same behavior as the CSLSet.", "Since the keys are considered \"equal\" the values might be updated in the map (in this case meaningless) but they don't swap the key with the new one (since it's equal).", "It does not have the behavior we wanted, an atomic put that replaces the Key in case of collisions.", "Can we please ensure that the \"atomic\" puts are honored?", "I migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.", "@Irfan First and foremost, we need HBASE-1577, but that is looking good.", "Beyond that, we need to think more about how to make this behave atomically.", "In thinking about it now, I do see some potential issues/race conditions that will lead to missing increments (with HBASE-1577 things will always _work_ but does not ensure atomicity of the increments).", "Need to think more.", "@Irfan  Excuse me... Got confused for a second.", "Once HBASE-1577 goes in, these absolutely *will* have atomic behavior.", "Each is guaranteed to have worked and performed the increment once it returns, and all readers will always see the latest value.", "Thanks for the patch Jon"], "SplitGT": [" Atomically increments a column value.", "If the column value already exists and is not a big-endian long, this could throw an exception."], "issueString": "incrementColumnValue does not write to WAL\nIncrementing never writes to the WAL.  Under failure scenarios, you will lose all increments since the last flush.\n\nDo we want to expose the option to the client as to whether to write to WAL or not?\nThis patch adds a writeToWAL argument and appends to the edit log on every increment, however the boolean is not exposed to the client in this patch and is default true. Wanted to discuss with others before going further.\n\nThoughts?\nIMHO, it's fine to expose the boolean to the client but there should be a client side API alternative which does not expose the boolean which internally sets writeToWAL as true. Everybody will be happy. Let's just do it. \n+1 patch coming...\nPlease review.\nWhy not have old incrementColumnValue call the new one with a flag set to 'true'?\nCode is fine.\n\nThis portion of the javadoc:\n\nbq. If the column value isn't long-like, this could throw an exception.\n\nI understand what you are saying, but I think this may just confuse users. Can you consider an alternate wording? \n2 things:\n- in the client code you build a 'get' that isnt used.\n- we are writing to the WAL _after_ the increment, this may be the only way to do it effectively, but we should also think about this carefully.\n@Ryan I think the Get is just some old leftover when it was used to be sent over that way and should now be removed.\n- Removes unnecessary Get in client code\n- Removed duplicate server calls in the client code, default just calls full w/ true\n- Replaced increment javadoc with \"Atomically increments a column value. If the column value already exists and is not a big-endian long, this could throw an exception.\"  Is that more clear?\n\n@Ryan I got most the way through splitting up Store.incrementColumnValue logic so we figure out what we have to do first, then write to wal, then perform the insert.  The issue is when we increment in-place in the memcache.  If we truly don't want to write until the wal append, we'd have to make a copy of the KV, perform the increment on it, and then either increment the original memcache value or swap.  This more or less negates the in-place increment optimization.  Thoughts?\nPatch looks good except for the identified weirdness where we write to WAL after the update.  To do this properly you'd need to row lock and make a copy.  Is that too onerous a price to pay?\nRyan notes that old code did row lock and was fast.\n\nSo, we should go ahead and pull the KV from memcache, copy it, increment it, add to WAL, then insert back into memcache?  It won't be too slow?  Maybe only do this if write to wal is enabled?\nWhy is there a race if a row lock is outstanding Ryan?\nWe are already getting a row lock, there is no race condition that I can see.\n\nI'd really like the increment-in-place over making a new KV each time we increment.  The downside is if the regionserver dies in the brief window after the update and before the WAL append, we lose a single increment.  If it dies on either side, then nothing would change.\n\nThe upside is we don't allocate a new KV each time.  That's what I'm after with increments, doing them in place and not doing a new insertion each time (and new allocation).  I have an internal ID assignment system that uses custom patches on 0.19.  Was hoping to drop our custom patches and use this out of the box, but if we copy the KV each time I'll likely patch it to not do that :)  I'd rather not lose any increments but the window is very small and you lose at most a single increment.  If a KV is 100 bytes, and I increment just 1M times in a day, i've unnecessarily allocated 100MB (and the worst kind, lots of small 100 byte allocations that stick around for a bit).\n\nWe still satisfy the property of \"Once the increment has returned successfully, it is safe\".  This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).\nbq. We still satisfy the property of \"Once the increment has returned successfully, it is safe\". This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).\n\nThis makes sense to me. \na row lock is only for writing.  The race happens when someone is reading the value you are incrementing.  They could copy a partially updated view into the RPC buffer, and the user could see something odd and unexpected (values going down for example).\n\nIf we used a ConcurrentSkipListSet, we can't put a duplicate, which requires us to temporarily remove the KeyValue, then put it back, opens a hole whereby we can look for a value and it not be there.  \n\nSo to support not-in-place modification we need to also move to a ConcurrentSkipListMap in memcache.\n/me man, i love OSS\n\nSomeone please review 1577 -- it converts memcache to CSLM from CSLS.\nSo, interesting, need to operate on copy so readers don't see partially changed KV.  That makes sense.\nFor now, let's just make the copy and make sure things are safe.  In addition to 1577.  Good stuff ryan.\n\nStack, you've brought your improper usage of /me onto the web now!\nWill post patch tomorrow\nNew patch that copies the existing KV instead of incrementing in-place.\n\nThis is now broken.  The CSLMap has the same behavior as the CSLSet.  Since the keys are considered \"equal\" the values might be updated in the map (in this case meaningless) but they don't swap the key with the new one (since it's equal).  It does not have the behavior we wanted, an atomic put that replaces the Key in case of collisions.\nCan we please ensure that the \"atomic\" puts are honored? \n\nI migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.\n@Irfan First and foremost, we need HBASE-1577, but that is looking good.  Beyond that, we need to think more about how to make this behave atomically.  In thinking about it now, I do see some potential issues/race conditions that will lead to missing increments (with HBASE-1577 things will always _work_ but does not ensure atomicity of the increments).  Need to think more.\n@Irfan  Excuse me... Got confused for a second.  Once HBASE-1577 goes in, these absolutely *will* have atomic behavior.  Each is guaranteed to have worked and performed the increment once it returns, and all readers will always see the latest value.\nThanks for the patch Jon\n", "issueSearchSentences": ["I migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.", "Replaced increment javadoc with \"Atomically increments a column value.", "Why not have old incrementColumnValue call the new one with a flag set to 'true'?", "incrementColumnValue does not write to WAL", "If the column value already exists and is not a big-endian long, this could throw an exception.\""], "issueSearchScores": [0.6766738295555115, 0.6211980581283569, 0.6128181219100952, 0.5829896330833435, 0.5489481687545776]}
{"aId": 39, "code": "public void convertToKeyOnly() {\n    // KV format:  <keylen/4><valuelen/4><key/keylen><value/valuelen>\n    // Rebuild as: <keylen/4><0/4><key/keylen>\n    byte [] newBuffer = new byte[getKeyLength() + (2 * Bytes.SIZEOF_INT)];\n    System.arraycopy(this.bytes, this.offset, newBuffer, 0, newBuffer.length);\n    Bytes.putInt(newBuffer, Bytes.SIZEOF_INT, 0);\n    this.bytes = newBuffer;\n    this.offset = 0;\n    this.length = newBuffer.length;\n  }", "comment": " Converts this KeyValue to only contain the key portion (the value is changed to be null). This method does a full copy of the backing byte array and does not modify the original byte array of this KeyValue. This method is used by KeyOnlyFilter and is an advanced feature ofKeyValue, proceed with caution.", "issueId": "HBASE-3211", "issueStringList": ["Key (Index) Only Fetches", "When you retrieve data from HBase you get Key (Row+Column+Timestamp) + Values.", "It would be nice to have a mode where we only fetch the keys (i.e.", "the index) but not the values.", "Cool.", "I think we should be able to do this somewhat trivially with a filter.", "Will take a look.", "we could fake this out by creating new KeyValue entries in the read path that skip the value part, and you'd end up with a bunch of Results that have Keys but the value length = 0.", "That sounds great.", "Assigning to Jonathan since he was planning to help on this.", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/", "Review request for hbase, stack and Kannan Muthukkaruppan.", "Summary", "Adds a new filter, KeyOnlyFilter.", "The idea is that this will make it so only the key portion of all the KVs are returned.", "Could imagine a few use cases where you just need the keys/index not the values.", "We have one where we have giant rows with big values and want to just get the qualifiers/versions w/o values.", "Adds a new method in KeyValue, convertToKeyOnly().", "From javadoc:", "Converts this KeyValue to only contain the key portion (the value is", "changed to be null).", "This method does a full copy of the backing byte", "array and does not modify the original byte array of this KeyValue.", "<p>", "This method is used by {@link KeyOnlyFilter} and is an advanced feature of", "KeyValue, proceed with caution.", "This addresses bug HBASE-3211.", "http://issues.apache.org/jira/browse/HBASE-3211", "Diffs", "trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java 1033617", "trunk/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java PRE-CREATION", "trunk/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java 1033617", "trunk/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java 1033617", "trunk/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java 1033617", "Diff: http://review.cloudera.org/r/1208/diff", "Testing", "Test of the KV method added to TestKeyValue.", "Test of KeyOnlyFilter added to TestFilter.", "Both passing.", "Thanks,", "Jonathan", "Message from: stack@duboce.net", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1890", "Ship it!", "Looks fine to me.", "That kv copy is ugly but what else can you do?", "stack", "Message from: \"Ryan Rawson\" <ryanobjc@gmail.com>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1891", "trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java", "<http://review.cloudera.org/r/1208/#comment6117>", "I'm torn here, this is making the implementation easy, but KeyValues have been immutable to date.", "While no one shares KeyValues between threads or scanners, and ideally no one should, this seems dangerous.", "Ryan", "Message from: \"Kannan Muthukkaruppan\" <kannan@facebook.com>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1893", "Ship it!", "Neat-O!", "Thanks for cranking this out so quickly.", "Kannan", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "bq.", "On 2010-11-10 15:24:52, stack wrote:", "bq.", "> Looks fine to me.", "That kv copy is ugly but what else can you do?", "Definitely can't modify the original buffer, so it's the only choice.", "In this case, it's not a huge deal because we'll do these allocations, return the result, and then immediately be done with the memory and will have no references to it.", "Should be okay on GC.", "One potential optimization would be to do one big rewrite of the KVs at the end rather as we go.", "Instead of allocating individual byte[] for each KV, you could potentially do one big byte[] behind all the key-only KVs.", "This gets way more complicated and I'm not sure it's worth it.", "Was going for minimal approach.", "In the filter unit test, I'm also going add an additional assert on commit (and verifying still passes).", "The test verifies the values are not the same but we should actually explicitly also assert that the value is 0 length.", "Thanks!", "Jonathan", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1890", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "bq.", "On 2010-11-10 15:25:07, Ryan Rawson wrote:", "bq.", "> trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199", "bq.", "> <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>", "bq.", ">", "bq.", ">     I'm torn here, this is making the implementation easy, but KeyValues have been immutable to date.", "While no one shares KeyValues between threads or scanners, and ideally no one should, this seems dangerous.", "bq.", ">", "bq.", ">", "It doesn't actually touch the original byte[] so does not actually destroy/mutate the underlying data in any way.", "Agreed it's still potentially \"dangerous\" but that's why I've added the nice warning message in javadoc :)", "Jonathan", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1891", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "bq.", "On 2010-11-10 15:30:44, Kannan Muthukkaruppan wrote:", "bq.", "> Neat-O!", "Thanks for cranking this out so quickly.", "Thanks for your help to keep this a super simple change!", "Jonathan", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1893", "Final patch for commit.", "Same as one reviewed on RB, just adds an extra assert in the unit test.", "Still passing.", "Committed to trunk.", "Thanks for reviews everyone.", "Thanks Kannan for helping.", "Actual final patch.", "Forgot to add the new file on the last one.", "Message from: \"Nicolas\" <nspiegelberg@facebook.com>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1896", "trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java", "<http://review.cloudera.org/r/1208/#comment6119>", "Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.", "Maybe this is too much of a one-off case...", "Nicolas", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "bq.", "On 2010-11-10 16:01:22, Nicolas wrote:", "bq.", "> trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199", "bq.", "> <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>", "bq.", ">", "bq.", ">     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.", "Maybe this is too much of a one-off case...", "Not sure I completely follow.", "You're saying the modification would happen outside the filter?", "No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.", "In any case, yeah, I would not be for adding another ReturnCode just for this.", "Jonathan", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1896", "Message from: \"Nicolas\" <nspiegelberg@facebook.com>", "bq.", "On 2010-11-10 16:01:22, Nicolas wrote:", "bq.", "> trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199", "bq.", "> <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>", "bq.", ">", "bq.", ">     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.", "Maybe this is too much of a one-off case...", "bq.", "bq.", "Jonathan Gray wrote:", "bq.", "Not sure I completely follow.", "You're saying the modification would happen outside the filter?", "No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.", "bq.", "bq.", "In any case, yeah, I would not be for adding another ReturnCode just for this.", "I suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.", "If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.", "Nicolas", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1896", "Message from: \"Jonathan Gray\" <jgray@apache.org>", "bq.", "On 2010-11-10 16:01:22, Nicolas wrote:", "bq.", "> trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199", "bq.", "> <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>", "bq.", ">", "bq.", ">     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.", "Maybe this is too much of a one-off case...", "bq.", "bq.", "Jonathan Gray wrote:", "bq.", "Not sure I completely follow.", "You're saying the modification would happen outside the filter?", "No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.", "bq.", "bq.", "In any case, yeah, I would not be for adding another ReturnCode just for this.", "bq.", "bq.", "Nicolas wrote:", "bq.", "I suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.", "If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.", "But a user would have to knowingly use this filter, right?", "And the filter only has one purpose of mutating the KVs.", "I do agree with what you're saying at some level but not sure what a note in the interface would do.", "This is so if writing other filters, you would know that other filters in the chain could modify the KV?", "How would you behave differently then in that case?", "Jonathan", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1896", "Message from: \"Nicolas\" <nspiegelberg@facebook.com>", "bq.", "On 2010-11-10 16:01:22, Nicolas wrote:", "bq.", "> trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199", "bq.", "> <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>", "bq.", ">", "bq.", ">     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.", "Maybe this is too much of a one-off case...", "bq.", "bq.", "Jonathan Gray wrote:", "bq.", "Not sure I completely follow.", "You're saying the modification would happen outside the filter?", "No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.", "bq.", "bq.", "In any case, yeah, I would not be for adding another ReturnCode just for this.", "bq.", "bq.", "Nicolas wrote:", "bq.", "I suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.", "If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.", "bq.", "bq.", "Jonathan Gray wrote:", "bq.", "But a user would have to knowingly use this filter, right?", "And the filter only has one purpose of mutating the KVs.", "I do agree with what you're saying at some level but not sure what a note in the interface would do.", "This is so if writing other filters, you would know that other filters in the chain could modify the KV?", "How would you behave differently then in that case?", "I guess the overall worry just comes in because this is a map() operation and not a filter().", "Granted, the user should reasonably know what they're doing.", "One theoretical future scenario: someone wants to get all Keys for large objects that match some Value filter.", "In that case, you have an order of operations problem between the two filters.", "Maybe that's material for future refactoring when we hit that case.", "Just cautioning that the filter/map 2-phase paradigm is a common problem that's been solved before and we're avoiding that in favor of a lightweight solution.", "Ignorance is bliss as long as we understanding what we're ignoring and why.", "Nicolas", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1896", "Message from: \"Ryan Rawson\" <ryanobjc@gmail.com>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1208/#review1903", "trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java", "<http://review.cloudera.org/r/1208/#comment6125>", "my concern is far more down to earth here, the problem is that KeyValue is an immutable object that _sometimes_ mutates.", "Having this function call:", "public KeyValue getKeyOnly() {", "insert implementation that doesnt modify this here", "}", "would be better.", "Ryan"], "SplitGT": [" Converts this KeyValue to only contain the key portion (the value is changed to be null).", "This method does a full copy of the backing byte array and does not modify the original byte array of this KeyValue.", "This method is used by KeyOnlyFilter and is an advanced feature ofKeyValue, proceed with caution."], "issueString": "Key (Index) Only Fetches\nWhen you retrieve data from HBase you get Key (Row+Column+Timestamp) + Values. \n\nIt would be nice to have a mode where we only fetch the keys (i.e. the index) but not the values.\n\n\n\n\nCool.  I think we should be able to do this somewhat trivially with a filter.  Will take a look.\nwe could fake this out by creating new KeyValue entries in the read path that skip the value part, and you'd end up with a bunch of Results that have Keys but the value length = 0.\nThat sounds great. Assigning to Jonathan since he was planning to help on this.\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/\n-----------------------------------------------------------\n\nReview request for hbase, stack and Kannan Muthukkaruppan.\n\n\nSummary\n-------\n\nAdds a new filter, KeyOnlyFilter.  The idea is that this will make it so only the key portion of all the KVs are returned.  Could imagine a few use cases where you just need the keys/index not the values.  We have one where we have giant rows with big values and want to just get the qualifiers/versions w/o values.\n\nAdds a new method in KeyValue, convertToKeyOnly().  From javadoc:\n\n  /**\n   * Converts this KeyValue to only contain the key portion (the value is\n   * changed to be null).  This method does a full copy of the backing byte\n   * array and does not modify the original byte array of this KeyValue.\n   * <p>\n   * This method is used by {@link KeyOnlyFilter} and is an advanced feature of\n   * KeyValue, proceed with caution.\n   */\n\n\nThis addresses bug HBASE-3211.\n    http://issues.apache.org/jira/browse/HBASE-3211\n\n\nDiffs\n-----\n\n  trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java 1033617 \n  trunk/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java PRE-CREATION \n  trunk/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java 1033617 \n  trunk/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java 1033617 \n  trunk/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java 1033617 \n\nDiff: http://review.cloudera.org/r/1208/diff\n\n\nTesting\n-------\n\nTest of the KV method added to TestKeyValue.  Test of KeyOnlyFilter added to TestFilter.  Both passing.\n\n\nThanks,\n\nJonathan\n\n\n\nMessage from: stack@duboce.net\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1890\n-----------------------------------------------------------\n\nShip it!\n\n\nLooks fine to me.  That kv copy is ugly but what else can you do?\n\n- stack\n\n\n\n\nMessage from: \"Ryan Rawson\" <ryanobjc@gmail.com>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1891\n-----------------------------------------------------------\n\n\n\ntrunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java\n<http://review.cloudera.org/r/1208/#comment6117>\n\n    I'm torn here, this is making the implementation easy, but KeyValues have been immutable to date.  While no one shares KeyValues between threads or scanners, and ideally no one should, this seems dangerous.\n    \n    \n\n\n- Ryan\n\n\n\n\nMessage from: \"Kannan Muthukkaruppan\" <kannan@facebook.com>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1893\n-----------------------------------------------------------\n\nShip it!\n\n\nNeat-O! Thanks for cranking this out so quickly.\n\n- Kannan\n\n\n\n\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n\nbq.  On 2010-11-10 15:24:52, stack wrote:\nbq.  > Looks fine to me.  That kv copy is ugly but what else can you do?\n\nDefinitely can't modify the original buffer, so it's the only choice.\n\nIn this case, it's not a huge deal because we'll do these allocations, return the result, and then immediately be done with the memory and will have no references to it.  Should be okay on GC.\n\nOne potential optimization would be to do one big rewrite of the KVs at the end rather as we go.  Instead of allocating individual byte[] for each KV, you could potentially do one big byte[] behind all the key-only KVs.  This gets way more complicated and I'm not sure it's worth it.  Was going for minimal approach.\n\nIn the filter unit test, I'm also going add an additional assert on commit (and verifying still passes).  The test verifies the values are not the same but we should actually explicitly also assert that the value is 0 length.\n\nThanks!\n\n\n- Jonathan\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1890\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n\nbq.  On 2010-11-10 15:25:07, Ryan Rawson wrote:\nbq.  > trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199\nbq.  > <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>\nbq.  >\nbq.  >     I'm torn here, this is making the implementation easy, but KeyValues have been immutable to date.  While no one shares KeyValues between threads or scanners, and ideally no one should, this seems dangerous.\nbq.  >     \nbq.  >\n\nIt doesn't actually touch the original byte[] so does not actually destroy/mutate the underlying data in any way.  Agreed it's still potentially \"dangerous\" but that's why I've added the nice warning message in javadoc :)\n\n\n- Jonathan\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1891\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n\nbq.  On 2010-11-10 15:30:44, Kannan Muthukkaruppan wrote:\nbq.  > Neat-O! Thanks for cranking this out so quickly.\n\nThanks for your help to keep this a super simple change!\n\n\n- Jonathan\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1893\n-----------------------------------------------------------\n\n\n\n\nFinal patch for commit.  Same as one reviewed on RB, just adds an extra assert in the unit test.  Still passing.\nCommitted to trunk.  Thanks for reviews everyone.  Thanks Kannan for helping.\nActual final patch.  Forgot to add the new file on the last one.\nMessage from: \"Nicolas\" <nspiegelberg@facebook.com>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1896\n-----------------------------------------------------------\n\n\n\ntrunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java\n<http://review.cloudera.org/r/1208/#comment6119>\n\n    Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.  Maybe this is too much of a one-off case...\n\n\n- Nicolas\n\n\n\n\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n\nbq.  On 2010-11-10 16:01:22, Nicolas wrote:\nbq.  > trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199\nbq.  > <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>\nbq.  >\nbq.  >     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.  Maybe this is too much of a one-off case...\n\nNot sure I completely follow.  You're saying the modification would happen outside the filter?  No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.\n\nIn any case, yeah, I would not be for adding another ReturnCode just for this.\n\n\n- Jonathan\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1896\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Nicolas\" <nspiegelberg@facebook.com>\n\n\nbq.  On 2010-11-10 16:01:22, Nicolas wrote:\nbq.  > trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199\nbq.  > <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>\nbq.  >\nbq.  >     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.  Maybe this is too much of a one-off case...\nbq.  \nbq.  Jonathan Gray wrote:\nbq.      Not sure I completely follow.  You're saying the modification would happen outside the filter?  No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.\nbq.      \nbq.      In any case, yeah, I would not be for adding another ReturnCode just for this.\n\nI suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.  If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.\n\n\n- Nicolas\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1896\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Jonathan Gray\" <jgray@apache.org>\n\n\nbq.  On 2010-11-10 16:01:22, Nicolas wrote:\nbq.  > trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199\nbq.  > <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>\nbq.  >\nbq.  >     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.  Maybe this is too much of a one-off case...\nbq.  \nbq.  Jonathan Gray wrote:\nbq.      Not sure I completely follow.  You're saying the modification would happen outside the filter?  No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.\nbq.      \nbq.      In any case, yeah, I would not be for adding another ReturnCode just for this.\nbq.  \nbq.  Nicolas wrote:\nbq.      I suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.  If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.\n\nBut a user would have to knowingly use this filter, right?  And the filter only has one purpose of mutating the KVs.  I do agree with what you're saying at some level but not sure what a note in the interface would do.  This is so if writing other filters, you would know that other filters in the chain could modify the KV?  How would you behave differently then in that case?\n\n\n- Jonathan\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1896\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Nicolas\" <nspiegelberg@facebook.com>\n\n\nbq.  On 2010-11-10 16:01:22, Nicolas wrote:\nbq.  > trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1199\nbq.  > <http://review.cloudera.org/r/1208/diff/1/?file=17147#file17147line1199>\nbq.  >\nbq.  >     Would it be more straightforward to have a ReturnCode.MODIFY, that signals an include but suggests that the program must call modifyKeyValue() to get the transformed data.  Maybe this is too much of a one-off case...\nbq.  \nbq.  Jonathan Gray wrote:\nbq.      Not sure I completely follow.  You're saying the modification would happen outside the filter?  No one needs to call modifyKeyValue() to get the transformed data, it's done in the filter.\nbq.      \nbq.      In any case, yeah, I would not be for adding another ReturnCode just for this.\nbq.  \nbq.  Nicolas wrote:\nbq.      I suggested this alternative because users normally expect filters to do immutable operations on the data itself, and you're introducing side effects.  If we stay with this paradigm, it's probably best to add a note in Filter.filterKeyValue() that the KeyValue may be modified.\nbq.  \nbq.  Jonathan Gray wrote:\nbq.      But a user would have to knowingly use this filter, right?  And the filter only has one purpose of mutating the KVs.  I do agree with what you're saying at some level but not sure what a note in the interface would do.  This is so if writing other filters, you would know that other filters in the chain could modify the KV?  How would you behave differently then in that case?\n\nI guess the overall worry just comes in because this is a map() operation and not a filter().  Granted, the user should reasonably know what they're doing.  One theoretical future scenario: someone wants to get all Keys for large objects that match some Value filter.  In that case, you have an order of operations problem between the two filters.  Maybe that's material for future refactoring when we hit that case.  Just cautioning that the filter/map 2-phase paradigm is a common problem that's been solved before and we're avoiding that in favor of a lightweight solution.  Ignorance is bliss as long as we understanding what we're ignoring and why.\n\n\n- Nicolas\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1896\n-----------------------------------------------------------\n\n\n\n\nMessage from: \"Ryan Rawson\" <ryanobjc@gmail.com>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1208/#review1903\n-----------------------------------------------------------\n\n\n\ntrunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java\n<http://review.cloudera.org/r/1208/#comment6125>\n\n    my concern is far more down to earth here, the problem is that KeyValue is an immutable object that _sometimes_ mutates. Having this function call:\n    \n    public KeyValue getKeyOnly() {\n     // insert implementation that doesnt modify this here\n    }\n    \n    would be better.\n    \n\n\n- Ryan\n\n\n\n\n", "issueSearchSentences": ["Converts this KeyValue to only contain the key portion (the value is", "array and does not modify the original byte array of this KeyValue.", "Instead of allocating individual byte[] for each KV, you could potentially do one big byte[] behind all the key-only KVs.", "I'm torn here, this is making the implementation easy, but KeyValues have been immutable to date.", "The idea is that this will make it so only the key portion of all the KVs are returned."], "issueSearchScores": [0.6320573091506958, 0.6179702877998352, 0.600041389465332, 0.592546820640564, 0.5781213045120239]}
{"aId": 40, "code": "public static String getShortTextFormat(Message m) {\n    if (m == null) return \"null\";\n    if (m instanceof ScanRequest) {\n      // This should be small and safe to output.  No data.\n      return TextFormat.shortDebugString(m);\n    } else if (m instanceof RegionServerReportRequest) {\n      // Print a short message only, just the servername and the requests, not the full load.\n      RegionServerReportRequest r = (RegionServerReportRequest)m;\n      return \"server \" + TextFormat.shortDebugString(r.getServer()) +\n        \" load { numberOfRequests: \" + r.getLoad().getNumberOfRequests() + \" }\";\n    } else if (m instanceof RegionServerStartupRequest) {\n      // Should be small enough.\n      return TextFormat.shortDebugString(m);\n    } else if (m instanceof MutationProto) {\n      return toShortString((MutationProto)m);\n    }\n    return \"TODO: \" + m.getClass().toString();\n  }", "comment": " Return short version of Message toString'd, shorter than TextFormat#shortDebugString.", "issueId": "HBASE-8366", "issueStringList": ["HBaseServer logs the full query.", "We log the query when we have an error.", "As a results, the logs are not readable when using stuff like multi.", "As a side note, this is as well a security issue (no need to encrypt the network and the storage if the logs contain everything).", "I'm not removing the full log line here; but just ask and I do it :-).", "Along with this, it also prints response which sometimes is quite large.", "HBaseServer#setResponse:", "{code}", "if (LOG.isDebugEnabled()) {", "LOG.debug(\"Header \" + TextFormat.shortDebugString(header) +", "\", result \" + (result != null?", "TextFormat.shortDebugString(result): \"null\"));", "}", "{code}", "This bloats the log file, usually unnecessary.", "Reading the TextFormat code doesn't tell me we are really shortening the response size.", "It should rather be a trace level, or let's just print the header and move result to trace level.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12579199/8366.v1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 site{color}.", "The mvn site goal succeeds with this patch.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.TestZooKeeper", "org.apache.hadoop.hbase.master.TestTableLockManager", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//console", "This message is automatically generated.", "After some thinking, I think the best option is to remove both of them (mine + the one mentioned by Himanshu).", "That's the only viable long term option.", "I will do this if nobody objects.", "Sorry for this lads.", "I am going to come back and fix this (thanks for filing the issue).", "TextFormat was useful debugging the ipc but yeah, too verbose.", "On other hand, because we are all pb now, we can log a TextFormat shorthand and print out stuff like region and row which will help when query is tooSlow or tooBig.", "TextFormat is not subclassable so it would be a hbase form of TextFormat.", "I can assign this to myself since I have a notion of how it should be if that is ok w/ you lot.", "I'm fine if you do it :-).", "+1!", "+1.", "Thanks Stack.", "Let me do different than this [~nkeywal]:", "{code}", "+      String param = (this.param != null ?", "TextFormat.shortDebugString(this.param) : \"\");", "+      if (param.length() > 100) {", "+        param = param.substring(100) + \" [...]\";", "+      }", "{code}", "I haven't looked but my guess is that we have already made String that is as large as the Message by this point.", "Let me try and do something that avoids that.", "I'll be back...", "Imho, we should just not log the params at all, or only in trace.", "This would solve the risk of leaking infos.", "Same opinion as Nicolas.", "Display the message on the logs, and the params only in the trace level.", "That way we can still see the messages, and we still have an option to see the parameters if we want to debug that...", "Without any restart.", "Patch that errs on the side of printing nothing rather than print data.", "I went through where we print out pbs and removed or shortened the output.", "Same patch only w/ better name.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12584750/8366v2.txt", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:red}-1 tests included{color}.", "The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color:green}+1 hadoop1.0{color}.", "The patch compiles against the hadoop 1.0 profile.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 site{color}.", "The mvn site goal succeeds with this patch.", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//console", "This message is automatically generated.", "Any chance of a +1?", "+1, thanks for doing the patch.", "Looks good to me.", "> +   * Return short version of Message toString'd, shorter than TextFormat#regionServerStartup.", "Probably you want TextFormat#shortDebugString there.", "> +    return \"TODO: \" + m.getClass().toString();", "I like this rather than a fall back to something that could spew.", "+1 (other than the nit noted by Andrew).", "Thanks.", "Thanks lads.", "Applied the patch (w/ Andrew's fix) to 0.95 and trunk.", "Integrated in HBase-TRUNK #4142 (See [https://builds.apache.org/job/HBase-TRUNK/4142/])", "HBASE-8366 HBaseServer logs the full query (Revision 1486232)", "Result = SUCCESS", "stack :", "Files :", "hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java", "hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClient.java", "hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/QosFunction.java", "Integrated in hbase-0.95 #214 (See [https://builds.apache.org/job/hbase-0.95/214/])", "HBASE-8366 HBaseServer logs the full query (Revision 1486233)", "Result = SUCCESS", "stack :", "Files :", "hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java", "hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClient.java", "hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java", "hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java", "hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/QosFunction.java"], "SplitGT": [" Return short version of Message toString'd, shorter than TextFormat#shortDebugString."], "issueString": "HBaseServer logs the full query.\nWe log the query when we have an error. As a results, the logs are not readable when using stuff like multi.\n\nAs a side note, this is as well a security issue (no need to encrypt the network and the storage if the logs contain everything). I'm not removing the full log line here; but just ask and I do it :-).\nAlong with this, it also prints response which sometimes is quite large. HBaseServer#setResponse:\n{code}\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Header \" + TextFormat.shortDebugString(header) +\n            \", result \" + (result != null? TextFormat.shortDebugString(result): \"null\"));\n        }\n{code}\nThis bloats the log file, usually unnecessary. Reading the TextFormat code doesn't tell me we are really shortening the response size. It should rather be a trace level, or let's just print the header and move result to trace level.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12579199/8366.v1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.TestZooKeeper\n                  org.apache.hadoop.hbase.master.TestTableLockManager\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/5337//console\n\nThis message is automatically generated.\nAfter some thinking, I think the best option is to remove both of them (mine + the one mentioned by Himanshu). That's the only viable long term option. I will do this if nobody objects.\nSorry for this lads.  I am going to come back and fix this (thanks for filing the issue).  TextFormat was useful debugging the ipc but yeah, too verbose.  On other hand, because we are all pb now, we can log a TextFormat shorthand and print out stuff like region and row which will help when query is tooSlow or tooBig.  TextFormat is not subclassable so it would be a hbase form of TextFormat.  I can assign this to myself since I have a notion of how it should be if that is ok w/ you lot.\nI'm fine if you do it :-).\n+1!\n+1.\nThanks Stack.\nLet me do different than this [~nkeywal]:\n\n{code}\n+      String param = (this.param != null ? TextFormat.shortDebugString(this.param) : \"\");\n+      if (param.length() > 100) {\n+        param = param.substring(100) + \" [...]\";\n+      }\n{code}\n\nI haven't looked but my guess is that we have already made String that is as large as the Message by this point.  Let me try and do something that avoids that.  I'll be back...\nImho, we should just not log the params at all, or only in trace. This would solve the risk of leaking infos.\nSame opinion as Nicolas. Display the message on the logs, and the params only in the trace level. That way we can still see the messages, and we still have an option to see the parameters if we want to debug that... Without any restart.\nPatch that errs on the side of printing nothing rather than print data.\n\nI went through where we print out pbs and removed or shortened the output.\nSame patch only w/ better name.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12584750/8366v2.txt\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/5823//console\n\nThis message is automatically generated.\nAny chance of a +1?\n+1, thanks for doing the patch.\nLooks good to me.\n\n> +   * Return short version of Message toString'd, shorter than TextFormat#regionServerStartup.\n\nProbably you want TextFormat#shortDebugString there.\n\n> +    return \"TODO: \" + m.getClass().toString();\n\nI like this rather than a fall back to something that could spew.\n\n\n+1 (other than the nit noted by Andrew). Thanks. \nThanks lads.  Applied the patch (w/ Andrew's fix) to 0.95 and trunk.\nIntegrated in HBase-TRUNK #4142 (See [https://builds.apache.org/job/HBase-TRUNK/4142/])\n    HBASE-8366 HBaseServer logs the full query (Revision 1486232)\n\n     Result = SUCCESS\nstack : \nFiles : \n* /hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java\n* /hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClient.java\n* /hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/QosFunction.java\n\nIntegrated in hbase-0.95 #214 (See [https://builds.apache.org/job/hbase-0.95/214/])\n    HBASE-8366 HBaseServer logs the full query (Revision 1486233)\n\n     Result = SUCCESS\nstack : \nFiles : \n* /hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java\n* /hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClient.java\n* /hbase/branches/0.95/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java\n* /hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java\n* /hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/QosFunction.java\n\n", "issueSearchSentences": ["> +   * Return short version of Message toString'd, shorter than TextFormat#regionServerStartup.", "TextFormat.shortDebugString(this.param) : \"\");", "Probably you want TextFormat#shortDebugString there.", "TextFormat.shortDebugString(result): \"null\"));", "Reading the TextFormat code doesn't tell me we are really shortening the response size."], "issueSearchScores": [0.6323418021202087, 0.586336612701416, 0.5783796310424805, 0.5504752397537231, 0.5381210446357727]}
{"aId": 41, "code": "public static Properties makeZKProps(HBaseConfiguration conf) {\n    // First check if there is a zoo.cfg in the CLASSPATH. If so, simply read\n    // it and grab its configuration properties.\n    ClassLoader cl = HQuorumPeer.class.getClassLoader();\n    InputStream inputStream = cl.getResourceAsStream(ZOOKEEPER_CONFIG_NAME);\n    if (inputStream != null) {\n      try {\n        return parseZooCfg(conf, inputStream);\n      } catch (IOException e) {\n        LOG.warn(\"Cannot read \" + ZOOKEEPER_CONFIG_NAME +\n                 \", loading from XML files\", e);\n      }\n    }\n\n    // Otherwise, use the configuration options from HBase's XML files.\n    Properties zkProperties = new Properties();\n\n    // Directly map all of the hbase.zookeeper.property.KEY properties.\n    for (Entry<String, String> entry : conf) {\n      String key = entry.getKey();\n      if (key.startsWith(ZK_CFG_PROPERTY)) {\n        String zkKey = key.substring(ZK_CFG_PROPERTY_SIZE);\n        String value = entry.getValue();\n        // If the value has variables substitutions, need to do a get.\n        if (value.contains(VARIABLE_START)) {\n          value = conf.get(key);\n        }\n        zkProperties.put(zkKey, value);\n      }\n    }\n\n    // Create the server.X properties.\n    int peerPort = conf.getInt(\"hbase.zookeeper.peerport\", 2888);\n    int leaderPort = conf.getInt(\"hbase.zookeeper.leaderport\", 3888);\n\n    String[] serverHosts = conf.getStrings(ZOOKEEPER_QUORUM, \"localhost\");\n    for (int i = 0; i < serverHosts.length; ++i) {\n      String serverHost = serverHosts[i];\n      String address = serverHost + \":\" + peerPort + \":\" + leaderPort;\n      String key = \"server.\" + i;\n      zkProperties.put(key, address);\n    }\n\n    return zkProperties;\n  }", "comment": " If there is a zoo.cfg in the classpath, simply read it in.", "issueId": "HBASE-1606", "issueStringList": ["Remove zoo.cfg, put config options into hbase-site.xml", "From HBASE-1551:", "Here's my current thinking:", "Move all of the ZooKeeper config paraments into hbase-*.xml using zookeeper.property.KEY = VALUE.", "Add a special property for the list of quorum servers, say zookeeper.quorum.", "This option can default to \"localhost\".", "If there is a zoo.cfg present in the classpath, use its data above the zookeeper.property.KEY options.", "When we need to instantiate something to talk to ZooKeeper, we simply create a new HBaseConfiguration and call some method on it e.g.", "toZooKeeperProperties().", "This method will iterate through the zookeeper.property.KEY and turn each into the appropriate ZooKeeper configurations (i.e.", "KEY=VALUE).", "It will generate", "the server.X property from the zookeeper.quorum configuration option.", "As mentioned above, if there is a zoo.cfg in the classpath, overwrite the data with its configuration.", "This will return a Properties object that can be used to construct the appropriate ZooKeeper config and start/talk to their servers.", "For start/stop management of full ZK quorum cluster, use something like my ZKServerTool in the patch (modified of course) to do the parsing mentioned above and turn it", "into a simple line-by-line list of quorum servers.", "As I do in this patch, the bin/zookeepers.sh can then simply call bin/hbase o.a.h.h.z.ZKServerTool to get the list of hosts.", "If you want something like a conf/zookeepers you can simply run ZKServerTool yourself.", "The benefits from all this are:", "One place for all ZK configuration.", "No duplicate setting of parameters.", "No more nasty zoo.cfg.", "Give the user what they're already used to, a single XML config file.", "New user only need edit zookeeper.quorum to get full cluster.", "Programmable control of what ZK one is talking to.", "And some of Stack's comments:", "@nitay, I think you need to keep the hbase zookeeper config inside of an hbase namespace.", "The Hadoop Configuation system is a floozy.", "It will go with anyone who calls load resource on it pulling in their properties.", "I could see that out on a MR task, the Configuration could have all kinds of pollution in it.", "Would suggest an hbase prefix - hbase.zookeeper prefix?", "@nitay on \"if zoo.cfg in the CLASSPATH\", that might work.", "We might want to try narrow the places we look on the CLASSPATH.", "But lets start open and narrow later (we probably want this to be open as possible at mo.", "until we learn more about the cloudera config.)", "@nitay on toZooKeeperProperties, do we have to expose that?", "Can't we just pass a HBaseConfiguation to the HBase ZK Wrapper (I'm not up on latest dev here so this might be an off suggestion)", "Enjoy zoo.cfg while you can boys.", "It's time to say goodbye.", "In this patch:", "Remove zoo.cfg from conf/ and src/test.", "Remove zoo.cfg from build.xml and stargate build.xml jar targets.", "Add properties hbase.zookeeper.quorum, hbase.zookeeper.peerport, and hbase.zookeeper.leaderport.", "Together, these form the server.X lines in the zoo.cfg.", "Add a property for every zoo.cfg option named hbase.zookeeper.property.KEY.", "Specifically, this adds:", "hbase.zookeeper.property.tickTime", "hbase.zookeeper.property.initLimit", "hbase.zookeeper.property.syncLimit", "hbase.zookeeper.property.dataDir", "hbase.zookeeper.property.clientPort", "hbase.zookeeper.property.maxClientCnxns", "All of our ZooKeeper clients, which go through ZooKeeperWrapper, and our server wrapper, HQuorumPeer, end up calling HQuorumPeer.makeZKProps() to get a Properties of the ZooKeeper configuration.", "In that method, if there is a zoo.cfg in the classpath, simply load the config from it.", "Otherwise, map each of the hbase.zookeeper.property.X options directly to the Properties and create the server.X lines from the first three properties mentioned.", "Note that this now means you can programatically change the ZooKeeper quorum used by setting the \"hbase.zookeeper.quorum\" property and passing in your altered HBaseConfiguration object.", "Along with this, there is no more static loading of a ZooKeeper config.", "Each ZooKeeperWrapper does it on their own, so you can have multiple HTable or whatnot clients talking to different servers.", "Ready for review.", "Same patch, rebased to latest trunk.", "+1 on patch.", "Thanks for summary of changes above.", "Its missing doc of this new way of doing things but can do in different issue.", "Also, would suggest on commit that the hadoop-default.xml comment that prefaces hbase.zookeeper.properties notes that anything that can be in zoo.cfg can be added to hbase-default.xml as long has it has requisite prefix.", "I ran a little local cluster with patch in place.", "All works fine."], "SplitGT": [" If there is a zoo.cfg in the classpath, simply read it in."], "issueString": "Remove zoo.cfg, put config options into hbase-site.xml\nFrom HBASE-1551:\n\nHere's my current thinking:\n\n    * Move all of the ZooKeeper config paraments into hbase-*.xml using zookeeper.property.KEY = VALUE.\n    * Add a special property for the list of quorum servers, say zookeeper.quorum. This option can default to \"localhost\".\n    * If there is a zoo.cfg present in the classpath, use its data above the zookeeper.property.KEY options.\n    * When we need to instantiate something to talk to ZooKeeper, we simply create a new HBaseConfiguration and call some method on it e.g. toZooKeeperProperties().\n      This method will iterate through the zookeeper.property.KEY and turn each into the appropriate ZooKeeper configurations (i.e. KEY=VALUE). It will generate\n      the server.X property from the zookeeper.quorum configuration option. As mentioned above, if there is a zoo.cfg in the classpath, overwrite the data with its configuration.\n      This will return a Properties object that can be used to construct the appropriate ZooKeeper config and start/talk to their servers.\n    * For start/stop management of full ZK quorum cluster, use something like my ZKServerTool in the patch (modified of course) to do the parsing mentioned above and turn it\n      into a simple line-by-line list of quorum servers. As I do in this patch, the bin/zookeepers.sh can then simply call bin/hbase o.a.h.h.z.ZKServerTool to get the list of hosts.\n      If you want something like a conf/zookeepers you can simply run ZKServerTool yourself.\n\nThe benefits from all this are:\n\n    * One place for all ZK configuration. No duplicate setting of parameters.\n    * No more nasty zoo.cfg. Give the user what they're already used to, a single XML config file.\n    * New user only need edit zookeeper.quorum to get full cluster.\n    * Programmable control of what ZK one is talking to.\n\n\nAnd some of Stack's comments:\n\n@nitay, I think you need to keep the hbase zookeeper config inside of an hbase namespace. The Hadoop Configuation system is a floozy. It will go with anyone who calls load resource on it pulling in their properties. I could see that out on a MR task, the Configuration could have all kinds of pollution in it. Would suggest an hbase prefix - hbase.zookeeper prefix?\n\n@nitay on \"if zoo.cfg in the CLASSPATH\", that might work. We might want to try narrow the places we look on the CLASSPATH. But lets start open and narrow later (we probably want this to be open as possible at mo. until we learn more about the cloudera config.)\n\n@nitay on toZooKeeperProperties, do we have to expose that? Can't we just pass a HBaseConfiguation to the HBase ZK Wrapper (I'm not up on latest dev here so this might be an off suggestion)\n\nEnjoy zoo.cfg while you can boys. It's time to say goodbye.\n\nIn this patch:\n- Remove zoo.cfg from conf/ and src/test.\n- Remove zoo.cfg from build.xml and stargate build.xml jar targets.\n- Add properties hbase.zookeeper.quorum, hbase.zookeeper.peerport, and hbase.zookeeper.leaderport. Together, these form the server.X lines in the zoo.cfg.\n- Add a property for every zoo.cfg option named hbase.zookeeper.property.KEY. Specifically, this adds:\n-- hbase.zookeeper.property.tickTime\n-- hbase.zookeeper.property.initLimit\n-- hbase.zookeeper.property.syncLimit\n-- hbase.zookeeper.property.dataDir\n-- hbase.zookeeper.property.clientPort\n-- hbase.zookeeper.property.maxClientCnxns\n\n- All of our ZooKeeper clients, which go through ZooKeeperWrapper, and our server wrapper, HQuorumPeer, end up calling HQuorumPeer.makeZKProps() to get a Properties of the ZooKeeper configuration. In that method, if there is a zoo.cfg in the classpath, simply load the config from it. Otherwise, map each of the hbase.zookeeper.property.X options directly to the Properties and create the server.X lines from the first three properties mentioned.\n\nNote that this now means you can programatically change the ZooKeeper quorum used by setting the \"hbase.zookeeper.quorum\" property and passing in your altered HBaseConfiguration object. Along with this, there is no more static loading of a ZooKeeper config. Each ZooKeeperWrapper does it on their own, so you can have multiple HTable or whatnot clients talking to different servers.\nReady for review.\nSame patch, rebased to latest trunk.\n+1 on patch.\n\nThanks for summary of changes above.\n\nIts missing doc of this new way of doing things but can do in different issue.  Also, would suggest on commit that the hadoop-default.xml comment that prefaces hbase.zookeeper.properties notes that anything that can be in zoo.cfg can be added to hbase-default.xml as long has it has requisite prefix.\n\nI ran a little local cluster with patch in place.  All works fine.\n", "issueSearchSentences": ["All of our ZooKeeper clients, which go through ZooKeeperWrapper, and our server wrapper, HQuorumPeer, end up calling HQuorumPeer.makeZKProps() to get a Properties of the ZooKeeper configuration.", "This will return a Properties object that can be used to construct the appropriate ZooKeeper config and start/talk to their servers.", "When we need to instantiate something to talk to ZooKeeper, we simply create a new HBaseConfiguration and call some method on it e.g.", "One place for all ZK configuration.", "Also, would suggest on commit that the hadoop-default.xml comment that prefaces hbase.zookeeper.properties notes that anything that can be in zoo.cfg can be added to hbase-default.xml as long has it has requisite prefix."], "issueSearchScores": [0.6717861294746399, 0.6203684210777283, 0.5682618618011475, 0.5673539638519287, 0.5476499795913696]}
{"aId": 44, "code": "public long incrementColumnValue(final byte [] row, final byte [] family, \n      final byte [] qualifier, final long amount, final boolean writeToWAL)\n  throws IOException {\n    NullPointerException npe = null;\n    if (row == null) {\n      npe = new NullPointerException(\"row is null\");\n    } else if (family == null) {\n      npe = new NullPointerException(\"column is null\");\n    }\n    if (npe != null) {\n      IOException io = new IOException(\n          \"Invalid arguments to incrementColumnValue\", npe);\n      throw io;\n    }\n    return connection.getRegionServerWithRetries(\n        new ServerCallable<Long>(connection, tableName, row) {\n          public Long call() throws IOException {\n            return server.incrementColumnValue(\n                location.getRegionInfo().getRegionName(), row, family, \n                qualifier, amount, writeToWAL);\n          }\n        }\n    );\n  }", "comment": " Atomically increments a column value. If the column value already exists and is not a big-endian long, this could throw an exception.", "issueId": "HBASE-1563", "issueStringList": ["incrementColumnValue does not write to WAL", "Incrementing never writes to the WAL.", "Under failure scenarios, you will lose all increments since the last flush.", "Do we want to expose the option to the client as to whether to write to WAL or not?", "This patch adds a writeToWAL argument and appends to the edit log on every increment, however the boolean is not exposed to the client in this patch and is default true.", "Wanted to discuss with others before going further.", "Thoughts?", "IMHO, it's fine to expose the boolean to the client but there should be a client side API alternative which does not expose the boolean which internally sets writeToWAL as true.", "Everybody will be happy.", "Let's just do it.", "+1 patch coming...", "Please review.", "Why not have old incrementColumnValue call the new one with a flag set to 'true'?", "Code is fine.", "This portion of the javadoc:", "bq.", "If the column value isn't long-like, this could throw an exception.", "I understand what you are saying, but I think this may just confuse users.", "Can you consider an alternate wording?", "2 things:", "in the client code you build a 'get' that isnt used.", "we are writing to the WAL _after_ the increment, this may be the only way to do it effectively, but we should also think about this carefully.", "@Ryan I think the Get is just some old leftover when it was used to be sent over that way and should now be removed.", "Removes unnecessary Get in client code", "Removed duplicate server calls in the client code, default just calls full w/ true", "Replaced increment javadoc with \"Atomically increments a column value.", "If the column value already exists and is not a big-endian long, this could throw an exception.\"", "Is that more clear?", "@Ryan I got most the way through splitting up Store.incrementColumnValue logic so we figure out what we have to do first, then write to wal, then perform the insert.", "The issue is when we increment in-place in the memcache.", "If we truly don't want to write until the wal append, we'd have to make a copy of the KV, perform the increment on it, and then either increment the original memcache value or swap.", "This more or less negates the in-place increment optimization.", "Thoughts?", "Patch looks good except for the identified weirdness where we write to WAL after the update.", "To do this properly you'd need to row lock and make a copy.", "Is that too onerous a price to pay?", "Ryan notes that old code did row lock and was fast.", "So, we should go ahead and pull the KV from memcache, copy it, increment it, add to WAL, then insert back into memcache?", "It won't be too slow?", "Maybe only do this if write to wal is enabled?", "Why is there a race if a row lock is outstanding Ryan?", "We are already getting a row lock, there is no race condition that I can see.", "I'd really like the increment-in-place over making a new KV each time we increment.", "The downside is if the regionserver dies in the brief window after the update and before the WAL append, we lose a single increment.", "If it dies on either side, then nothing would change.", "The upside is we don't allocate a new KV each time.", "That's what I'm after with increments, doing them in place and not doing a new insertion each time (and new allocation).", "I have an internal ID assignment system that uses custom patches on 0.19.", "Was hoping to drop our custom patches and use this out of the box, but if we copy the KV each time I'll likely patch it to not do that :)  I'd rather not lose any increments but the window is very small and you lose at most a single increment.", "If a KV is 100 bytes, and I increment just 1M times in a day, i've unnecessarily allocated 100MB (and the worst kind, lots of small 100 byte allocations that stick around for a bit).", "We still satisfy the property of \"Once the increment has returned successfully, it is safe\".", "This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).", "bq.", "We still satisfy the property of \"Once the increment has returned successfully, it is safe\".", "This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).", "This makes sense to me.", "a row lock is only for writing.", "The race happens when someone is reading the value you are incrementing.", "They could copy a partially updated view into the RPC buffer, and the user could see something odd and unexpected (values going down for example).", "If we used a ConcurrentSkipListSet, we can't put a duplicate, which requires us to temporarily remove the KeyValue, then put it back, opens a hole whereby we can look for a value and it not be there.", "So to support not-in-place modification we need to also move to a ConcurrentSkipListMap in memcache.", "me man, i love OSS", "Someone please review 1577 -- it converts memcache to CSLM from CSLS.", "So, interesting, need to operate on copy so readers don't see partially changed KV.", "That makes sense.", "For now, let's just make the copy and make sure things are safe.", "In addition to 1577.", "Good stuff ryan.", "Stack, you've brought your improper usage of /me onto the web now!", "Will post patch tomorrow", "New patch that copies the existing KV instead of incrementing in-place.", "This is now broken.", "The CSLMap has the same behavior as the CSLSet.", "Since the keys are considered \"equal\" the values might be updated in the map (in this case meaningless) but they don't swap the key with the new one (since it's equal).", "It does not have the behavior we wanted, an atomic put that replaces the Key in case of collisions.", "Can we please ensure that the \"atomic\" puts are honored?", "I migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.", "@Irfan First and foremost, we need HBASE-1577, but that is looking good.", "Beyond that, we need to think more about how to make this behave atomically.", "In thinking about it now, I do see some potential issues/race conditions that will lead to missing increments (with HBASE-1577 things will always _work_ but does not ensure atomicity of the increments).", "Need to think more.", "@Irfan  Excuse me... Got confused for a second.", "Once HBASE-1577 goes in, these absolutely *will* have atomic behavior.", "Each is guaranteed to have worked and performed the increment once it returns, and all readers will always see the latest value.", "Thanks for the patch Jon"], "SplitGT": [" Atomically increments a column value.", "If the column value already exists and is not a big-endian long, this could throw an exception."], "issueString": "incrementColumnValue does not write to WAL\nIncrementing never writes to the WAL.  Under failure scenarios, you will lose all increments since the last flush.\n\nDo we want to expose the option to the client as to whether to write to WAL or not?\nThis patch adds a writeToWAL argument and appends to the edit log on every increment, however the boolean is not exposed to the client in this patch and is default true. Wanted to discuss with others before going further.\n\nThoughts?\nIMHO, it's fine to expose the boolean to the client but there should be a client side API alternative which does not expose the boolean which internally sets writeToWAL as true. Everybody will be happy. Let's just do it. \n+1 patch coming...\nPlease review.\nWhy not have old incrementColumnValue call the new one with a flag set to 'true'?\nCode is fine.\n\nThis portion of the javadoc:\n\nbq. If the column value isn't long-like, this could throw an exception.\n\nI understand what you are saying, but I think this may just confuse users. Can you consider an alternate wording? \n2 things:\n- in the client code you build a 'get' that isnt used.\n- we are writing to the WAL _after_ the increment, this may be the only way to do it effectively, but we should also think about this carefully.\n@Ryan I think the Get is just some old leftover when it was used to be sent over that way and should now be removed.\n- Removes unnecessary Get in client code\n- Removed duplicate server calls in the client code, default just calls full w/ true\n- Replaced increment javadoc with \"Atomically increments a column value. If the column value already exists and is not a big-endian long, this could throw an exception.\"  Is that more clear?\n\n@Ryan I got most the way through splitting up Store.incrementColumnValue logic so we figure out what we have to do first, then write to wal, then perform the insert.  The issue is when we increment in-place in the memcache.  If we truly don't want to write until the wal append, we'd have to make a copy of the KV, perform the increment on it, and then either increment the original memcache value or swap.  This more or less negates the in-place increment optimization.  Thoughts?\nPatch looks good except for the identified weirdness where we write to WAL after the update.  To do this properly you'd need to row lock and make a copy.  Is that too onerous a price to pay?\nRyan notes that old code did row lock and was fast.\n\nSo, we should go ahead and pull the KV from memcache, copy it, increment it, add to WAL, then insert back into memcache?  It won't be too slow?  Maybe only do this if write to wal is enabled?\nWhy is there a race if a row lock is outstanding Ryan?\nWe are already getting a row lock, there is no race condition that I can see.\n\nI'd really like the increment-in-place over making a new KV each time we increment.  The downside is if the regionserver dies in the brief window after the update and before the WAL append, we lose a single increment.  If it dies on either side, then nothing would change.\n\nThe upside is we don't allocate a new KV each time.  That's what I'm after with increments, doing them in place and not doing a new insertion each time (and new allocation).  I have an internal ID assignment system that uses custom patches on 0.19.  Was hoping to drop our custom patches and use this out of the box, but if we copy the KV each time I'll likely patch it to not do that :)  I'd rather not lose any increments but the window is very small and you lose at most a single increment.  If a KV is 100 bytes, and I increment just 1M times in a day, i've unnecessarily allocated 100MB (and the worst kind, lots of small 100 byte allocations that stick around for a bit).\n\nWe still satisfy the property of \"Once the increment has returned successfully, it is safe\".  This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).\nbq. We still satisfy the property of \"Once the increment has returned successfully, it is safe\". This weird scenario would actually only happen in a case where the increment started but never returned (so client should not assume it was successful anyways).\n\nThis makes sense to me. \na row lock is only for writing.  The race happens when someone is reading the value you are incrementing.  They could copy a partially updated view into the RPC buffer, and the user could see something odd and unexpected (values going down for example).\n\nIf we used a ConcurrentSkipListSet, we can't put a duplicate, which requires us to temporarily remove the KeyValue, then put it back, opens a hole whereby we can look for a value and it not be there.  \n\nSo to support not-in-place modification we need to also move to a ConcurrentSkipListMap in memcache.\n/me man, i love OSS\n\nSomeone please review 1577 -- it converts memcache to CSLM from CSLS.\nSo, interesting, need to operate on copy so readers don't see partially changed KV.  That makes sense.\nFor now, let's just make the copy and make sure things are safe.  In addition to 1577.  Good stuff ryan.\n\nStack, you've brought your improper usage of /me onto the web now!\nWill post patch tomorrow\nNew patch that copies the existing KV instead of incrementing in-place.\n\nThis is now broken.  The CSLMap has the same behavior as the CSLSet.  Since the keys are considered \"equal\" the values might be updated in the map (in this case meaningless) but they don't swap the key with the new one (since it's equal).  It does not have the behavior we wanted, an atomic put that replaces the Key in case of collisions.\nCan we please ensure that the \"atomic\" puts are honored? \n\nI migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.\n@Irfan First and foremost, we need HBASE-1577, but that is looking good.  Beyond that, we need to think more about how to make this behave atomically.  In thinking about it now, I do see some potential issues/race conditions that will lead to missing increments (with HBASE-1577 things will always _work_ but does not ensure atomicity of the increments).  Need to think more.\n@Irfan  Excuse me... Got confused for a second.  Once HBASE-1577 goes in, these absolutely *will* have atomic behavior.  Each is guaranteed to have worked and performed the increment once it returns, and all readers will always see the latest value.\nThanks for the patch Jon\n", "issueSearchSentences": ["I migrated lot of my code to use the incrementColumnValue() because of the atomic nature of this call and it really reduces the data footprint and also the query time when aggregating the cell values.", "incrementColumnValue does not write to WAL", "Why not have old incrementColumnValue call the new one with a flag set to 'true'?", "Replaced increment javadoc with \"Atomically increments a column value.", "If the column value already exists and is not a big-endian long, this could throw an exception.\""], "issueSearchScores": [0.7297472953796387, 0.7190792560577393, 0.6821458339691162, 0.660371720790863, 0.6147751212120056]}
{"aId": 52, "code": "@Deprecated\n  public long initialize() throws IOException {\n    return initialize(null);\n  }", "comment": " Used only by tests and SplitTransaction to reopen the region.", "issueId": "HBASE-7786", "issueStringList": ["Consolidate HRegion creation/opening API", "Currently we have 4 ways to instantiate an HRegion.", "HRegion.createHRegion()", "HRegion.openHRegion()", "HRegion.newHRegion() + r.initialize()", "new HRegion() + r.initialize()", "Aside from tests and HMerge and SplitTransaction code everyone use createHRegion() and openHRegion().", "To avoid errors due to missing initialization calls I think we should limit the access to newHRegion() and the constructor.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12568418/HBASE-7786-v0.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 15 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.regionserver.wal.TestWALReplay", "org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//console", "This message is automatically generated.", "{quote}", "+   * Used only by tests and SplitTransaction to reopen the region.", "+   * You should use createHRegion() or openHRegion()", "{quote}", "Is it possible to get rid of it instead?", "It just does redirect to different method, might as well have it in the tests themselves.", "{quote}", "/**", "+  /*", "{quote}", "Is this intentional?", "{quote}", "HRegion createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)", "{quote}", "Is this used anywhere except for the other overload?", "Otherwise looks reasonable.", "...assuming the tests pass :)", "{quote}", "{code}", "Used only by tests and SplitTransaction to reopen the region.", "You should use createHRegion() or openHRegion()", "{code}", "Is it possible to get rid of it instead?", "It just does redirect to different method, might as well have it in the tests themselves.", "{quote}", "Nah... the idea was naving initialize() private, since the user should not know about that... but a couple of tests uses the constructor new HRegion() { @override method() } to slightly change the behaviour...", "I guess I can mock it, or set the REGION_IMPL but it makes the test less readable.", "{quote}", "HRegion createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)", "Is this used anywhere except for the other overload?", "{quote}", "This is used only by the split code, and inside HRegion just to make newHRegion() private", "Thanks for doing housekeeping.", "{code}", "+   * Used only by tests and SplitTransaction to reopen the region.", "+   * You should use createHRegion() or openHRegion()", "{code}", "Deprecate if you are not going to remove as Sergey suggests?", "+1", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12568432/HBASE-7786-v1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 15 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//console", "This message is automatically generated.", "{quote}Deprecate if you are not going to remove as Sergey suggests?", "{quote}", "It will be nice having some decorator to mark methods as \"Exposed for testing\" do we have something similar?", "{quote}", "bq.", "createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)", "This is used only by the split code, and inside HRegion just to make newHRegion() private", "{quote}", "Oh, I see, they are in different files.", "+1 then, +1 on deprecating the test call.", "[~mbertozzi] IIRC, I have seen some one argue this an anti-pattern but I like it: http://docs.guava-libraries.googlecode.com/git-history/v10.0.1/javadoc/com/google/common/annotations/VisibleForTesting.html", "yeah VisibleForTesting seems good, but I was thinking more something like", "{code}", "public void methodUsedOnlyByTests() {", "if (!testing) throw new UnsupportedOperationException():", "do stuff", "}", "{code}", "just to make sure that a user is not using that...", "I'm thinking to a situation where we remove that method because we know is only for test and we break binary compatibility.", "yeah VisibleForTesting seems good, but I was thinking more something like", "{code}", "public void methodUsedOnlyByTests() {", "if (!testing) throw new UnsupportedOperationException():", "do stuff", "}", "{code}", "just to make sure that a user is not using that...", "I'm thinking to a situation where we remove that method because we know is only for test and we break binary compatibility.", "{color:green}+1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12568472/HBASE-7786-v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 15 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:green}+1 javadoc{color}.", "The javadoc tool did not generate any warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:green}+1 findbugs{color}.", "The patch does not introduce any new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:green}+1 lineLengths{color}.", "The patch does not introduce lines longer than 100", "{color:green}+1 core tests{color}.", "The patch passed unit tests in .", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//console", "This message is automatically generated.", "if there're no other comments I'm going to commit this one, since is just code moved", "+1", "committed to trunk, thanks for the review guys", "Integrated in HBase-TRUNK #3864 (See [https://builds.apache.org/job/HBase-TRUNK/3864/])", "HBASE-7786 Consolidate HRegion creation/opening API (Revision 1444212)", "Result = FAILURE", "mbertozzi :", "Files :", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java", "Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #399 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/399/])", "HBASE-7786 Consolidate HRegion creation/opening API (Revision 1444212)", "Result = FAILURE", "mbertozzi :", "Files :", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java", "hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java", "hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java"], "SplitGT": [" Used only by tests and SplitTransaction to reopen the region."], "issueString": "Consolidate HRegion creation/opening API\nCurrently we have 4 ways to instantiate an HRegion.\n * HRegion.createHRegion()\n * HRegion.openHRegion()\n * HRegion.newHRegion() + r.initialize()\n * new HRegion() + r.initialize()\n\nAside from tests and HMerge and SplitTransaction code everyone use createHRegion() and openHRegion(). To avoid errors due to missing initialization calls I think we should limit the access to newHRegion() and the constructor.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12568418/HBASE-7786-v0.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.regionserver.wal.TestWALReplay\n                  org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4368//console\n\nThis message is automatically generated.\n{quote}\n+   * Used only by tests and SplitTransaction to reopen the region.\n+   * You should use createHRegion() or openHRegion()\n{quote}\nIs it possible to get rid of it instead? It just does redirect to different method, might as well have it in the tests themselves.\n\n{quote}\n-  /**\n+  /*\n{quote}\nIs this intentional?\n\n{quote}\nHRegion createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)\n{quote}\nIs this used anywhere except for the other overload?\n\nOtherwise looks reasonable.\n\n...assuming the tests pass :)\n{quote}\n{code}\nUsed only by tests and SplitTransaction to reopen the region.\nYou should use createHRegion() or openHRegion()\n{code}\nIs it possible to get rid of it instead? It just does redirect to different method, might as well have it in the tests themselves.\n{quote}\nNah... the idea was naving initialize() private, since the user should not know about that... but a couple of tests uses the constructor new HRegion() { @override method() } to slightly change the behaviour... I guess I can mock it, or set the REGION_IMPL but it makes the test less readable.\n\n{quote}\nHRegion createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)\nIs this used anywhere except for the other overload?\n{quote}\nThis is used only by the split code, and inside HRegion just to make newHRegion() private\nThanks for doing housekeeping.\n\n{code}\n+   * Used only by tests and SplitTransaction to reopen the region.\n+   * You should use createHRegion() or openHRegion()\n{code}\n\nDeprecate if you are not going to remove as Sergey suggests?\n\n+1\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12568432/HBASE-7786-v1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4371//console\n\nThis message is automatically generated.\n{quote}Deprecate if you are not going to remove as Sergey suggests?{quote}\nIt will be nice having some decorator to mark methods as \"Exposed for testing\" do we have something similar?\n{quote}\nbq. createDaughterRegion(final HRegionInfo hri, final Path daughterTmpDir)\nThis is used only by the split code, and inside HRegion just to make newHRegion() private\n{quote}\nOh, I see, they are in different files.\n\n+1 then, +1 on deprecating the test call.\n\n[~mbertozzi] IIRC, I have seen some one argue this an anti-pattern but I like it: http://docs.guava-libraries.googlecode.com/git-history/v10.0.1/javadoc/com/google/common/annotations/VisibleForTesting.html\nyeah VisibleForTesting seems good, but I was thinking more something like\n{code}\npublic void methodUsedOnlyByTests() {\n    if (!testing) throw new UnsupportedOperationException():\n    // do stuff\n}\n{code}\n\njust to make sure that a user is not using that...\nI'm thinking to a situation where we remove that method because we know is only for test and we break binary compatibility.\nyeah VisibleForTesting seems good, but I was thinking more something like\n{code}\npublic void methodUsedOnlyByTests() {\n    if (!testing) throw new UnsupportedOperationException():\n    // do stuff\n}\n{code}\n\njust to make sure that a user is not using that...\nI'm thinking to a situation where we remove that method because we know is only for test and we break binary compatibility.\n{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12568472/HBASE-7786-v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 15 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in .\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/4374//console\n\nThis message is automatically generated.\nif there're no other comments I'm going to commit this one, since is just code moved\n+1\ncommitted to trunk, thanks for the review guys\nIntegrated in HBase-TRUNK #3864 (See [https://builds.apache.org/job/HBase-TRUNK/3864/])\n    HBASE-7786 Consolidate HRegion creation/opening API (Revision 1444212)\n\n     Result = FAILURE\nmbertozzi : \nFiles : \n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java\n\nIntegrated in HBase-TRUNK-on-Hadoop-2.0.0 #399 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/399/])\n    HBASE-7786 Consolidate HRegion creation/opening API (Revision 1444212)\n\n     Result = FAILURE\nmbertozzi : \nFiles : \n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java\n\n", "issueSearchSentences": ["Nah... the idea was naving initialize() private, since the user should not know about that... but a couple of tests uses the constructor new HRegion() { @override method() } to slightly change the behaviour...", "To avoid errors due to missing initialization calls I think we should limit the access to newHRegion() and the constructor.", "if (!testing) throw new UnsupportedOperationException():", "if (!testing) throw new UnsupportedOperationException():", "Used only by tests and SplitTransaction to reopen the region."], "issueSearchScores": [0.45213770866394043, 0.40231508016586304, 0.39362937211990356, 0.39362937211990356, 0.32526350021362305]}
{"aId": 53, "code": "public static void cleanRestoreDir(Job job, String snapshotName) throws IOException {\n    TableSnapshotInputFormatImpl.cleanRestoreDir(job, snapshotName);\n  }", "comment": " clean restore directory after snapshot scan job", "issueId": "HBASE-26485", "issueStringList": ["Introduce a method to clean restore directory after Snapshot Scan", "SnapshotScan is widely used in our company.", "However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time.", "So maybe we can introduce a method for users to clean the snapshot restore directory after job."], "SplitGT": [" clean restore directory after snapshot scan job"], "issueString": "Introduce a method to clean restore directory after Snapshot Scan\nSnapshotScan is widely used in our company. However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time. So maybe we can introduce a method for users to clean the snapshot restore directory after job.\n", "issueSearchSentences": ["So maybe we can introduce a method for users to clean the snapshot restore directory after job.", "However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time.", "Introduce a method to clean restore directory after Snapshot Scan", "SnapshotScan is widely used in our company."], "issueSearchScores": [0.7390189170837402, 0.6465071439743042, 0.6065527200698853, 0.2278587520122528]}
{"aId": 55, "code": "private boolean tryRecoveringExpiredZKSession() throws InterruptedException,\n      IOException, KeeperException {\n    this.zooKeeper = new ZooKeeperWatcher(conf, MASTER + \":\"\n        + address.getPort(), this);\n\n    if (!becomeActiveMaster()) {\n      return false;\n    }\n    initializeZKBasedSystemTrackers();\n    // Update in-memory structures to reflect our earlier Root/Meta assignment.\n    assignRootAndMeta();\n    // process RIT if any\n    this.assignmentManager.processRegionsInTransition();\n    return true;\n  }", "comment": " Initialize all ZK based system trackers. Assign root and meta.", "issueId": "HBASE-3210", "issueStringList": ["HBASE-1921 for the new master", "HBASE-1921 was lost when writing the new master code.", "I guess it's going to be much harder to implement now, but I think it's a critical feature to have considering the reasons that brought me do it in the old master.", "There's already a test in TestZooKeeper which has been disabled a while ago.", "First draft of my patch for review.", "Here is what is being done now:", "1.", "When the primary master's Abort is triggered from ZK Node listener during a ZK session expiry event, we first try to see if we can restore the ZK session.", "We ignore the abort trigger and continue working as primary master, if we can successfully restore the ZK session.", "2.", "A successful ZK session recovery involves the following.", "a.", "Create a ZK Session", "b.", "Try becoming the primary master again.", "(so that we don't step onto secondary master's toes)", "c. Initialize all ZK based trackers.", "This includes the AssignmentManager, CatalogTracker,", "RegionServerTracker and ClusterSTatusTracker.", "d. Assign Root and Meta.", "(We just ensure that our local memory structures are correctly updated to reflect our earlier Root/Meta assignments)", "e. Process RIT if any, that came in during our blackout.", "3.", "Refactored the Master startup logic so that we can reuse them during a master session recovery attempt.", "JD/Stack: Can one of your review this closely as it involves some minor refactoring to master startup?", "Also, JD worked on HBASE-1921 earlier and has the full context.", "Also, I reenabled the disabled test in TestZooKeeper that tests for master session expiry.", "All the tests are passing on my local.", "Subbu: Your patch looks great (as does your reenabling of TZK).", "I'm up for committing it -- was going to run all tests first first though since a pretty significant change -- but your patch is 4x the time it needs to be since the bulk is formatting only changes.", "Would you mind resubmitting the patch absent the formatting changes.", "Try also to keep lines < 80.", "Good stuff Subbu.", "Sure deal.", "will do it ASAP.", "Thanks Stack for the review.", "attaching the patch with corrected formatting and line breaks.", "running unit tests to ensure this change breaks nothing else", "Comments after first pass:", "becomeActiveMaster and recoverableFromZKSessionExpiry needs complete javadoc (the @return tag is empty)", "should recoverableFromZKSessionExpiry be called tryRecoveringExpiredSession?", "in recoverableFromZKSessionExpiry, you do a big if(becomeActiveMaster()) and then return true, or false.", "What we usually prefer for readability is to instead do something like:", "{code}", "if (!becomeActiveMaster()) {", "return false;", "}", "Initialize ZK based trackers since we now have a new ZK session.", "initilizeZKBasedSystemTrackers();", "Update in-memory strutures to reflect our earlier Root/Meta assignment.", "assignRootAndMeta();", "process RIT if any", "this.assignmentManager.processRegionsInTransition();", "return true;", "{code}", "Instead of:", "{code}", "return recoverableFromZKSessionExpiry() ?", "false : true;", "{code}", "do", "{code}", "return !recoverableFromZKSessionExpiry();", "{code}", "abortNow in abortNow() is set multiple times to true.", "Review how you use that variable.", "There's a typo in initilizeZKBasedSystemTrackers", "JD:", "thanks for your comments.", "Will address all your call outs.", "FYI, all tests pass for posted patch.", "Attached latest patch that addresses JD's comments/suggestions.", "Thanks Stack for running the tests and I did it too.", "thanks again.", "Committed to TRUNK.", "Thank you for the patch Subbu."], "SplitGT": [" Initialize all ZK based system trackers.", "Assign root and meta."], "issueString": "HBASE-1921 for the new master\nHBASE-1921 was lost when writing the new master code. I guess it's going to be much harder to implement now, but I think it's a critical feature to have considering the reasons that brought me do it in the old master. There's already a test in TestZooKeeper which has been disabled a while ago.\nFirst draft of my patch for review.\n\nHere is what is being done now:\n\n1. When the primary master's Abort is triggered from ZK Node listener during a ZK session expiry event, we first try to see if we can restore the ZK session. We ignore the abort trigger and continue working as primary master, if we can successfully restore the ZK session.\n\n2. A successful ZK session recovery involves the following.\n   a. Create a ZK Session \n   b. Try becoming the primary master again. (so that we don't step onto secondary master's toes)\n   c. Initialize all ZK based trackers. This includes the AssignmentManager, CatalogTracker,      \n      RegionServerTracker and ClusterSTatusTracker.\n   d. Assign Root and Meta. (We just ensure that our local memory structures are correctly updated to reflect our earlier Root/Meta assignments)\n   e. Process RIT if any, that came in during our blackout.\n\n3. Refactored the Master startup logic so that we can reuse them during a master session recovery attempt.\n\n\n  \n\nJD/Stack: Can one of your review this closely as it involves some minor refactoring to master startup? Also, JD worked on HBASE-1921 earlier and has the full context.\nAlso, I reenabled the disabled test in TestZooKeeper that tests for master session expiry.\n\nAll the tests are passing on my local. \nSubbu: Your patch looks great (as does your reenabling of TZK).  I'm up for committing it -- was going to run all tests first first though since a pretty significant change -- but your patch is 4x the time it needs to be since the bulk is formatting only changes.  Would you mind resubmitting the patch absent the formatting changes.  Try also to keep lines < 80.  Good stuff Subbu.\nSure deal. will do it ASAP. \n\nThanks Stack for the review.\nattaching the patch with corrected formatting and line breaks.\n\n\nrunning unit tests to ensure this change breaks nothing else\nComments after first pass:\n\n - becomeActiveMaster and recoverableFromZKSessionExpiry needs complete javadoc (the @return tag is empty)\n - should recoverableFromZKSessionExpiry be called tryRecoveringExpiredSession?\n - in recoverableFromZKSessionExpiry, you do a big if(becomeActiveMaster()) and then return true, or false. What we usually prefer for readability is to instead do something like:\n\n{code}\nif (!becomeActiveMaster()) {\n  return false;\n}\n// Initialize ZK based trackers since we now have a new ZK session.\ninitilizeZKBasedSystemTrackers();\n// Update in-memory strutures to reflect our earlier Root/Meta assignment.\nassignRootAndMeta();\n// process RIT if any\nthis.assignmentManager.processRegionsInTransition();\nreturn true;\n{code}\n - Instead of:\n\n{code}\n return recoverableFromZKSessionExpiry() ? false : true;\n{code}\ndo\n{code}\n return !recoverableFromZKSessionExpiry();\n{code}\n\n - abortNow in abortNow() is set multiple times to true. Review how you use that variable.\n - There's a typo in initilizeZKBasedSystemTrackers\nJD:\n\nthanks for your comments. Will address all your call outs.\nFYI, all tests pass for posted patch.\nAttached latest patch that addresses JD's comments/suggestions.\n\nThanks Stack for running the tests and I did it too.\n\nthanks again.\nCommitted to TRUNK.  Thank you for the patch Subbu.\n", "issueSearchSentences": ["We ignore the abort trigger and continue working as primary master, if we can successfully restore the ZK session.", "in recoverableFromZKSessionExpiry, you do a big if(becomeActiveMaster()) and then return true, or false.", "When the primary master's Abort is triggered from ZK Node listener during a ZK session expiry event, we first try to see if we can restore the ZK session.", "should recoverableFromZKSessionExpiry be called tryRecoveringExpiredSession?", "return !recoverableFromZKSessionExpiry();"], "issueSearchScores": [0.7113204598426819, 0.6801794767379761, 0.6725168228149414, 0.6522547602653503, 0.620632529258728]}
{"aId": 56, "code": "private void connectionEvent(WatchedEvent event) {\n    switch(event.getState()) {\n      case SyncConnected:\n        // Update our identifier.  Otherwise ignore.\n        LOG.debug(this.identifier + \" connected\");\n        // Now, this callback can be invoked before the this.zookeeper is set.\n        // Wait a little while.\n        long finished = System.currentTimeMillis() +\n          this.conf.getLong(\"hbase.zookeeper.watcher.sync.connected.wait\", 2000);\n        while (System.currentTimeMillis() < finished) {\n          Threads.sleep(1);\n          if (this.zooKeeper != null) break;\n        }\n        if (this.zooKeeper == null) {\n          LOG.error(\"ZK is null on connection event -- see stack trace \" +\n            \"for the stack trace when constructor was called on this zkw\",\n            this.constructorCaller);\n          throw new NullPointerException(\"ZK is null\");\n        }\n        this.identifier = this.identifier + \"-0x\" +\n          Long.toHexString(this.zooKeeper.getSessionId());\n        break;\n\n      // Abort the server if Disconnected or Expired\n      // TODO: \u00c5ny reason to handle these two differently?\n      case Disconnected:\n        LOG.info(prefix(\"Received Disconnected from ZooKeeper, ignoring\"));\n        break;\n\n      case Expired:\n        String msg = prefix(this.identifier + \" received expired from \" +\n          \"ZooKeeper, aborting\");\n        // TODO: One thought is to add call to ZooKeeperListener so say,\n        // ZooKeperNodeTracker can zero out its data values.\n        if (this.abortable != null) this.abortable.abort(msg,\n            new KeeperException.SessionExpiredException());\n        break;\n    }\n  }", "comment": " If Disconnected or Expired, this should shutdown the cluster. But, since we send a KeeperException.SessionExpiredException along with the abort call, it's possible for the Abortable to catch it and try to create a new session with ZooKeeper. This is what the client does in HCM.", "issueId": "HBASE-3095", "issueStringList": ["Client needs to reconnect if it expires its zk session", "Clients use an HConnection down in their guts to connect to the hbase cluster.", "Master-is-running and root-region-location are up in zk.", "Setup of a new HConnection sets up a connection to ZooKeeper.", "If the session with ZK expires for whatever reason -- in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long GC, well, it'll be frustrating to users if we do not just try and resetup the zk connection.", "I'm putting this in 0.90 for now.", "Its marked a major bug rather than critical or blocker so could get punted.", "My sense though is that this is something that could become an irritant.", "Lets see.", "I'll let the issue stew some before going about fixing.", "It looks like Benoit fixed this for the old-school ZooKeeperWrapper.", "His fix should work for the new stuff.", "Its not there currently.", "Bring it in.", "See HBASE-2849.", "Message from: \"Jean-Daniel Cryans\" <jdcryans@apache.org>", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1167/", "Review request for hbase.", "Summary", "Changes ZKW to send the SessionExpiredException in the abort call so that clients can intercept it and reconnect to ZooKeeper by first clearing up their state and establishing a new connection.", "This addresses bug HBASE-3095.", "http://issues.apache.org/jira/browse/HBASE-3095", "Diffs", "trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1030759", "trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1030759", "trunk/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java 1030759", "Diff: http://review.cloudera.org/r/1167/diff", "Testing", "Adapted the client session expiration unit test.", "Thanks,", "Jean-Daniel", "Message from: stack@duboce.net", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1167/#review1804", "Ship it!", "Excellent.", "Nice and clean.", "trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java", "<http://review.cloudera.org/r/1167/#comment5868>", "Why a new exception?", "Why not pass the one that got us here?", "(Or, maybe there isn't one?", "Or you can't get at it)", "stack", "Message from: \"Jean-Daniel Cryans\" <jdcryans@apache.org>", "bq.", "On 2010-11-04 12:30:24, stack wrote:", "bq.", "> /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java, line 326", "bq.", "> <http://review.cloudera.org/r/1167/diff/1/?file=16666#file16666line326>", "bq.", ">", "bq.", ">     Why a new exception?", "Why not pass the one that got us here?", "(Or, maybe there isn't one?", "Or you can't get at it)", "Yeah there's no exception leading us there, it's the ZK event.", "Will add more documentation tho.", "Jean-Daniel", "This is an automatically generated e-mail.", "To reply, visit:", "http://review.cloudera.org/r/1167/#review1804", "Committed to trunk the patch I posted on RB plus those comments on top of ZKW.connectionEvent:", "{noformat}", "If Disconnected or Expired, this should shutdown the cluster.", "But, since", "we send a KeeperException.SessionExpiredException along with the abort", "call, it's possible for the Abortable to catch it and try to create a new", "session with ZooKeeper.", "This is what the client does in HCM.", "{noformat}"], "SplitGT": [" If Disconnected or Expired, this should shutdown the cluster.", "But, since we send a KeeperException.SessionExpiredException along with the abort call, it's possible for the Abortable to catch it and try to create a new session with ZooKeeper.", "This is what the client does in HCM."], "issueString": "Client needs to reconnect if it expires its zk session\nClients use an HConnection down in their guts to connect to the hbase cluster.  Master-is-running and root-region-location are up in zk.   Setup of a new HConnection sets up a connection to ZooKeeper.  If the session with ZK expires for whatever reason -- in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long GC, well, it'll be frustrating to users if we do not just try and resetup the zk connection.\nI'm putting this in 0.90 for now.  Its marked a major bug rather than critical or blocker so could get punted.  My sense though is that this is something that could become an irritant.  Lets see.  I'll let the issue stew some before going about fixing.\nIt looks like Benoit fixed this for the old-school ZooKeeperWrapper.  His fix should work for the new stuff.  Its not there currently.  Bring it in.  See HBASE-2849.\nMessage from: \"Jean-Daniel Cryans\" <jdcryans@apache.org>\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1167/\n-----------------------------------------------------------\n\nReview request for hbase.\n\n\nSummary\n-------\n\nChanges ZKW to send the SessionExpiredException in the abort call so that clients can intercept it and reconnect to ZooKeeper by first clearing up their state and establishing a new connection.\n\n\nThis addresses bug HBASE-3095.\n    http://issues.apache.org/jira/browse/HBASE-3095\n\n\nDiffs\n-----\n\n  /trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1030759 \n  /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1030759 \n  /trunk/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java 1030759 \n\nDiff: http://review.cloudera.org/r/1167/diff\n\n\nTesting\n-------\n\nAdapted the client session expiration unit test.\n\n\nThanks,\n\nJean-Daniel\n\n\n\nMessage from: stack@duboce.net\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1167/#review1804\n-----------------------------------------------------------\n\nShip it!\n\n\nExcellent.  Nice and clean.\n\n\n/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java\n<http://review.cloudera.org/r/1167/#comment5868>\n\n    Why a new exception?  Why not pass the one that got us here? (Or, maybe there isn't one? Or you can't get at it)\n\n\n- stack\n\n\n\n\nMessage from: \"Jean-Daniel Cryans\" <jdcryans@apache.org>\n\n\nbq.  On 2010-11-04 12:30:24, stack wrote:\nbq.  > /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java, line 326\nbq.  > <http://review.cloudera.org/r/1167/diff/1/?file=16666#file16666line326>\nbq.  >\nbq.  >     Why a new exception?  Why not pass the one that got us here? (Or, maybe there isn't one? Or you can't get at it)\n\nYeah there's no exception leading us there, it's the ZK event. Will add more documentation tho.\n\n\n- Jean-Daniel\n\n\n-----------------------------------------------------------\nThis is an automatically generated e-mail. To reply, visit:\nhttp://review.cloudera.org/r/1167/#review1804\n-----------------------------------------------------------\n\n\n\n\nCommitted to trunk the patch I posted on RB plus those comments on top of ZKW.connectionEvent:\n\n{noformat}\n* If Disconnected or Expired, this should shutdown the cluster. But, since\n* we send a KeeperException.SessionExpiredException along with the abort\n* call, it's possible for the Abortable to catch it and try to create a new\n* session with ZooKeeper. This is what the client does in HCM.\n{noformat}\n", "issueSearchSentences": ["Yeah there's no exception leading us there, it's the ZK event.", "Committed to trunk the patch I posted on RB plus those comments on top of ZKW.connectionEvent:", "Changes ZKW to send the SessionExpiredException in the abort call so that clients can intercept it and reconnect to ZooKeeper by first clearing up their state and establishing a new connection.", "session with ZooKeeper.", "Setup of a new HConnection sets up a connection to ZooKeeper."], "issueSearchScores": [0.4881479740142822, 0.4776551425457001, 0.4585637152194977, 0.4414498209953308, 0.4177020192146301]}
{"aId": 58, "code": "public void acquireDelegationToken(final FileSystem fs)\n      throws IOException {\n    String tokenKind;\n    String scheme = fs.getUri().getScheme();\n    if (SWEBHDFS_SCHEME.equalsIgnoreCase(scheme)) {\n      tokenKind = SWEBHDFS_TOKEN_KIND.toString();\n    } else if (WEBHDFS_SCHEME.equalsIgnoreCase(scheme)) {\n      tokenKind = WEBHDFS_TOKEN_KIND.toString();\n    } else if (HDFS_URI_SCHEME.equalsIgnoreCase(scheme)) {\n      tokenKind = HDFS_DELEGATION_KIND.toString();\n    } else {\n      LOG.warn(\"Unknown FS URI scheme: \" + scheme);\n      // Preserve default behavior\n      tokenKind = HDFS_DELEGATION_KIND.toString();\n    }\n\n    acquireDelegationToken(tokenKind, fs);\n  }", "comment": " Acquire the delegation token for the specified filesystem.", "issueId": "HBASE-22313", "issueStringList": ["Add a method to FsDelegationToken to accept token kind", "The acquireDelegationToken method [1]\u00a0defaults to checking for delegation token of kind \"HDFS_DELEGATION_TOKEN\" before fetching it from the FileSystem.", "It would be helpful to have a method that accepts the token kind and fetches delegation token from UserProvider for that token kind.", "[1]\u00a0-\u00a0[https://github.com/apache/hbase/blob/rel/2.1.4/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java#L67]", "I have a patch with similar purpose, I think: HBASE-20900", "Thanks for linking this.", "But isn't your patch still only use HDFS_DELEGATION_TOKEN kind?", "Is the purpose to support a different file system other than HDFS?", "Yes that's correct", "I am working on a patch for this.", "I ll attach to this issue once done.", "Thanks!", "I have attached the patch and also have a pull request out\u00a0[https://github.com/apache/hbase/pull/199]\u00a0.", "Let me know if anything else is needed.", "Thanks!", "Looks like the pull request got accepted.", "I have also updated the patch with the changes from the PR.", "Is there anything I need to do to get this completed?", "Thanks!", "Pushed to branch-1, branch-2 and master.", "Thanks [~venki09] for contributing!", "[~venki09] could you add Release Notes to this ticket?", "Results for branch branch-2", "[build #2126 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//General_Nightly_Build_Report/]", "(x) {color:red}-1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(/) {color:green}+1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}", "Results for branch branch-1", "[build #981 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(x) {color:red}-1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//General_Nightly_Build_Report/]", "(x) {color:red}-1 jdk7 checks{color}", "For more information [see jdk7 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//JDK7_Nightly_Build_Report/]", "(/) {color:green}+1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "[~psomogyi] is there an example I can follow for the release notes?", "Thanks!", "Results for branch master", "[build #1287 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1287/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(/) {color:green}+1 general checks{color}", "For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(x) {color:red}-1 jdk8 hadoop3 checks{color}", "For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//JDK8_Nightly_Build_Report_(Hadoop3)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details.", "(/) {color:green}+1 client integration test{color}"], "SplitGT": [" Acquire the delegation token for the specified filesystem."], "issueString": "Add a method to FsDelegationToken to accept token kind\nThe acquireDelegationToken method [1]\u00a0defaults to checking for delegation token of kind \"HDFS_DELEGATION_TOKEN\" before fetching it from the FileSystem. It would be helpful to have a method that accepts the token kind and fetches delegation token from UserProvider for that token kind.\r\n\r\n[1]\u00a0-\u00a0[https://github.com/apache/hbase/blob/rel/2.1.4/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java#L67]\u00a0\nI have a patch with similar purpose, I think: HBASE-20900\nThanks for linking this. But isn't your patch still only use HDFS_DELEGATION_TOKEN kind?\nIs the purpose to support a different file system other than HDFS?\nYes that's correct\nI am working on a patch for this. I ll attach to this issue once done. Thanks!\nI have attached the patch and also have a pull request out\u00a0[https://github.com/apache/hbase/pull/199]\u00a0. Let me know if anything else is needed. Thanks!\nLooks like the pull request got accepted. I have also updated the patch with the changes from the PR. Is there anything I need to do to get this completed? Thanks!\nPushed to branch-1, branch-2 and master.\r\n\r\nThanks [~venki09] for contributing!\n[~venki09] could you add Release Notes to this ticket?\nResults for branch branch-2\n\t[build #2126 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//General_Nightly_Build_Report/]\n\n\n\n\n(x) {color:red}-1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(/) {color:green}+1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2126//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\nResults for branch branch-1\n\t[build #981 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(x) {color:red}-1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//General_Nightly_Build_Report/]\n\n\n(x) {color:red}-1 jdk7 checks{color}\n-- For more information [see jdk7 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//JDK7_Nightly_Build_Report/]\n\n\n(/) {color:green}+1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-1/981//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n[~psomogyi] is there an example I can follow for the release notes? Thanks!\nResults for branch master\n\t[build #1287 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1287/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(/) {color:green}+1 general checks{color}\n-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//General_Nightly_Build_Report/]\n\n\n\n\n(/) {color:green}+1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n(x) {color:red}-1 jdk8 hadoop3 checks{color}\n-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1287//JDK8_Nightly_Build_Report_(Hadoop3)/]\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n\n(/) {color:green}+1 client integration test{color}\n\n", "issueSearchSentences": ["The acquireDelegationToken method [1]\u00a0defaults to checking for delegation token of kind \"HDFS_DELEGATION_TOKEN\" before fetching it from the FileSystem.", "[1]\u00a0-\u00a0[https://github.com/apache/hbase/blob/rel/2.1.4/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java#L67]", "Add a method to FsDelegationToken to accept token kind", "But isn't your patch still only use HDFS_DELEGATION_TOKEN kind?", "It would be helpful to have a method that accepts the token kind and fetches delegation token from UserProvider for that token kind."], "issueSearchScores": [0.7164711952209473, 0.5550488829612732, 0.4050629138946533, 0.3542146682739258, 0.32137173414230347]}
{"aId": 61, "code": "public static UserPermission toUserPermission(org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.UserPermission proto) {\n    return new UserPermission(proto.getUser().toByteArray(),\n        toTablePermission(proto.getPermission()));\n  }", "comment": " Converts a user permission proto to a client user permission object.", "issueId": "HBASE-20635", "issueStringList": ["Support to convert the shaded user permission proto to client user permission object", "Currently we have API to build the protobuf UserPermission to client user permission in AccessControlUtil but we cannot do the same when we use shaded protobufs.", "{noformat}", "Converts a user permission proto to a client user permission object.", "@param proto the protobuf UserPermission", "@return the converted UserPermission", "public static UserPermission toUserPermission(AccessControlProtos.UserPermission proto) {", "return new UserPermission(proto.getUser().toByteArray(),", "toTablePermission(proto.getPermission()));", "}", "{noformat}", "[~rajeshbabu] do you just need to use {{org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil}} instead?", ":)", "I don't, however, see a conversion for", "{noformat}", "UserPermission toUserPermission(org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.UserPermission){noformat}", "like you have in your patch.", "Should this be added to ShadedAccessControlUtil?", "(not sure what exactly you need for Phoenix)", "[~elserj]", "bq.Rajeshbabu Chintaguntla do you just need to use org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil instead?", "Yes it's really helpful.", "Uploaded simplified patch adding adding the toUserPermission in ShadedAccessControlUtil.", "Thanks for the information.", "bq.", "Should this be added to ShadedAccessControlUtil?", "(not sure what exactly you need for Phoenix)", "In Phoenix one of the place if we use custom access controller then trying to get the user permissions from AccessControlService.Interface using below method.", "{noformat}", "<code>rpc GetUserPermissions(.hbase.pb.GetUserPermissionsRequest) returns (.hbase.pb.GetUserPermissionsResponse);</code>", "public abstract void getUserPermissions(", "org.apache.hbase.thirdparty.com.google.protobuf.RpcController controller,", "org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.GetUserPermissionsRequest request,", "org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.GetUserPermissionsResponse> done);", "{noformat}", "Here the GetUserPermissionsResponse is shaded and the user permissions list returned by it also shaded ones.", "But there is no particular API to convert the shaded user permission to proto to UserPermission object.", "So this patch is required to do the same.", "Thanks.", "| (x) *{color:red}-1 overall{color}* |", "\\\\", "\\\\", "|| Vote || Subsystem || Runtime || Comment ||", "| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated.", "{color} |", "|| || || || {color:brown} Prechecks {color} ||", "| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns.", "{color} |", "| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags.", "{color} |", "| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests.", "Please justify why no new tests are needed for this patch.", "Also please list what manual steps were performed to verify this patch.", "{color} |", "|| || || || {color:brown} master Compile Tests {color} ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 53s{color} | {color:green} master passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 36s{color} | {color:green} master passed {color} |", "| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} master passed {color} |", "| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m  3s{color} | {color:green} branch has no errors when building our shaded downstream artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 56s{color} | {color:green} master passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} master passed {color} |", "|| || || || {color:brown} Patch Compile Tests {color} ||", "| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 52s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |", "| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 32s{color} | {color:red} hbase-client: The patch generated 1 new + 50 unchanged - 0 fixed = 51 total (was 50) {color} |", "| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues.", "{color} |", "| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 58s{color} | {color:green} patch has no errors when building our shaded downstream artifacts.", "{color} |", "| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 37s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0.", "{color} |", "| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |", "| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |", "|| || || || {color:brown} Other Tests {color} ||", "| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 14s{color} | {color:green} hbase-client in the patch passed.", "{color} |", "| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m  9s{color} | {color:green} The patch does not generate ASF License warnings.", "{color} |", "| {color:black}{color} | {color:black} {color} | {color:black} 39m 42s{color} | {color:black} {color} |", "\\\\", "\\\\", "|| Subsystem || Report/Notes ||", "| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |", "| JIRA Issue | HBASE-20635 |", "| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12926873/HBASE-20635_v2.patch |", "| Optional Tests |  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |", "| uname | Linux 542ab0f6ced6 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 GNU/Linux |", "| Build tool | maven |", "| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/component/dev-support/hbase-personality.sh |", "| git revision | master / b002b0b8b0 |", "| maven | version: Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T19:49:05Z) |", "| Default Java | 1.8.0_171 |", "| findbugs | v3.1.0-RC3 |", "| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/artifact/patchprocess/diff-checkstyle-hbase-client.txt |", "|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/testReport/ |", "| Max.", "process+thread count | 260 (vs. ulimit of 10000) |", "| modules | C: hbase-client U: hbase-client |", "| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/console |", "| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |", "This message was automatically generated.", "lgtm", "v2 looks fine to me too", "[~stack] you ok with this for branch-2.0?", "Small translation for shaded-protocol User pb that Phoenix wanted to us (some internal grant logic it does to make it feel \"natural\")", "Pushed to branch-2 and master for now.", "Thanks Rajeshbabu!"], "SplitGT": [" Converts a user permission proto to a client user permission object."], "issueString": "Support to convert the shaded user permission proto to client user permission object\nCurrently we have API to build the protobuf UserPermission to client user permission in AccessControlUtil but we cannot do the same when we use shaded protobufs.\r\n{noformat}\r\n  /**\r\n   * Converts a user permission proto to a client user permission object.\r\n   *\r\n   * @param proto the protobuf UserPermission\r\n   * @return the converted UserPermission\r\n   */\r\n  public static UserPermission toUserPermission(AccessControlProtos.UserPermission proto) {\r\n    return new UserPermission(proto.getUser().toByteArray(),\r\n        toTablePermission(proto.getPermission()));\r\n  }\r\n{noformat}\n[~rajeshbabu] do you just need to use {{org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil}} instead? :)\r\n\r\nI don't, however, see a conversion for\r\n{noformat}\r\nUserPermission toUserPermission(org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.UserPermission){noformat}\r\nlike you have in your patch. Should this be added to ShadedAccessControlUtil? (not sure what exactly you need for Phoenix)\n[~elserj] \r\nbq.Rajeshbabu Chintaguntla do you just need to use org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil instead? \r\nYes it's really helpful. Uploaded simplified patch adding adding the toUserPermission in ShadedAccessControlUtil. Thanks for the information.\r\n\r\nbq. Should this be added to ShadedAccessControlUtil? (not sure what exactly you need for Phoenix)\r\nIn Phoenix one of the place if we use custom access controller then trying to get the user permissions from AccessControlService.Interface using below method.\r\n{noformat}\r\n      /**\r\n       * <code>rpc GetUserPermissions(.hbase.pb.GetUserPermissionsRequest) returns (.hbase.pb.GetUserPermissionsResponse);</code>\r\n       */\r\n      public abstract void getUserPermissions(\r\n          org.apache.hbase.thirdparty.com.google.protobuf.RpcController controller,\r\n          org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.GetUserPermissionsRequest request,\r\n          org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.GetUserPermissionsResponse> done);\r\n{noformat}\r\nHere the GetUserPermissionsResponse is shaded and the user permissions list returned by it also shaded ones. But there is no particular API to convert the shaded user permission to proto to UserPermission object. So this patch is required to do the same.\r\nThanks.\n| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} master Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 53s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 36s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m  3s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 56s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} master passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 52s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 32s{color} | {color:red} hbase-client: The patch generated 1 new + 50 unchanged - 0 fixed = 51 total (was 50) {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 58s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 37s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 14s{color} | {color:green} hbase-client in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m  9s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 39m 42s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |\r\n| JIRA Issue | HBASE-20635 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12926873/HBASE-20635_v2.patch |\r\n| Optional Tests |  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |\r\n| uname | Linux 542ab0f6ced6 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/component/dev-support/hbase-personality.sh |\r\n| git revision | master / b002b0b8b0 |\r\n| maven | version: Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T19:49:05Z) |\r\n| Default Java | 1.8.0_171 |\r\n| findbugs | v3.1.0-RC3 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/artifact/patchprocess/diff-checkstyle-hbase-client.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/testReport/ |\r\n| Max. process+thread count | 260 (vs. ulimit of 10000) |\r\n| modules | C: hbase-client U: hbase-client |\r\n| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/13130/console |\r\n| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n\nlgtm\nv2 looks fine to me too\n[~stack] you ok with this for branch-2.0? Small translation for shaded-protocol User pb that Phoenix wanted to us (some internal grant logic it does to make it feel \"natural\")\nPushed to branch-2 and master for now. Thanks Rajeshbabu!\n", "issueSearchSentences": ["public static UserPermission toUserPermission(AccessControlProtos.UserPermission proto) {", "@param proto the protobuf UserPermission", "But there is no particular API to convert the shaded user permission to proto to UserPermission object.", "UserPermission toUserPermission(org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos.UserPermission){noformat}", "return new UserPermission(proto.getUser().toByteArray(),"], "issueSearchScores": [0.9175148606300354, 0.8411638736724854, 0.7255904078483582, 0.7166541814804077, 0.6790482997894287]}
{"aId": 62, "code": "private List<ServerName> getExcludedServersForSystemTable(\n      boolean checkForMinVersion) {\n    // TODO: This should be a cached list kept by the ServerManager rather than calculated on each\n    // move or system region assign. The RegionServerTracker keeps list of online Servers with\n    // RegionServerInfo that includes Version.\n    List<Pair<ServerName, String>> serverList = master.getServerManager().getOnlineServersList()\n      .stream()\n      .map(s->new Pair<>(s, master.getRegionServerVersion(s)))\n      .collect(Collectors.toList());\n    if (serverList.isEmpty()) {\n      return Collections.emptyList();\n    }\n    String highestVersion = Collections.max(serverList,\n      (o1, o2) -> VersionInfo.compareVersion(o1.getSecond(), o2.getSecond())).getSecond();\n    if (checkForMinVersion) {\n      if (!DEFAULT_MIN_VERSION_MOVE_SYS_TABLES_CONFIG.equals(minVersionToMoveSysTables)) {\n        int comparedValue = VersionInfo.compareVersion(minVersionToMoveSysTables, highestVersion);\n        if (comparedValue > 0) {\n          return Collections.emptyList();\n        }\n      }\n    }\n    return serverList.stream()\n      .filter(pair -> !pair.getSecond().equals(highestVersion))\n      .map(Pair::getFirst)\n      .collect(Collectors.toList());\n  }", "comment": " For system table, we must assign regions to a server with highest version.", "issueId": "HBASE-22923", "issueStringList": ["hbase:meta is assigned to localhost when we downgrade the hbase version", "When we downgrade the hbase version\uff08rsgroup enable\uff09, we found that the hbase:meta table could not be assigned.", "{code:java}", "master.AssignmentManager: Failed assignment of hbase:meta,,1.1588230740 to localhost,1,1, trying to assign elsewhere instead; try=1 of 10 java.io.IOException: Call to localhost/127.0.0.1:1 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:1", "{code}", "hbase group list:", "HBASE_META group\uff08hbase:meta and other system tables\uff09", "default group", "1.Down grade all servers in HBASE_META first", "2.higher version servers is in default", "3.hbase:meta assigned to localhost", "For system table, we assign them to a server with highest version.", "AssignmentManager#getExcludedServersForSystemTable", "But did not consider the rsgroup.", "When the nodes of system table group are all in the list of exclude nodes,\u00a0can we assign the system table across the group?", "bq.1.Down grade all servers in HBASE_META first", "Is this order imp?", "The versions in nodes of system group can be kept higher only while panning this upgrade/downgrade?", "Can u explain why this sequence was a must?", "Here is what can reproduce this easily (and it can happen unknowingly in production):", "# Create 'system' RSGroup.", "Move all system tables to this RSGroup.", "# Now we have system tables in system RSGroup and all other tables in default RSGroup.", "# Bring up new RegionServer on higher version.", "Since it's IP address is not yet known to master, it will be added to 'default' RSGroup by default (or let's say unknowingly one of default RSGroup's RegionServer is restarted and brought to higher version during rolling upgrade).", "# One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.", "And we will get the same error as reported by [~wenbang]\u00a0as per Jira description.", "Just had a quick glance, and it seems that we have purposefully kept RSGroupInfoManager (together with it's coproc endpoint) away from hbase-server (hbase-rsgroup not reachable) in branch-1 and branch-2, and it is again moved back to hbase-server module in trunk.", "[~zhangduo]\u00a0[~zghao] does this mean branch-1 and 2 have no way for AM to interact with RSGroup APIs directly?", "If we have a way, it would be better for AM to identify that any RS brought up with higher version than rest of RS should belong to 'system' RSGroup (for meta to move) and if it doesn't belong to system RSGroup, then do not move meta region to it regardless of the version difference.", "[~anoop.hbase]", "[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement: _All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules.", "This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.", "So that's the historical reason for the separation, and I assume for compatibility guideline reasons hbase-rsgroups was only merged into the core modules in master branch for HBase 3.", "Anyway, in your scenario, this seems like the key problem:", "{quote}Bring up new RegionServer on higher version.\u00a0\u00a0[...]", "One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.", "{quote}", "This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables, thus changing how we do rolling upgrades, thus changing the requirements for this behavior.", "At the very least we can make it optional.", "{quote}[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement:\u00a0_All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules.", "This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.", "{quote}", "I see, makes sense.", "{quote}This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables", "{quote}", "Agree, this behaviour is in line with any incompatible changes: user table region hosted on higher version RS can face issues connecting to meta hosted on lower version RS due to various reasons: coproc endpoint or schema incompatibilities.", "{quote}At the very least we can make it optional.", "{quote}", "Yeah that's better idea.", "In fact, we can introduce config for min version required to move system tables to.", "That would be much better.", "This way, operator would be willing to live with meta and other system table regions continue to live on lower versioned RS.", "In major case, we would want to define the value as next major version (assuming meta schema and coproc changes introduced in major version).", "For instance, setting min version to \"2.0.0\" would mean during upgrade from 1.6.0 -> 1.6.1 -> 1.7.0 would not have to worry about moving system regions to higher versioned RS but anytime we upgrade to HBase 2, AM will assign system table regions to higher versioned RS.", "Will come up with PR soon.", "Thank you\u00a0[~apurtell]!", "Thanks for the reviews [~apurtell] [~bharathv].", "Results for branch branch-1", "[build #142 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142/]: (x) *{color:red}-1 overall{color}*", "details (if available):", "(x) {color:red}-1 general checks{color}", "For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//General_Nightly_Build_Report/]", "(/) {color:green}+1 jdk7 checks{color}", "For more information [see jdk7 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK7_Nightly_Build_Report/]", "(x) {color:red}-1 jdk8 hadoop2 checks{color}", "For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK8_Nightly_Build_Report_(Hadoop2)/]", "(/) {color:green}+1 source release artifact{color}", "See build output for details."], "SplitGT": [" For system table, we must assign regions to a server with highest version."], "issueString": "hbase:meta is assigned to localhost when we downgrade the hbase version\nWhen we downgrade the hbase version\uff08rsgroup enable\uff09, we found that the hbase:meta table could not be assigned.\r\n{code:java}\r\nmaster.AssignmentManager: Failed assignment of hbase:meta,,1.1588230740 to localhost,1,1, trying to assign elsewhere instead; try=1 of 10 java.io.IOException: Call to localhost/127.0.0.1:1 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:1\r\n{code}\r\nhbase group list:\r\n\r\n\u00a0 HBASE_META group\uff08hbase:meta and other system tables\uff09\r\n\r\n\u00a0 default group\r\n\r\n1.Down grade all servers in HBASE_META first\r\n\r\n2.higher version servers is in default\r\n\r\n3.hbase:meta assigned to localhost\r\n\r\nFor system table, we assign them to a server with highest version.\r\n\r\nAssignmentManager#getExcludedServersForSystemTable\r\n\r\nBut did not consider the rsgroup.\nWhen the nodes of system table group are all in the list of exclude nodes,\u00a0can we assign the system table across the group?\nbq.1.Down grade all servers in HBASE_META first\r\nIs this order imp? The versions in nodes of system group can be kept higher only while panning this upgrade/downgrade?\r\nCan u explain why this sequence was a must?\nHere is what can reproduce this easily (and it can happen unknowingly in production):\r\n # Create 'system' RSGroup. Move all system tables to this RSGroup.\r\n # Now we have system tables in system RSGroup and all other tables in default RSGroup.\r\n # Bring up new RegionServer on higher version. Since it's IP address is not yet known to master, it will be added to 'default' RSGroup by default (or let's say unknowingly one of default RSGroup's RegionServer is restarted and brought to higher version during rolling upgrade).\r\n # One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup. And we will get the same error as reported by [~wenbang]\u00a0as per Jira description.\r\n\r\n\u00a0\r\n\r\nJust had a quick glance, and it seems that we have purposefully kept RSGroupInfoManager (together with it's coproc endpoint) away from hbase-server (hbase-rsgroup not reachable) in branch-1 and branch-2, and it is again moved back to hbase-server module in trunk.\r\n\r\n[~zhangduo]\u00a0[~zghao] does this mean branch-1 and 2 have no way for AM to interact with RSGroup APIs directly? If we have a way, it would be better for AM to identify that any RS brought up with higher version than rest of RS should belong to 'system' RSGroup (for meta to move) and if it doesn't belong to system RSGroup, then do not move meta region to it regardless of the version difference.\r\n\r\n[~anoop.hbase]\n[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement: _All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules. This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.\u00a0\r\n\r\nSo that's the historical reason for the separation, and I assume for compatibility guideline reasons hbase-rsgroups was only merged into the core modules in master branch for HBase 3.\u00a0\r\n\r\nAnyway, in your scenario, this seems like the key problem:\r\n{quote}Bring up new RegionServer on higher version.\u00a0\u00a0[...]\u00a0One dedicated thread in AM will try to assign meta (and other system tables) to newly brought RS with higher version but will fail to bring it online because newly brought up RS is not under jurisdiction of system RSGroup.\u00a0\r\n\r\n{quote}\r\nThis behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables, thus changing how we do rolling upgrades, thus changing the requirements for this behavior. At the very least we can make it optional.\u00a0\n{quote}[~vjasani]\u00a0Way back when RSGroups was first proposed, there were concerns about it, and to resolve those concerns the community imposed this requirement:\u00a0_All RSgroups changes must be optional, and confined to the hbase-rsgroups module_, with only case by case exceptions for enabling plug points in hbase-server and other core modules. This requirement demanded the\u00a0admin APIs for rsgroups should also be in the hbase-rsgroups module, and this is the reason for RSGroupInfoManager.\u00a0\r\n{quote}\r\nI see, makes sense.\r\n{quote}This behavior is introduced specifically for rolling upgrade, because META might need to be migrated, and earlier thinking was a regionserver could do it, but latest thinking is it is better for the master to migrate META and other tables\r\n{quote}\r\nAgree, this behaviour is in line with any incompatible changes: user table region hosted on higher version RS can face issues connecting to meta hosted on lower version RS due to various reasons: coproc endpoint or schema incompatibilities.\r\n{quote}At the very least we can make it optional.\r\n{quote}\r\nYeah that's better idea. In fact, we can introduce config for min version required to move system tables to. That would be much better. This way, operator would be willing to live with meta and other system table regions continue to live on lower versioned RS. In major case, we would want to define the value as next major version (assuming meta schema and coproc changes introduced in major version). For instance, setting min version to \"2.0.0\" would mean during upgrade from 1.6.0 -> 1.6.1 -> 1.7.0 would not have to worry about moving system regions to higher versioned RS but anytime we upgrade to HBase 2, AM will assign system table regions to higher versioned RS.\r\n\r\nWill come up with PR soon. Thank you\u00a0[~apurtell]!\nThanks for the reviews [~apurtell] [~bharathv].\nResults for branch branch-1\n\t[build #142 on builds.a.o|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142/]: (x) *{color:red}-1 overall{color}*\n----\ndetails (if available):\n\n(x) {color:red}-1 general checks{color}\n-- For more information [see general report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//General_Nightly_Build_Report/]\n\n\n(/) {color:green}+1 jdk7 checks{color}\n-- For more information [see jdk7 report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK7_Nightly_Build_Report/]\n\n\n(x) {color:red}-1 jdk8 hadoop2 checks{color}\n-- For more information [see jdk8 (hadoop2) report|https://ci-hadoop.apache.org/job/HBase/job/HBase%20Nightly/job/branch-1/142//JDK8_Nightly_Build_Report_(Hadoop2)/]\n\n\n\n\n(/) {color:green}+1 source release artifact{color}\n-- See build output for details.\n\n", "issueSearchSentences": ["AssignmentManager#getExcludedServersForSystemTable", "For system table, we assign them to a server with highest version.", "2.higher version servers is in default", "In fact, we can introduce config for min version required to move system tables to.", "{quote}Bring up new RegionServer on higher version.\u00a0\u00a0[...]"], "issueSearchScores": [0.7186505794525146, 0.6348006725311279, 0.5965086221694946, 0.5473142862319946, 0.5454346537590027]}
{"aId": 63, "code": "public List<HRegionInfo> getTableRegions(final byte[] tableName) throws IOException\n  {\n      CatalogTracker ct = getCatalogTracker();\n      List<HRegionInfo> Regions;\n      try {\n          Regions = MetaReader.getTableRegions(ct, tableName, true);\n        } finally {\n          cleanupCatalogTracker(ct);\n        }\n      \n      return Regions;\t  \n  }", "comment": " get the regions of a given table.", "issueId": "HBASE-2556", "issueStringList": ["Add convenience method to HBaseAdmin to get a collection of HRegionInfo objects for each table", "When we try to list all of the regions associated with a table, (e.g.", "in webapps/master/master.jsp or thrift/ThriftServer.java#getTableRegions, we either have to make do with HServerInfo objects or use the HTable#getRegionsInfo method.", "It may be useful to add a method to the HBaseAdmin object instead.", "It includes the followings:", "1.", "Add a new method getTableRegions in HBaseAdmin.", "2.", "Add unit test for the new method in TestAdmin.java.", "3.", "Update ThriftServer to use the new method to get regions for a given table.", "4.", "Add unit test in TestThriftServer", "It seems like master.jsp or table.jsp has changed since Jeff opened the issue.", "No update is made on the web serlet part.", "Committed to TRUNK.", "Thanks for the patch Ming Ma", "Integrated in HBase-TRUNK #1950 (See [https://builds.apache.org/hudson/job/HBase-TRUNK/1950/])"], "SplitGT": [" get the regions of a given table."], "issueString": "Add convenience method to HBaseAdmin to get a collection of HRegionInfo objects for each table\nWhen we try to list all of the regions associated with a table, (e.g. in webapps/master/master.jsp or thrift/ThriftServer.java#getTableRegions, we either have to make do with HServerInfo objects or use the HTable#getRegionsInfo method. It may be useful to add a method to the HBaseAdmin object instead.\nIt includes the followings:\n\n1. Add a new method getTableRegions in HBaseAdmin.\n2. Add unit test for the new method in TestAdmin.java.\n3. Update ThriftServer to use the new method to get regions for a given table.\n4. Add unit test in TestThriftServer\n\nIt seems like master.jsp or table.jsp has changed since Jeff opened the issue. No update is made on the web serlet part.\nCommitted to TRUNK.  Thanks for the patch Ming Ma\nIntegrated in HBase-TRUNK #1950 (See [https://builds.apache.org/hudson/job/HBase-TRUNK/1950/])\n    \n", "issueSearchSentences": ["in webapps/master/master.jsp or thrift/ThriftServer.java#getTableRegions, we either have to make do with HServerInfo objects or use the HTable#getRegionsInfo method.", "Update ThriftServer to use the new method to get regions for a given table.", "When we try to list all of the regions associated with a table, (e.g.", "Add convenience method to HBaseAdmin to get a collection of HRegionInfo objects for each table", "Add a new method getTableRegions in HBaseAdmin."], "issueSearchScores": [0.6421055793762207, 0.6253211498260498, 0.6171762943267822, 0.6020668148994446, 0.5386441946029663]}
{"aId": 64, "code": "public static void cleanRestoreDir(Job job, String snapshotName) throws IOException {\n    Configuration conf = job.getConfiguration();\n    Path restoreDir = new Path(conf.get(RESTORE_DIR_KEY));\n    FileSystem fs = restoreDir.getFileSystem(conf);\n    if (!fs.exists(restoreDir)) {\n      LOG.warn(\"{} doesn't exist on file system, maybe it's already been cleaned\", restoreDir);\n      return;\n    }\n    if (!fs.delete(restoreDir, true)) {\n      LOG.warn(\"Failed clean restore dir {} for snapshot {}\", restoreDir, snapshotName);\n    }\n    LOG.debug(\"Clean restore directory {} for {}\", restoreDir,  snapshotName);\n  }", "comment": " clean restore directory after snapshot scan job", "issueId": "HBASE-26485", "issueStringList": ["Introduce a method to clean restore directory after Snapshot Scan", "SnapshotScan is widely used in our company.", "However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time.", "So maybe we can introduce a method for users to clean the snapshot restore directory after job."], "SplitGT": [" clean restore directory after snapshot scan job"], "issueString": "Introduce a method to clean restore directory after Snapshot Scan\nSnapshotScan is widely used in our company. However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time. So maybe we can introduce a method for users to clean the snapshot restore directory after job.\n", "issueSearchSentences": ["So maybe we can introduce a method for users to clean the snapshot restore directory after job.", "However, after the snapshot scan job, the restore directory is not cleaned, and this maybe puts a lot of pressure on HDFS after a long time.", "Introduce a method to clean restore directory after Snapshot Scan", "SnapshotScan is widely used in our company."], "issueSearchScores": [0.8215963840484619, 0.7635579705238342, 0.6686506271362305, 0.2645935118198395]}
{"aId": 65, "code": "public int approximateCount(Key key) {\n    int res = Integer.MAX_VALUE;\n    int[] h = hash.hash(key);\n    hash.clear();\n    for (int i = 0; i < nbHash; i++) {\n      // find the bucket\n      int wordNum = h[i] >> 4;          // div 16\n      int bucketShift = (h[i] & 0x0f) << 2;  // (mod 16) * 4\n      \n      long bucketMask = 15L << bucketShift;\n      long bucketValue = (buckets[wordNum] & bucketMask) >>> bucketShift;\n      if (bucketValue < res) res = (int)bucketValue;\n    }\n    if (res != Integer.MAX_VALUE) {\n      return res;\n    } else {\n      return 0;\n    }\n  }", "comment": " how many times the key was added to the filter.", "issueId": "HBASE-908", "issueStringList": ["Add approximate counting to CountingBloomFilter", "CountingBloomFilter can return approximate count of how many times any particular key was added to the filter.", "This means that we can use a CountingBloomFilter like an approximate map of key->count, for values of count significantly smaller than the bucket size (15 in this implementation).", "This patch adds a new method, approximateCount(Key).", "There is also a JUnit test to test this functionality.", "Committed.", "Thanks for the patch Andrzej."], "SplitGT": [" how many times the key was added to the filter."], "issueString": "Add approximate counting to CountingBloomFilter\nCountingBloomFilter can return approximate count of how many times any particular key was added to the filter. This means that we can use a CountingBloomFilter like an approximate map of key->count, for values of count significantly smaller than the bucket size (15 in this implementation).\nThis patch adds a new method, approximateCount(Key). There is also a JUnit test to test this functionality.\nCommitted.  Thanks for the patch Andrzej.\n", "issueSearchSentences": ["This patch adds a new method, approximateCount(Key).", "CountingBloomFilter can return approximate count of how many times any particular key was added to the filter.", "This means that we can use a CountingBloomFilter like an approximate map of key->count, for values of count significantly smaller than the bucket size (15 in this implementation).", "Add approximate counting to CountingBloomFilter", "There is also a JUnit test to test this functionality."], "issueSearchScores": [0.5907905101776123, 0.5798158645629883, 0.5706709623336792, 0.40314793586730957, 0.22777390480041504]}
{"aId": 69, "code": "private void matchAndRemoveSuffixFromPipeline(List<? extends Segment> suffix) {\n    if (suffix.isEmpty()) {\n      return;\n    }\n    if (pipeline.size() < suffix.size()) {\n      throw new IllegalStateException(\n          \"CODE-BUG:pipleine size:[\" + pipeline.size() + \"],suffix size:[\" + suffix.size()\n              + \"],pipeline size must greater than or equals suffix size\");\n    }\n\n    ListIterator<? extends Segment> suffixIterator = suffix.listIterator(suffix.size());\n    ListIterator<? extends Segment> pipelineIterator = pipeline.listIterator(pipeline.size());\n    int count = 0;\n    while (suffixIterator.hasPrevious()) {\n      Segment suffixSegment = suffixIterator.previous();\n      Segment pipelineSegment = pipelineIterator.previous();\n      if (suffixSegment != pipelineSegment) {\n        throw new IllegalStateException(\"CODE-BUG:suffix last:[\" + count + \"]\" + suffixSegment\n            + \" is not pipleline segment:[\" + pipelineSegment + \"]\");\n      }\n      count++;\n    }\n\n    for (int index = 1; index <= count; index++) {\n      pipeline.pollLast();\n    }\n\n  }", "comment": " Checking that the Segments in suffix input parameter is same as the Segments in CompactionPipeline#pipeline one by one from the last element to the first element ofsuffix.", "issueId": "HBASE-26384", "issueStringList": ["Segment already flushed to hfile may still be remained in CompactingMemStore", "When  {{CompactingMemStore}} prepares flushing, {{CompactingMemStore.snapshot}} invokes  following {{CompactingMemStore.pushPipelineToSnapshot}} method to get {{Snapshot}}, following line 570 and line 575 uses {{CompactionPipeline#version}} to track whether the Segments in {{CompactionPipeline#pipeline}} has changed since it gets {{VersionedSegmentsList}}  in line 570 before emptying {{CompactionPipeline#pipeline}} in line 575.", "{code:java}", "565    private void pushPipelineToSnapshot() {", "566        int iterationsCnt = 0;", "567        boolean done = false;", "568        while (!done) {", "569              iterationsCnt++;", "570              VersionedSegmentsList segments = pipeline.getVersionedList();", "571              pushToSnapshot(segments.getStoreSegments());", "572              // swap can return false in case the pipeline was updated by ongoing compaction", "573             // and the version increase, the chance of it happenning is very low", "574             // In Swap: don't close segments (they are in snapshot now) and don't update the region size", "575            done = pipeline.swap(segments, null, false, false);", ".......", "}", "{code}", "However, when {{CompactingMemStore#inMemoryCompaction}} executes {{CompactionPipeline#flattenOneSegment}}, it does not change  {{CompactionPipeline#version}} , if there is an  {{in memeory compaction}} which executes  {{CompactingMemStore#flattenOneSegment}} between above line 570 and line 575, the  {{CompactionPipeline#version}} not change, but the {{Segment}} in {{CompactionPipeline}} has changed.", "Because {{CompactionPipeline#version}} not change,  {{pipeline.swap}} in above line 575 could think it is safe to invoke following {{CompactionPipeline#swapSuffix}} method to remove {{Segment}} in {{CompactionPipeline}} , but the {{Segment}} in {{CompactionPipeline}} has changed because of {{CompactingMemStore#flattenOneSegment}} , so the {{Segment}} not removed in following line 295 and still remaining in {{CompactionPipeline}}.", "{code:java}", "293  private void swapSuffix(List<?", "extends Segment> suffix, ImmutableSegment segment,", "294         boolean closeSegmentsInSuffix) {", "295      pipeline.removeAll(suffix);", "296      if(segment != null) pipeline.addLast(segment);", "....", "{code}", "However {{CompactingMemStore.snapshot}} think it is successful and continues to flush the {{Segment}} got by {{CompactingMemStore.snapshot}}  as normal, but the {{Segment}} with the same cells still be left in {{CompactingMemStore}}.", "Leaving {{Segment}} which already flushed in {{MemStore}} is dangerous: if a Major Compaction before the left {{Segment}} flushing, there may be data erroneous.", "My Fix in the PR is as following:", "# Increasing the {{CompactionPipeline#version}}  in {{CompactingMemStore#flattenOneSegment}} .", "Branch-2 has this problem but master not\uff0c because the branch-2 patch for HBASE-18375 omitting this.", "# For {{CompactionPipeline#swapSuffix}}  , explicitly checking that the {{Segment}} in {{suffix}} input parameter is same as the {{Segment}} in {{pipeline}} one by one from", "the last element to the first element of {{suffix}} \uff0c I think explicitly throwing Exception is better than hiding error and causing  subtle problem.", "I made separate PRs for master and branch-2 so the code for master and brach-2 could consistent and master could also has UTs for this problem.", "[PR#3777|https://github.com/apache/hbase/pull/3777] is for master and [PR#3779|https://github.com/apache/hbase/pull/3779] is for branch-2.The difference between them is patch for brach-2 including following code in {{CompactionPipeline.replaceAtIndex}} which not included in  branch-2 patch for HBASE-18375:", "{code:java}", "the version increment is indeed needed, because the swap uses removeAll() method of the", "linked-list that compares the objects to find what to remove.", "The flattening changes the segment object completely (creation pattern) and so", "swap will not proceed correctly after concurrent flattening.", "version++;", "{code}", "[~zhangduo], Would you please help me have a review at your convenience?", "It is just making the branch-2 in line with master and adding UTs for the problem , thank you very much.", "Pushed to branch-2.4+.", "Thanks [~comnetwork]!"], "SplitGT": [" Checking that the Segments in suffix input parameter is same as the Segments in CompactionPipeline#pipeline one by one from the last element to the first element ofsuffix."], "issueString": "Segment already flushed to hfile may still be remained in CompactingMemStore \nWhen  {{CompactingMemStore}} prepares flushing, {{CompactingMemStore.snapshot}} invokes  following {{CompactingMemStore.pushPipelineToSnapshot}} method to get {{Snapshot}}, following line 570 and line 575 uses {{CompactionPipeline#version}} to track whether the Segments in {{CompactionPipeline#pipeline}} has changed since it gets {{VersionedSegmentsList}}  in line 570 before emptying {{CompactionPipeline#pipeline}} in line 575.  \r\n  {code:java}\r\n  565    private void pushPipelineToSnapshot() {\r\n  566        int iterationsCnt = 0;\r\n  567        boolean done = false;\r\n  568        while (!done) {\r\n  569              iterationsCnt++;\r\n  570              VersionedSegmentsList segments = pipeline.getVersionedList();\r\n  571              pushToSnapshot(segments.getStoreSegments());\r\n  572              // swap can return false in case the pipeline was updated by ongoing compaction\r\n  573             // and the version increase, the chance of it happenning is very low\r\n  574             // In Swap: don't close segments (they are in snapshot now) and don't update the region size\r\n  575            done = pipeline.swap(segments, null, false, false);\r\n                    .......\r\n  }\r\n   {code}\r\nHowever, when {{CompactingMemStore#inMemoryCompaction}} executes {{CompactionPipeline#flattenOneSegment}}, it does not change  {{CompactionPipeline#version}} , if there is an  {{in memeory compaction}} which executes  {{CompactingMemStore#flattenOneSegment}} between above line 570 and line 575, the  {{CompactionPipeline#version}} not change, but the {{Segment}} in {{CompactionPipeline}} has changed.  Because {{CompactionPipeline#version}} not change,  {{pipeline.swap}} in above line 575 could think it is safe to invoke following {{CompactionPipeline#swapSuffix}} method to remove {{Segment}} in {{CompactionPipeline}} , but the {{Segment}} in {{CompactionPipeline}} has changed because of {{CompactingMemStore#flattenOneSegment}} , so the {{Segment}} not removed in following line 295 and still remaining in {{CompactionPipeline}}. \r\n  {code:java}\r\n  293  private void swapSuffix(List<? extends Segment> suffix, ImmutableSegment segment,\r\n  294         boolean closeSegmentsInSuffix) {\r\n  295      pipeline.removeAll(suffix);\r\n  296      if(segment != null) pipeline.addLast(segment);\r\n             ....\r\n{code}\r\n\r\nHowever {{CompactingMemStore.snapshot}} think it is successful and continues to flush the {{Segment}} got by {{CompactingMemStore.snapshot}}  as normal, but the {{Segment}} with the same cells still be left in {{CompactingMemStore}}. Leaving {{Segment}} which already flushed in {{MemStore}} is dangerous: if a Major Compaction before the left {{Segment}} flushing, there may be data erroneous.\r\n\r\nMy Fix in the PR is as following:\r\n# Increasing the {{CompactionPipeline#version}}  in {{CompactingMemStore#flattenOneSegment}} .\r\n   Branch-2 has this problem but master not\uff0c because the branch-2 patch for HBASE-18375 omitting this. \r\n# For {{CompactionPipeline#swapSuffix}}  , explicitly checking that the {{Segment}} in {{suffix}} input parameter is same as the {{Segment}} in {{pipeline}} one by one from \r\n   the last element to the first element of {{suffix}} \uff0c I think explicitly throwing Exception is better than hiding error and causing  subtle problem.\r\n\r\nI made separate PRs for master and branch-2 so the code for master and brach-2 could consistent and master could also has UTs for this problem.\r\n[PR#3777|https://github.com/apache/hbase/pull/3777] is for master and [PR#3779|https://github.com/apache/hbase/pull/3779] is for branch-2.The difference between them is patch for brach-2 including following code in {{CompactionPipeline.replaceAtIndex}} which not included in  branch-2 patch for HBASE-18375:\r\n{code:java}\r\n    // the version increment is indeed needed, because the swap uses removeAll() method of the\r\n    // linked-list that compares the objects to find what to remove.\r\n    // The flattening changes the segment object completely (creation pattern) and so\r\n    // swap will not proceed correctly after concurrent flattening.\r\n    version++;\r\n{code}\r\n\r\n \r\n\r\n\n[~zhangduo], Would you please help me have a review at your convenience? It is just making the branch-2 in line with master and adding UTs for the problem , thank you very much.\nPushed to branch-2.4+.\r\n\r\nThanks [~comnetwork]!\n", "issueSearchSentences": ["# For {{CompactionPipeline#swapSuffix}}  , explicitly checking that the {{Segment}} in {{suffix}} input parameter is same as the {{Segment}} in {{pipeline}} one by one from", "295      pipeline.removeAll(suffix);", "extends Segment> suffix, ImmutableSegment segment,", "Because {{CompactionPipeline#version}} not change,  {{pipeline.swap}} in above line 575 could think it is safe to invoke following {{CompactionPipeline#swapSuffix}} method to remove {{Segment}} in {{CompactionPipeline}} , but the {{Segment}} in {{CompactionPipeline}} has changed because of {{CompactingMemStore#flattenOneSegment}} , so the {{Segment}} not removed in following line 295 and still remaining in {{CompactionPipeline}}.", "294         boolean closeSegmentsInSuffix) {"], "issueSearchScores": [0.6638199687004089, 0.6048095226287842, 0.5908600091934204, 0.5866661071777344, 0.5486578941345215]}
{"aId": 70, "code": "public static TableName getTableLockName(TableName tn) {\n    byte[] tableName = tn.getName();\n    return TableName.valueOf(Bytes.add(tableName, MobConstants.MOB_TABLE_LOCK_SUFFIX));\n  }", "comment": " Gets the table name used in the table lock.", "issueId": "HBASE-12820", "issueStringList": ["Use table lock instead of MobZookeeper", "We had a lock to synchronize the major compaction, and sweep tool.", "Now we will have MR-less mob compaction in the HBase, and need the lock as well.", "And the table lock is a better choice.", "In this JIRA, clean the MobZookeeper code and use TableLockManager instead.", "Upload the patch.", "[~jmhsieh], [~anoopsamjohn] and [~ram_krish], please help look at it.", "Thanks!", "The patch is uploaded to the RB, you could find it through https://reviews.apache.org/r/29703/.", "Upload the patch V2 to remove the redundant empty space.", "Update the patch according to the comments of [~jmhsieh] and [~ram_krish].", "All the test cases (including the non-mob ones) are passed.", "Just one comment in RB.", "Will commit after that.", "Thanks [~jingchengdu].", "Thanks [~ram_krish].", "I've replied it in RB.", "Update the patch according to Ram's comments.", "Thanks [~ram_krish]", "Sorry for the delay..", "I have some Qs /comments.", "Pls hold off commit Ram.", "TableLock is a broader one and we might delay other lock needed ops unwanted.", "This is my major concern.", "Had some discussion with Jingcheng and some ways have come out..", "He would add details.", "Pls don't commit.", "Thanks Anoop [~anoopsamjohn].", "Currently we use table locks in the major compaction of mob-enabled column, sweeper and native mob compaction.", "The major compactions of mob-enabled columns use read locks, and the other two use write locks.", "The write locks might block the usages other than mob, for instance the region split, etc.", "So is it ok to use a dummy table name for the table lock used in the mob?", "For instance, use tableName + \".mobLock\" as the dummy table name?", "Please advise.", "Thanks.", "Upload the patch V5 to use a table lock with a dummy table name in mob.", "Hi [~jmhsieh], [~anoopsamjohn] and [~ram_krish], please review and comment.", "Thanks.", "Update the patch according to Ram's comments.", "Update the patch V7 according to Anoop's comments.", "Thanks [~anoopsamjohn].", "Pushed to branch hbase-11339.", "Thanks Jingcheng."], "SplitGT": [" Gets the table name used in the table lock."], "issueString": "Use table lock instead of MobZookeeper\nWe had a lock to synchronize the major compaction, and sweep tool. Now we will have MR-less mob compaction in the HBase, and need the lock as well. And the table lock is a better choice. In this JIRA, clean the MobZookeeper code and use TableLockManager instead.\nUpload the patch. [~jmhsieh], [~anoopsamjohn] and [~ram_krish], please help look at it. Thanks!\nThe patch is uploaded to the RB, you could find it through https://reviews.apache.org/r/29703/.\nUpload the patch V2 to remove the redundant empty space.\nUpdate the patch according to the comments of [~jmhsieh] and [~ram_krish]. All the test cases (including the non-mob ones) are passed.\nJust one comment in RB. Will commit after that. Thanks [~jingchengdu].\nThanks [~ram_krish]. I've replied it in RB.\nUpdate the patch according to Ram's comments. Thanks [~ram_krish]\nSorry for the delay.. I have some Qs /comments.  Pls hold off commit Ram.\nTableLock is a broader one and we might delay other lock needed ops unwanted. This is my major concern.\n\nHad some discussion with Jingcheng and some ways have come out.. He would add details.\n\nPls don't commit.\nThanks Anoop [~anoopsamjohn].\nCurrently we use table locks in the major compaction of mob-enabled column, sweeper and native mob compaction.\nThe major compactions of mob-enabled columns use read locks, and the other two use write locks.\nThe write locks might block the usages other than mob, for instance the region split, etc.\nSo is it ok to use a dummy table name for the table lock used in the mob? For instance, use tableName + \".mobLock\" as the dummy table name? Please advise. Thanks.\nUpload the patch V5 to use a table lock with a dummy table name in mob.\nHi [~jmhsieh], [~anoopsamjohn] and [~ram_krish], please review and comment. Thanks.\nUpdate the patch according to Ram's comments.\nUpdate the patch V7 according to Anoop's comments. Thanks [~anoopsamjohn].\nPushed to branch hbase-11339.  Thanks Jingcheng.\n", "issueSearchSentences": ["For instance, use tableName + \".mobLock\" as the dummy table name?", "So is it ok to use a dummy table name for the table lock used in the mob?", "And the table lock is a better choice.", "Use table lock instead of MobZookeeper", "TableLock is a broader one and we might delay other lock needed ops unwanted."], "issueSearchScores": [0.7569596767425537, 0.6756187677383423, 0.6649520397186279, 0.6243922114372253, 0.6180981397628784]}
{"aId": 73, "code": "private void lock(final Lock lock, final int multiplier)\n      throws RegionTooBusyException, InterruptedIOException {\n    try {\n      final long waitTime = Math.min(maxBusyWaitDuration,\n        busyWaitDuration * Math.min(multiplier, maxBusyWaitMultiplier));\n      if (!lock.tryLock(waitTime, TimeUnit.MILLISECONDS)) {\n        throw new RegionTooBusyException(\n          \"failed to get a lock in \" + waitTime + \"ms\");\n      }\n    } catch (InterruptedException ie) {\n      LOG.info(\"Interrupted while waiting for a lock\");\n      InterruptedIOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    }\n  }", "comment": " Throw RegionTooBusyException if failed to get the lock in time.", "issueId": "HBASE-6423", "issueStringList": ["Writes should not block reads on blocking updates to memstores", "We have a big data use case where we turn off WAL and have a ton of reads and writes.", "We found that:", "1. flushing a memstore takes a while (GZIP compression)", "2. incoming writes cause the new memstore to grow in an unbounded fashion", "3. this triggers blocking memstore updates", "4. in turn, this causes all the RPC handler threads to block on writes to that memstore", "5. we are not able to read during this time as RPC handlers are blocked", "At a higher level, we should not hold up the RPC threads while blocking updates, and we should build in some sort of rate control.", "Linking three related tickets where there was some earlier discussion of this", "Any update on this issue?", "I attached a patch: trunk-6423.patch.", "It is consistent with current behavior.", "However, you can adjust the waiting time.", "Patch looks good.", "{code}", "+      if (timeToWait < 0L) {", "+        this.updatesBlockedMs.add(now - startTime);", "+        LOG.info(\"Failed to unblocking updates for region \" + this + \" '\"", "+          + Thread.currentThread().getName() + \"' in time.", "The region is still busy.", "\");", "+        throw new RegionTooBusyException(\"region is flushing\");", "{code}", "typo: to unblocking updates -> to unblock updates", "Should we include -timeToWait, the amount of time we are unable to wait in the exception message ?", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12554076/trunk-6423.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 5 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 99 warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 22 new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.io.TestHeapSize", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//console", "This message is automatically generated.", "Addressed Ted's comments.", "Fixed TestHeapSize.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12554204/trunk-6423_v2.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 5 new or modified tests.", "{color:red}-1 patch{color}.", "The patch command could not apply the patch.", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3366//console", "This message is automatically generated.", "Rebased to trunk latest.", "{color:red}-1 overall{color}.", "Here are the results of testing the latest attachment", "http://issues.apache.org/jira/secure/attachment/12554219/trunk-6423_v2.1.patch", "against trunk revision .", "{color:green}+1 @author{color}.", "The patch does not contain any @author tags.", "{color:green}+1 tests included{color}.", "The patch appears to include 5 new or modified tests.", "{color:green}+1 hadoop2.0{color}.", "The patch compiles against the hadoop 2.0 profile.", "{color:red}-1 javadoc{color}.", "The javadoc tool appears to have generated 99 warning messages.", "{color:green}+1 javac{color}.", "The applied patch does not increase the total number of javac compiler warnings.", "{color:red}-1 findbugs{color}.", "The patch appears to introduce 22 new Findbugs (version 1.3.9) warnings.", "{color:green}+1 release audit{color}.", "The applied patch does not increase the total number of release audit warnings.", "{color:red}-1 core tests{color}.", "The patch failed these unit tests:", "org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster", "org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor", "Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//testReport/", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html", "Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html", "Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//console", "This message is automatically generated.", "Needs release note of new config.", "You think Long.MAX_VALUE as default?", "In trunk why not have it a reasonable value and in 0.94 have your MAX_VALUE?", "Should default be socket timeout?", "What is this comment about?", "+    // block waiting for the lock for closing", "lock.writeLock().lock();", "No busy waiting when closing?", "I see the comment in a few places.", "Removing getRecentFlushInfo is ok because HRegion is annotated private?", "This is a little ugly:", "+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);", "+      lock(this.updatesLock.readLock(), multiplier);", "You pass in multiplier.", "Why not pass in calculated time to busy wait?", "Have a little function that consults multiplier and", "does Math.min, etc., and figures how long to wait?", "+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);", "For example, put the above into the lock method?", "Pass in size?", "Fix your capialization of the 'N' in +   * @throws RegioNTooBusyException if failed to get the lock in time", "Can we backport?", "So write locks are unbounded, its just the reads that we'll give up on.", "How does this address Karthik's original comment in the issue?", "Reads fail faster releasing handlers so we can read/write against other memstores other than the one that has a write lock on it (and is", "currently at its limit)?", "Good patch Jimmy", "@Stack, I addressed couple of your comments and put the patch on RB: https://reviews.apache.org/r/8149/.", "It will make the review easier.", "This patch will make sure it is possible that we don't wait for the lock forever, so that the IPC handler is released if the region is busy.", "Therefore, other none-busy regions can be accessed.", "That's how we address Karthik's original comment in the issue.", "Sure, we can backport, once we are ready to integrate it into the trunk branch.", "This part's a bit ugly:", "{code}", "+  protected Configuration createConf() {", "+    Configuration conf = HBaseConfiguration.create();", "+    if (busyWaitDuration != null) {", "+      conf.set(\"hbase.busy.wait.duration\", busyWaitDuration);", "+    }", "+    return conf;", "+  }", "{code}", "We're using the configuration pass this information around?", "Why isn't it in the configuration in the first place?", "[~jxiang] You doing the createConf stuff in the test that Lars above refers too because there is no setup in this old junit3 TestHRegion and you need to get your custom config.", "in there each time test runs?", "If so, +1 on commit.", "Otherwise, what Lars asks.", "The goal is to make busyWaitDuration a parameter for this test and its derived tests.", "It is okay to make configuration a parameter instead, if it is not changed during the tests.", "I will take a look and post a new patch.", "You don't this in your new patch?", "{code}", "+    if (busyWaitDuration != null) {", "+      conf.set(\"hbase.busy.wait.duration\", busyWaitDuration);", "+    }", "{code}", "Should you be doing the above?", "I do it only in the constructor(#TestHRegionBusyWait) now since I made conf a member variable.", "You don't do it in TestHRegion.", "Should you?", "You used to set it if unset in previous patch.", "No, we don't need to do it in TestHRegion.", "We want TestHRegion to run without the setting.", "conf is a member variable now.", "TestHRegionBusyWait has all the tests in TestHRegion and", "its own tests, with the setting.", "I understand now.", "+1 on commit.", "Thanks for the review.", "Added some minor change.", "Also uploaded a patch for 0.94.", "I will commit it tonight if no objection.", "Integrated into 0.94 and trunk.", "Thanks all.", "Addendum that fixes TestHeapSize", "+1 on addendum", "Jimmy confirmed that addendum is correct.", "Addendum integrated to trunk and 0.94"], "SplitGT": [" Throw RegionTooBusyException if failed to get the lock in time."], "issueString": "Writes should not block reads on blocking updates to memstores\nWe have a big data use case where we turn off WAL and have a ton of reads and writes. We found that:\n\n1. flushing a memstore takes a while (GZIP compression)\n2. incoming writes cause the new memstore to grow in an unbounded fashion\n3. this triggers blocking memstore updates\n4. in turn, this causes all the RPC handler threads to block on writes to that memstore\n5. we are not able to read during this time as RPC handlers are blocked\n\nAt a higher level, we should not hold up the RPC threads while blocking updates, and we should build in some sort of rate control.\nLinking three related tickets where there was some earlier discussion of this\nAny update on this issue?\nI attached a patch: trunk-6423.patch.  It is consistent with current behavior.  However, you can adjust the waiting time.\nPatch looks good.\n{code}\n+      if (timeToWait < 0L) {\n+        this.updatesBlockedMs.add(now - startTime);\n+        LOG.info(\"Failed to unblocking updates for region \" + this + \" '\"\n+          + Thread.currentThread().getName() + \"' in time. The region is still busy.\");\n+        throw new RegionTooBusyException(\"region is flushing\");\n{code}\ntypo: to unblocking updates -> to unblock updates\nShould we include -timeToWait, the amount of time we are unable to wait in the exception message ?\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12554076/trunk-6423.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 99 warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 22 new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.io.TestHeapSize\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3361//console\n\nThis message is automatically generated.\nAddressed Ted's comments. Fixed TestHeapSize.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12554204/trunk-6423_v2.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified tests.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3366//console\n\nThis message is automatically generated.\nRebased to trunk latest.\n{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12554219/trunk-6423_v2.1.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified tests.\n\n    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 99 warning messages.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:red}-1 findbugs{color}.  The patch appears to introduce 22 new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n     {color:red}-1 core tests{color}.  The patch failed these unit tests:\n                       org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster\n                  org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor\n\nTest results: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html\nConsole output: https://builds.apache.org/job/PreCommit-HBASE-Build/3369//console\n\nThis message is automatically generated.\nNeeds release note of new config.\n\nYou think Long.MAX_VALUE as default?  In trunk why not have it a reasonable value and in 0.94 have your MAX_VALUE?\n\nShould default be socket timeout?\n\nWhat is this comment about?\n\n+    // block waiting for the lock for closing\n     lock.writeLock().lock();\n\nNo busy waiting when closing?\n\nI see the comment in a few places.\n\nRemoving getRecentFlushInfo is ok because HRegion is annotated private?\n\nThis is a little ugly:\n\n+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);\n+      lock(this.updatesLock.readLock(), multiplier);\n\nYou pass in multiplier.  Why not pass in calculated time to busy wait?  Have a little function that consults multiplier and\ndoes Math.min, etc., and figures how long to wait?\n\n+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);\n\nFor example, put the above into the lock method?  Pass in size?\n\nFix your capialization of the 'N' in +   * @throws RegioNTooBusyException if failed to get the lock in time\n\nCan we backport?\n\nSo write locks are unbounded, its just the reads that we'll give up on.  How does this address Karthik's original comment in the issue?\nReads fail faster releasing handlers so we can read/write against other memstores other than the one that has a write lock on it (and is \ncurrently at its limit)?\n\nGood patch Jimmy\n\n@Stack, I addressed couple of your comments and put the patch on RB: https://reviews.apache.org/r/8149/. It will make the review easier.\n\nThis patch will make sure it is possible that we don't wait for the lock forever, so that the IPC handler is released if the region is busy.  Therefore, other none-busy regions can be accessed.  That's how we address Karthik's original comment in the issue.\n\nSure, we can backport, once we are ready to integrate it into the trunk branch.\nThis part's a bit ugly:\n{code}\n+  protected Configuration createConf() {\n+    Configuration conf = HBaseConfiguration.create();\n+    if (busyWaitDuration != null) {\n+      conf.set(\"hbase.busy.wait.duration\", busyWaitDuration);\n+    }\n+    return conf;\n+  }\n{code}\n\nWe're using the configuration pass this information around? Why isn't it in the configuration in the first place?\n[~jxiang] You doing the createConf stuff in the test that Lars above refers too because there is no setup in this old junit3 TestHRegion and you need to get your custom config. in there each time test runs?\n\nIf so, +1 on commit.  Otherwise, what Lars asks.\nThe goal is to make busyWaitDuration a parameter for this test and its derived tests.\nIt is okay to make configuration a parameter instead, if it is not changed during the tests.\nI will take a look and post a new patch.\nYou don't this in your new patch?\n\n{code}\n+    if (busyWaitDuration != null) {\n+      conf.set(\"hbase.busy.wait.duration\", busyWaitDuration);\n+    }\n{code}\n\nShould you be doing the above?\nI do it only in the constructor(#TestHRegionBusyWait) now since I made conf a member variable.\nYou don't do it in TestHRegion.  Should you?  You used to set it if unset in previous patch.\nNo, we don't need to do it in TestHRegion. We want TestHRegion to run without the setting.\nconf is a member variable now.  TestHRegionBusyWait has all the tests in TestHRegion and\nits own tests, with the setting.\nI understand now.  +1 on commit.\nThanks for the review.  Added some minor change.  Also uploaded a patch for 0.94.\n\nI will commit it tonight if no objection.\nIntegrated into 0.94 and trunk.  Thanks all.\nAddendum that fixes TestHeapSize\n+1 on addendum\nJimmy confirmed that addendum is correct.\n\nAddendum integrated to trunk and 0.94\n", "issueSearchSentences": ["+      lock(this.updatesLock.readLock(), multiplier);", "+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);", "+      int multiplier = Math.min(acquiredLocks.size(), maxBusyWaitMultiplier);", "This patch will make sure it is possible that we don't wait for the lock forever, so that the IPC handler is released if the region is busy.", "lock.writeLock().lock();"], "issueSearchScores": [0.7226008176803589, 0.6706689596176147, 0.6706689596176147, 0.6561165452003479, 0.5636837482452393]}
{"aId": 79, "code": "HServerLoad getLoad(String serverName) {\n    HServerInfo hsi = this.onlineServers.get(serverName);\n    if (hsi == null) return null;\n    return hsi.getLoad();\n  }", "comment": " Make server load accessible to AssignmentManager", "issueId": "HBASE-3676", "issueStringList": ["Update region server load for AssignmentManager through regionServerReport()", "Currently the following method only calls serverManager.regionServerReport():", "public HMsg [] regionServerReport(HServerInfo serverInfo, HMsg msgs[],", "HRegionInfo[] mostLoadedRegions)", "This means AssignmentManager doesn't have valid server load information.", "The following method would be added to AssignmentManager:", "public void regionServerReport(HServerInfo serverInfo, HRegionInfo[] mostLoadedRegions)", "For HBASE-1502, we would figure out how to store load information through zk.", "Also includes change of regionLoad to TreeMap in HServerLoad.java", "This would facilitate making better load balancing decision in the future.", "Bringing into 0.92 but going to wait on commit...", "TestMultiParallel, TestRegionRebalancing, TestLoadBalancer, TestMasterObserver all passed.", "It turns out that ServerManager.processRegionServerAllsWell() is doing server info refresh.", "This patch removes the new method in AssignmentManager and adds the following method to ServerManager:", "{code}", "HServerLoad getLoad(String serverName) {", "{code}", "so that server load is accessible to AssignmentManager", "The following tests pass for the latest patch: TestMasterObserver, TestRegionRebalancing, TestMultiParallel, TestLoadBalancer", "Applied to TRUNK.", "Thanks for the patch Ted Yu (FYI, 80 chars per line and no tabs in future)."], "SplitGT": [" Make server load accessible to AssignmentManager"], "issueString": "Update region server load for AssignmentManager through regionServerReport()\nCurrently the following method only calls serverManager.regionServerReport():\n  public HMsg [] regionServerReport(HServerInfo serverInfo, HMsg msgs[],\n    HRegionInfo[] mostLoadedRegions)\nThis means AssignmentManager doesn't have valid server load information.\n\nThe following method would be added to AssignmentManager:\npublic void regionServerReport(HServerInfo serverInfo, HRegionInfo[] mostLoadedRegions)\n\nFor HBASE-1502, we would figure out how to store load information through zk.\nAlso includes change of regionLoad to TreeMap in HServerLoad.java\nThis would facilitate making better load balancing decision in the future.\nBringing into 0.92 but going to wait on commit...\nTestMultiParallel, TestRegionRebalancing, TestLoadBalancer, TestMasterObserver all passed.\nIt turns out that ServerManager.processRegionServerAllsWell() is doing server info refresh.\nThis patch removes the new method in AssignmentManager and adds the following method to ServerManager:\n{code}\n  HServerLoad getLoad(String serverName) {\n{code}\nso that server load is accessible to AssignmentManager\nThe following tests pass for the latest patch: TestMasterObserver, TestRegionRebalancing, TestMultiParallel, TestLoadBalancer\nApplied to TRUNK. Thanks for the patch Ted Yu (FYI, 80 chars per line and no tabs in future).\n", "issueSearchSentences": ["HServerLoad getLoad(String serverName) {", "so that server load is accessible to AssignmentManager", "Also includes change of regionLoad to TreeMap in HServerLoad.java", "This means AssignmentManager doesn't have valid server load information.", "public void regionServerReport(HServerInfo serverInfo, HRegionInfo[] mostLoadedRegions)"], "issueSearchScores": [0.9017976522445679, 0.5340902209281921, 0.5119071006774902, 0.4858465790748596, 0.46777454018592834]}
